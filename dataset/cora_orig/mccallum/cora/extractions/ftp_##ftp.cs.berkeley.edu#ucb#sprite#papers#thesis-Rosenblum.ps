URL: ftp://ftp.cs.berkeley.edu/ucb/sprite/papers/thesis-Rosenblum.ps
Refering-URL: http://www.cs.berkeley.edu/projects/sprite/sprite.papers.html
Root-URL: 
Title: The Design and Implementation of a Log-structured File System Copyright 1992  
Author: Mendel Rosenblum 
Abstract-found: 0
Intro-found: 1
Reference: <institution> hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh References </institution>
Reference: 1. <author> John K. Ousterhout, Herve Da Costa, David Harrison, John A. Kunze, Mike Kupfer, and James G. Thompson, </author> <title> ``A TraceDriven Analysis of the Unix 4.2 BSD File System,'' </title> <booktitle> Proceedings of the 10th Symposium on Operating Systems Principles, </booktitle> <pages> pp. 15-24 ACM, </pages> <year> (1985). </year>
Reference-contexts: Crash recovery time in a log-structured file system does not grow with the disk capacity. Log-structured file systems are based on the assumption that files are cached in main memory and that increasing memory sizes will make caches more and more effective at satisfying read requests <ref> [1] </ref>. As a result, disk traffic will become dominated by writes. This condition is precisely the opposite of the read-dominated workloads that almost all previous disk storage managers were designed for. The changed workloads require a write-optimized rather than read-optimized disk access technique. <p> All disk resident items such as files, directories, and metadata blocks can be cached in the disk cache. The Unix disk cache is maintained using a least recently used algorithm that exploits locality and reuse in file system access patterns. Much research literature exists on read caching <ref> [1, 12-14] </ref>. Studies have found cache hits rates of 80-90%. This rate is high enough to significantly reduce the disk delays experienced by Unix applications. From the storage manager's view there is no disadvantage to read caching. <p> Large file caches will reduce the number of requests that must go to disk. This will effectively decouple application performance from that of the disk. For workloads that contain much locality, such as those measured in the office/engineering environment <ref> [1, 15] </ref>, a file cache much smaller than the disk can have a hit rate high enough to significantly reduce the average wait time of a application and the number of read requests needed from disk storage. <p> Office and engineering applications tend to be dominated by accesses to small files; several studies 26 have measured mean file sizes of only a few kilobytes <ref> [1, 15, 36, 37] </ref>. This environment also contains applications that access very large files. Most of these accesses are sequential although random access does occur and is heavily used by some applications.
Reference: 2. <author> Michael L. Kazar, Bruce W. Leverett, Owen T. Anderson, Vasilis Apostolides, Beth A. Bottos, Sailesh Chutani, Craig F. Everhart, W. Anthony Mason, Shu-Tsui Tu, and Edward R. Zayas, </author> <title> ``DEcorum File System Architectural Overview,'' </title> <booktitle> Proceedings of the USENIX 1990 Summer Conference, </booktitle> <pages> pp. </pages> <month> 151-164 (June </month> <year> 1990). </year>
Reference-contexts: The changed workloads require a write-optimized rather than read-optimized disk access technique. The notion of logging is not new; it has been used extensively in database systems and a number of recent file systems have incorporated a log as an auxiliary structure to speed up writes and crash recovery <ref> [2, 3] </ref>. However, these other systems use the log only for temporary storage; the permanent home for information is in a traditional random-access storage structure on disk. In contrast, a log-structured file system stores data permanently in the log; there is no other structure on disk. <p> Logging has also appeared in several general-purpose file systems including Alpine [27] and a reimplementation of the Cedar file system [3]. Several recent implementations of the Unix file system interface have used logging to speed crash recovery and improve performance. Storage managers such as Episode <ref> [2] </ref> and AIX's JFS [28] use write ahead logging on changes to metadata structures to avoid the ordering and fsck problems in Unix FFS. The Echo storage manager [29] uses logging on all modifications to improve performance and reliability of user modifications. <p> If the file cache works well and absorbs the read requests then the technique fails. 7.6. Contemporary disk storage managers Several disk storage managers for Unix workloads have been proposed and built concurrently with Sprite LFS. Two such systems are the Episode <ref> [2] </ref> and Echo [29] disk storage managers. The designs used in these storage managers build upon earlier research efforts in logging file systems including Cedar [3] and Alpine [27]. This section compares Sprite LFS to these systems.
Reference: 3. <author> Robert B. Hagmann, </author> <title> ``Reimplementing the Cedar File System Using Logging and Group Commit,'' </title> <booktitle> Proceedings of the 11th Symposium on Operating Systems Principles, </booktitle> <pages> pp. </pages> <month> 155-162 (Nov </month> <year> 1987). </year>
Reference-contexts: The changed workloads require a write-optimized rather than read-optimized disk access technique. The notion of logging is not new; it has been used extensively in database systems and a number of recent file systems have incorporated a log as an auxiliary structure to speed up writes and crash recovery <ref> [2, 3] </ref>. However, these other systems use the log only for temporary storage; the permanent home for information is in a traditional random-access storage structure on disk. In contrast, a log-structured file system stores data permanently in the log; there is no other structure on disk. <p> Logging is most commonly used in commercial database management systems. These systems are used in an environment where write caching is not acceptable and small updates are common. Logging has also appeared in several general-purpose file systems including Alpine [27] and a reimplementation of the Cedar file system <ref> [3] </ref>. Several recent implementations of the Unix file system interface have used logging to speed crash recovery and improve performance. Storage managers such as Episode [2] and AIX's JFS [28] use write ahead logging on changes to metadata structures to avoid the ordering and fsck problems in Unix FFS. <p> Two such systems are the Episode [2] and Echo [29] disk storage managers. The designs used in these storage managers build upon earlier research efforts in logging file systems including Cedar <ref> [3] </ref> and Alpine [27]. This section compares Sprite LFS to these systems. Each of these systems uses logging to increase performance and reduce crash recovery time yet they use the log differently.
Reference: 4. <author> John K. Ousterhout, Andrew R. Cherenson, Frederick Douglis, Michael N. Nelson, and Brent B. Welch, </author> <title> ``The Sprite Network Operating System,'' </title> <journal> IEEE Computer 21(2) pp. </journal> <month> 23-36 </month> <year> (1988). </year>
Reference-contexts: The concept of log-structured file systems is demonstrated by a prototype log-structured file system called Sprite LFS, which is in production use as part of the Sprite network operating system <ref> [4] </ref>. Benchmark programs demonstrate that, for small files, the raw writing speed of Sprite LFS is more than an order of magnitude greater than that of the commonly used Unix file system [5].
Reference: 5. <author> Marshall K. McKusick, </author> <title> ``A Fast File System for Unix,'' </title> <journal> Transactions on Computer Systems 2(3) pp. 181-197 ACM, </journal> <year> (1984). </year>
Reference-contexts: Benchmark programs demonstrate that, for small files, the raw writing speed of Sprite LFS is more than an order of magnitude greater than that of the commonly used Unix file system <ref> [5] </ref>. Even for other workloads, such as those including reads and large-file accesses, Sprite LFS is at least as fast as Unix in all cases but one (files read sequentially after being written randomly). Long-term measurements of Sprite LFS in production use show the inherent advantages of log-structured file systems. <p> Every 30 seconds a process performs a sync system call that writes all the modified blocks in the main memory cache to their home on disk. 2.3.1. Berkeley Unix Fast File System In the early 1980s a major revision of the Unix file system was done at Berkeley <ref> [5] </ref>. The resulting file system, called the Berkeley Fast File System (Unix FFS), has become one of the most widely used and studied Unix file systems. Unix FFS changed the on-disk layout in a number of ways that improved performance. <p> Although having fragments complicates the design, fragments are necessary to avoid large amounts of disk space being lost to internal fragmentation. Studies showed that with a 4096 byte block as much as 50% disk space would be lost on normal workloads <ref> [5] </ref>. Unix FFS keeps the basic on-disk format of inodes and indirect blocks that was present in the initial Unix implementation. However, the inodes are no longer clustered together in their own region. Instead, Unix FFS breaks the disk into regions of adjacent cylinders called cylinder groups. <p> The behavior differences between the two file systems can be attributed directly to the the disk storage manager part of the file system code. The SunOS file system also provides an interesting comparison point because the file system and its performance characteristics are widely published in the literature <ref> [5, 20] </ref>. However, SunOS shares no code in common with Sprite LFS so differences in performance may be due to other factors than the disk storage management technique used.
Reference: 6. <author> Dennis Ritchie and Ken Thompson, </author> <title> ``UNIX time-sharing system,'' </title> <journal> Bell System Technical Journal 57(6) pp. </journal> <year> 1905-1929 </year> <month> (July </month> <year> 1978). </year>
Reference-contexts: The Unix office/engineering environment is characterized by applications for document preparation, program development, simulation studies, computer aided design (CAD), and other miscellaneous programs running under the Unix operating system programming interface <ref> [6] </ref>. There are a number of advantages to using this environment as a focus for research. Unix applications make heavy use of the file system and the workloads they generate are well documented in the research literature. <p> These effects provide the motivation for the design of the log-structured file system presented in Chapter 4. 2.1. Unix file system abstraction The Unix file system interface <ref> [6] </ref> is a commonly used one in the office/engineering environment. It defines a set of abstractions and operations that must be supported by the disk storage manager. The choice of the Unix file system interface has serious implications for a disk storage manager. <p> Unix file systems have traditionally tolerated some loss of data at a crash in order to increase performance. In other environments such as a bank's database system reducing reliability to increase performance would be unacceptable. 2.3. Unix file system implementation The initial Unix file system implementation <ref> [6] </ref> used very simple disk storage management techniques and contained few of the optimizations presented in this chapter. Nevertheless it is interesting to study because it forms the base for many of the existing implementations of the Unix file system interface.
Reference: 7. <author> David A. Patterson, Garth Gibson, and Randy H. Katz, </author> <title> ``A Case for Redundant Arrays of Inexpensive Disks (RAID),'' </title> <booktitle> ACM SIGMOD 88, </booktitle> <pages> pp. </pages> <month> 109-116 (June </month> <year> 1988). </year>
Reference-contexts: This makes the Unix environment workloads particularly challenging for disk storage managers. It also puts the applications in jeopardy of becoming I/O bound on future faster processors. The Unix environment also presents problems when mapping the disk request pattern onto disk array technology such as RAIDs <ref> [7] </ref>. RAID technology can dramatically increase the performance of the disk storage subsystem but the small disk request size of current Unix disk storage managers prevent efficient use of this technology. Some environments, such as those containing large sequential accesses, already take full advantage of RAIDs. <p> While CPUs have been doubling in speed each year, disk performance has taken ten years to double. 3.1.2.1. Disk array technology Although the performance of individual disks is being held back by mechanics, disk storage performance is being improved by the use of disk arrays or RAIDs <ref> [7, 31, 32] </ref>. By spreading (striping) the data over many disks, storage managers can significantly improve the throughput and transfer bandwidth of the disk storage by accessing many disks in parallel. <p> The techniques used to implement redundancy vary widely in design and performance characteristics. This dissertation focuses on the techniques used for RAID level 4 and RAID level 5 <ref> [7] </ref>. These techniques maintain blocks containing a parity value calculated over several data blocks. These parity blocks can be used to reconstruct data loss due to failure of a disk.
Reference: 8. <author> John K. Ousterhout and Fred Douglis, </author> <title> ``Beating the I/O Bottleneck: A Case for Log-structured File Systems,'' </title> <type> UCB/CSD 88/467, </type> <institution> Computer Science Division (EECS), University of California, Berke-ley, Berkeley, </institution> <address> CA (Oct 1988). </address>
Reference-contexts: The design, Sprite LFS, is the only published complete design for a log-structured file system. The design fills the gaps left in the initial proposal for a log-structured file system <ref> [8] </ref> by using a segmented log structure to handle the log wrapping upon itself. g The design demonstrates the log-structured file system as a high performance disk storage management technique that will scale well with the technology changes of the 1990s.
Reference: 9. <author> R. Daley and P. Neumann, </author> <title> ``A General Purpose File System for Secondary Storage,'' </title> <booktitle> Proceedings of the AFIPS Fall Joint Computer Conference, </booktitle> <pages> pp. </pages> <month> 213-229 </month> <year> (1965). </year>
Reference-contexts: The basic abstractions and implementations in use today date back to the 1960s <ref> [9] </ref>. This section describes these abstractions and implementation techniques with particular focus on the disk storage manager designs for the Unix office/engineering environment. A brief outline of the Unix file system implementation is presented to provide a reference point for the discussion of disk storage manager optimization techniques.
Reference: 10. <author> Lawrence J. Kenah and Simon F. Bate, </author> <title> VAX/VMS internals and data structures, </title> <publisher> Digital Press, </publisher> <address> Bed-ford, Mass (1984). </address>
Reference-contexts: The choice of the Unix file system interface has serious implications for a disk storage manager. One of the most important implications is that applications provide little if any information about their access patterns to data stored in the file system. For example, unlike file systems such as VMS <ref> [10] </ref> and MVS, the Unix file system interface provides no information about the eventual file size, file lifetime, or access pattern that the file will have. Storage managers for this environment must operate efficiently with no ``hints'' from the application. The basic data storage abstraction in Unix is the file. <p> Several extant-based systems have been built and used including VMS <ref> [10] </ref>, DTSS [18], MVS, and others [19]. These system allocate large files in a small number (e.g. 16) of large extents. When the file grows and fills one extent, another extent is added to the file.
Reference: 11. <author> Marshall Kirk McKusick, William N. Joy, Samuel J. Leffler, and Robert S. Fabry, </author> <title> ``Fsck The UNIX File System Check Program,'' Unix System Manager's Manual - 4.3 BSD Virtual VAX-11 Version, </title> <booktitle> USENIX, </booktitle> <month> (Apr </month> <year> 1986). </year>
Reference-contexts: Disk storage manager designs should match if not exceed this level of reliability. Beside the loss of data, crashes may leave the Unix disk structures in an inconsistent state that requires repair before they can be accessed again. After a crash a special crash recovery program fsck <ref> [11] </ref> must be run on the file system before it can be accessed. As described in Section 2.5.2, fsck is time consuming and can greatly increase the restart time of the system after a crash. <p> Disk storage managers that utilize scavenger programs are free of the limitations on write caching and disk sorting imposed by the ordered writes. This allows the storage manager to take full advantage of these performance enhancing features. For example, the Unix file system uses a scavenger program fsck <ref> [11] </ref> to restore consistency after a crash. Fsck checks for the inconsistencies such as disk blocks claimed by more than one inode and/or the free list and corrupted or incorrect directory entries.
Reference: 12. <author> Alan Jay Smith, </author> <title> ``Disk Cache-Miss Ratio Analysis and Design Considerations,'' </title> <journal> ACM Transactions on Computer Systems 3(3) pp. </journal> <month> 161-203 </month> <year> (1985). </year>
Reference: 13. <author> Michael N. Nelson, </author> <title> ``Physical Memory Management in a Network Operating System,'' </title> <type> UCB/CSD 88/471, </type> <institution> Computer Science Division (EECS), University of California, Berkeley, Berkeley, </institution> <address> CA (Nov 1988). </address>
Reference-contexts: Memory used to cache files can not be used for supporting the application's other memory requirements. Care must be taken not to give so much memory to the storage manager that applications can not perform acceptably. Nelson's PhD thesis <ref> [13] </ref> contains a study and suggested solution of trading off space between virtual memory and the storage manager's cache. 10 Prefetching Prefetching is a technique that, like read caching, can improve the performance of read requests by using main memory.
Reference: 14. <author> James G. Thompson, </author> <title> ``Efficient Analysis of Caching Systems,'' </title> <type> UCB/CSD 87/374, </type> <institution> Computer Science Division (EECS), University of California, Berkeley, Berkeley, </institution> <address> CA (1987). </address>
Reference: 15. <author> Mary G. Baker, John H. Hartman, Michael D. Kupfer, Ken W. Shirriff, and John K. Ousterhout, </author> <title> ``Measurements of a Distributed File System,'' </title> <booktitle> Proceedings of the 13th Symposium on Operating Systems Principles, </booktitle> <pages> pp. 198-212 ACM, </pages> <month> (Oct </month> <year> 1991). </year>
Reference-contexts: Prefetching works well when the access patterns can be consistency predicted and works poorly on workloads that can not be predicted. Prefetching in Unix works well because sequential access is the most common access pattern <ref> [15] </ref>. Write caching Just as read caching uses memory to improve the performance of reads, it is possible to use memory to speed writes using a technique called write caching. <p> Large file caches will reduce the number of requests that must go to disk. This will effectively decouple application performance from that of the disk. For workloads that contain much locality, such as those measured in the office/engineering environment <ref> [1, 15] </ref>, a file cache much smaller than the disk can have a hit rate high enough to significantly reduce the average wait time of a application and the number of read requests needed from disk storage. <p> Office and engineering applications tend to be dominated by accesses to small files; several studies 26 have measured mean file sizes of only a few kilobytes <ref> [1, 15, 36, 37] </ref>. This environment also contains applications that access very large files. Most of these accesses are sequential although random access does occur and is heavily used by some applications. <p> For these reasons a write-efficient storage manager will still be in demand. 3.3.2. Future workloads Just as with technology, it is possible to use current trends in workload changes to extrapolate future workloads. A study by members of the Sprite project <ref> [15] </ref> examined changes in the office/engineering workload over a ten year period. The study found that the workload evolved to contain more I/O, greater bursts of I/O, and larger maximum file sizes. One recent area of storage manager research that will facilitate new workloads is multimedia I/O [39, 40]. <p> Use of Sprite LFS as a production file system for periods of up to a year has captured this effect for log-structured file systems. Alternative evaluation techniques such as trace-driven simulations seem to be limited to time periods measured in days or weeks. For example, Baker <ref> [15] </ref> collected 192 hours of traces for a file caching study. Traces of this length are sufficient for file cache analysis but insufficient to examine the important issues in disk storage management. Without an implementation in production use the real overheads of fragmentation can not be known.
Reference: 16. <author> Jim Gray, </author> <title> ``Notes on Data Base Operating Systems,'' in Operating Systems, An Advanced Course, </title> <publisher> Springer-Verlag (1979). </publisher> <pages> 90 </pages>
Reference-contexts: Should the system crash during this window an application could ``lose'' modifications it made. For this reason, applications that depend on storage writes being permanent cannot use write caching. Prominent examples of such applications are data base management systems <ref> [16] </ref>. Data base systems need a guarantee that once the modification request returns, the changes will not be lost. Even in an environment such as Unix office/engineering where some data loss in crashes is been tolerated, limits on write caching are needed to reduce the chances of lost data. <p> Database storage management The use of write-ahead logging for crash recovery and high performance was pioneered in database systems <ref> [16, 58] </ref>. Most commercial database managers use some form of logging. The Sprite LFS design borrows heavily from this area. The Sprite LFS crash recovery mechanism of checkpoints and roll forward is similar to ``redo log'' techniques used in database systems and object repositories [59].
Reference: 17. <author> Robbert van Renesse, Andrew S. Tanenbaum, and Annita Wilschut, </author> <title> ``The Design of a High-Performance File Server,'' </title> <address> IR-178, Vrije Universiteit Amsterdam (Nov 1988). </address>
Reference-contexts: Few general purpose storage managers require objects to be contiguously allocated and those that do either require frequent defragmentation programs to be run or else they restrict the operations that can be performed. One such system that does contiguous allocation is the Bullet <ref> [17] </ref> file system. Contiguous allocation gives Bullet high performance on sequential accesses but the cost is high. Bullet requires that the file system be ``repacked'' at night to reclaim space lost to fragmentation. In addition, Bullet has restrictive access semantics that prohibit updating or file growth.
Reference: 18. <author> Philip D. L. Koch, </author> <title> ``Disk File Allocation Based on the Buddy System,'' </title> <journal> Transactions on Computer Systems 5(4) pp. </journal> <month> 352-370 </month> <year> (1987). </year>
Reference-contexts: Several extant-based systems have been built and used including VMS [10], DTSS <ref> [18] </ref>, MVS, and others [19]. These system allocate large files in a small number (e.g. 16) of large extents. When the file grows and fills one extent, another extent is added to the file.
Reference: 19. <author> Mike Powell, </author> <title> ``The DEMOS File System,'' </title> <booktitle> Proceedings of the 6th Symposium on Operating Systems Principles, </booktitle> <pages> pp. 33-42 ACM, </pages> <month> (Nov </month> <year> 1977). </year>
Reference-contexts: Several extant-based systems have been built and used including VMS [10], DTSS [18], MVS, and others <ref> [19] </ref>. These system allocate large files in a small number (e.g. 16) of large extents. When the file grows and fills one extent, another extent is added to the file. Although extent-based storage managers ease the storage allocation problems of contiguous allocation they do not totally eliminate the problem.
Reference: 20. <author> Larry McVoy and Steve Kleiman, </author> <title> ``Extent-like Performance from a UNIX File System,'' </title> <booktitle> Proceedings of the USENIX 1991 Winter Conference, </booktitle> <month> (Jan </month> <year> 1991). </year>
Reference-contexts: Unless conflicting allocations occurred, the Unix FFS file system tends to allocate files contiguously or nearly contiguously. These modifications allow the file system to approximate the sequential access performance of contiguous allocation without resorting to large extents which would be wasteful for small files <ref> [20] </ref>. The Unix FFS allocation algorithm works well until the disk becomes nearly full. The reason for this is that as the disk fills it becomes harder to do contiguous or near-contiguous allocation. <p> The cost of skip-sector allocation is that only half the disk bandwidth can be used. Simple clustering of sequential file requests into larger transfer units allows SunOS (a Unix FFS derivative file system) to achieve a write cost of 1.0 <ref> [20] </ref>. 53 workloads, Unix FFS does substationally worse. Measurements show small-file workloads utilize at most 5-10% of the disk bandwidth, for a write cost of 10-20 (see [45] and Figure 6-1 in Section 6.2.1 for specific measurements). <p> The behavior differences between the two file systems can be attributed directly to the the disk storage manager part of the file system code. The SunOS file system also provides an interesting comparison point because the file system and its performance characteristics are widely published in the literature <ref> [5, 20] </ref>. However, SunOS shares no code in common with Sprite LFS so differences in performance may be due to other factors than the disk storage management technique used. <p> The performance advantage of Sprite LFS on sequential writes is an artifact of the implementation. It is possible for a block-oriented file system to group requests together to improve performance. For example, a newer version of SunOS <ref> [20] </ref> groups write requests and should therefore have performance equivalent to Sprite LFS. The sequential read following the file's creation proceeds at the same rate for each file system.
Reference: 21. <author> Margo I. Seltzer, Peter M. Chen, and John K. Ousterhout, </author> <title> ``Disk Scheduling Revisited,'' </title> <booktitle> Proceedings of the Winter 1990 USENIX Technical Conference, </booktitle> <month> (January </month> <year> 1990). </year>
Reference-contexts: Disk scheduling algorithms have been carefully studied and many different algorithms can be found in the literature. They use techniques such as sorting the requests by shortest seek time first or scanning of the disk processing the requests as they are encounted. Recent studies <ref> [21, 22] </ref> that use modern disk characteristics found that sorting large numbers of random requests can increase the disk efficiency from around 7% to 25% of the disk's maximum bandwidth. <p> The benefit from this scheduling will be only a modest increase in performance. Scheduling as many as 1000 randomly written blocks per disk only allows 25% of the maximum disk efficiency to be achieved <ref> [21] </ref>. This is much greater than the 5% to 10% achieved without scheduling but is still a factor of four less than the disk's maximum bandwidth. With most reads being satisfied from caches in main memory and most writes still going to disk, disk I/O patterns will be fundamentally altered.
Reference: 22. <author> Carl Staelin, </author> <title> ``High Performance File System Design,'' CS-TR-347-91, </title> <type> Princeton Technical Report, </type> <institution> Princeton, </institution> <address> NJ (October 1991). </address>
Reference-contexts: Disk scheduling algorithms have been carefully studied and many different algorithms can be found in the literature. They use techniques such as sorting the requests by shortest seek time first or scanning of the disk processing the requests as they are encounted. Recent studies <ref> [21, 22] </ref> that use modern disk characteristics found that sorting large numbers of random requests can increase the disk efficiency from around 7% to 25% of the disk's maximum bandwidth.
Reference: 23. <author> Jim Gray, </author> <title> ``A Census of Tandem System Availability Between 1985 and 1990,'' </title> <type> TR 90.1 (Part Number 33579), </type> <institution> Tandem Computers Inc., Cupertino, </institution> <note> CA (January 1990). </note>
Reference-contexts: This section examines techniques for recovering the storage after a system crash. The next chapter will argue that the reliability of disk hardware will be improving in reliability and availability due to technology such as disk arrays. The major cause of failures of systems will be software crashes <ref> [23] </ref>. It is the job of the storage manager to recover quickly from these crashes and ensure high availability of the storage. The definition and semantics of crash recovery vary depending on the environment being supported.
Reference: 24. <author> William H. Paxton, </author> <title> ``A Client-Based Transaction System to Maintain Data Integrity,'' </title> <booktitle> Proceedings of the 7th Symposium on Operating Systems Principles, </booktitle> <pages> pp. 18-23 ACM,, </pages> <year> (1979). </year>
Reference: 25. <author> M. Fridrich and W. </author> <title> Older, ``The Felix File Server,'' </title> <booktitle> Proceedings of the 8th Symposium on Operating Systems Principles, </booktitle> <pages> pp. 37-46 ACM, </pages> <year> (1981). </year>
Reference: 26. <author> M. Schroeder, D. Gifford, and R. Needham, </author> <title> ``A Caching File System for a Programmer's Workstation,'' </title> <booktitle> Proceedings of the 10th Symposium on Operating Systems Principles, </booktitle> <pages> pp. 25-34 ACM, </pages> <year> (1985). </year>
Reference-contexts: Request performance is of great importance in this environment; write caching and loss of same data has traditionally been tolerated for increased performance. Because of the small file size and the high rate of update to files, the office/engineering environment causes problems for storage managers. Fortunately, file caching <ref> [26] </ref> has proven very effective for read requests in this environment. The remaining challenge for storage managers is to handle the write requests. Unlike the transaction processing environment, high levels of concurrency are typically not present in the office/engineering environment.
Reference: 27. <author> Mark R. Brown, Karen N. Kolling, and Edward A. Taft, </author> <title> ``The Alpine File System,'' </title> <journal> Transactions on Computer Systems 3(4) pp. </journal> <month> 261-293 </month> <year> (1985). </year>
Reference-contexts: Logging is most commonly used in commercial database management systems. These systems are used in an environment where write caching is not acceptable and small updates are common. Logging has also appeared in several general-purpose file systems including Alpine <ref> [27] </ref> and a reimplementation of the Cedar file system [3]. Several recent implementations of the Unix file system interface have used logging to speed crash recovery and improve performance. <p> Two such systems are the Episode [2] and Echo [29] disk storage managers. The designs used in these storage managers build upon earlier research efforts in logging file systems including Cedar [3] and Alpine <ref> [27] </ref>. This section compares Sprite LFS to these systems. Each of these systems uses logging to increase performance and reduce crash recovery time yet they use the log differently.
Reference: 28. <author> A. Chang, M. F. Mergen, R. K. Rader, J. A. Roberts, and S. L. Porter, </author> <title> ``Evolution of storage facilities in AIX Version 3 for RISC System/6000 processors,'' </title> <journal> IBM Journal of Research and Development 34(1) pp. </journal> <month> 105-109 (Jan </month> <year> 1990). </year>
Reference-contexts: Logging has also appeared in several general-purpose file systems including Alpine [27] and a reimplementation of the Cedar file system [3]. Several recent implementations of the Unix file system interface have used logging to speed crash recovery and improve performance. Storage managers such as Episode [2] and AIX's JFS <ref> [28] </ref> use write ahead logging on changes to metadata structures to avoid the ordering and fsck problems in Unix FFS. The Echo storage manager [29] uses logging on all modifications to improve performance and reliability of user modifications.
Reference: 29. <author> Andy Hisgen, Andrew Birrell, Chuck Jerian, Timothy Mann, Michael Schroeder, and Garret Swart, </author> <title> ``Granularity and Semantic Level of Replication in the Echo Distributed File System,'' </title> <booktitle> IEEE Workshop on the Management of Replicated Data, </booktitle> <month> (Nov </month> <year> 1990). </year>
Reference-contexts: Storage managers such as Episode [2] and AIX's JFS [28] use write ahead logging on changes to metadata structures to avoid the ordering and fsck problems in Unix FFS. The Echo storage manager <ref> [29] </ref> uses logging on all modifications to improve performance and reliability of user modifications. The databases and file systems using logging will be discussed in more detail in Chapter 7. 2.6. <p> If the file cache works well and absorbs the read requests then the technique fails. 7.6. Contemporary disk storage managers Several disk storage managers for Unix workloads have been proposed and built concurrently with Sprite LFS. Two such systems are the Episode [2] and Echo <ref> [29] </ref> disk storage managers. The designs used in these storage managers build upon earlier research efforts in logging file systems including Cedar [3] and Alpine [27]. This section compares Sprite LFS to these systems.
Reference: 30. <author> G. </author> <title> Amdahl, ``Validity of the Single Processor Approach to Achieving Large Scale Computing Capabilities,'' </title> <booktitle> Proc. AFIPS Spring Joint Computer Conference, </booktitle> <month> (Apr </month> <year> 1967). </year>
Reference-contexts: The numbers from 1975 and 1985 are taken from disk storage manager design papers published in those years. The numbers for 1995 are generated from predictions and future product announcements in the literature. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh This example illustrates what has come to be known as Amdahl's law <ref> [30] </ref>. If part of a system is sped up and part is not, the overall performance will be held back by the part not improved. For computing systems of the 1990s, the part that is improving is the processor and the part that is not is the disk.
Reference: 31. <author> Peter M. Chen, </author> <title> ``An Evaluation of Redundant Arrays of Inexpensive Disks using an Amdahl 5890,'' </title> <type> UCB/CSD 89/506, </type> <institution> Computer Science Division (EECS), University of California, Berkeley, </institution> <address> Berke-ley, CA (May 1989). </address>
Reference-contexts: While CPUs have been doubling in speed each year, disk performance has taken ten years to double. 3.1.2.1. Disk array technology Although the performance of individual disks is being held back by mechanics, disk storage performance is being improved by the use of disk arrays or RAIDs <ref> [7, 31, 32] </ref>. By spreading (striping) the data over many disks, storage managers can significantly improve the throughput and transfer bandwidth of the disk storage by accessing many disks in parallel.
Reference: 32. <author> M. Y. Kim, </author> <title> ``Synchronized Disk Interleaving,'' </title> <journal> IEEE Transactions on Computers C-35(11) pp. </journal> <month> 978-988 (Nov </month> <year> 1986). </year>
Reference-contexts: While CPUs have been doubling in speed each year, disk performance has taken ten years to double. 3.1.2.1. Disk array technology Although the performance of individual disks is being held back by mechanics, disk storage performance is being improved by the use of disk arrays or RAIDs <ref> [7, 31, 32] </ref>. By spreading (striping) the data over many disks, storage managers can significantly improve the throughput and transfer bandwidth of the disk storage by accessing many disks in parallel.
Reference: 33. <author> Control Data Corporation, </author> <title> Product Specification for Wren IV SCSI Model 94171-344, Control Data OEM Product Sales, </title> <address> Minneapolis MN (Jan 1988). </address>
Reference-contexts: On the other hand, large requests are limited by the bandwidth of the storage media which will see large improvements using disk array technology. The actual sizes of small and large requests depends on the disk technology used. For disk of the late 1980s such as the Wren IV <ref> [33] </ref> with an average seek time of 17.5 milliseconds, 3600 RPM rotational speed, and maximum transfer bandwidth of 1.3 megabytes per second, a small request would be less than 12 kilobytes in size and a large request over 100 kilobytes. <p> The first device is a Wren IV disk <ref> [33] </ref> which has a maximum transfer bandwidth of 1.3 megabytes a second and an average access delay of 29.5 milliseconds (17.5 ms average seek, 8 ms average rotational delay, and 4 milliseconds controller overhead).
Reference: 34. <author> Ethan Miller, </author> <title> ``Input/Output Behavior of Supercomputing Applications,'' </title> <type> UCB/CSD 91/616, </type> <institution> Computer Science Division (EECS), University of California, Berkeley, Berkeley, </institution> <note> CA (January 1991). </note>
Reference-contexts: Supercomputer workloads Supercomputer workloads are typified by the use of large data files in scientific experiments. A typical example would be a program that processes the output of sensors from a physics experiment. Access is mainly sequential to very large files <ref> [34] </ref>. Because of the high speed of the processor in this environment, the performance of the storage manager is of great importance. The high bandwidth provided by disk array technology is a good match for the large sequential transfer of the supercomputer environment.
Reference: 35. <author> O. G. Johnson, </author> <title> ``Three-Dimensional Wave Equation Computations on Vector Computers,'' </title> <booktitle> Proceedings of the IEEE 72(1)(January 1984). </booktitle>
Reference-contexts: The high bandwidth provided by disk array technology is a good match for the large sequential transfer of the supercomputer environment. In fact, the supercomputer environment has been a pioneer in the development of high bandwidth disk array techniques <ref> [35] </ref>. Using techniques such as contiguous allocation, current storage manager designs will achieve close to 100% of the disk maximum bandwidth on large transfers. Any additional performance improvements must come from hardware and not the storage manager techniques.
Reference: 36. <author> M. Satyanarayanan, </author> <title> ``A Study of File Sizes and Functional Lifetimes,'' </title> <booktitle> Proceedings of the 8th Symposium on Operating Systems Principles, </booktitle> <pages> pp. 96-108 ACM, </pages> <year> (1981). </year>
Reference-contexts: Office and engineering applications tend to be dominated by accesses to small files; several studies 26 have measured mean file sizes of only a few kilobytes <ref> [1, 15, 36, 37] </ref>. This environment also contains applications that access very large files. Most of these accesses are sequential although random access does occur and is heavily used by some applications.
Reference: 37. <author> Edward D. Lazowska, John Zahorjan, David R Cheriton, and Willy Zwaenepoel, </author> <title> ``File Access Performance of Diskless Workstations,'' </title> <journal> Transactions on Computer Systems 4(3) pp. </journal> <month> 238-268 (Aug </month> <year> 1986). </year>
Reference-contexts: Office and engineering applications tend to be dominated by accesses to small files; several studies 26 have measured mean file sizes of only a few kilobytes <ref> [1, 15, 36, 37] </ref>. This environment also contains applications that access very large files. Most of these accesses are sequential although random access does occur and is heavily used by some applications.
Reference: 38. <author> Garth A. Gibson, </author> <title> ``Redundant Disk Arrays: Reliable, Parallel Secondary Storage,'' </title> <type> UCB/CSD 91/613, </type> <institution> Computer Science Division (EECS), University of California, Berkeley, Berkeley, </institution> <note> CA 91 (December 1990). </note>
Reference-contexts: Because these disks have no mechanical parts they offer zero seek and rotational latencies. Unfortunately, the current technology trends indicate that these solid-state disks will not be cost competitive with magnetic disk until the year 2000 at the earliest <ref> [38] </ref>. This still leaves magnetic disk as the storage media of choice for the 1990s. It is possible that less dramatic changes will occur in storage technology that could alter storage manager design. An example would be the inclusion of fast non-volatile memory in the storage hierarchy.
Reference: 39. <author> Keith Muller and Joseph Pasquale, </author> <title> ``A High Performance Multi-Structured File System Design,'' </title> <booktitle> Proceedings of the 13th Symposium on Operating Systems Principles, </booktitle> <pages> pp. </pages> <month> 56-67 (Oct </month> <year> 1991). </year>
Reference-contexts: The study found that the workload evolved to contain more I/O, greater bursts of I/O, and larger maximum file sizes. One recent area of storage manager research that will facilitate new workloads is multimedia I/O <ref> [39, 40] </ref>. Multimedia workloads are characterized by access to continuous media [41] such as audio or video. The challenges posed to storage systems by continuous media are more stringent workload requirements rather than new workload access patterns. The continuous media requires strict guarantees on the 27 performance of storage managers.
Reference: 40. <author> Venkat Rangan and Harrick Vin, </author> <title> ``Designing File Systems Digital for Video and Audio,'' </title> <booktitle> Proceedings of the 13th Symposium on Operating Systems Principles, </booktitle> <pages> pp. </pages> <month> 81-95 (Oct </month> <year> 1991). </year>
Reference-contexts: The study found that the workload evolved to contain more I/O, greater bursts of I/O, and larger maximum file sizes. One recent area of storage manager research that will facilitate new workloads is multimedia I/O <ref> [39, 40] </ref>. Multimedia workloads are characterized by access to continuous media [41] such as audio or video. The challenges posed to storage systems by continuous media are more stringent workload requirements rather than new workload access patterns. The continuous media requires strict guarantees on the 27 performance of storage managers.
Reference: 41. <author> Ramesh Govindan and David Anderson, </author> <title> ``Scheduling and IPC Mechanisms for Continuous Media,'' </title> <booktitle> Proceedings of the 13th Symposium on Operating Systems Principles, </booktitle> <pages> pp. </pages> <month> 68-80 (Oct </month> <year> 1991). </year>
Reference-contexts: The study found that the workload evolved to contain more I/O, greater bursts of I/O, and larger maximum file sizes. One recent area of storage manager research that will facilitate new workloads is multimedia I/O [39, 40]. Multimedia workloads are characterized by access to continuous media <ref> [41] </ref> such as audio or video. The challenges posed to storage systems by continuous media are more stringent workload requirements rather than new workload access patterns. The continuous media requires strict guarantees on the 27 performance of storage managers.
Reference: 42. <author> ANSI, </author> <title> ``Common Command Set (CCS) of the Small Computer System Interface (SCSI),'' X3T9.2/85-52, ANSI Standard Committe of X3 (1989). </title>
Reference-contexts: These include: g The location of the last log region written. This is used to locate the end of the log, which becomes the starting location for the next log write. g Information needed by the ``df'' command <ref> [42] </ref>, a command that returns disk free space and other information about the file system.
Reference: 43. <author> Robert B. Hagmann, </author> <title> ``A Crash Recovery Scheme for a Memory-Resident Database System,'' </title> <journal> IEEE Transactions on Computers C-35(9)(Sep 1986). </journal>
Reference-contexts: Without this synchronization, an application could add new modifications to the file cache as fast as the checkpoint can write them out, preventing the checkpoint from finishing. The current implementation of Sprite LFS does more synchronization than is strictly necessary during checkpoints. There are more complicated techniques <ref> [43] </ref> of getting consistent checkpoints without blocking all modifications. These techniques work by forgoing the synchronization and checkpointing the data to disk. The algorithms detect in operations that could cause inconsistencies during the writing and record these operations so that a consistent checkpoint can be generated after a crash. <p> Main-memory database systems Although most storage manager designs are optimized for read performance, a few designs exist that attempt to improve write performance. Main-memory database systems are the most obvious examples of this approach <ref> [43, 60, 62, 63] </ref>. They are disk storage mangers in which the entire storage fits in memory so disk reads are only needed during cold start. Because read performance is not important, the evaluation criteria for main-memory data base systems are write performance and crash recovery time. <p> Because their data fits in memory and reads are not important, main-memory database designs focus on the speed of logging and checkpoint operations. Checkpoints are performed by transferring the entire memory image of the database to disk as fast as possible. Hagmann's system <ref> [43] </ref> writes the memory image into a circular buffer of sections that are similar to Sprite LFS segments. Cleaning of sections is not needed because the contents of a section are always overwritten by the next checkpoint.
Reference: 44. <author> Steve Kleiman, ``Vnodes: </author> <title> An Architecture for Multiple File System Types in Sun UNIX,'' </title> <booktitle> Proceedings of the Winter 1986 USENIX Technical Conference, </booktitle> <pages> pp. </pages> <month> 238-247 (January </month> <year> 1986). </year>
Reference-contexts: Although the current Sprite LFS implementation is closely tied to the internal structure of the Sprite kernel, the design described is not limited to being implemented in Sprite. The vnode interface commonly found in Unix systems <ref> [44] </ref> provides a point at which the Sprite LFS design could be added to the Unix kernel. 4.7. Summary This chapter presented design and implementation techniques for log-structured file systems. <p> One disadvantage of the Sprite LFS implementation was that transfer of the implementation to systems other than Sprite was difficult. The implementation was closely tied to the internal Sprite file system design which is sufficiently different from standard Unix file systems internals such as vnodes <ref> [44] </ref> that it would be easier to rewrite it rather than port the Sprite LFS code. This meant that distribution of Sprite LFS prototype to other sites for evaluation and use was not possible.
Reference: 45. <author> John K. Ousterhout, </author> <title> ``Why Aren't Operating Systems Getting Faster As Fast as Hardware?,'' </title> <booktitle> Proceedings of the USENIX 1990 Summer Conference, </booktitle> <pages> pp. </pages> <month> 247-256 (June </month> <year> 1990). </year>
Reference-contexts: Measurements show small-file workloads utilize at most 5-10% of the disk bandwidth, for a write cost of 10-20 (see <ref> [45] </ref> and Figure 6-1 in Section 6.2.1 for specific measurements). <p> The reason is that the machines being used are not fast enough to be disk-bound with the current workloads. For example, Figure 6-5 shows a comparison of Sprite LFS, Sprite OFS, and SunOS running the modified Andrew benchmark <ref> [45] </ref>. The Andrew benchmark was designed to measure the file system's handling of common Unix requests. Over the entire benchmark, Sprite LFS is only 5% faster than Sprite OFS and 21% faster than SunOS. The reason for the limited speedup is that the benchmark is CPU-bound.
Reference: 46. <author> Carl Staelin and Hector Garcia-Molina, </author> <title> ``Clustering Active Disk Data To Improve Disk Performance,'' CS-TR-283-90, </title> <type> Princeton Technical Report, </type> <institution> Princeton, </institution> <address> NJ (September 1990). </address>
Reference-contexts: When cleaning is not active this space can be used for caching files. Rather than enhancing write performance, the reorganization during cleaning can implement a policy that attempts to improve further read performance. Recent studies <ref> [46] </ref> have demonstrated the benefits of reorganizing data on disk to improve read performance. Some reorganization is already done in Sprite LFS.
Reference: 47. <author> Brent Ballinger Welch, </author> <title> ``Naming, State Management, and User-Level Extensions in the Sprite Distributed File System,'' </title> <type> UCB/CSD 90/567, </type> <institution> Computer Science Division (EECS), University of Cali-fornia, Berkeley, Berkeley, </institution> <address> CA (April 1990). </address>
Reference-contexts: Synthetic workload analysis The experiments using synthetic workloads consist of running benchmark programs that generate file system requests and comparing their performance on Sprite LFS and two other file systems. The two other file systems studied were the original Sprite file system (OFS) <ref> [47] </ref> and the Berkeley FFS running in SunOS 4.0.3 (SunOS). OFS provides an interesting comparison point for Sprite LFS because both systems reside in the Sprite kernel and they share most of the file system code.
Reference: 48. <author> Liba Svobodova, </author> <title> Management of Object Histories in the Swallow Repository, </title> <institution> MIT Laboratory for Computer Science, </institution> <address> Cambridge, MA (1980). </address>
Reference: 49. <author> Liba Svobodova, </author> <title> ``A Reliable Object-Oriented Data Repository for a Distributed Computer System,'' </title> <booktitle> Proceedings of the 8th Symposium on Operating Systems Principles, </booktitle> <pages> pp. 47-58 ACM, </pages> <year> (1981). </year>
Reference: 50. <author> D. Reed and Liba Svobodova, ``SWALLOW: </author> <title> A Distributed Data Storage System for a Local Network,'' </title> <booktitle> Local Networks for Computer Communications, </booktitle> <pages> pp. 355-373 North-Holland, </pages> <year> (1981). </year>
Reference: 51. <author> Ross S. Finlayson and David R. Cheriton, </author> <title> ``Log Files: An Extended File Service Exploiting Write-Once Storage,'' </title> <booktitle> Proceedings of the 11th Symposium on Operating Systems Principles, </booktitle> <pages> pp. 129-148 ACM, </pages> <month> (Nov </month> <year> 1987). </year>
Reference-contexts: Storage systems that use a log as the primary data structure have appeared in several proposals for building file systems on write-once media. Examples of such systems are Swallow [48-50], Log Files <ref> [51] </ref>, the Optical File Cabinet [52], and others [53]. All these systems write changes in an append-only fashion to a log-like structure and maintain indexing information much like the Sprite LFS inode map and inodes for quickly locating and reading files.
Reference: 52. <author> Jason Gait, </author> <title> ``The Optical File Cabinet: A Random Access File System for Write-Once Optical Disks,'' </title> <journal> IEEE Computer 21(6) pp. </journal> <month> 11-22 </month> <year> (1988). </year>
Reference-contexts: Storage systems that use a log as the primary data structure have appeared in several proposals for building file systems on write-once media. Examples of such systems are Swallow [48-50], Log Files [51], the Optical File Cabinet <ref> [52] </ref>, and others [53]. All these systems write changes in an append-only fashion to a log-like structure and maintain indexing information much like the Sprite LFS inode map and inodes for quickly locating and reading files.
Reference: 53. <author> Terry Laskodi, Bob Eifrig, and Jason Gait, </author> <title> ``A UNIX File System for a Write-Once Optical Disk,'' </title> <booktitle> Proceedings of the USENIX 1988 Summer Conference, </booktitle> <pages> pp. </pages> <month> 51-60 </month> <year> (1988). </year>
Reference-contexts: Storage systems that use a log as the primary data structure have appeared in several proposals for building file systems on write-once media. Examples of such systems are Swallow [48-50], Log Files [51], the Optical File Cabinet [52], and others <ref> [53] </ref>. All these systems write changes in an append-only fashion to a log-like structure and maintain indexing information much like the Sprite LFS inode map and inodes for quickly locating and reading files.
Reference: 54. <author> H. G. Baker, </author> <title> ``List Processing in Real Time on a Serial Computer,'' A.I. </title> <type> Working Paper 139, </type> <institution> MIT-AI Lab, </institution> <address> Boston, MA (April 1977). </address>
Reference-contexts: Swallow also uses copying to remove all live storage from an entire WORM-disk platter so the platter can be replaced. 7.2. Main-memory storage management The segment cleaning approach used in Sprite LFS to reclaim log space acts much like scavenging garbage collectors developed for programming languages <ref> [54, 55] </ref>. Scavenging garbage collectors work by copying the live data together leaving the deleted data in-place and available for reallocation. Early scavenging collectors [54] maintained two regions for data: old space and new space. New storage is allocated in the new space. <p> Main-memory storage management The segment cleaning approach used in Sprite LFS to reclaim log space acts much like scavenging garbage collectors developed for programming languages [54, 55]. Scavenging garbage collectors work by copying the live data together leaving the deleted data in-place and available for reallocation. Early scavenging collectors <ref> [54] </ref> maintained two regions for data: old space and new space. New storage is allocated in the new space. During garbage collection the live objects in the new space are copied and compacted into the old space.
Reference: 55. <author> D. Ungar, </author> <title> ``Generation Scavenging: A Non-Disruptive High Performance Storage Reclamation Algorithm,'' </title> <booktitle> Proceedings of the Software Engineering Symposium on Practical Software Development Environments, </booktitle> <pages> pp. </pages> <month> 157-167 (Apr </month> <year> 1984). </year>
Reference-contexts: Swallow also uses copying to remove all live storage from an entire WORM-disk platter so the platter can be replaced. 7.2. Main-memory storage management The segment cleaning approach used in Sprite LFS to reclaim log space acts much like scavenging garbage collectors developed for programming languages <ref> [54, 55] </ref>. Scavenging garbage collectors work by copying the live data together leaving the deleted data in-place and available for reallocation. Early scavenging collectors [54] maintained two regions for data: old space and new space. New storage is allocated in the new space. <p> The copying and compaction of the live objects during garbage collection is exactly the operation performed by segment cleaning. By using the age sorting of blocks during segment cleaning Sprite LFS separates files into generations much like generational garbage collection schemes <ref> [55, 56] </ref>. Generational garbage collection 82 techniques keep several regions of storage allocated in a hierarchy. All allocations are made in one region called the new space. Storage that lives is copied into a higher level (older generation) in the hierarchy.
Reference: 56. <author> Henry Lieberman and Carl Hewitt, </author> <title> ``A Real-Time Garbage Collector Based on the Lifetimes of Objects,'' </title> <journal> Communications of the ACM 26(6) pp. </journal> <month> 419-429 </month> <year> (1983). </year>
Reference-contexts: The copying and compaction of the live objects during garbage collection is exactly the operation performed by segment cleaning. By using the age sorting of blocks during segment cleaning Sprite LFS separates files into generations much like generational garbage collection schemes <ref> [55, 56] </ref>. Generational garbage collection 82 techniques keep several regions of storage allocated in a hierarchy. All allocations are made in one region called the new space. Storage that lives is copied into a higher level (older generation) in the hierarchy.
Reference: 57. <author> Benjamin Zorn, Paul Hilfinger, Kinson Ho, and James Laarus, </author> <title> ``SPUR Lisp: Deisgn and Implementation,'' </title> <type> UCB/CSD 87/373, </type> <institution> Computer Science Division (EECS), University of California, Berkeley, Berkeley, </institution> <address> CA (September 1987). </address>
Reference-contexts: In spite of the similarities between segment cleaning and generational garage collection schemes, there are several major points of difference. Generation collectors typically have a small number of spaces. For example, the SPUR lisp garage collector has four generations <ref> [57] </ref>. Sprite LFS file systems have a large number of segments. Each segment forms its own generation, and the segments as a whole form a continuum of age brackets.
Reference: 58. <author> R. Peterson and J. Strickland, </author> <title> ``LOG Write-Ahead Protocols and IMS/VS Logging,'' </title> <booktitle> Proceedings of the Second ACM SIACT-SIGMOD Symposium on Principles of Database Systems, </booktitle> <pages> pp. </pages> <month> 216-243 (March 1893). </month>
Reference-contexts: Database storage management The use of write-ahead logging for crash recovery and high performance was pioneered in database systems <ref> [16, 58] </ref>. Most commercial database managers use some form of logging. The Sprite LFS design borrows heavily from this area. The Sprite LFS crash recovery mechanism of checkpoints and roll forward is similar to ``redo log'' techniques used in database systems and object repositories [59].
Reference: 59. <author> Brian M. Oki, Barbara H. Liskov, and Robert W. Scheifler, </author> <title> ``Reliable Object Storage to Support Atomic Actions,'' </title> <booktitle> Proceedings of the 10th Symposium on Operating Systems Principles, </booktitle> <pages> pp. 147-159 ACM, </pages> <year> (1985). </year> <month> 92 </month>
Reference-contexts: Most commercial database managers use some form of logging. The Sprite LFS design borrows heavily from this area. The Sprite LFS crash recovery mechanism of checkpoints and roll forward is similar to ``redo log'' techniques used in database systems and object repositories <ref> [59] </ref>. The logs in these systems contain a list of operations that potentially must be redone after a crash. During recovery each log entry is examined and the operation redone if its effects are missing.
Reference: 60. <author> David J. DeWitt, Randy H. Katz, Frank Olken, L. D. Shapiro, Mike R. Stonebraker, and David Wood, </author> <title> ``Implementation Techniques for Main Memory Database Systems,'' </title> <booktitle> Proceedings of SIG-MOD 1984, </booktitle> <pages> pp. </pages> <month> 1-8 (June </month> <year> 1984). </year>
Reference-contexts: Collecting data in the file cache and writing it to disk in large writes provides a performance benefit similar to group commit in database systems <ref> [60] </ref>. Using group commit, database managers collect many changes to the data together and write them to the log in a single disk transfer. Doing one larger transfer rather than many smaller transfers allows the the data base manager to use the disk and CPU more efficiently. <p> Main-memory database systems Although most storage manager designs are optimized for read performance, a few designs exist that attempt to improve write performance. Main-memory database systems are the most obvious examples of this approach <ref> [43, 60, 62, 63] </ref>. They are disk storage mangers in which the entire storage fits in memory so disk reads are only needed during cold start. Because read performance is not important, the evaluation criteria for main-memory data base systems are write performance and crash recovery time.
Reference: 61. <author> Klaus Elhardt and Rudolf Bayer, </author> <title> ``A Database Cache for High Performance and Fast Restart in Database Systems,'' </title> <journal> ACM Transactions on Database Systems 9(4) pp. </journal> <month> 503-525 (December </month> <year> 1984). </year>
Reference-contexts: The overhead of this double write can be as high as a factor for two for workloads traditionally supported by the Unix file systems. 83 One database system that uses techniques similar to Sprite LFS is Elhardt and Bayer's Database Cache <ref> [61] </ref>. The Database Cache design sequentially writes all modified blocks to a disk that is treated as a circular buffer. In addition, a separate copy of the database is maintained using traditional update in place techniques.
Reference: 62. <author> Kenneth Salem and Hector Garcia-Molina, </author> <title> ``Crash Recovery Mechanisms for Main Storage Database Systems,'' </title> <institution> CS-TR-034-86, Princeton University, Princeton, </institution> <address> NJ (1986). </address>
Reference-contexts: Main-memory database systems Although most storage manager designs are optimized for read performance, a few designs exist that attempt to improve write performance. Main-memory database systems are the most obvious examples of this approach <ref> [43, 60, 62, 63] </ref>. They are disk storage mangers in which the entire storage fits in memory so disk reads are only needed during cold start. Because read performance is not important, the evaluation criteria for main-memory data base systems are write performance and crash recovery time.
Reference: 63. <author> Tobin J. Lehman and Michael J. Carey, </author> <title> ``A Recovery Algorithm of A High-Performance Memory-Resident Database System,'' </title> <booktitle> SIGMOD 1987, </booktitle> (). 
Reference-contexts: Main-memory database systems Although most storage manager designs are optimized for read performance, a few designs exist that attempt to improve write performance. Main-memory database systems are the most obvious examples of this approach <ref> [43, 60, 62, 63] </ref>. They are disk storage mangers in which the entire storage fits in memory so disk reads are only needed during cold start. Because read performance is not important, the evaluation criteria for main-memory data base systems are write performance and crash recovery time. <p> Fuzzy dumps would be useful to limit the overhead of Sprite LFS checkpoints. Since the disk accesses of both checkpoints and the log are sequential, main-memory data bases use the disk very efficiently. Another main-memory database by Lehman and Carey <ref> [63] </ref> writes the checkpoint as a pure-threaded log. The fragmentation due to pure-threading is not presented. Although the main-memory techniques have high write performance and fast crash recovery, they are also limited to databases that fit in memory.
Reference: 64. <author> Jon Solworth and Cyril Orji, </author> <title> ``Write-Only Disk Caches,'' </title> <booktitle> 1990 ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pp. </pages> <month> 123-132 </month> <year> (1990). </year>
Reference-contexts: Other write-optimized storage managers Another write-optimization technique is to attempt to hide the cost of the writes by doing them during the disk overhead times for disk reads. This approach, used in a system called the Write-only Disk Cache <ref> [64] </ref>, delays writing modifications until a read request forces the disk head to move near the location where the changes are to be written. By ``hiding'' the write requests in the overhead for the read requests the system can process writes with almost zero overhead.

References-found: 65

