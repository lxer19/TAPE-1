URL: ftp://ftp.cs.unc.edu/pub/publications/techreports/94-034.ps.Z
Refering-URL: http://www.cs.unc.edu/Research/graphics/pubs.html
Root-URL: http://www.cs.unc.edu
Title: Case Study: Observing a Volume Rendered Fetus within a Pregnant Patient  
Author: Andrei State, David T. Chen, Chris Tector, Andrew Brandt, Hong Chen, Ryutarou Ohbuchi, Mike Bajura and Henry Fuchs 
Affiliation: University of North Carolina  
Abstract: Augmented reality systems with see-through head - mounted displays have been used primarily for applications that are possible with today's computational capabilities. We explore possibilities for a particular applicationin-place, real-time 3D ultrasound visualizationwithout concern for such limitations. The question is not How well could we currently visualize the fetus in real time, but How well could we see the fetus if we had sufficient compute power? Our video sequence shows a 3D fetus within a pregnant woman's abdomenthe way this would look to a HMD user. Technical problems in making the sequence are discussed. This experience exposed limitations of current augmented reality systems; it may help define the capabilities of future systems needed for applications as demanding as real-time medical visualization. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> McCann, H.A., Sharp, J.S., Kinter, T.M., McEwan, C.N., Barillot, C., and Greenleaf, J.F. </author> <title> Multidimensional Ultrasonic Imaging for Cardiology. </title> <booktitle> Proc. IEEE 76.9 (1988): </booktitle> <pages> 1063-1073. </pages>
Reference-contexts: an augmented reality system displaying live ultrasound data in real time and properly registered in 3D space within a scanned subject would be a powerful and intuitive tool; it could be used for needle-guided biopsies, obstetrics, cardiology, etc. 2 Previous work Many researchers have attempted visualization of 3D echography data <ref> [1, 2, 3, 4] </ref>; some have volume visualized data sets that were acquired as a series of hand-guided 2D echography slices with 6DOF [5, 6].
Reference: 2. <author> Lalouche, R.C., Bickmore, D., Tessler, F., Mankovich, H. K., and Kangaraloo, H. </author> <title> Three-dimensional reconstruction of ultrasound images. SPIE89, Medical Imaging. </title> <booktitle> SPIE, </booktitle> <year> 1989. </year> <pages> 59-66. </pages>
Reference-contexts: an augmented reality system displaying live ultrasound data in real time and properly registered in 3D space within a scanned subject would be a powerful and intuitive tool; it could be used for needle-guided biopsies, obstetrics, cardiology, etc. 2 Previous work Many researchers have attempted visualization of 3D echography data <ref> [1, 2, 3, 4] </ref>; some have volume visualized data sets that were acquired as a series of hand-guided 2D echography slices with 6DOF [5, 6].
Reference: 3. <author> Pini, R., Monnini, E., Masotti, L., Novins, K. L., Greenberg, D. P., Greppi, B., Cerofolini, M., and Devereux, R. B. </author> <title> Echocardiographic Three-Dimensional Visualization of the Heart. 3D Imaging in Medicine . Ed. </title> <editor> Fuchs, H. Hhne, K. H., and Pizer, S. M. </editor> <booktitle> NATO ASI Series. </booktitle> <address> Travemnde, Germany: </address> <publisher> Springer-Verlag, </publisher> <year> 1990. </year> <booktitle> F 60: </booktitle> <pages> 263-274. </pages>
Reference-contexts: an augmented reality system displaying live ultrasound data in real time and properly registered in 3D space within a scanned subject would be a powerful and intuitive tool; it could be used for needle-guided biopsies, obstetrics, cardiology, etc. 2 Previous work Many researchers have attempted visualization of 3D echography data <ref> [1, 2, 3, 4] </ref>; some have volume visualized data sets that were acquired as a series of hand-guided 2D echography slices with 6DOF [5, 6].
Reference: 4. <institution> Tomographic Technologies, GMBH. 4D Tomographic Ultrasound, A clinical study. </institution> <year> 1993. </year>
Reference-contexts: an augmented reality system displaying live ultrasound data in real time and properly registered in 3D space within a scanned subject would be a powerful and intuitive tool; it could be used for needle-guided biopsies, obstetrics, cardiology, etc. 2 Previous work Many researchers have attempted visualization of 3D echography data <ref> [1, 2, 3, 4] </ref>; some have volume visualized data sets that were acquired as a series of hand-guided 2D echography slices with 6DOF [5, 6].
Reference: 5. <author> Ganapathy, U., and Kaufman, A. </author> <title> 3D acquisition and visualization of ultrasound data. </title> <booktitle> Visualization in Biomedical Computing 1992. </booktitle> <address> Chapel Hill, NC: </address> <booktitle> SPIE, 1992. </booktitle> <volume> 1808: </volume> <pages> 535-545. </pages>
Reference-contexts: a powerful and intuitive tool; it could be used for needle-guided biopsies, obstetrics, cardiology, etc. 2 Previous work Many researchers have attempted visualization of 3D echography data [1, 2, 3, 4]; some have volume visualized data sets that were acquired as a series of hand-guided 2D echography slices with 6DOF <ref> [5, 6] </ref>. Compared to echography imaging by current state -of-the-art 2D scanners, such volume visualizations promise to reduce the difficulty of mentally combining 2D echography slices into a coherent 3D volume.
Reference: 6. <author> Ohbuchi, R., Chen, D., and Fuchs, H. </author> <title> Incremental volume reconstruction and rendering for 3D ultrasound imaging. </title> <booktitle> Visualization in Biomedical Computing 1992. </booktitle> <address> Chapel Hill, NC: </address> <booktitle> SPIE, 1992. </booktitle> <volume> 1808: </volume> <pages> 312-323. </pages>
Reference-contexts: a powerful and intuitive tool; it could be used for needle-guided biopsies, obstetrics, cardiology, etc. 2 Previous work Many researchers have attempted visualization of 3D echography data [1, 2, 3, 4]; some have volume visualized data sets that were acquired as a series of hand-guided 2D echography slices with 6DOF <ref> [5, 6] </ref>. Compared to echography imaging by current state -of-the-art 2D scanners, such volume visualizations promise to reduce the difficulty of mentally combining 2D echography slices into a coherent 3D volume. <p> Reconstruction. The echography pixels are positioned and resampled into a regularly gridded volume, using a simple approximation algorithm based on a linear combination of Gaussian weighting functions which are translated and scaled to minimize artifacts <ref> [6, 11] </ref>. Size and shape of an echography pixel in world (or tracker) space are approximated by a point spread function which falls off away from the world space position as a non - spherical Gaussian.
Reference: 7. <author> Bajura, M., Fuchs, H., and Ohbuchi, R. </author> <title> Merging Virtual Objects with the Real World: Seeing Ultrasound Imagery within the Patient. </title> <booktitle> Computer Graphics (Proceedings of SIGGRAPH92) 26.2 (1992): </booktitle> <pages> 203-210. </pages>
Reference-contexts: However, almost all of these systems have used conventional stationary video monitors for presentation, so that a user must still mentally fuse 3D volume images on the monitor with the 3D volume of the patient. One system <ref> [7] </ref> tried to visualize live 2D echography images in-place within the patient using a see -through HMD system. <p> augmented reality, it could show only a few image slices (no 3D volume) at a relatively low frame rate Those few ultrasound images appeared to be pasted in front of the patients body rather than fixed within it. 3 Near-real-time visualization system In January 1993 we attempted to improve upon <ref> [7] </ref> with a system designed to perform real-time, in-place volume visualization of a live human subject. It contained two major real-time features: a continuously updated and rendered volume, and an image compositor. The volume was updated from a series of 2D echography images acquired by a tracked ultrasound transducer.
Reference: 8. <author> Wang,, J., Chi, V., and Fuchs, H. </author> <title> A Real-Time Optical 3D Tracker for Head-Mounted Display System. </title> <booktitle> Computer Graphics (Proceedings of 1990 Symposium on Interactive 3D Graphics) 24.2 (1990): </booktitle> <pages> 205-215. </pages>
Reference-contexts: The optical distortion of the lens is determined by imaging a grid pattern (Plate2, inset). A circularly symmetric model based on a 5th degree polynomial is used. Our optical tracking system is described in <ref> [8] </ref>; the calibration methods used for it are described in [9]. 4.2 Real-time acquisition Unburdened by visualization processing needs, this phase takes place in true real-time. Both the ultrasound images and the HMD camera images are recorded at 30fps on Sony D2 digital tape recorders.
Reference: 9. <author> Gottschalk, S. </author> <title> Autocalibration for Virtual Environment Tracking Hardware. </title> <booktitle> Computer Graphics (Proceedings of SIGGRAPH93) 27 (1993): </booktitle> <pages> 65-72. </pages>
Reference-contexts: The optical distortion of the lens is determined by imaging a grid pattern (Plate2, inset). A circularly symmetric model based on a 5th degree polynomial is used. Our optical tracking system is described in [8]; the calibration methods used for it are described in <ref> [9] </ref>. 4.2 Real-time acquisition Unburdened by visualization processing needs, this phase takes place in true real-time. Both the ultrasound images and the HMD camera images are recorded at 30fps on Sony D2 digital tape recorders.
Reference: 10. <institution> Digital Low-Pass Filter Without Phase Shift. NASA Tech Briefs KSC-11471. John F. Kennedy Space Center, Florida. </institution>
Reference-contexts: The major steps in generating the composite are: Tracking noise filtering. The tracking data we acquire fluctuates even if the tracked target remains stationary. Such noise causes misregistration between video and CG imagery. To reduce this effect, a noncausal low-pass filter without phase shift is used <ref> [10] </ref>, with cutoff frequencies of 1Hz for the transducer tracker and 6Hz for the HMD tracker. Reconstruction.
Reference: 11. <author> Ohbuchi, R. </author> <title> Incremental Acquisition and Visualization of 3D Ultrasound Images. </title> <type> Doctoral dissertation. </type> <institution> University of North Carolina at Chapel Hill, Computer Science Department, </institution> <year> 1994. </year>
Reference-contexts: Reconstruction. The echography pixels are positioned and resampled into a regularly gridded volume, using a simple approximation algorithm based on a linear combination of Gaussian weighting functions which are translated and scaled to minimize artifacts <ref> [6, 11] </ref>. Size and shape of an echography pixel in world (or tracker) space are approximated by a point spread function which falls off away from the world space position as a non - spherical Gaussian.
Reference: 12. <author> Neumann, U., State, A., Chen, H., Fuchs, H., Cullip, T. J., Fang, Q., Lavoie, M., and Rhoades, J. </author> <title> Interactive Multimodal Volume Visualization for a Distributed Radiation-Treatment Planning Simulator. </title> <type> Technical Report TR94-040. </type> <institution> University of North Carolina at Chapel Hill, Computer Science Department, </institution> <year> 1994. </year>
Reference-contexts: Since an image frame and its tracking information are related by the frame's time code, digitization of echography frames from video tape and volume reconstruction can take place automatically on a workstation under program control. Visualization. A volume renderer vol2 <ref> [12] </ref> running on a graphics multicomputer Pixel-Planes5 is used to render the reconstructed volume (s) from viewpoints matching those of the HMD-mounted camera.
Reference: 13. <author> Cullip, T. J., and Neumann, U. </author> <title> Accelerating Volume Reconstruction With 3D Texture Hardware. </title> <type> Technical Report TR93-027. </type> <institution> University of North Carolina at Chapel Hill, Computer Science Department, </institution> <year> 1994. </year>
Reference-contexts: What resources would be required to present an on - line visualization of comparable quality in an augmented-reality system? We expect advances in volume rendering software and hardware to soon provide highspeed stereoscopic rendering of volumetric data sets (see for example <ref> [13] </ref>). As for reconstruction, our volume contained nearly 4 times as many voxels as the one used in the 1993 experiment. Since the latter was being reconstructed at a rate of about 1 Hz, we need an increase of 2 orders of magnitude in computational speed for the reconstruction subsystem.
References-found: 13

