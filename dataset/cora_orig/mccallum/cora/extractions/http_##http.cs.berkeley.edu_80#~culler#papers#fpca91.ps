URL: http://http.cs.berkeley.edu:80/~culler/papers/fpca91.ps
Refering-URL: http://http.cs.berkeley.edu:80/~culler/papers/
Root-URL: http://www.cs.berkeley.edu
Title: Compiler-Controlled Multithreading for Lenient Parallel Languages 1  
Author: Klaus Erik Schauser David E. Culler Thorsten von Eicken 
Address: Berkeley  
Affiliation: Computer Science Division Electrical Engineering and Computer Sciences University of California,  
Abstract: Tolerance to communication latency and inexpensive synchronization are critical for general-purpose computing on large multiprocessors. Fast dynamic scheduling is required for powerful non-strict parallel languages. However, machines that support rapid switching between multiple execution threads remain a design challenge. This paper explores how multithreaded execution can be addressed as a compilation problem, to achieve switching rates approaching what hardware mechanisms might provide. Compiler-controlled multithreading is examined through compilation of a lenient parallel language, Id90, for a threaded abstract machine, TAM. A key feature of TAM is that synchronization is explicit and occurs only at the start of a thread, so that a simple cost model can be applied. A scheduling hierarchy allows the compiler to schedule logically related threads closely together in time and to use registers across threads. Remote communication is via message sends and split-phase memory accesses. Messages and memory replies are received by compiler-generated message handlers which rapidly integrate these events with thread scheduling. To compile Id90 for TAM, we employ a new parallel intermediate form, dual-graphs, with distinct control and data arcs. This provides a clean framework for partitioning the program into threads, scheduling threads, and managing registers under asynchronous execution. The compilation process is described and preliminary measurements of its effectiveness are discussed. Dynamic execution measurements are obtained via a second compilation step, which translates TAM into native code for existing machines with instrumentation incorporated. These measurements show that the cost of compiler-controlled multithreading is within a small factor of the cost of control flow in sequential languages. 
Abstract-found: 1
Intro-found: 1
Reference: [ACC + 90] <author> R. Alverson, D. Callahan, D. Cummings, B. Koblenz, A. Porterfield, and B. Smith. </author> <title> The Tera Computer System. </title> <booktitle> In Proc. of the 1990 Int. Conf. on Supercomputing, </booktitle> <pages> pages 1-6, </pages> <address> Amsterdam, </address> <year> 1990. </year>
Reference-contexts: 1 Introduction Multithreaded execution appears to be a key ingredient in general purpose parallel computing systems. Many researchers suggest that processors should support multiple instruction streams and switch very rapidly between them in response to remote memory reference latencies or synchronization <ref> [AI87, Smi90, HF88, ALKK90, ACC + 90] </ref>. However, the proposed architectural solutions make thread scheduling invisible to the compiler, preventing it from applying optimizations that might reduce the cost of thread switching or improve scheduling based on analysis of the program.
Reference: [ACM88] <author> Arvind, D. E. Culler, and G. K. Maa. </author> <title> Assessing the Benefits of Fine-Grain Parallelism in Dataflow Programs. </title> <journal> The Int. Journal of Supercomputer Applications, </journal> <volume> 2(3), </volume> <month> November </month> <year> 1988. </year>
Reference-contexts: Finally, code quality is evaluated on several benchmarks. 2 Language Issues Several studies have demonstrated that by exposing parallelism at all levels ample parallelism is available on a broad class of programs <ref> [ACM88, Cul90, AE88, AHN88] </ref>. Exposing parallelism at all levels requires that functions or arbitrary expressions be able to execute and possibly return results before all operands are computed. Data structures must be able to be accessed or passed around while components are still being computed.
Reference: [AE88] <author> Arvind and K. Ekanadham. </author> <title> Future Scientific Programming on Parallel Machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5(5) </volume> <pages> 460-493, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: Finally, code quality is evaluated on several benchmarks. 2 Language Issues Several studies have demonstrated that by exposing parallelism at all levels ample parallelism is available on a broad class of programs <ref> [ACM88, Cul90, AE88, AHN88] </ref>. Exposing parallelism at all levels requires that functions or arbitrary expressions be able to execute and possibly return results before all operands are computed. Data structures must be able to be accessed or passed around while components are still being computed. <p> Paraffins [AHN88] enumerates the distinct isomers of paraffins of size up to 14. Gamteb is a Monte Carlo neutron transport code [BCS + 89]. It is highly recursive with many conditionals. Simple is a hydrodynamics and heat conduction code widely used as an application benchmark, rewritten in Id <ref> [CHR78, AE88] </ref>. One iteration is run on 50 fi 50 matrices. Our current compiler performs only a limited form of redundant arc elimination and does no switch or merge combining. Registers are used only for thread local values.
Reference: [AHN88] <author> Arvind, S. K. Heller, and R. S. Nikhil. </author> <title> Programming Generality and Parallel Computers. </title> <booktitle> In Proc. of the Fourth Int. Symp. on Biological and Artificial Intelligence Systems, </booktitle> <pages> pages 255-286. </pages> <address> ESCOM (Leider), Trento, Italy, </address> <month> September </month> <year> 1988. </year>
Reference-contexts: Finally, code quality is evaluated on several benchmarks. 2 Language Issues Several studies have demonstrated that by exposing parallelism at all levels ample parallelism is available on a broad class of programs <ref> [ACM88, Cul90, AE88, AHN88] </ref>. Exposing parallelism at all levels requires that functions or arbitrary expressions be able to execute and possibly return results before all operands are computed. Data structures must be able to be accessed or passed around while components are still being computed. <p> DTW implements a dynamic time warp algorithm used in discrete word speech recognition [Sah91]. The size of the test template and number of cepstral coefficients is 100. Speech is used to determine cepstral coefficients for speech processing. We take 10240 speech samples and compute 30 cepstral coefficients. Paraffins <ref> [AHN88] </ref> enumerates the distinct isomers of paraffins of size up to 14. Gamteb is a Monte Carlo neutron transport code [BCS + 89]. It is highly recursive with many conditionals. Simple is a hydrodynamics and heat conduction code widely used as an application benchmark, rewritten in Id [CHR78, AE88].
Reference: [AI87] <author> Arvind and R. A. </author> <title> Iannucci. Two Fundamental Issues in Multiprocessing. </title> <booktitle> In Proc. of DFVLR - Conf. 1987 on Par. Proc. in Science and Eng., </booktitle> <address> Bonn-Bad Godesberg, </address> <publisher> W. </publisher> <address> Germany, </address> <month> June </month> <year> 1987. </year>
Reference-contexts: 1 Introduction Multithreaded execution appears to be a key ingredient in general purpose parallel computing systems. Many researchers suggest that processors should support multiple instruction streams and switch very rapidly between them in response to remote memory reference latencies or synchronization <ref> [AI87, Smi90, HF88, ALKK90, ACC + 90] </ref>. However, the proposed architectural solutions make thread scheduling invisible to the compiler, preventing it from applying optimizations that might reduce the cost of thread switching or improve scheduling based on analysis of the program.
Reference: [ALKK90] <author> A. Agarwal, B. Lim, D. Kranz, and J. Kubiatowicz. </author> <month> APRIL: </month> <title> A Processor Architecture for Multiprocessing. </title> <booktitle> In Proc. of the 17th Ann. Int. Symp. on Comp. Arch., </booktitle> <pages> pages 104-114, </pages> <address> Seattle, Washington, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Multithreaded execution appears to be a key ingredient in general purpose parallel computing systems. Many researchers suggest that processors should support multiple instruction streams and switch very rapidly between them in response to remote memory reference latencies or synchronization <ref> [AI87, Smi90, HF88, ALKK90, ACC + 90] </ref>. However, the proposed architectural solutions make thread scheduling invisible to the compiler, preventing it from applying optimizations that might reduce the cost of thread switching or improve scheduling based on analysis of the program.
Reference: [ANP87] <author> Arvind, R. S. Nikhil, and K. K. Pingali. I-Structures: </author> <title> Data Structures for Parallel Computing. </title> <type> Technical Report CSG Memo 269, </type> <institution> MIT Lab for Comp. Sci., </institution> <type> 545 Tech. </type> <institution> Square, </institution> <address> Cambridge, MA, </address> <month> February </month> <year> 1987. </year> <title> (Also in Proc. of the Graph Reduction Workshop, </title> <address> Santa Fe, NM. </address> <month> October </month> <year> 1986.). </year>
Reference-contexts: TAM provides a specialized form of send to support split-phase access to data structures. The heap is assumed to be distributed over processors, so access to a data element may require interprocessor communication. In addition, accesses may be synchronizing, as with I-structures <ref> [ANP87] </ref> where a read of an empty element is deferred until the corresponding write takes place. I-structure operations generate a request for a particular heap location and the response is received by an inlet. Meanwhile, the processor continues with other enabled threads.
Reference: [BCS + 89] <author> P. J. Burns, M. Christon, R. Schweitzer, O. M. Lubeck, H. J. Wasserman, M. L. Simmons, and D. V. Pryor. </author> <title> Vectorization of Monte-Carlo Particle Transport: An Architectural Study using the LANL Benchmark Gamteb. </title> <booktitle> In Proc. Supercomputing '89. IEEE Computer Society and ACM SIGARCH, </booktitle> <address> New York, NY, </address> <month> November </month> <year> 1989. </year>
Reference-contexts: Speech is used to determine cepstral coefficients for speech processing. We take 10240 speech samples and compute 30 cepstral coefficients. Paraffins [AHN88] enumerates the distinct isomers of paraffins of size up to 14. Gamteb is a Monte Carlo neutron transport code <ref> [BCS + 89] </ref>. It is highly recursive with many conditionals. Simple is a hydrodynamics and heat conduction code widely used as an application benchmark, rewritten in Id [CHR78, AE88]. One iteration is run on 50 fi 50 matrices.
Reference: [CFR + 89] <author> R. Cytron, J. Ferrante, B. K. Rosen, M. N. Wegman, and F. K. Zadeck. </author> <title> An Efficient Method of Computing Static Single Assignment Form. </title> <booktitle> In Proc. of the 16th Annual ACM Symp. on Principles of Progr. Lang., </booktitle> <pages> pages 25-35, </pages> <address> Los Angeles, </address> <month> January </month> <year> 1989. </year>
Reference-contexts: Dual graphs are similar in form to data structures used in most optimizing compilers, but the key differences are that they describe parallel control flow and are in static single assignment form <ref> [CFR + 89] </ref>. Compilation to TAM involves a series of transformations on the dual graph, described below.
Reference: [CHR78] <author> W. P. Crowley, C. P. Hendrickson, and T. E. Rudy. </author> <title> The SIMPLE code. </title> <type> Technical Report UCID 17715, </type> <institution> Lawrence Livermore Laboratory, </institution> <month> February </month> <year> 1978. </year>
Reference-contexts: Paraffins [AHN88] enumerates the distinct isomers of paraffins of size up to 14. Gamteb is a Monte Carlo neutron transport code [BCS + 89]. It is highly recursive with many conditionals. Simple is a hydrodynamics and heat conduction code widely used as an application benchmark, rewritten in Id <ref> [CHR78, AE88] </ref>. One iteration is run on 50 fi 50 matrices. Our current compiler performs only a limited form of redundant arc elimination and does no switch or merge combining. Registers are used only for thread local values.
Reference: [CSS + 91] <author> D. Culler, A. Sah, K. Schauser, T. von Eicken, and J. Wawrzynek. </author> <title> Fine-grain Parallelism with Minimal Hardware Support: A Compiler-Controlled Threaded Abstract Machine. </title> <booktitle> In Proc. of 4th Int. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Santa-Clara, CA, </address> <month> April </month> <year> 1991. </year> <note> (Also available as Technical Report UCB/CSD 91/591, </note> <institution> CS Div., University of California at Berkeley). </institution>
Reference-contexts: Synchronization, thread scheduling and storage management are explicit in the machine language and exposed to the compiler. TAM is presented elsewhere <ref> [CSS + 91, vESC91] </ref>; in this section we describe the salient features of TAM as a compilation target. A primary design goal in TAM is to provide a means of exploiting locality, even under asynchronous execution, to minimize the overhead of multithreading. <p> The scheduling queue is contained within the frames, and the last thread executed in a quantum, called the leave thread, includes code to locate the next activation and fork to a designated enter thread within that activation. Empirically, quanta often cross many points of possible suspension <ref> [CSS + 91] </ref>. Thus, it is advantageous to keep values in registers between threads that the compiler cannot prove will execute in a single quantum. The compiler can construct leave and enter threads that save and restore specific registers if the guess proves incorrect.
Reference: [Cul90] <author> D. E. Culler. </author> <title> Managing Parallelism and Resources in Scientific Dataflow Programs. </title> <type> Technical Report 446, </type> <institution> MIT Lab for Comp. Sci., </institution> <month> March </month> <year> 1990. </year> <type> (PhD Thesis, </type> <institution> Dept. of EECS, MIT). </institution>
Reference-contexts: Finally, code quality is evaluated on several benchmarks. 2 Language Issues Several studies have demonstrated that by exposing parallelism at all levels ample parallelism is available on a broad class of programs <ref> [ACM88, Cul90, AE88, AHN88] </ref>. Exposing parallelism at all levels requires that functions or arbitrary expressions be able to execute and possibly return results before all operands are computed. Data structures must be able to be accessed or passed around while components are still being computed. <p> The program graph for the lookup example includes a function DEF node, enclosing a LOOP node, enclosing a IF node, as shown in Figure 2. The figure shows the corresponding dual graph representation, using a 1-bounded loop <ref> [Cul90] </ref>. The four arguments enter at the inlet nodes at the top of the graph. Since the function is strict in all its arguments, their control outputs are joined before the merge.
Reference: [GH90] <author> V. G. Grafe and J. E. Hoch. </author> <title> The Epsilon-2 Hybrid Dataflow Architecture. </title> <booktitle> In Proc. of Compcon90, </booktitle> <pages> pages 88-93, </pages> <address> San Francisco, CA, </address> <month> March </month> <year> 1990. </year>
Reference-contexts: By compiling via TAM, we have achieved more than two orders of magnitude performance improvement over graph interpreters on conventional machines, making this Id implementation competitive with machines supporting dynamic instruction scheduling in hardware <ref> [PC90, SYH + 89, GH90, Ian88] </ref>. By constraining how dual-graphs are partitioned, we can generate TAM code that closely models these other target architectures. <p> Iannucci integrates thread generation and register assignment to a limited extent; registers are assumed to vanish at every possible suspension point or control transfer. This style of register usage is incorporated in recent dataflow machines, including 14 Monsoon [PT91], Epsilon <ref> [GH90] </ref> and EM-4 [SYH + 89], allowing partitioning similar to the hybrid model. * Threads produced by dataflow partitioning without merging reflect the limited thread capability of most dataflow machines.
Reference: [Hal85] <author> R. H. Halstead, Jr. </author> <title> Multilisp: A Language for Concurrent Symbolic Computation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(4) </volume> <pages> 501-538, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: However, the proposed architectural solutions make thread scheduling invisible to the compiler, preventing it from applying optimizations that might reduce the cost of thread switching or improve scheduling based on analysis of the program. Inherently parallel languages, such as Id [Nik90] and Multilisp <ref> [Hal85] </ref>, require that small execution threads be scheduled dynamically, even if executed on a uniprocessor [Tra88]. Traub's theoretical work demonstrates how to minimize thread switching for these languages on sequential machines.
Reference: [HF88] <author> R. H. Halstead, Jr. and T. Fujita. MASA: </author> <title> a Multithreaded Processor Architecture for Parallel Symbolic Computing. </title> <booktitle> In Proc. of the 15th Int. Symp. on Comp. Arch., </booktitle> <pages> pages 443-451, </pages> <address> Hawaii, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: 1 Introduction Multithreaded execution appears to be a key ingredient in general purpose parallel computing systems. Many researchers suggest that processors should support multiple instruction streams and switch very rapidly between them in response to remote memory reference latencies or synchronization <ref> [AI87, Smi90, HF88, ALKK90, ACC + 90] </ref>. However, the proposed architectural solutions make thread scheduling invisible to the compiler, preventing it from applying optimizations that might reduce the cost of thread switching or improve scheduling based on analysis of the program.
Reference: [Ian88] <author> R. A. </author> <title> Iannucci. Toward a Dataflow/von Neumann Hybrid Architecture. </title> <booktitle> In Proc. 15th Int. Symp. on Comp. Arch., </booktitle> <pages> pages 131-140, </pages> <address> Hawaii, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: At the same time, none require dynamic scheduling throughout. Thus, it makes sense to investigate hybrid execution models <ref> [Ian88, NA89] </ref>, where statically ordered threads are scheduled 2 dynamically. Our TAM model takes this idea one step further by exposing the scheduling of threads to the compiler as well, so that dynamic scheduling is done without hardware support. <p> Dependence sets partitioning is far more powerful. It finds safe partitions by grouping together nodes which depend on the same set of input nodes (inlets, merges and labels). This guarantees that there are no cyclic dependences within a partition. This is a variant of Iannucci's method of dependence sets <ref> [Ian88] </ref>. It groups overlapping fan-out trees into a partition. Dependence sets is powerful and practical to implement. <p> By compiling via TAM, we have achieved more than two orders of magnitude performance improvement over graph interpreters on conventional machines, making this Id implementation competitive with machines supporting dynamic instruction scheduling in hardware <ref> [PC90, SYH + 89, GH90, Ian88] </ref>. By constraining how dual-graphs are partitioned, we can generate TAM code that closely models these other target architectures. <p> We compare and produce code for the following forms of partitioning. * For TAM we use our best partitioning: dependence sets partitioning with merging. * Threads produced using dependence sets partitioning without merging correspond closely to Scheduling Quanta in Iannucci's hybrid architecture <ref> [Ian88] </ref>. Iannucci integrates thread generation and register assignment to a limited extent; registers are assumed to vanish at every possible suspension point or control transfer.
Reference: [NA89] <author> R. S. Nikhil and Arvind. </author> <title> Can Dataflow Subsume von Neumann Computing? In Proc. </title> <booktitle> of the 16th Annual Int. Symp. on Comp. Arch., </booktitle> <address> Jerusalem, Israel, </address> <month> May </month> <year> 1989. </year>
Reference-contexts: At the same time, none require dynamic scheduling throughout. Thus, it makes sense to investigate hybrid execution models <ref> [Ian88, NA89] </ref>, where statically ordered threads are scheduled 2 dynamically. Our TAM model takes this idea one step further by exposing the scheduling of threads to the compiler as well, so that dynamic scheduling is done without hardware support.
Reference: [Nik90] <author> R. S. Nikhil. </author> <note> Id (Version 90.0) Reference Manual. Technical Report CSG Memo, to appear, </note> <institution> MIT Lab for Comp. Sci., </institution> <type> 545 Tech. </type> <institution> Square, </institution> <address> Cambridge, MA, </address> <year> 1990. </year> <month> 20 </month>
Reference-contexts: However, the proposed architectural solutions make thread scheduling invisible to the compiler, preventing it from applying optimizations that might reduce the cost of thread switching or improve scheduling based on analysis of the program. Inherently parallel languages, such as Id <ref> [Nik90] </ref> and Multilisp [Hal85], require that small execution threads be scheduled dynamically, even if executed on a uniprocessor [Tra88]. Traub's theoretical work demonstrates how to minimize thread switching for these languages on sequential machines.
Reference: [Nik91] <author> R. S. Nikhil. </author> <title> The Parallel Programming Language Id and its Compilation for Parallel Machines. </title> <booktitle> In Proc. Workshop on Massive Parallelism, </booktitle> <address> Amalfi, Italy, October 1989. </address> <publisher> Academic Press, </publisher> <year> 1991. </year> <note> Also: CSG Memo 313, </note> <institution> MIT Laboratory for Computer Science, 545 Technology Square, </institution> <address> Cambridge, MA 02139, USA. </address>
Reference-contexts: If cons and flat are strict, this exhibits no parallelism. Under lenient execution, the list is constructed in parallel <ref> [Nik91] </ref>. The contrived function two_things returns a pair containing the square of its first argument and the product of its two arguments. It can compute and return x*x before y is available, which enhances parallelism.
Reference: [PC90] <author> G. M. Papadopoulos and D. E. Culler. Monsoon: </author> <title> an Explicit Token-Store Architecture. </title> <booktitle> In Proc. of the 17th Annual Int. Symp. on Comp. Arch., </booktitle> <address> Seattle, Washington, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: two_things x y = (x*x, x*y); def cube x = -a,b = two_things (x,a) in b-; def strange x p = -a,b,c = if p then (bb,x,aa) else (x,aa,bb); aa = 3*a; in c-; None of these examples present problems for a machine with dynamic instruction scheduling such as Monsoon <ref> [PC90] </ref>. At the same time, none require dynamic scheduling throughout. Thus, it makes sense to investigate hybrid execution models [Ian88, NA89], where statically ordered threads are scheduled 2 dynamically. <p> By compiling via TAM, we have achieved more than two orders of magnitude performance improvement over graph interpreters on conventional machines, making this Id implementation competitive with machines supporting dynamic instruction scheduling in hardware <ref> [PC90, SYH + 89, GH90, Ian88] </ref>. By constraining how dual-graphs are partitioned, we can generate TAM code that closely models these other target architectures.
Reference: [PT91] <author> G. M. Papadopoulos and K. R. Traub. </author> <title> Multithreading: A Revisionist View of Dataflow Architectures. </title> <booktitle> In Proc. of the 18th Int. Symp. on Comp. Arch., </booktitle> <pages> pages 342-351, </pages> <address> Toronto, Canada, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: Iannucci integrates thread generation and register assignment to a limited extent; registers are assumed to vanish at every possible suspension point or control transfer. This style of register usage is incorporated in recent dataflow machines, including 14 Monsoon <ref> [PT91] </ref>, Epsilon [GH90] and EM-4 [SYH + 89], allowing partitioning similar to the hybrid model. * Threads produced by dataflow partitioning without merging reflect the limited thread capability of most dataflow machines. <p> However, partitioning has substantial impact on control overhead. Our implementation on conventional state-of-the-art processors provides a baseline against which novel multithreaded machines can be judged. Surprisingly, the compilation techniques developed for conventional machines can improve the performance of these novel architectures as well <ref> [PT91] </ref>. While these results are encouraging, there is considerable room for improvement. Partitioning of conditionals will improve significantly when switch and merge combining are implemented. Redundant arc elimination is currently quite primitive.
Reference: [Sah91] <author> A. Sah. </author> <title> Parallel Language Support for Shared memory multiprocessors. </title> <type> Master's thesis, </type> <institution> Computer Science Div., University of California at Berkeley, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: Thirty iterations are run on matrices of size 100 fi 100. DTW implements a dynamic time warp algorithm used in discrete word speech recognition <ref> [Sah91] </ref>. The size of the test template and number of cepstral coefficients is 100. Speech is used to determine cepstral coefficients for speech processing. We take 10240 speech samples and compute 30 cepstral coefficients. Paraffins [AHN88] enumerates the distinct isomers of paraffins of size up to 14.
Reference: [Sch91] <author> K. E. Schauser. </author> <title> Compiling Dataflow into Threads. </title> <type> Technical report, </type> <institution> Computer Science Div., University of California, </institution> <address> Berkeley CA 94720, </address> <year> 1991. </year> <type> (MS Thesis, </type> <institution> Dept. of EECS, </institution> <note> UCB). </note>
Reference-contexts: It has a data output, but neither a control input nor a control output, since the value is known at compile time. Dual graphs are generated by expanding dataflow program graph instructions. This is a local transformation described by expansion rules for individual program graph nodes <ref> [Sch91] </ref>. A program graph arc expands into a data arc and a control arc in the dual graph. In many cases, one or the other will prove unnecessary and be eliminated. <p> Also, all dependence arcs must cross partitions. Finally, the entry count for any valid execution of the partition is constant. These properties imply the following lemma. (We will omit proofs throughout this paper. The interested reader is referred to <ref> [Sch91] </ref>.) Lemma 1 A safe partition can be mapped into a TAM thread. Our partitioning algorithm first identifies small safe partitions. Then, these basic partitions are iteratively merged into larger safe partitions by applying simple merge rules, eliminating redundant control arcs, and combining switches and merges until the process converges. <p> The power of dependence sets and dominance sets partitioning lies in the fact that they can work across different fan-out or fan-in trees. It is possible to find examples where dependence sets partitioning is superior to dominance sets partitioning, and vice versa <ref> [Sch91] </ref>. A better choice may be to combine the two forms of basic partitioning. 5.3 Redundant Arc Elimination The goal of redundant arc elimination is to reduce synchronization cost. Eliminating a control arc between two partitions avoids a fork and decreases the entry count of the target partition.
Reference: [Smi90] <author> B. Smith. </author> <title> Keynote Address. </title> <booktitle> Proc. of the 17th Annual Int. Symp. on Comp. Arch., </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Multithreaded execution appears to be a key ingredient in general purpose parallel computing systems. Many researchers suggest that processors should support multiple instruction streams and switch very rapidly between them in response to remote memory reference latencies or synchronization <ref> [AI87, Smi90, HF88, ALKK90, ACC + 90] </ref>. However, the proposed architectural solutions make thread scheduling invisible to the compiler, preventing it from applying optimizations that might reduce the cost of thread switching or improve scheduling based on analysis of the program.
Reference: [SYH + 89] <author> S. Sakai, Y. Yamaguchi, K. Hiraki, Y. Kodama, and T. Yuba. </author> <title> An Architecture of a Dataflow Single Chip Processor. </title> <booktitle> In Proc. of the 16th Annual Int. Symp. on Comp. Arch., </booktitle> <pages> pages 46-53, </pages> <address> Jerusalem, Israel, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: By compiling via TAM, we have achieved more than two orders of magnitude performance improvement over graph interpreters on conventional machines, making this Id implementation competitive with machines supporting dynamic instruction scheduling in hardware <ref> [PC90, SYH + 89, GH90, Ian88] </ref>. By constraining how dual-graphs are partitioned, we can generate TAM code that closely models these other target architectures. <p> Iannucci integrates thread generation and register assignment to a limited extent; registers are assumed to vanish at every possible suspension point or control transfer. This style of register usage is incorporated in recent dataflow machines, including 14 Monsoon [PT91], Epsilon [GH90] and EM-4 <ref> [SYH + 89] </ref>, allowing partitioning similar to the hybrid model. * Threads produced by dataflow partitioning without merging reflect the limited thread capability of most dataflow machines.
Reference: [Tra86] <author> K. R. Traub. </author> <title> A Compiler for the MIT Tagged-Token Dataflow Architecture. </title> <type> Technical Report TR-370, </type> <institution> MIT Lab for Comp. Sci., </institution> <type> 545 Tech. </type> <institution> Square, </institution> <address> Cambridge, MA, </address> <month> August </month> <year> 1986. </year> <type> (MS Thesis, </type> <institution> Dept. of EECS, MIT). </institution>
Reference-contexts: The lenient parallel language Id90 is taken as a starting point for the study, using the MIT compiler to produce dataflow program graphs <ref> [Tra86] </ref>. <p> This involves analysis of expected quantum boundaries, frame and register assignment under asynchronous thread scheduling, and generation of inlets. 4 Dual Graphs Compilation of Id90 to TAM begins after generation of dataflow program graphs <ref> [Tra86] </ref>. Program graphs are a hierarchical graphical intermediate form that facilitates powerful high level optimizations. The meaning of program graphs is given in terms of a dataflow firing rule, so control flow is implicitly prescribed by the dynamic propagation of values.
Reference: [Tra88] <author> K. R. Traub. </author> <title> Sequential Implementation of Lenient Programming Languages. </title> <type> Technical Report TR-417, </type> <institution> MIT Lab for Comp. Sci., </institution> <type> 545 Tech. </type> <institution> Square, </institution> <address> Cambridge, MA, </address> <month> September </month> <year> 1988. </year> <type> (PhD Thesis, </type> <institution> Dept. of EECS, MIT). </institution>
Reference-contexts: Inherently parallel languages, such as Id [Nik90] and Multilisp [Hal85], require that small execution threads be scheduled dynamically, even if executed on a uniprocessor <ref> [Tra88] </ref>. Traub's theoretical work demonstrates how to minimize thread switching for these languages on sequential machines. <p> It can compute and return x*x before y is available, which enhances parallelism. In fact, it must be able to do so, since the first result can be used as the second argument, as in the unusual function cube. The final example, due to Traub <ref> [Tra88] </ref>, has three mutually recursive bindings, where the cyclic dependence through the conditional must be resolved dynamically. def lookup_array A T = -(al,ah) = bounds A; (tl,th) = bounds T in -array (al,ah) of [i] = (lookup A [i] T tl th) || i &lt;- al to ah--; def lookup v <p> This aspect is constrained partly by the language and partly by the execution model. The language dictates which portions of the program can be scheduled statically and which require dynamic synchronization. An elegant theoretical framework for addressing the language requirements is provided by Traub's work <ref> [Tra88] </ref>. The execution model places further constraints on partitioning, since synchronization only occurs at the entry to a thread and conditional execution occurs only between threads. These constraints simplify treatment of the language requirements. <p> Identifying portions of the dual graph that can be executed as a thread is called partitioning. 5 Partitioning The key step in compiling a lenient language for a machine that executes instruction sequences is partitioning the program into statically schedulable entities <ref> [Tra88] </ref>. Fundamental limits on partitioning are imposed by dependence cycles that can only be resolved dynamically. In Id90 these arise due to conditionals, function calls, and access to I-structures.
Reference: [vESC91] <author> T. von Eicken, K. E. Schauser, and D. E. Culler. TL0: </author> <title> An Implementation of the TAM Threaded Abstract Machine, Version 2.1. </title> <type> Technical Report, </type> <institution> Computer Science Div., University of California at Berkeley, </institution> <year> 1991. </year>
Reference-contexts: Synchronization, thread scheduling and storage management are explicit in the machine language and exposed to the compiler. TAM is presented elsewhere <ref> [CSS + 91, vESC91] </ref>; in this section we describe the salient features of TAM as a compilation target. A primary design goal in TAM is to provide a means of exploiting locality, even under asynchronous execution, to minimize the overhead of multithreading.
References-found: 28

