URL: http://www.ics.uci.edu/~pedrod/ijcai95.ps.gz
Refering-URL: http://www.ics.uci.edu/~mlearn/MLPapers.html
Root-URL: 
Email: pedrod@ics.uci.edu  
Title: Rule Induction and Instance-Based Learning: A Unified Approach  
Author: Pedro Domingos 
Note: 10 with 95% confidence).  
Address: Irvine, California 92717, U.S.A.  
Affiliation: Department of Information and Computer Science University of California, Irvine  
Abstract: This paper presents a new approach to inductive learning that combines aspects of instance-based learning and rule induction in a single simple algorithm. The RISE system searches for rules in a specific-to-general fashion, starting with one rule per training example, and avoids some of the difficulties of separate-and-conquer approaches by evaluating each proposed induction step globally, i.e., through an efficient procedure that is equivalent to checking the accuracy of the rule set as a whole on every training example. Classification is performed using a best-match strategy, and reduces to nearest-neighbor if all generalizations of instances were rejected. An extensive empirical study shows that RISE consistently achieves higher accuracies than state-of-the-art representatives of its "parent" paradigms (PEBLS and CN2), and also outperforms a decision-tree learner (C4.5) in 13 out of 15 test domains (in 
Abstract-found: 1
Intro-found: 1
Reference: [ Aha and Bankert, 1994 ] <author> D. W. Aha and R. L. Bankert. </author> <title> Feature selection for case-based classification of cloud types: An empirical comparison. </title> <booktitle> In Proc. AAAI-94 Workshop on Case-Based Reasoning, </booktitle> <pages> pages 106-112, </pages> <year> 1994. </year>
Reference-contexts: Compared with IBL algorithms, RISE has the crucial advantage of being able to select different sets of relevant features in different sections of the instance space. Several well-known methods for removing irrelevant features in nearest-neighbor classifiers exist <ref> [ Aha and Bankert, 1994 ] </ref> , but the decisions they make are very coarse, applying to the whole instance space at once. The same consideration holds for the many feature-weighting schemes that have been proposed [ Mohri and Tanaka, 1994 ] .
Reference: [ Aha et al., 1991 ] <author> D. W. Aha, D. Kibler, and M. K. Al-bert. </author> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 37-66, </pages> <year> 1991. </year>
Reference-contexts: 1 Introduction Several well-developed approaches to inductive learning currently exist, among them induction of decision trees [ Quinlan, 1993a ] , rule induction [ Clark and Niblett, 1989 ] , and instance-based learning <ref> [ Aha et al., 1991 ] </ref> . While accuracy in many practical domains is still far from 100%, it is unclear how much, if any, improvement is still possible with current methods.
Reference: [ Belew et al., 1992 ] <author> R. K. Belew, J. McInerney, and N. N. Schraudolph. </author> <title> Evolving networks: Using the genetic algorithm with connectionist learning. </title> <editor> In C. G. Langton, C. Taylor, J. D. Farmer, and S. Rasmussen, editors, </editor> <booktitle> Artificial Life II, </booktitle> <pages> pages 511-547. </pages> <publisher> Addison-Wesley, </publisher> <address> Redwood City, CA, </address> <year> 1992. </year>
Reference-contexts: but combining different paradigms from RISE's: decision trees, IBL and linear machines [ Brodley, 1993 ] , decision trees and rules [ Quin-lan, 1987 ] , decision trees and perceptrons [ Utgoff, 1989 ] , rules and Bayesian classification [ Smyth et al., 1990 ] , back-propagation and genetic algorithms <ref> [ Belew et al., 1992 ] </ref> , etc.
Reference: [ Brodley, 1993 ] <author> C. E. Brodley. </author> <title> Addressing the selective superiority problem: Automatic algorithm/model class selection. </title> <booktitle> In Proc. 10th Machine Learning Conf., </booktitle> <pages> pages 17-24, </pages> <year> 1993. </year>
Reference-contexts: Empirical studies have also shown repeatedly that each approach works best in some, but not all, domains; this has been termed the selective superiority problem <ref> [ Brodley, 1993 ] </ref> . Ideally, we would like to have an algorithm that in each domain of interest performs as well as the best of the algorithms above, or better. <p> One way to attempt this is by combining two or more of the basic approaches into an algorithm that will behave as the most appropriate of them in each situation. This line of research may be termed "empirical multi-strategy learning" [ Michalski and Tecuci, 1994 ] . MCS <ref> [ Brodley, 1993 ] </ref> , KBNGE [ Wettschereck, 1994 ] and ITRULE [ Smyth et al., 1990 ] are examples of systems of this type. Two induction paradigms with largely complementary strengths and weaknesses are rule induction and instance-based learning (IBL). <p> Several algorithms proposed in the literature can be seen as empirical multi-strategy learners, but combining different paradigms from RISE's: decision trees, IBL and linear machines <ref> [ Brodley, 1993 ] </ref> , decision trees and rules [ Quin-lan, 1987 ] , decision trees and perceptrons [ Utgoff, 1989 ] , rules and Bayesian classification [ Smyth et al., 1990 ] , back-propagation and genetic algorithms [ Belew et al., 1992 ] , etc.
Reference: [ Catlett, 1991 ] <author> J. Catlett. </author> <title> Megainduction: A test flight. </title> <booktitle> In Proc. 8th Machine Learning Conf., </booktitle> <pages> pages 589-604, </pages> <year> 1991. </year>
Reference-contexts: RISE has not been optimized, however, and several important components of the system are amenable to such optimization. Beyond that, window-ing and other sampling techniques can be used without expected loss in accuracy <ref> [ Catlett, 1991 ] </ref> . Also, even though RISE's memory cost is much smaller than that of a simple nearest-neighbor classifier, the rule sets it produces are not as compact as those output by C4.5 or CN2.
Reference: [ Clark and Boswell, 1991 ] <author> P. Clark and R. Boswell. </author> <title> Rule induction with CN2: Some recent improvements. </title> <booktitle> In Proc. EWSL-91, </booktitle> <pages> pages 151-163, </pages> <year> 1991. </year>
Reference-contexts: The version of RISE thus selected is the one described in previous sections. RISE was then compared with state-of-the-art representatives of other approaches on the remaining 15 domains: PEBLS 2.1 for IBL [ Cost and Salzberg, 1993 ] , CN2 6.1 for rule induction <ref> [ Clark and Boswell, 1991 ] </ref> , and C4.5 for induction of decision trees [ Quin-lan, 1993a ] .
Reference: [ Clark and Niblett, 1989 ] <author> P. Clark and T. Niblett. </author> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 261-283, </pages> <year> 1989. </year>
Reference-contexts: 1 Introduction Several well-developed approaches to inductive learning currently exist, among them induction of decision trees [ Quinlan, 1993a ] , rule induction <ref> [ Clark and Niblett, 1989 ] </ref> , and instance-based learning [ Aha et al., 1991 ] . While accuracy in many practical domains is still far from 100%, it is unclear how much, if any, improvement is still possible with current methods. <p> Since E V S , and assuming that A C, which is generally the case, the smaller of those values dominates the complexity of the initialization phase, and both therefore constitute upper bounds on the time complexity of the whole algorithm in their respective situations. Results in <ref> [ Clark and Niblett, 1989 ] </ref> show that the ba Table 3: Experimental results. Domain RISE Default Conf. PEBLS Conf. CN2 Conf. C4.5 Conf.
Reference: [ Cost and Salzberg, 1993 ] <author> S. Cost and S. Salzberg. </author> <title> A weighted nearest neighbor algorithm for learning with symbolic features. </title> <journal> Machine Learning, </journal> <volume> 10 </volume> <pages> 57-78, </pages> <year> 1993. </year>
Reference-contexts: The essential idea behind VDM-type metrics is that two values should be considered similar if they make similar class predictions, and dissimilar if their predictions diverge. This has been found to give good results in several domains <ref> [ Cost and Salzberg, 1993 ] </ref> . Notice that, in particular, SV DM (x i ; x j ) is always 0 if i = j. <p> The version of RISE thus selected is the one described in previous sections. RISE was then compared with state-of-the-art representatives of other approaches on the remaining 15 domains: PEBLS 2.1 for IBL <ref> [ Cost and Salzberg, 1993 ] </ref> , CN2 6.1 for rule induction [ Clark and Boswell, 1991 ] , and C4.5 for induction of decision trees [ Quin-lan, 1993a ] .
Reference: [ DeGroot, 1986 ] <author> M. H. </author> <title> DeGroot. Probability and Statistics. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, 2nd edition, </address> <year> 1986. </year>
Reference-contexts: The comparison with PEBLS in not conclusive (only 85% confidence). The sign test, however, can be misled by very small, insignificant differences; a more sensitive procedure is the Wilcoxon signed-ranks test <ref> [ DeGroot, 1986 ] </ref> , which also takes into account the relative magnitudes of the differences between each pair of accuracies being compared.
Reference: [ Duda and Hart, 1973 ] <author> R. O. Duda and P. E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: In regions of overlap, NGE arbitrarily assigns all examples to the class of the most specific hyperrectangle. In contrast, RISE's learning strategy approximates the optimal decision rule of placing the boundary between two classes at the point where the density of examples from one overtakes that of the other <ref> [ Duda and Hart, 1973 ] </ref> . This is because a rule is started from each example, and its generalization halts when it would include more examples of other classes than of the example's one.
Reference: [ Golding and Rosenbloom, 1991 ] <author> A. R. Golding and P. S. Rosenbloom. </author> <title> Improving rule-based systems through case-based reasoning. </title> <booktitle> In Proc. AAAI-91, </booktitle> <pages> pages 22-27, </pages> <year> 1991. </year>
Reference-contexts: Unlike RISE, FCLS employs different representations for rules and exemplars, and uses a separate-and-conquer strategy similar to that of its AQ ancestors. Golding and Rosenbloom's Anapron system <ref> [ Golding and Rosenbloom, 1991 ] </ref> combines case-based and rule-based reasoning in a name-pronunciation task. It differs substantially from RISE in that it does not learn rules, but rather makes use of a pre-existing knowledge base. It also treats cases and rules separately, and employs a different matching procedure.
Reference: [ Holte et al., 1989 ] <author> R. C. Holte, L. E. Acker, and B. W. Porter. </author> <title> Concept learning and the problem of small disjuncts. </title> <booktitle> In Proc. IJCAI-89, </booktitle> <pages> pages 813-818, </pages> <year> 1989. </year>
Reference-contexts: However, they can only form axis-parallel frontiers in the instance space, and they have trouble recognizing exceptions, or in general small, low-frequency sections of the space; this is known as the small disjuncts problem <ref> [ Holte et al., 1989 ] </ref> . Further, their general-to-specific, "separate and conquer" search strategy causes them to suffer from the splintering problem: as induction progresses, the amount of data left for further learning dwindles rapidly, leading to wrong decisions or insufficient specialization due to lack of adequate statistical support.
Reference: [ Michalski and Tecuci, 1994 ] <editor> R. Michalski and G. Tecuci, editors. </editor> <title> Machine Learning: A Mul-tistrategy Approach. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1994. </year>
Reference-contexts: One way to attempt this is by combining two or more of the basic approaches into an algorithm that will behave as the most appropriate of them in each situation. This line of research may be termed "empirical multi-strategy learning" <ref> [ Michalski and Tecuci, 1994 ] </ref> . MCS [ Brodley, 1993 ] , KBNGE [ Wettschereck, 1994 ] and ITRULE [ Smyth et al., 1990 ] are examples of systems of this type. Two induction paradigms with largely complementary strengths and weaknesses are rule induction and instance-based learning (IBL).
Reference: [ Michalski et al., 1986 ] <author> R. S. Michalski, I. Mozetic, J. Hong, and N. Lavrac. </author> <title> The multi-purpose incremental learning system AQ15 and its testing application to three medical domains. </title> <booktitle> In Proc. AAAI-86, </booktitle> <pages> pages 1041-1045, </pages> <year> 1986. </year>
Reference-contexts: Quinlan [ Quinlan, 1993b ] has successfully combined IBL with trees and other methods, but for the purpose of regression as opposed to classification, performing this combination only at classification time, and in a way that depends critically on the predicted value being continuous. AQ15 <ref> [ Michalski et al., 1986 ] </ref> is a rule induction system that employs best-match classification. Its approach was carried further in the FCLS system [ Zhang, 1990 ] , which combines rules with exemplars in an attempt to alleviate the small disjuncts problem.
Reference: [ Mohri and Tanaka, 1994 ] <author> T. Mohri and H. Tanaka. </author> <title> An optimal weighting criterion of case indexing for both numeric and symbolic attributes. </title> <booktitle> In Proc. AAAI-94 Workshop on Case-Based Reasoning, </booktitle> <pages> pages 123-127, </pages> <year> 1994. </year>
Reference-contexts: Several well-known methods for removing irrelevant features in nearest-neighbor classifiers exist [ Aha and Bankert, 1994 ] , but the decisions they make are very coarse, applying to the whole instance space at once. The same consideration holds for the many feature-weighting schemes that have been proposed <ref> [ Mohri and Tanaka, 1994 ] </ref> . Rule induction systems are able to detect that certain features are relevant only in the context of others, and RISE shares this ability: a feature may be dropped in some rules, but not others.
Reference: [ Murphy and Aha, 1995 ] <author> P. M. Murphy and D. W. Aha. </author> <title> UCI repository of machine learning databases. Machine-readable data repository, </title> <institution> Department of Information and Computer Science, University of Cali-fornia at Irvine, </institution> <address> Irvine, CA, </address> <year> 1995. </year>
Reference-contexts: In practice only a small number of steps may actually be required. 4 Experiments and Results 4.1 Experimental Design In order to verify if RISE's expected benefits are observed in practice, experiments were carried out on 30 datasets from the UCI repository <ref> [ Murphy and Aha, 1995 ] </ref> . Half of the domains were first used to select a default version of RISE by 10-fold cross-validation.
Reference: [ Niblett, 1987 ] <author> T. Niblett. </author> <title> Constructing decision trees in noisy domains. </title> <booktitle> In Proc. EWSL-87, </booktitle> <pages> pages 67-78, </pages> <year> 1987. </year>
Reference-contexts: When two or more rules are equally close to a test example, the rule that was most accurate on the training set wins. So as to not unduly favor more specific rules, the Laplace-corrected accuracy is used <ref> [ Niblett, 1987 ] </ref> : LAcc (R) = N corr (R) + 1 N won (R) + C where R is any rule, C is the number of classes, N won (R) is the total number or examples won by R, N corr (R) is the number of examples among those
Reference: [ Quinlan, 1987 ] <author> J. R. Quinlan. </author> <title> Generating production rules from decision trees. </title> <booktitle> In Proc. IJCAI-87, </booktitle> <pages> pages 304-307, </pages> <year> 1987. </year>
Reference-contexts: C4.5RULES, the version of C4.5 that converts trees to rules, was chosen because rules have been observed to achieve the highest accuracies <ref> [ Quinlan, 1987 ] </ref> , and because they are more di rectly comparable to RISE. The default classifier, which assigns all test examples to the most frequent class, was also included in the study to provide a baseline. The default versions of all algorithms were used.
Reference: [ Quinlan, 1993a ] <author> J. R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: 1 Introduction Several well-developed approaches to inductive learning currently exist, among them induction of decision trees <ref> [ Quinlan, 1993a ] </ref> , rule induction [ Clark and Niblett, 1989 ] , and instance-based learning [ Aha et al., 1991 ] . While accuracy in many practical domains is still far from 100%, it is unclear how much, if any, improvement is still possible with current methods.
Reference: [ Quinlan, 1993b ] <author> J. R. Quinlan. </author> <title> Combining instance-based and model-based learning. </title> <booktitle> In Proc. 10th Machine Learning Conf., </booktitle> <pages> pages 236-243, </pages> <year> 1993. </year>
Reference-contexts: Quinlan <ref> [ Quinlan, 1993b ] </ref> has successfully combined IBL with trees and other methods, but for the purpose of regression as opposed to classification, performing this combination only at classification time, and in a way that depends critically on the predicted value being continuous.
Reference: [ Salzberg, 1991 ] <author> S. Salzberg. </author> <title> A nearest hyperrectangle learning method. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 251-276, </pages> <year> 1991. </year>
Reference-contexts: It differs substantially from RISE in that it does not learn rules, but rather makes use of a pre-existing knowledge base. It also treats cases and rules separately, and employs a different matching procedure. A system more similar to RISE is EACH/NGE <ref> [ Salzberg, 1991 ] </ref> , which produces and uses hyperrectan-gles generalized from specific instances.
Reference: [ Schaffer, 1994 ] <author> C. Schaffer. </author> <title> A conservation law for generalization performance. </title> <booktitle> In Proc. 11th Machine Learning Conf., </booktitle> <pages> pages 259-265, </pages> <year> 1994. </year>
Reference-contexts: While it is now clearly understood that induction is a "zero-sum game", and thus this goal is unachievable for the set of all mathematically possible domains <ref> [ Schaffer, 1994 ] </ref> , it may well be possible to produce learners that perform better on a wide variety of real-world domains, at the cost of worse performance in domains that never occur in practice.
Reference: [ Smyth et al., 1990 ] <author> P. Smyth, R. M. Goodman, and C. Higgins. </author> <title> A hybrid rule-based/Bayesian classifier. </title> <booktitle> In Proc. ECAI-90, </booktitle> <pages> pages 610-615, </pages> <year> 1990. </year>
Reference-contexts: This line of research may be termed "empirical multi-strategy learning" [ Michalski and Tecuci, 1994 ] . MCS [ Brodley, 1993 ] , KBNGE [ Wettschereck, 1994 ] and ITRULE <ref> [ Smyth et al., 1990 ] </ref> are examples of systems of this type. Two induction paradigms with largely complementary strengths and weaknesses are rule induction and instance-based learning (IBL). <p> proposed in the literature can be seen as empirical multi-strategy learners, but combining different paradigms from RISE's: decision trees, IBL and linear machines [ Brodley, 1993 ] , decision trees and rules [ Quin-lan, 1987 ] , decision trees and perceptrons [ Utgoff, 1989 ] , rules and Bayesian classification <ref> [ Smyth et al., 1990 ] </ref> , back-propagation and genetic algorithms [ Belew et al., 1992 ] , etc.
Reference: [ Stanfill and Waltz, 1986 ] <author> C. Stanfill and D. Waltz. </author> <title> Toward memory-based reasoning. </title> <journal> Communications of the ACM, </journal> <volume> 29 </volume> <pages> 1213-1228, </pages> <year> 1986. </year>
Reference-contexts: The distance measure used is a combination of Euclidean distance for numeric attributes, and a simplified version of Stanfill and Waltz's value difference metric for symbolic attributes <ref> [ Stanfill and Waltz, 1986 ] </ref> . Let E = (e 1 ; e 2 ; . . . ; e A ; c E ) be an example with value e i for the ith attribute and class c E .
Reference: [ Utgoff, 1989 ] <author> P. E. Utgoff. </author> <title> Perceptron trees: A case study in hybrid concept representations. </title> <journal> Connection Science, </journal> <volume> 1 </volume> <pages> 377-391, </pages> <year> 1989. </year>
Reference-contexts: Several algorithms proposed in the literature can be seen as empirical multi-strategy learners, but combining different paradigms from RISE's: decision trees, IBL and linear machines [ Brodley, 1993 ] , decision trees and rules [ Quin-lan, 1987 ] , decision trees and perceptrons <ref> [ Utgoff, 1989 ] </ref> , rules and Bayesian classification [ Smyth et al., 1990 ] , back-propagation and genetic algorithms [ Belew et al., 1992 ] , etc.
Reference: [ Wettschereck and Dietterich, 1995 ] <author> D. Wettschereck and T. Dietterich. </author> <title> An experimental comparison of the nearest-neighbor and nearest-hyperrectangle algorithms. </title> <booktitle> Machine Learning, </booktitle> <year> 1995. </year> <note> To appear. </note>
Reference: [ Wettschereck, 1994 ] <author> D. Wettschereck. </author> <title> A hybrid nearest-neighbor and nearest-hyperrectangle algorithm. </title> <booktitle> In Proc. 7th ECML, </booktitle> <year> 1994. </year>
Reference-contexts: This line of research may be termed "empirical multi-strategy learning" [ Michalski and Tecuci, 1994 ] . MCS [ Brodley, 1993 ] , KBNGE <ref> [ Wettschereck, 1994 ] </ref> and ITRULE [ Smyth et al., 1990 ] are examples of systems of this type. Two induction paradigms with largely complementary strengths and weaknesses are rule induction and instance-based learning (IBL). <p> Recently Wettschereck and Dietterich [1995] have carried out a detailed comparison of NGE and k-nearest-neighbor (kNN), and designed an algorithm that combines the two <ref> [ Wettschereck, 1994 ] </ref> , but does not achieve greater accuracy than kNN alone. They found that NGE performs substantially worse than kNN, and that the chief cause of this is NGE's use of overlapping rectangles.
Reference: [ Zhang, 1990 ] <author> J. Zhang. </author> <title> A method that combines inductive learning with exemplar-based learning. </title> <booktitle> In Proc. 2nd International IEEE Conf. on Tools for Artificial Intelligence, </booktitle> <pages> pages 31-37, </pages> <year> 1990. </year>
Reference-contexts: AQ15 [ Michalski et al., 1986 ] is a rule induction system that employs best-match classification. Its approach was carried further in the FCLS system <ref> [ Zhang, 1990 ] </ref> , which combines rules with exemplars in an attempt to alleviate the small disjuncts problem. Unlike RISE, FCLS employs different representations for rules and exemplars, and uses a separate-and-conquer strategy similar to that of its AQ ancestors.
References-found: 28

