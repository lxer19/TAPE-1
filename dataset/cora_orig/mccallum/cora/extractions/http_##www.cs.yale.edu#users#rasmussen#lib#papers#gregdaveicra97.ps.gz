URL: http://www.cs.yale.edu/users/rasmussen/lib/papers/gregdaveicra97.ps.gz
Refering-URL: http://www.cs.yale.edu/users/rasmussen/research.html
Root-URL: http://www.cs.yale.edu
Title: Image-based Prediction of Landmark Features for Mobile Robot Navigation  
Author: Gregory D. Hager David Kriegman Erliang Yeh Christopher Rasmussen 
Address: New Haven, CT 06520  
Affiliation: Computer Science Electrical Engineering Electrical Engineering Computer Science Yale University,  
Note: 1997 IEEE International Conference on Robotics and Automation  
Abstract: We have been developing an architecture for vision-based navigation which relies on continuous feedback from visual "landmarks" to control robot motion. In this approach, landmarks are consistently located and acquired as they come into view. To make this process efficient and robust, it is important that the image locations of these features can be predicted from available image information. In this article, we discuss methods for direct image-based prediction of point and line features for a mobile system operating on a planar surface. Preliminary experimental results suggest that image-based prediction can be performed efficiently and with sufficient accuracy to ensure robust acquisition of navigational landmarks. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Atiya and G. Hager. </author> <title> Real-time vision-based robot localization. </title> <journal> IEEE Trans. Robotics and Automation, </journal> <volume> 9(6) </volume> <pages> 785-800, </pages> <year> 1993. </year>
Reference-contexts: 1 Introduction Most prior research on robot navigation has focused on developing methods for computing and/or controlling the position of a mobile system with respect to some geometric or topological map <ref> [1, 3, 9, 11, 12, 13, 14, 16] </ref>. However, there are applications where the ability to explicitly represent and reason about geometry is not essential. <p> This problem offers a number of challenges to many published approaches to robot navigation. Given that the environment and the allowable paths through it may may change often and abruptly, it is unlikely that a prior geometric model will be available, eliminating approaches which rely on them <ref> [1, 11, 12] </ref>. For similar reasons, "situated" approaches which implicitly use strong assumptions about the environment are also inapplicable [3, 9]. While it is possible to arrange fiducial markers for such a task, it is onerous to erect and calibrate such markers.
Reference: [2] <author> E. Barett, M. Brill, N. Haag, and P. Payton. </author> <title> Invariant linear methods in photogrammetry and model matching. </title> <editor> In J. Mundy and A. Zisserman, editors, </editor> <booktitle> Geometric Invariance in Computer Vision, </booktitle> <pages> pages 277-292. </pages> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: In a purely sensor-based approach, prediction must be performed solely on image information. This problem is closely related to the image transfer problem discussed in the area of projective geometry applied to vision <ref> [2, 5, 8, 15] </ref>. <p> We are to predict the location of each of the remaining n m markers. The problem of predicting the location of markers or features without explicitly reconstructing their 3-D Euclidean location is known in photogrammetry as image transfer <ref> [2] </ref>. Using projective geometry and projective invariance, methods for performing image transfer have been developed for point and line features under a variety of assumptions about the configurations of the features, the camera model (orthographic projection, affine, perspective, projective), and availability of camera calibration [2, 5, 8]. <p> Using projective geometry and projective invariance, methods for performing image transfer have been developed for point and line features under a variety of assumptions about the configurations of the features, the camera model (orthographic projection, affine, perspective, projective), and availability of camera calibration <ref> [2, 5, 8] </ref>. For point features, Barrett et. al. show that linear methods can be used to transfer points from two map images to a third image if eight additional points are observed in all three images [2]. <p> For point features, Barrett et. al. show that linear methods can be used to transfer points from two map images to a third image if eight additional points are observed in all three images <ref> [2] </ref>. Hartley's methods for projective reconstruction of lines can be used for line transfer, and it requires observing thirteen lines in all three images [8]. <p> Unfortunately, we have observed that the results are unacceptably noisy, perhaps due to inaccuracies in the camera calibration coupled with image noise propagating through nonlinear equations. Instead, we have developed a simpler method based on a specialization of the method by Barrett et. al. <ref> [2] </ref>. <p> Expanding the determinant and grouping terms yields an equation involving sixteen unique combinations of observables multiplied by coefficients. These coefficients can be expressed as determinants of matrices composed of rows from A and C: In the general case, it is shown in <ref> [2] </ref> that the first seven of these coefficients are zero. <p> This provides a linear constraint on the values of the observations of these points <ref> [2] </ref> . For the case of planar motion, we can specialize this result and show that the coefficients of pp; q q and p are zero. This would mean that six points determine an invariant relationship. Suppose that we also know the image coordinates of the center of projection.
Reference: [3] <author> S. P. Engelson and D. McDermott. </author> <title> Error correction in mobile robot map learning. </title> <booktitle> In Proc. IEEE Int. Conf. Robotics and Automation, </booktitle> <address> Nice, France, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Most prior research on robot navigation has focused on developing methods for computing and/or controlling the position of a mobile system with respect to some geometric or topological map <ref> [1, 3, 9, 11, 12, 13, 14, 16] </ref>. However, there are applications where the ability to explicitly represent and reason about geometry is not essential. <p> For similar reasons, "situated" approaches which implicitly use strong assumptions about the environment are also inapplicable <ref> [3, 9] </ref>. While it is possible to arrange fiducial markers for such a task, it is onerous to erect and calibrate such markers.
Reference: [4] <author> B. Espiau, F. Chaumette, and P. Rives. </author> <title> A New Approach to Visual Servoing in Robotics. </title> <journal> IEEE Trans. Robotics and Automation, </journal> <volume> 8 </volume> <pages> 313-326, </pages> <year> 1992. </year>
Reference-contexts: Navigation is posed as the problem of moving from place to place|that is, from view to view|using techniques developed in the area of visual servoing <ref> [4, 10] </ref>. In order to make this problem precise, we define the following terms (detailed more fully in [6]). We use the term marker to denote any visual entity that is in some way visually distinctive so that it can be tracked as the robot moves.
Reference: [5] <author> S. Fairley, I. Reid, and D. Murray. </author> <title> Transfer of fixation for an active stereo platform via affine structure recovery. </title> <booktitle> In Int. Conf. Computer Vision, </booktitle> <pages> pages 1100-1105, </pages> <year> 1995. </year>
Reference-contexts: In a purely sensor-based approach, prediction must be performed solely on image information. This problem is closely related to the image transfer problem discussed in the area of projective geometry applied to vision <ref> [2, 5, 8, 15] </ref>. <p> Using projective geometry and projective invariance, methods for performing image transfer have been developed for point and line features under a variety of assumptions about the configurations of the features, the camera model (orthographic projection, affine, perspective, projective), and availability of camera calibration <ref> [2, 5, 8] </ref>. For point features, Barrett et. al. show that linear methods can be used to transfer points from two map images to a third image if eight additional points are observed in all three images [2].
Reference: [6] <author> G. Hager and C. Rasmussen. </author> <title> Robot navigation using image sequences. </title> <booktitle> In Proc. AAAI, </booktitle> <pages> pp. 938-943, </pages> <year> 1996. </year>
Reference-contexts: Our choice of vision is based on the fact that it can observe large areas to find useful "landmarks" for defining the path, and our desire to use passive sensing techniques. In a previous paper <ref> [6] </ref>, we outlined the general architecture of such a vision-based navigation system. The central idea in this design is to constantly track image features used as landmarks for navigation. <p> Navigation is posed as the problem of moving from place to place|that is, from view to view|using techniques developed in the area of visual servoing [4, 10]. In order to make this problem precise, we define the following terms (detailed more fully in <ref> [6] </ref>). We use the term marker to denote any visual entity that is in some way visually distinctive so that it can be tracked as the robot moves. A scene is a set of markers which are tracked concurrently in an image.
Reference: [7] <author> G. Hager and K. Toyama. XVision: </author> <title> Combining image warping and geometric constraints for fast visual tracking. </title> <booktitle> In Proc. ECCV, </booktitle> <pages> pp. 507-517. </pages> <publisher> Springer Ver-lag, </publisher> <year> 1996. </year>
Reference-contexts: In a previous paper [6], we outlined the general architecture of such a vision-based navigation system. The central idea in this design is to constantly track image features used as landmarks for navigation. Such tracking is cheap and simple <ref> [7] </ref>; it quickly and continually reduces image information to the time history of a small set of feature locations. This time history is learned once (the teaching phase), and subsequent motion is defined by controlling the robot so as to replicate the learned feature time history.
Reference: [8] <author> R. </author> <title> Hartley. A linear method for reconstruction from lines and points. </title> <booktitle> In Int. Conf. Computer Vision, </booktitle> <pages> pp. 882-887, </pages> <year> 1995. </year>
Reference-contexts: In a purely sensor-based approach, prediction must be performed solely on image information. This problem is closely related to the image transfer problem discussed in the area of projective geometry applied to vision <ref> [2, 5, 8, 15] </ref>. <p> Using projective geometry and projective invariance, methods for performing image transfer have been developed for point and line features under a variety of assumptions about the configurations of the features, the camera model (orthographic projection, affine, perspective, projective), and availability of camera calibration <ref> [2, 5, 8] </ref>. For point features, Barrett et. al. show that linear methods can be used to transfer points from two map images to a third image if eight additional points are observed in all three images [2]. <p> Hartley's methods for projective reconstruction of lines can be used for line transfer, and it requires observing thirteen lines in all three images <ref> [8] </ref>. In both of these general methods, the camera can be at an arbitrary 3-D position and orientation, and the camera may have different calibration parameters at each location. <p> For point and/or line correspondences, methods have been developed for estimating the trifocal tensor when the cameras are uncalibrated and in arbitrary locations <ref> [8, 15] </ref>. Now, consider the image of a 3-D line as shown in define a plane. In turn, the intersection of this plane with the image plane defines a line which can be measured in the image. <p> Hartley observes that for three uncalibrated cameras, this same set of equations holds where E; F and G are arbitrary 3 fi 3 matrices forming the trifocal tensor <ref> [8] </ref>. While the cross product in (5) defines three scalar equations, only two of these are linearly independent. Furthermore, the elements of the 3 fi 3 matrices E; F and G enter linearly in this constraint.
Reference: [9] <author> I. Horswill. Polly: </author> <title> A vision-based artificial agent. </title> <booktitle> In Proc. AAAI, </booktitle> <pages> pp. 824-829, </pages> <year> 1992. </year>
Reference-contexts: 1 Introduction Most prior research on robot navigation has focused on developing methods for computing and/or controlling the position of a mobile system with respect to some geometric or topological map <ref> [1, 3, 9, 11, 12, 13, 14, 16] </ref>. However, there are applications where the ability to explicitly represent and reason about geometry is not essential. <p> For similar reasons, "situated" approaches which implicitly use strong assumptions about the environment are also inapplicable <ref> [3, 9] </ref>. While it is possible to arrange fiducial markers for such a task, it is onerous to erect and calibrate such markers.
Reference: [10] <author> S. Hutchinson, G. Hager, and P. Corke. </author> <title> A tutorial on visual servo control. </title> <journal> IEEE Trans. Robotics and Automation, </journal> <volume> 12(5) </volume> <pages> 651-670, </pages> <year> 1996. </year>
Reference-contexts: Navigation is posed as the problem of moving from place to place|that is, from view to view|using techniques developed in the area of visual servoing <ref> [4, 10] </ref>. In order to make this problem precise, we define the following terms (detailed more fully in [6]). We use the term marker to denote any visual entity that is in some way visually distinctive so that it can be tracked as the robot moves.
Reference: [11] <author> A. Kosaka and A. Kak. </author> <title> Fast vision-guided mobile robot navigation using model-based reasoning and prediction of uncertainties. CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> 56(3) </volume> <pages> 271-329, </pages> <year> 1993. </year>
Reference-contexts: 1 Introduction Most prior research on robot navigation has focused on developing methods for computing and/or controlling the position of a mobile system with respect to some geometric or topological map <ref> [1, 3, 9, 11, 12, 13, 14, 16] </ref>. However, there are applications where the ability to explicitly represent and reason about geometry is not essential. <p> This problem offers a number of challenges to many published approaches to robot navigation. Given that the environment and the allowable paths through it may may change often and abruptly, it is unlikely that a prior geometric model will be available, eliminating approaches which rely on them <ref> [1, 11, 12] </ref>. For similar reasons, "situated" approaches which implicitly use strong assumptions about the environment are also inapplicable [3, 9]. While it is possible to arrange fiducial markers for such a task, it is onerous to erect and calibrate such markers. <p> One central component of this architecture is the ability to effectively predict and acquire landmarks as they come into view. Given a geometric model, prediction could be handled using a well-understood combination of odometry and estimation <ref> [11, 12] </ref>. In a purely sensor-based approach, prediction must be performed solely on image information. This problem is closely related to the image transfer problem discussed in the area of projective geometry applied to vision [2, 5, 8, 15].
Reference: [12] <author> D. J. Kriegman, E. Triendl, and T. O. Binford. </author> <title> Stereo vision and navigation in buildings for mobile robots. </title> <journal> IEEE Trans. Robotics and Automation, </journal> <volume> 5(6) </volume> <pages> 792-803, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: 1 Introduction Most prior research on robot navigation has focused on developing methods for computing and/or controlling the position of a mobile system with respect to some geometric or topological map <ref> [1, 3, 9, 11, 12, 13, 14, 16] </ref>. However, there are applications where the ability to explicitly represent and reason about geometry is not essential. <p> This problem offers a number of challenges to many published approaches to robot navigation. Given that the environment and the allowable paths through it may may change often and abruptly, it is unlikely that a prior geometric model will be available, eliminating approaches which rely on them <ref> [1, 11, 12] </ref>. For similar reasons, "situated" approaches which implicitly use strong assumptions about the environment are also inapplicable [3, 9]. While it is possible to arrange fiducial markers for such a task, it is onerous to erect and calibrate such markers. <p> One central component of this architecture is the ability to effectively predict and acquire landmarks as they come into view. Given a geometric model, prediction could be handled using a well-understood combination of odometry and estimation <ref> [11, 12] </ref>. In a purely sensor-based approach, prediction must be performed solely on image information. This problem is closely related to the image transfer problem discussed in the area of projective geometry applied to vision [2, 5, 8, 15].
Reference: [13] <author> B. Kuipers and Y. Byun. </author> <title> A robot exploration and mapping strategy based on a semantic hierarchy of spatial representations. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <volume> 8 </volume> <pages> 47-63, </pages> <year> 1991. </year>
Reference-contexts: 1 Introduction Most prior research on robot navigation has focused on developing methods for computing and/or controlling the position of a mobile system with respect to some geometric or topological map <ref> [1, 3, 9, 11, 12, 13, 14, 16] </ref>. However, there are applications where the ability to explicitly represent and reason about geometry is not essential. <p> Finally, the fact that the robot may need to navigate accurately in large open areas suggests that topological approaches based on place recognition and approaches relying on range-limited sensing such as sonar <ref> [13, 14] </ref> may have difficulty supporting accurate and reliable motion. Our aim is to develop a vision-based navigation system capable of performing these types of tasks.
Reference: [14] <author> J. L. Leonard and H. F. Durrant-Whyte. </author> <title> Mobile robot localization by tracking geometric beacons. </title> <journal> IEEE Trans. Robotics and Automation, </journal> <volume> 7(3) </volume> <pages> 376-380, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Most prior research on robot navigation has focused on developing methods for computing and/or controlling the position of a mobile system with respect to some geometric or topological map <ref> [1, 3, 9, 11, 12, 13, 14, 16] </ref>. However, there are applications where the ability to explicitly represent and reason about geometry is not essential. <p> Finally, the fact that the robot may need to navigate accurately in large open areas suggests that topological approaches based on place recognition and approaches relying on range-limited sensing such as sonar <ref> [13, 14] </ref> may have difficulty supporting accurate and reliable motion. Our aim is to develop a vision-based navigation system capable of performing these types of tasks.
Reference: [15] <author> A. Shashua and M. Werman. </author> <title> Trilinearity of three perspective views and its associated tensor. </title> <booktitle> In ICCV95, </booktitle> <pages> pp. 920-925, </pages> <year> 1995. </year>
Reference-contexts: In a purely sensor-based approach, prediction must be performed solely on image information. This problem is closely related to the image transfer problem discussed in the area of projective geometry applied to vision <ref> [2, 5, 8, 15] </ref>. <p> For point and/or line correspondences, methods have been developed for estimating the trifocal tensor when the cameras are uncalibrated and in arbitrary locations <ref> [8, 15] </ref>. Now, consider the image of a 3-D line as shown in define a plane. In turn, the intersection of this plane with the image plane defines a line which can be measured in the image.
Reference: [16] <author> C. Taylor and D. Kriegman. </author> <title> Algorithms for vision-based exploration. </title> <editor> In K. Goldberg, D. Halperin, J. Latombe, and R. Wilson, editors, </editor> <booktitle> The Algorithmic Foundations of Robotics. </booktitle> <editor> A. K. Peters, </editor> <address> Boston, MA, </address> <year> 1995. </year>
Reference-contexts: 1 Introduction Most prior research on robot navigation has focused on developing methods for computing and/or controlling the position of a mobile system with respect to some geometric or topological map <ref> [1, 3, 9, 11, 12, 13, 14, 16] </ref>. However, there are applications where the ability to explicitly represent and reason about geometry is not essential.
Reference: [17] <author> S. Ullman and R. Basri. </author> <title> Recognition by a linear combination of models. </title> <journal> IEEE Trans. Pattern Anal. Mach. Intelligence, </journal> <volume> 13 </volume> <pages> 992-1006, </pages> <year> 1991. </year>
Reference: [18] <author> J. Weng, T. Huang, and N. Ahuja. </author> <title> Motion and structure from line correspondences: Closed-form solution, uniqueness, </title> <journal> and optimization. IEEE Trans. Pattern Anal. Mach. Intelligence, </journal> <volume> 14(3) </volume> <pages> 318-336, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: For three images of the same line, it is shown in a paper by Weng, Huang and Ahuja <ref> [18] </ref> that the nor-mals to the corresponding planes are related by: n 2 fi 4 0 En 1 0 F n 1 0 Gn 1 5 = 0 (5) where n i denotes the normal to the plane defined by a line and camera center i. <p> For the same reasons, transfer is impossible when two camera centers are coincident or when the three camera centers are collinear. Like the third degeneracy mentioned above, transfer is not possible if one of the lines lies in horizon plane. As noted in <ref> [18] </ref>, there are other degeneracies for structure from motion from straight lines that are likely to apply to this case of image transfer under constrained motion. 3.3 Relaxing Assumptions When formulating the transfer problem in Section 2, we assumed that the camera was neither tilted nor rotated about the optical axis;
References-found: 18

