URL: http://kmi.open.ac.uk/techreports/papers/kmi-tr-29.ps.gz
Refering-URL: http://kmi.open.ac.uk/techreports/kmi-tr-list.html
Root-URL: 
Title: Robust Parameter Learning in Bayesian Networks with Missing Data  
Author: Marco Ramoni Paola Sebastiani 
Date: July 1996 (revised October 1996)  
Affiliation: Knowledge Media Institute  
Pubnum: KMI-TR-29  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> W. L. Buntine. </author> <title> Operations for learning with graphical models. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 159-225, </pages> <year> 1994. </year>
Reference-contexts: Although in their original concept bbns were mainly designed to encode the knowledge of human experts, their statistical roots soon prompted for the development of methods to learn them directly from databases of cases rather than from the insight of human domain experts <ref> [2, 1, 4] </ref>. This choice can be extremely rewarding when the domain of applications generates large amounts of statistical information and aspects of the domain knowledge are still unknown or controversial, or too complex to be encoded as subjective probabilities of few domain experts.
Reference: [2] <author> G.F. Cooper and E. Herskovitz. </author> <title> A bayesian method for the induction of probabilistic networks from data. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 309-347, </pages> <year> 1992. </year>
Reference-contexts: Although in their original concept bbns were mainly designed to encode the knowledge of human experts, their statistical roots soon prompted for the development of methods to learn them directly from databases of cases rather than from the insight of human domain experts <ref> [2, 1, 4] </ref>. This choice can be extremely rewarding when the domain of applications generates large amounts of statistical information and aspects of the domain knowledge are still unknown or controversial, or too complex to be encoded as subjective probabilities of few domain experts.
Reference: [3] <author> R G Cowel, A.P. Dawid, and P. Sebastiani. </author> <title> A comparison of sequential learning methods for incomplete data. </title> <booktitle> In Bayesian Statistics 5, </booktitle> <pages> pages 533-542. </pages> <publisher> Clarendon Press, Oxford, </publisher> <year> 1996. </year>
Reference-contexts: Exact analysis would require the computation of the joint posterior distribution of the parameters given each possible completion of the database, and then mix these over all possible completions. This is apparently infeasible. A deterministic method, proposed by [7] and further developed by <ref> [3] </ref>, provides a way to approximate the exact posterior distribution by processing data sequentially. An alternative approach is a stochastic approximation of the posterior distribution using for instance Markov Chain Monte Carlo (MCMC) methods, such as the Gibbs Sampling.
Reference: [4] <author> D. Heckerman, D. Geiger, </author> <title> and D.M. Chickering. Learning bayesian networks: The combinations of knowledge and statistical data. </title> <journal> Machine Learning, </journal> <volume> 20 </volume> <pages> 197-243, </pages> <year> 1995. </year>
Reference-contexts: Although in their original concept bbns were mainly designed to encode the knowledge of human experts, their statistical roots soon prompted for the development of methods to learn them directly from databases of cases rather than from the insight of human domain experts <ref> [2, 1, 4] </ref>. This choice can be extremely rewarding when the domain of applications generates large amounts of statistical information and aspects of the domain knowledge are still unknown or controversial, or too complex to be encoded as subjective probabilities of few domain experts.
Reference: [5] <author> M. Ramoni. </author> <title> Ignorant influence diagrams. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1808-1814, </pages> <address> S. Mateo, CA, 1995. </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: In this way, our learning algorithm returns probability intervals which account for the reliability of the information available in the database. These intervals can be then propagated using current techniques, such as <ref> [5] </ref>. An experimental comparison between our method and a stochastic method shows a remarkable difference in accuracy between the two methods and the computational advantages of our deterministic method with respect to the stochastic one.
Reference: [6] <author> M. Ramoni and P. Sebastiani. </author> <title> Robust learning with missing data. </title> <type> Technical Report KMi-TR-28, </type> <institution> Knowledge Media Institute, The Open University, </institution> <year> 1997. </year>
Reference-contexts: Such extreme estimates can be easily computed from the frequencies of incomplete cases in the database. Full details can be found in <ref> [6] </ref>, we report here only the main result. <p> Further experimental results, comparing the two methods when data are missing at random, using different network topologies and larger databases, are reported in <ref> [6] </ref>. 7 Robust Parameter Learning in Bayesian Networks with Missing Data percentage of entries in the database. Sample of 500 cases. 8 Robust Parameter Learning in Bayesian Networks with Missing Data 5. Conclusions This paper introduced a new method to learn conditional probabilities in a bbn from a database.
Reference: [7] <author> D.J. Spiegelhalter and S.L. Lauritzen. </author> <title> Sequential updating of conditional probabilities on directed graphical structures. </title> <journal> Networks, </journal> <volume> 20 </volume> <pages> 157-224, </pages> <year> 1990. </year>
Reference-contexts: Exact analysis would require the computation of the joint posterior distribution of the parameters given each possible completion of the database, and then mix these over all possible completions. This is apparently infeasible. A deterministic method, proposed by <ref> [7] </ref> and further developed by [3], provides a way to approximate the exact posterior distribution by processing data sequentially. An alternative approach is a stochastic approximation of the posterior distribution using for instance Markov Chain Monte Carlo (MCMC) methods, such as the Gibbs Sampling.
Reference: [8] <author> A Thomas, D J Spiegelhalter, and W R Gilks. </author> <title> Bugs: A program to perform bayesian inference using gibbs sampling. </title> <booktitle> In Bayesian Statistics 4, </booktitle> <pages> pages 837-42. </pages> <publisher> Clarendon Press, Oxford, </publisher> <year> 1992. </year> <month> 9 </month>
Reference-contexts: The focus of these experiments was mainly to compare the robustness of the two methods when data are systematically missing in the database. We compared an implementation of our method to the implementation of the Gibbs Sampling provided by the program BUGS <ref> [8] </ref>.
References-found: 8

