URL: http://www.cs.brown.edu/people/sjg/Thesis.ps
Refering-URL: http://www.cs.brown.edu/people/sjg/
Root-URL: http://www.cs.brown.edu
Title: Edge-Based Best-First Chart Parsing  
Author: Sharon Goldwater 
Date: May 1, 1998  
Abstract: Natural language grammars are often very large and full of ambiguities, making standard computer parsers too slow to be practical for many tasks. Best-first parsing attempts to address this problem by preferentially working to expand subparses that are judged "good" by some probabilistic figure of merit. We explain the standard non-probabilistic and best-first chart parsing paradigms, then describe a new method of best-first parsing which improves upon previous work by ranking subparses at a more fine-grained level, speeding up parsing by approximately a factor of 20 over the best previous results. Moreover, these results are achieved with a higher level of accuracy than is obtained by parsing to exhaustion.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Robert J. Bobrow. </author> <title> Statistical agenda parsing. </title> <booktitle> In DARPA Speech and Language Workshop, </booktitle> <pages> pages 222-224, </pages> <year> 1990. </year>
Reference-contexts: As such, we can build a parser which uses statistics to guide the search, thus speeding up the parsing process. In "Best-first" probabilistic parsing, introduced by Bobrow <ref> [1] </ref> and Chitrao and Grishman [5], partial parses are ranked according to some probabilistic figure of merit (FOM), and higher ranked subparses are given priority in the search| that is, the parser preferentially works to expand subparses with higher FOMs.
Reference: [2] <author> Sharon Caraballo and Eugene Charniak. </author> <title> New figures of merit for best-first probabilistic chart parsing. Computational Linguistics, </title> <publisher> Forthcoming. </publisher>
Reference-contexts: Naturally, the speed of a best-first parser and the correctness of its results depend on the FOM used to rank constituents. Several FOMs have been proposed for this purpose [12, 9, 10]. One of the more extensive comparisons is presented by Caraballo and Charniak <ref> [2] </ref> (referred to henceforth as C&C). Our work relies heavily on C&C's results, which are described in section 3. Recent work by Goodman [7] uses an FOM which is similar to C&C's, but probably more accurate.
Reference: [3] <author> Eugene Charniak. </author> <title> Statistical Language Learning. </title> <publisher> MIT Press, </publisher> <address> Cam-bridge, MA, </address> <year> 1993. </year>
Reference-contexts: This list is used when we wish to reconstruct the parse tree of the sentence. Each completed constituent also stores such a list. The basic chart parsing algorithm is shown in Figure 1. For a full simulation of the algorithm on an example sentence, see section 1.4 of <ref> [3] </ref>. 3 Loop until the agenda is empty: 1. Remove a constituent, A, from the agenda. Let i be A's type. 2.
Reference: [4] <author> Eugene Charniak. </author> <title> Statistical parsing with a context-free grammar and word statistics. </title> <booktitle> In Proceedings of the Fourteenth National Confer 15 ence on Artificial Intelligence, </booktitle> <pages> pages 598-603, </pages> <address> Menlo Park, 1997. </address> <publisher> AAAI Press/MIT Press. </publisher>
Reference-contexts: For example, Charniak's lexicalized parser <ref> [4] </ref> uses word-based statistics in addition to tag-based statistics, achieving accuracy levels in the mid 80% range.
Reference: [5] <author> Mahesh V. Chitrao and Ralph Grishman. </author> <title> Statistical parsing of messages. </title> <booktitle> In DARPA Speech and Language Workshop, </booktitle> <pages> pages 263-266, </pages> <year> 1990. </year>
Reference-contexts: As such, we can build a parser which uses statistics to guide the search, thus speeding up the parsing process. In "Best-first" probabilistic parsing, introduced by Bobrow [1] and Chitrao and Grishman <ref> [5] </ref>, partial parses are ranked according to some probabilistic figure of merit (FOM), and higher ranked subparses are given priority in the search| that is, the parser preferentially works to expand subparses with higher FOMs.
Reference: [6] <author> Joshua Goodman. </author> <title> Parsing algorithms and metrics. </title> <booktitle> In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 177-183, </pages> <year> 1996. </year>
Reference-contexts: Another possibility is that our FOM is, in effect, partially imitating Goodman's "Labeled Recall" parser <ref> [6] </ref>, which tries to maximize labeled bracket recall with the test set, rather than returning the maximum likelihood parse. In any case, probably the most remarkable feature of these results is the small number of edges needed to find a good parse.
Reference: [7] <author> Joshua Goodman. </author> <title> Global thresholding and multiple-pass parsing. </title> <booktitle> In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, </booktitle> <pages> pages 11-25, </pages> <year> 1997. </year>
Reference-contexts: Several FOMs have been proposed for this purpose [12, 9, 10]. One of the more extensive comparisons is presented by Caraballo and Charniak [2] (referred to henceforth as C&C). Our work relies heavily on C&C's results, which are described in section 3. Recent work by Goodman <ref> [7] </ref> uses an FOM which is similar to C&C's, but probably more accurate. <p> All sentences of length greater than 40 were ignored for testing purposes as done in both C&C and Goodman <ref> [7] </ref>. We applied the left-factoring technique described above to the grammar. The accuracy of our parser is reported in terms of precision and recall. <p> And with a small sacrifice in accuracy, we are able to find a parse with as few as three times the required number. The significance of these numbers becomes apparent when comparing our results to those achieved by Goodman <ref> [7] </ref> and C&C. It is difficult to fully compare Goodman's results to ours, since he measures processor time 13 Table 1: Edges vs.
Reference: [8] <author> John E. Hopcroft and Jeffrey D. Ullman. </author> <title> Introduction to Automata Theory, Languages and Computation. </title> <publisher> Addison-Wesley, </publisher> <year> 1979. </year>
Reference-contexts: its immediate left and right for constituents to combine with and put any new constituents formed on the agenda. (This is essentially the CKY parsing algorithm, modified to handle unary rules.) To obtain G 1 from G 0 , we apply the technique of left-factoring, introduced by Hopcroft and Ulman <ref> [8] </ref>. In the original grammar, we have rules of the form A ! B 0 : : : B n : q, where q is the probability of the production A ! B 0 : : : B n .
Reference: [9] <author> Fred Kochman and Joseph Kupin. </author> <title> Calculating the probability of a partial parse of a sentence. </title> <booktitle> In DARPA Speech and Language Workshop, </booktitle> <pages> pages 273-240, </pages> <year> 1991. </year>
Reference-contexts: Naturally, the speed of a best-first parser and the correctness of its results depend on the FOM used to rank constituents. Several FOMs have been proposed for this purpose <ref> [12, 9, 10] </ref>. One of the more extensive comparisons is presented by Caraballo and Charniak [2] (referred to henceforth as C&C). Our work relies heavily on C&C's results, which are described in section 3.
Reference: [10] <author> David M. Magerman and Mitchell P. Marcus. </author> <title> Parsing the voyager domain using pearl. </title> <booktitle> In DARPA Speech and Language Workshop, </booktitle> <pages> pages 231-236, </pages> <year> 1991. </year>
Reference-contexts: Naturally, the speed of a best-first parser and the correctness of its results depend on the FOM used to rank constituents. Several FOMs have been proposed for this purpose <ref> [12, 9, 10] </ref>. One of the more extensive comparisons is presented by Caraballo and Charniak [2] (referred to henceforth as C&C). Our work relies heavily on C&C's results, which are described in section 3.
Reference: [11] <author> Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. </author> <title> Building a large annotated corpus of english: The penn treebank. </title> <journal> Computational Linguistics, </journal> <volume> 19 </volume> <pages> 313-330, </pages> <year> 1993. </year>
Reference-contexts: on lower-level items, the parser can rule out bad constituents even before completing them, enabling it to concentrate on better constituents and reach a parse faster. 5 Experimental Results For our experiment, we used a tree-bank grammar induced from sections 2 - 21 of the Penn Wall Street Journal text <ref> [11] </ref>, with section 22 reserved for testing. All sentences of length greater than 40 were ignored for testing purposes as done in both C&C and Goodman [7]. We applied the left-factoring technique described above to the grammar. The accuracy of our parser is reported in terms of precision and recall.
Reference: [12] <author> Scott Miller and Heidi Fox. </author> <title> Automatic grammar acquisition. </title> <booktitle> In Proceedings of the Human Language Technology Workshop, </booktitle> <pages> pages 268-271, </pages> <year> 1994. </year>
Reference-contexts: Naturally, the speed of a best-first parser and the correctness of its results depend on the FOM used to rank constituents. Several FOMs have been proposed for this purpose <ref> [12, 9, 10] </ref>. One of the more extensive comparisons is presented by Caraballo and Charniak [2] (referred to henceforth as C&C). Our work relies heavily on C&C's results, which are described in section 3.
References-found: 12

