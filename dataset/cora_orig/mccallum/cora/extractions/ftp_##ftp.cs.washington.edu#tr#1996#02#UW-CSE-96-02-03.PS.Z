URL: ftp://ftp.cs.washington.edu/tr/1996/02/UW-CSE-96-02-03.PS.Z
Refering-URL: http://www.cs.washington.edu/research/tr/tr-by-title.html
Root-URL: 
Title: The Influence of Caches on the Performance of Heaps  
Author: Anthony LaMarca Richard E. Ladner 
Keyword: Jones's.  
Date: February 29, 1996  
Address: Seattle, WA 98195  
Affiliation: Department of Computer Science and Engineering University of Washington  
Pubnum: Technical Report UW-CSE-96-02-03  
Abstract: As cycle times grow relative to memory speeds, the cache performance of algorithms has an increasingly large impact on overall performance. Unfortunately, most commonly used algorithms were not designed with cache performance in mind. This paper investigates the cache performance of implicit heaps. We present optimizations which significantly reduce the cache misses that heaps incur and improve their overall performance. We present an analytical model called collective analysis that allows cache performance to be predicted as a function of both cache configuration and algorithm configuration. As part of our investigation, we perform an analysis of the cache performance of both traditional heaps and our improved heaps in our model. In addition empirical data is given for five architectures to show the impact our optimizations have on overall performance. We also revisit a priority queue study originally performed by Jones [21]. Due to the increases in cache miss penalties, the relative performance result we obtain on today's machine differ greatly from the machines of only ten years ago. We compare the performance of implicit heaps, skew heaps and splay trees and discuss the difference between our results and 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, M. Horowitz, and J. Hennessy. </author> <title> An analytical cache model. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7:2:184-215, </volume> <year> 1989. </year>
Reference-contexts: An analytical cache model also has the inherent advantage that it helps a designer understand the algorithm and helps uncover possible optimizations. A number of researchers have employed hybrid modeling techniques in which a combination of trace-driven simulation and analytical models is used <ref> [1, 29] </ref>. These techniques compress an address trace into a few key parameters describing an application's behavior, and these are then used to drive an analytical cache model. The difference between the collective analysis framework we present and these techniques is that collective analysis does not involve any trace data.
Reference: [2] <author> J. Anderson and M. Lam. </author> <title> Global optimizations for parallelism and locality on scalable parallel machines. </title> <booktitle> In Proceedings of the 1993 ACM Symposium on Programming Languages Design and Implementation, </booktitle> <pages> pages 112-125. </pages> <publisher> ACM, </publisher> <year> 1993. </year>
Reference-contexts: In collective analysis, we make assumptions similar to Rao's about the distribution of references within cache regions. The result is that our formulas for cache performance are very similar to Rao's. The compiler community has produced many optimizations for improving the cache locality of code <ref> [5, 2, 37] </ref> and algorithms for deciding when these optimizations should be applied [22, 4, 16]. The vast majority of these compiler optimizations focus on loop nests and offer little improvement to pointer-based structures and algorithms whose reference patterns are difficult to predict.
Reference: [3] <author> S. Carlsson. </author> <title> An optimal algorithm for deleting the root of a heap. </title> <journal> Information Processing Letters, </journal> <volume> 37:2:117-120, </volume> <year> 1991. </year>
Reference-contexts: The reference patterns of the priority queue algorithms we examine in the paper are complicated enough that these compiler optimizations offer little benefit. Priority queues and heaps in particular have been well studied. Improved algorithms have been developed for building, adding and removing from heaps <ref> [23, 9, 3, 17, 15] </ref>. A number of pointer-based, self-balancing priority queues, including splay trees and skew heaps [30], have been developed since the heap was introduced. <p> In this paper, we do not consider the implication nor the optimization of other priority queue operations such as reduce-min or the merging of two priority queues. Heap have been well studied and there are numerous extensions and more sophisticated algorithms for adding and removing <ref> [23, 9, 3, 17, 15] </ref>. 2.1 The Memory System While machines have varying memory system configurations, almost all share some characteristics. Most new machines have a multi-level cache architecture where the cache closest to memory is typically direct-mapped and has a high miss penalty [18].
Reference: [4] <author> S. Carr, K. McKinley, and C. W. Tseng. </author> <title> Compiler optimizations for improving data locality. </title> <booktitle> In Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 252-262, </pages> <year> 1994. </year>
Reference-contexts: The result is that our formulas for cache performance are very similar to Rao's. The compiler community has produced many optimizations for improving the cache locality of code [5, 2, 37] and algorithms for deciding when these optimizations should be applied <ref> [22, 4, 16] </ref>. The vast majority of these compiler optimizations focus on loop nests and offer little improvement to pointer-based structures and algorithms whose reference patterns are difficult to predict.
Reference: [5] <author> M. Cierniak and Wei Li. </author> <title> Unifying data and control transformations for distributed shared-memory machines. </title> <booktitle> In Proceedings of the 1995 ACM Symposium on Programming Languages Design and Implementation, </booktitle> <pages> pages 205-217. </pages> <publisher> ACM, </publisher> <year> 1995. </year>
Reference-contexts: In collective analysis, we make assumptions similar to Rao's about the distribution of references within cache regions. The result is that our formulas for cache performance are very similar to Rao's. The compiler community has produced many optimizations for improving the cache locality of code <ref> [5, 2, 37] </ref> and algorithms for deciding when these optimizations should be applied [22, 4, 16]. The vast majority of these compiler optimizations focus on loop nests and offer little improvement to pointer-based structures and algorithms whose reference patterns are difficult to predict.
Reference: [6] <author> D. Clark. </author> <title> Cache performance of the VAX-11/780. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 1:1:24-37, </volume> <year> 1983. </year>
Reference-contexts: 1 Introduction The time to service a cache miss to memory has grown from 6 cycles for the Vax 11/780 to 120 for the AlphaServer 8400 <ref> [6, 14] </ref>. Cache miss penalties have grown to the point where good overall performance cannot be achieved without good cache performance. Unfortunately, many fundamental algorithms were developed without considering caching. In this paper, we perform a study of the cache performance of heaps [36].
Reference: [7] <author> E. Coffman and P. Denning. </author> <title> Operating Systems Theory. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1973. </year>
Reference-contexts: Systems with similar features include include Cprof [24] and SHMAP [12]. A study by Rao [27] examines the performance of page replacement policies using the inde 3 pendent reference model <ref> [7] </ref> to predict miss behavior. In collective analysis, we make assumptions similar to Rao's about the distribution of references within cache regions. The result is that our formulas for cache performance are very similar to Rao's. <p> Collectively, all of the processes represent the accesses to the entire virtual address space and hence represent the system's overall memory behavior. For purposes of the analysis we assume that the references to memory satisfy the independent reference assumption <ref> [7] </ref>. In this model each access is independent of all previous accesses, that is, the system is memoryless. Algorithms which exhibit very regular access patterns such as sequential traversals will not be accurately modeled in our framework because we make the independent reference assumption.
Reference: [8] <author> T. Cormen, C. Leiserson, and R. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> The MIT Press, </publisher> <address> Cam-bridge, MA, </address> <year> 1990. </year>
Reference-contexts: While execution time is the final measure of performance, it is too coarse-grained a metric to use in the intermediate stages of analysis and optimization of algorithms. The majority of researchers in the algorithm community compare algorithm performance using analyses in a unit-cost model. The RAM model <ref> [8] </ref> is used most commonly, and in this abstract architecture all basic operations, including reads and writes to memory, have unit cost. Unit-cost models have the advantage that they are simple, easy to use and produce results that are easily compared. <p> The general consensus is that, due to its low operation count and linear worst case behavior, Floyd's method is the preferred algorithm for building a heap from a set of keys <ref> [28, 34, 8] </ref>. In our experiments, we executed both build-heap algorithms on a set of uniformly distributed keys. As the literature suggests, Floyd's method executes far fewer instructions per key than does Repeated-Adds.
Reference: [9] <author> J. De Graffe and W. Kosters. </author> <title> Expected heights in heaps. BIT, </title> <address> 32:4:570-579, </address> <year> 1992. </year>
Reference-contexts: The reference patterns of the priority queue algorithms we examine in the paper are complicated enough that these compiler optimizations offer little benefit. Priority queues and heaps in particular have been well studied. Improved algorithms have been developed for building, adding and removing from heaps <ref> [23, 9, 3, 17, 15] </ref>. A number of pointer-based, self-balancing priority queues, including splay trees and skew heaps [30], have been developed since the heap was introduced. <p> In this paper, we do not consider the implication nor the optimization of other priority queue operations such as reduce-min or the merging of two priority queues. Heap have been well studied and there are numerous extensions and more sophisticated algorithms for adding and removing <ref> [23, 9, 3, 17, 15] </ref>. 2.1 The Memory System While machines have varying memory system configurations, almost all share some characteristics. Most new machines have a multi-level cache architecture where the cache closest to memory is typically direct-mapped and has a high miss penalty [18].
Reference: [10] <author> E. Doberkat. </author> <title> Inserting a new element into a heap. </title> <journal> BIT, </journal> <volume> 21 </volume> <pages> 225-269, </pages> <year> 1981. </year>
Reference-contexts: Doberkat <ref> [10] </ref> showed that independent of N , if the keys are uniformly distributed, add operations percolate the new item up only 1.6 levels on average. Doberkat [11] also studied the cost of the remove-min operation on a heap chosen at random from the set of legal heaps.
Reference: [11] <author> E. Doberkat. </author> <title> Deleting the root of a heap. </title> <journal> Acta Informatica, </journal> <volume> 17 </volume> <pages> 245-265, </pages> <year> 1982. </year>
Reference-contexts: Doberkat [10] showed that independent of N , if the keys are uniformly distributed, add operations percolate the new item up only 1.6 levels on average. Doberkat <ref> [11] </ref> also studied the cost of the remove-min operation on a heap chosen at random from the set of legal heaps. He showed that after remove-min swaps the last element to the root, the swapped element is percolated down more that (depth 1) levels on average.
Reference: [12] <author> J. Dongarra, O. Brewer, J. Kohl, and S. Fineberg. </author> <title> A tool to aid in the design, implementation, and understanding of matrix algorithms for parallel processors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 9:2:185-202, </volume> <month> June </month> <year> 1990. </year>
Reference-contexts: The Memspy system they developed allows the programmer to see a breakdown of cache misses by the four C's [19] for each data structure. Systems with similar features include include Cprof [24] and SHMAP <ref> [12] </ref>. A study by Rao [27] examines the performance of page replacement policies using the inde 3 pendent reference model [7] to predict miss behavior. In collective analysis, we make assumptions similar to Rao's about the distribution of references within cache regions.
Reference: [13] <author> M. Farrens, G. Tyson, and A. Pleszkun. </author> <title> A study of single-chip processor/cache organizations for large numbers of transistors. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 338-347, </pages> <year> 1994. </year>
Reference-contexts: Most cache performance analysis is currently done with hardware monitoring [33] or with trace-driven simulation <ref> [13, 35] </ref>. Neither of these solutions offers the benefits of an analytical cache model, namely the ability to quickly obtain estimates of cache performance for varying cache and algorithm configurations.
Reference: [14] <author> D. Fenwick, D. Foley, W. Gist, S. VanDoren, and D. Wissell. </author> <title> The AlphaServer 8000 series: High-end server platform development. </title> <journal> Digital Technical Journal, </journal> <volume> 7:1:43-65, </volume> <year> 1995. </year>
Reference-contexts: 1 Introduction The time to service a cache miss to memory has grown from 6 cycles for the Vax 11/780 to 120 for the AlphaServer 8400 <ref> [6, 14] </ref>. Cache miss penalties have grown to the point where good overall performance cannot be achieved without good cache performance. Unfortunately, many fundamental algorithms were developed without considering caching. In this paper, we perform a study of the cache performance of heaps [36]. <p> Most new machines have a multi-level cache architecture where the cache closest to memory is typically direct-mapped and has a high miss penalty [18]. The third level cache in the DEC AlphaServer 8400, for example, has a four megabyte capacity and and a miss penalty of 120 cycles <ref> [14] </ref>. The optimizations we present in this paper are intended to improve the performance of the data references of heaps in large caches with low degrees of associativity. 2.2 Why Look at Cache Performance? Traditional algorithm design and analysis has, for the most part, ignored caches.
Reference: [15] <author> Robert W. Floyd. </author> <title> Treesort 3. </title> <journal> Communications of the ACM, </journal> <volume> 7:12:701, </volume> <year> 1964. </year> <month> 27 </month>
Reference-contexts: The reference patterns of the priority queue algorithms we examine in the paper are complicated enough that these compiler optimizations offer little benefit. Priority queues and heaps in particular have been well studied. Improved algorithms have been developed for building, adding and removing from heaps <ref> [23, 9, 3, 17, 15] </ref>. A number of pointer-based, self-balancing priority queues, including splay trees and skew heaps [30], have been developed since the heap was introduced. <p> In this paper, we do not consider the implication nor the optimization of other priority queue operations such as reduce-min or the merging of two priority queues. Heap have been well studied and there are numerous extensions and more sophisticated algorithms for adding and removing <ref> [23, 9, 3, 17, 15] </ref>. 2.1 The Memory System While machines have varying memory system configurations, almost all share some characteristics. Most new machines have a multi-level cache architecture where the cache closest to memory is typically direct-mapped and has a high miss penalty [18]. <p> The first algorithm is the obvious and naive way, namely to start with an empty heap and repeatedly perform adds until the heap is built. We call this the Repeated-Adds algorithm. The second algorithm for building a heap is due to Floyd <ref> [15] </ref> and builds a heap in fewer instructions than Repeated-Adds. Floyd's method begins by treating the array of unordered keys as if it were a heap. It then starts half way into the heap and re-heapifies subtrees from the middle up until the entire heap is valid.
Reference: [16] <author> D. Gannon, W. Jalby, and K. Gallivan. </author> <title> Strategies for cache and local memory management by global program transformation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5:5:587-616, </volume> <month> Oct </month> <year> 1988. </year>
Reference-contexts: The result is that our formulas for cache performance are very similar to Rao's. The compiler community has produced many optimizations for improving the cache locality of code [5, 2, 37] and algorithms for deciding when these optimizations should be applied <ref> [22, 4, 16] </ref>. The vast majority of these compiler optimizations focus on loop nests and offer little improvement to pointer-based structures and algorithms whose reference patterns are difficult to predict.
Reference: [17] <author> G. Gonnet and J. Munro. </author> <title> Heaps on heaps. </title> <journal> SIAM Journal of Computing, </journal> <volume> 15:4:964-971, </volume> <year> 1986. </year>
Reference-contexts: The reference patterns of the priority queue algorithms we examine in the paper are complicated enough that these compiler optimizations offer little benefit. Priority queues and heaps in particular have been well studied. Improved algorithms have been developed for building, adding and removing from heaps <ref> [23, 9, 3, 17, 15] </ref>. A number of pointer-based, self-balancing priority queues, including splay trees and skew heaps [30], have been developed since the heap was introduced. <p> In this paper, we do not consider the implication nor the optimization of other priority queue operations such as reduce-min or the merging of two priority queues. Heap have been well studied and there are numerous extensions and more sophisticated algorithms for adding and removing <ref> [23, 9, 3, 17, 15] </ref>. 2.1 The Memory System While machines have varying memory system configurations, almost all share some characteristics. Most new machines have a multi-level cache architecture where the cache closest to memory is typically direct-mapped and has a high miss penalty [18].
Reference: [18] <author> J. Hennesey and D. Patterson. </author> <title> Computer Architecture A Quantitative Approach. </title> <publisher> Morgan Kaufman Publishers, Inc., </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: Most new machines have a multi-level cache architecture where the cache closest to memory is typically direct-mapped and has a high miss penalty <ref> [18] </ref>. The third level cache in the DEC AlphaServer 8400, for example, has a four megabyte capacity and and a miss penalty of 120 cycles [14]. <p> In our model, the cache has a block size of B bytes, where B C and is also a power of two. In order to simplify analysis, we only model direct mapped caches <ref> [18] </ref>, and in our model we do not distinguish reads from writes. We assume that items that are contiguous in the virtual address space map to contiguous cache locations, which means that we are modeling a virtually indexed cache [18]. <p> In order to simplify analysis, we only model direct mapped caches <ref> [18] </ref>, and in our model we do not distinguish reads from writes. We assume that items that are contiguous in the virtual address space map to contiguous cache locations, which means that we are modeling a virtually indexed cache [18]. Our model does not include a TLB, nor does it attempt to capture page faults due to physical memory limitations. 3.1.2 Applying the Model The goal of collective analysis is to approximate the memory behavior of an algorithm and predict its cache performance characteristics from this approximation. <p> In our analysis, we ignore this effect and assume that neighboring sets of siblings are never faulted in. This assumption also causes misses due to false sharing to be overlooked <ref> [18] </ref>. The basic structure of our decomposition is to create one or more processes for each level in the heap. Given a total of N elements, there are t = log d ((d 1)N + 1) levels in the heap.
Reference: [19] <author> M. Hill and A. Smith. </author> <title> Evaluating associativity in CPU caches. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 38:12:1612-1630, </volume> <year> 1989. </year>
Reference-contexts: Anderson et al augment trace-driven cache simulation by categorizing the cache misses by the type of miss and by the name of the data structure incurring the miss [25]. The Memspy system they developed allows the programmer to see a breakdown of cache misses by the four C's <ref> [19] </ref> for each data structure. Systems with similar features include include Cprof [24] and SHMAP [12]. A study by Rao [27] examines the performance of page replacement policies using the inde 3 pendent reference model [7] to predict miss behavior.
Reference: [20] <author> D. B. Johnson. </author> <title> Priority queues with update and finding minimum spanning trees. </title> <journal> Information Processing Letters, </journal> <volume> 4, </volume> <year> 1975. </year>
Reference-contexts: Increasing the fanout of the heap so that a set of siblings fills a cache block eliminates this waste. The remove-min operation will no longer load elements into the cache and not look at them. A d-heap is the generalization of a heap with fanout d rather than two <ref> [20] </ref>. Figure 5 shows a 4-heap and the way it lays in the cache when four elements fit per cache block. Unlike our previous optimization, this change will definitely have an impact on the dynamic instruction count of our heap operations.
Reference: [21] <author> D. Jones. </author> <title> An emperical comparison of priority-queue and event-set implementations. </title> <journal> Communications of the ACM, </journal> <volume> 29:4:300-311, </volume> <year> 1986. </year>
Reference-contexts: We develop an improved implicit heap which has better memory system and overall performance. In our experiments, our improved heap incurs as much as 65% fewer cache misses running in the hold 1 model <ref> [21] </ref>. The reduction in cache misses translates to speedups as high as 75% for a heap running in the hold model, and performing heapsort with our optimized heaps improved performance by as much as a factor of two. <p> In this paper we also reproduce a subset of the experiments performed by Jones comparing the performance of various priority queues <ref> [21] </ref>. In our experiments, we compare the performance of traditional heaps, our improved heaps, top-down skew heaps and both top-down and bottom-up splay trees. Our comparison of priority queues yielded very different results from Jones's original experiments. <p> This different approach raises opportunities not previously considered and allows us to improve performance beyond existing results. Evaluations of priority queue performance include Jones's study using various architectures and data distributions <ref> [21] </ref>, and a study by Naor et al which examines instruction counts as well as page fault behavior [26]. 1.3 Overview In Section 2 we describe the heap we use as the base of our study. <p> In Section 4 we present data from trace driven simulations and executions on various architectures to show the impact of our optimizations on both cache performance and overall performance of heaps. In Section 5 we revisit Jones's priority queue study <ref> [21] </ref>. We compare the performance of a traditional heap, an improved heap, skew heaps and splay trees in the hold model using exponentially distributed keys. <p> Heaps are often used in discrete event simulations as a priority queue to store the events. In order to measure the performance of heaps operating as an event queue, we analyze our heaps in the hold model <ref> [21] </ref>. In the hold model, the heap is initially seeded with some number of keys. <p> Our experiments are intended to be a reproduction of a subset of the priority queue study comparison performed by Jones in 1986 <ref> [21] </ref>. The results of Jones's experiments indicated that for the architecture of that time, pointer-based, self-balancing priority queues such as splay trees and skew heaps performed better than the simple implicit heaps. <p> In our framework, we performed a cache analysis of d-heaps, and our performance predictions closely match the results of a trace-driven simulation. Finally, we reproduced a subset of Jones's experiments <ref> [21] </ref> examining the performance of a number of priority queues. Our experiments showed that the low memory overhead of implicit heaps makes them an excellent choice as a priority queue, somewhat contradicting Jones's results.
Reference: [22] <author> K. Kennedy and K. McKinley. </author> <title> Optimizing for parallelism and data locality. </title> <booktitle> In Proceedings of the 1992 International Conference on Supercomputing, </booktitle> <pages> pages 323-334, </pages> <year> 1992. </year>
Reference-contexts: The result is that our formulas for cache performance are very similar to Rao's. The compiler community has produced many optimizations for improving the cache locality of code [5, 2, 37] and algorithms for deciding when these optimizations should be applied <ref> [22, 4, 16] </ref>. The vast majority of these compiler optimizations focus on loop nests and offer little improvement to pointer-based structures and algorithms whose reference patterns are difficult to predict.
Reference: [23] <author> D. E. Knuth. </author> <title> The Art of Computer Programming, vol III Sorting and Searching. </title> <address> Addison-Wesely, Reading, MA, </address> <year> 1973. </year>
Reference-contexts: The reference patterns of the priority queue algorithms we examine in the paper are complicated enough that these compiler optimizations offer little benefit. Priority queues and heaps in particular have been well studied. Improved algorithms have been developed for building, adding and removing from heaps <ref> [23, 9, 3, 17, 15] </ref>. A number of pointer-based, self-balancing priority queues, including splay trees and skew heaps [30], have been developed since the heap was introduced. <p> In this paper, we do not consider the implication nor the optimization of other priority queue operations such as reduce-min or the merging of two priority queues. Heap have been well studied and there are numerous extensions and more sophisticated algorithms for adding and removing <ref> [23, 9, 3, 17, 15] </ref>. 2.1 The Memory System While machines have varying memory system configurations, almost all share some characteristics. Most new machines have a multi-level cache architecture where the cache closest to memory is typically direct-mapped and has a high miss penalty [18]. <p> The cache should be divided into regions in such a way that the accesses to a 1 The only reference we found that proposed larger fanout heaps due to their reduced operation cost was an exercise by Knuth <ref> [23, Ex. 28 Pg. 158] </ref>. 10 particular region are uniformly distributed across that region. If the accesses are not uniformly distributed, the region should be subdivided into multiple regions.
Reference: [24] <author> A. Lebeck and D. Wood. </author> <title> Cache profiling and the spec benchmarks: a case study. </title> <booktitle> Computer, </booktitle> <address> 27:10:15-26, </address> <month> Oct </month> <year> 1994. </year>
Reference-contexts: The Memspy system they developed allows the programmer to see a breakdown of cache misses by the four C's [19] for each data structure. Systems with similar features include include Cprof <ref> [24] </ref> and SHMAP [12]. A study by Rao [27] examines the performance of page replacement policies using the inde 3 pendent reference model [7] to predict miss behavior. In collective analysis, we make assumptions similar to Rao's about the distribution of references within cache regions.
Reference: [25] <author> M. Martonosi, A. Gupta, and T. Anderson. Memspy: </author> <title> analyzing memory system bottlenecks in programs. </title> <booktitle> In Proceedings of the 1992 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 1-12, </pages> <year> 1992. </year>
Reference-contexts: Anderson et al augment trace-driven cache simulation by categorizing the cache misses by the type of miss and by the name of the data structure incurring the miss <ref> [25] </ref>. The Memspy system they developed allows the programmer to see a breakdown of cache misses by the four C's [19] for each data structure. Systems with similar features include include Cprof [24] and SHMAP [12].
Reference: [26] <author> D. Naor, C. Martel, and N. Matloff. </author> <title> Performance of priority queue structures in a virtual memory environment. </title> <journal> Computer Journal, </journal> <volume> 34:5:428-437, </volume> <month> Oct </month> <year> 1991. </year>
Reference-contexts: This different approach raises opportunities not previously considered and allows us to improve performance beyond existing results. Evaluations of priority queue performance include Jones's study using various architectures and data distributions [21], and a study by Naor et al which examines instruction counts as well as page fault behavior <ref> [26] </ref>. 1.3 Overview In Section 2 we describe the heap we use as the base of our study. We describe a typical memory system and present two optimizations of heaps for this typical memory system. In Section 3 we present collective analysis, a framework for predicting cache performance of algorithms.
Reference: [27] <author> G. Rao. </author> <title> Performance analysis of cache memories. </title> <journal> Journal of the ACM, </journal> <volume> 25:3:378-395, </volume> <year> 1978. </year>
Reference-contexts: The Memspy system they developed allows the programmer to see a breakdown of cache misses by the four C's [19] for each data structure. Systems with similar features include include Cprof [24] and SHMAP [12]. A study by Rao <ref> [27] </ref> examines the performance of page replacement policies using the inde 3 pendent reference model [7] to predict miss behavior. In collective analysis, we make assumptions similar to Rao's about the distribution of references within cache regions. <p> An access is a miss if it is not a hit. The following theorem is a reformulation of the results of Rao <ref> [27] </ref>. Our formulation differs from Rao's only in that we group together blocks into a region if the accesses are uniformly distributed in the region. This simplifies the analysis considerably.
Reference: [28] <author> R. Sedgewick. </author> <title> Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1988. </year>
Reference-contexts: The general consensus is that, due to its low operation count and linear worst case behavior, Floyd's method is the preferred algorithm for building a heap from a set of keys <ref> [28, 34, 8] </ref>. In our experiments, we executed both build-heap algorithms on a set of uniformly distributed keys. As the literature suggests, Floyd's method executes far fewer instructions per key than does Repeated-Adds.
Reference: [29] <author> J.P. Singh, H.S. Stone, and D.F. Thiebaut. </author> <title> A model of workloads and its use in miss-rate prediction for fully associative caches. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41:7:811-825, </volume> <year> 1992. </year>
Reference-contexts: An analytical cache model also has the inherent advantage that it helps a designer understand the algorithm and helps uncover possible optimizations. A number of researchers have employed hybrid modeling techniques in which a combination of trace-driven simulation and analytical models is used <ref> [1, 29] </ref>. These techniques compress an address trace into a few key parameters describing an application's behavior, and these are then used to drive an analytical cache model. The difference between the collective analysis framework we present and these techniques is that collective analysis does not involve any trace data.
Reference: [30] <author> D. Sleator and R. Tarjan. </author> <title> Self-adjusting binary search trees. </title> <journal> Journal of the ACM, </journal> <volume> 32:3:652-686, </volume> <year> 1985. </year>
Reference-contexts: Priority queues and heaps in particular have been well studied. Improved algorithms have been developed for building, adding and removing from heaps [23, 9, 3, 17, 15]. A number of pointer-based, self-balancing priority queues, including splay trees and skew heaps <ref> [30] </ref>, have been developed since the heap was introduced. The main difference between the work in this paper and previous work is that we focus on cache performance as well as instruction cost. This different approach raises opportunities not previously considered and allows us to improve performance beyond existing results. <p> The implementations of the skew heap and the bottom-up splay tree were taken from an archive of the code used in Jones's study. The top-down splay tree implementation is an adoption of Sleator and Tarjan's code <ref> [30] </ref>. As Jones did in his study, the queues were run in the hold model, and no work was performed between the remove-min and the add of each iteration (w = 0). In our experiments, a queue element consisted of an 8 byte key and no data.
Reference: [31] <author> Amitabh Srivastava and Alan Eustace. </author> <title> ATOM: A system for building customized program analysis tools. </title> <booktitle> In Proceedings of the 1994 ACM Symposium on Programming Languages Design and Implementation, </booktitle> <pages> pages 196-205. </pages> <publisher> ACM, </publisher> <year> 1994. </year> <month> 28 </month>
Reference-contexts: B ) An expression for can be derived by substituting these intensities into Equation 1 and reducing. 3.4 Validation In our study both our dynamic instruction counts and out cache simulation results were measured using Atom <ref> [31] </ref>. Atom is a toolkit developed by DEC for instrumenting codes on Alpha workstations. Dynamic instruction counts were obtained by inserting an increment to an instruction 15 counter after each instruction executed by the algorithm. <p> Our executions were run on a DEC Alphastation 250. Dynamic instruction counts and cache performance data was collected using Atom <ref> [31] </ref> (see Section 3.4). <p> The queue was then measured for 200,000 iterations. As before, executions were run on a DEC Alphastation 250. Dynamic instruction counts and cache performance data was collected using Atom <ref> [31] </ref> (see Section 3.4). Cache simulations were configured for a two megabyte direct-mapped cache and a 32 byte block size.
Reference: [32] <author> O. Temam, C. Fricker, and W. Jalby. </author> <title> Cache interference phenomena. </title> <booktitle> In Proceedings of the 1994 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 261-271, </pages> <year> 1994. </year>
Reference-contexts: The behavior of an algorithm is explicitly stated, and this serves as input to our model. While this makes the analysis more difficult to perform, it offers the advantage that our model can provide fast predictions for a variety of algorithm configurations and cache configurations. Temam, Fricker and Jalby <ref> [32] </ref> provide a purely analytical approach for predicting conflict misses in loop nests of numerical codes. In their model, the memory reference patterns are examined, and cache performance is predicted by determining when each data item is reused and how often this reuse is disrupted.
Reference: [33] <author> R. Uhlig, D. Nagle, T. Stanley, T. Mudge, S. Sechrest, and R. Brown. </author> <title> Design tradeoffs for software-managed TLBs. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 12:3:175-205, </volume> <year> 1994. </year>
Reference-contexts: When both analytical predictions and simulation results are available, we compare them to validate the accuracy of the predictions. 1.2 Related Work There has been a large amount of research on analyzing and improving cache performance. Most cache performance analysis is currently done with hardware monitoring <ref> [33] </ref> or with trace-driven simulation [13, 35]. Neither of these solutions offers the benefits of an analytical cache model, namely the ability to quickly obtain estimates of cache performance for varying cache and algorithm configurations.
Reference: [34] <author> M. Weiss. </author> <title> Data structures and algorithm analysis. </title> <publisher> Benjamin/Cummings Pub. Co., </publisher> <address> Redwood City, CA, </address> <year> 1995. </year>
Reference-contexts: The general consensus is that, due to its low operation count and linear worst case behavior, Floyd's method is the preferred algorithm for building a heap from a set of keys <ref> [28, 34, 8] </ref>. In our experiments, we executed both build-heap algorithms on a set of uniformly distributed keys. As the literature suggests, Floyd's method executes far fewer instructions per key than does Repeated-Adds.
Reference: [35] <author> H. Wen and J. L. Baer. </author> <title> Efficient trace-driven simulation methods for cache performance analysis. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9:3:222-241, </volume> <year> 1991. </year>
Reference-contexts: Most cache performance analysis is currently done with hardware monitoring [33] or with trace-driven simulation <ref> [13, 35] </ref>. Neither of these solutions offers the benefits of an analytical cache model, namely the ability to quickly obtain estimates of cache performance for varying cache and algorithm configurations.
Reference: [36] <author> J. W. Williams. </author> <title> Heapsort. </title> <journal> Communications of the ACM, </journal> <volume> 7:6:347-348, </volume> <year> 1964. </year>
Reference-contexts: Cache miss penalties have grown to the point where good overall performance cannot be achieved without good cache performance. Unfortunately, many fundamental algorithms were developed without considering caching. In this paper, we perform a study of the cache performance of heaps <ref> [36] </ref>. The main goal of our study is to understand and improve the memory system performance of heaps using both experimental and analytical tools. We collect experimental data from execution times on various machines and from instruction counts and cache miss ratios from trace driven simulations. <p> The 8-heap executes more instructions that the 4-heap and consequently executes slower initially. Eventually, however, the reduction in cache misses overcomes the difference in instruction count, and the 8-heap performs best for large data sets. 4.2 Heapsort We next examine the effects our improvements have on heapsort <ref> [36] </ref>. We compare heapsort built from traditional heaps and our aligned d-heaps. The traditional heaps were built using Floyd's method.
Reference: [37] <author> M. Wolf and M. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Proceedings of the 1991 ACM Symposium on Programming Languages Design and Implementation, </booktitle> <pages> pages 30-44. </pages> <publisher> ACM, </publisher> <year> 1991. </year> <month> 29 </month>
Reference-contexts: In collective analysis, we make assumptions similar to Rao's about the distribution of references within cache regions. The result is that our formulas for cache performance are very similar to Rao's. The compiler community has produced many optimizations for improving the cache locality of code <ref> [5, 2, 37] </ref> and algorithms for deciding when these optimizations should be applied [22, 4, 16]. The vast majority of these compiler optimizations focus on loop nests and offer little improvement to pointer-based structures and algorithms whose reference patterns are difficult to predict.
References-found: 37

