URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3851/3851.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Title: SYMMETRIC CAUCHY-LIKE PRECONDITIONERS FOR THE REGULARIZED SOLUTION OF 1-D ILL-POSED PROBLEMS  
Author: MISHA E. KILMER 
Keyword: Key words. Regularization, ill-posed problems, Toeplitz, Cauchy-like, preconditioner, conjugate gradient, minimal residual, normal equations, image processing, deblurring  
Date: December 4, 1997  
Note: AMS(MOS) subject classifications. 65R20, 45L10, 94A12  
Abstract: The discretization of integral equations can lead to systems involving symmetric Toeplitz matrices. We describe a preconditioning technique for the regularized solution of the related discrete ill-posed problem. We use discrete sine transforms to transform the system to one involving a Cauchy-like matrix. Based on the approach of Kilmer and O'Leary, the preconditioner is a symmetric, rank m fl approximation to the Cauchy-like matrix augmented by the identity. We shall show that if the kernel of the integral equation is smooth then the preconditioned matrix has two desirable properties; namely, the largest m fl magnitude eigenvalues are clustered around and bounded below by one, and that small magnitude eigenvalues remain small. We also show that the initialization cost is less than the initialization cost for the preconditioner introduced by Kilmer and O'Leary. Further, we describe a method for applying the preconditioner in O((n + 1) lg(n + 1)) operations when n+1 is a power of 2, and describe a variant of the MINRES algorithm to solve the symmetrically preconditioned problem. The preconditioned method is tested on two examples. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. F. Chan and P. C. Hansen, </author> <title> A look-ahead Levinson algorithm for general Toeplitz systems, </title> <journal> IEEE Trans. Signal Processing, </journal> <volume> 40 (1992), </volume> <pages> pp. 1079-1090. </pages>
Reference-contexts: However, when the necessary pivoting is incorporated into the fast or super-fast LDU factorization algorithms for Toeplitz matrices, these methods can become too expensive for our purposes, often requiring O (n 3 ) operations to factor an n fi n Toeplitz matrix <ref> [18, 6, 1] </ref>. To circumvent this problem, in [16] we advocated transforming T to a Cauchy-like matrix C using a particular unitary transformation. However, if T was symmetric, C in that case was no longer symmetric.
Reference: [2] <author> T. Finck, G. Heinig, and K. Rost, </author> <title> An inversion formula and fast algorithms for Cauchy-Vandermonde matrices, </title> <journal> Linear Algebra Appl., </journal> <volume> 183 (1993), </volume> <pages> pp. 179-191. </pages>
Reference-contexts: The next property gives some insight into how matrix-vector multiplications might be computed. Property 2. Let C 0 be the Cauchy matrix (C 0 ) ij = ! i ! j 0 i = j Following <ref> [2] </ref>, we observe C = ( i=1 diag ( ~ A (i) )C 0 diag ( ~ B (i) ) + diag (c);(4) where the superscript on ~ A and ~ B denotes the ith column of the generators, c denotes the vector with components c i , and diag ()
Reference: [3] <author> A. Gerasoulis, </author> <title> A fast algorithm for generalized Hilbert matrices, </title> <journal> Math. Comput., </journal> <volume> 50 (1988), </volume> <pages> pp. 179-188. </pages>
Reference-contexts: Let the polynomials h (x) and s (x) be defined according to h (x) = i=1 x ! i Note that h (! i ) = u i s 0 (! i ). Now Gerasoulis in <ref> [3] </ref> shows that the jth component of C 0 u can be determined through an appropriate evaluation of polynomials: z j = (h 0 (! j ) 2 Now s (x) = i=1 (x ! i ). <p> Thus, s (x) can be written in terms of an nth degree polynomial of the second kind as s (x) = 2 n U n (x) <ref> [3] </ref>.
Reference: [4] <author> A. Gerasoulis, M. D. Grigoriadis, and L. Sun, </author> <title> A fast algorithm for Trummer's problem, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 8 (1987), </volume> <pages> pp. </pages> <month> s135-s137. </month>
Reference-contexts: Then it is possible to compute C 0 u in O (N lg N ) operations using fast sine and cosine transformations via a variant of the algorithm of Gerasoulis et al <ref> [4] </ref> for solving Trummer's problem, which we now describe. Let the polynomials h (x) and s (x) be defined according to h (x) = i=1 x ! i Note that h (! i ) = u i s 0 (! i ).
Reference: [5] <author> G. H. Golub and C. V. Loan, </author> <title> Matrix Computations, </title> <publisher> Johns Hopkins University Press, </publisher> <year> 1989. </year> <note> Second Ed. </note>
Reference-contexts: Variant of MINRES. In this subsection we present a variant of MIN-RES for solving the symmetrically preconditioned problem M 1=2 CM 1=2 (M 1=2 y) = M 1=2 z which involves matrix vector multiplies with M 1 rather than M 1=2 (see <ref> [5, Section 10.3.1] </ref> and [7]).
Reference: [6] <author> M. H. Gutknecht and M. Hochbruck, </author> <title> Look-ahead Levinson and Schur algorithms for non-Hermitian Toeplitz systems, </title> <journal> Numer. Math., </journal> <volume> 70 (1995), </volume> <pages> pp. 181-227. </pages>
Reference-contexts: However, when the necessary pivoting is incorporated into the fast or super-fast LDU factorization algorithms for Toeplitz matrices, these methods can become too expensive for our purposes, often requiring O (n 3 ) operations to factor an n fi n Toeplitz matrix <ref> [18, 6, 1] </ref>. To circumvent this problem, in [16] we advocated transforming T to a Cauchy-like matrix C using a particular unitary transformation. However, if T was symmetric, C in that case was no longer symmetric.
Reference: [7] <author> M. Hanke, </author> <title> Conjugate Gradient Type Methods for Ill-Posed Problems, </title> <publisher> Longman Scientific and Technical, </publisher> <year> 1995. </year>
Reference-contexts: However, the displacement rank of the augmented matrix will remain the same as the original matrix ( 4). Iterative Krylov subspace methods can be used as regularization techniques <ref> [16, 15, 8, 7] </ref>. They can be particularly efficient on problems involving Toeplitz matrices since multiplication of a Toeplitz matrix times a vector can be done quickly (see [14]). <p> To circumvent this problem, in [16] we advocated transforming T to a Cauchy-like matrix C using a particular unitary transformation. However, if T was symmetric, C in that case was no longer symmetric. This lack of symmetry prevents one from applying such algorithms as MINRES or MR-II <ref> [7] </ref> to solve the transformed system | rather, an algorithm such as CGLS, which requires roughly twice as much work per iteration, must be used. <p> The interested reader is referred to [8] for a discussion of how the L-curve method can be used to determine an appropriate regularization parameter for MR-II and to <ref> [7] </ref> for how Morozov's discrepancy principle can be used to find a regularization parameter for MINRES (see also [11]). 7.1. Determining M 1 . <p> Variant of MINRES. In this subsection we present a variant of MIN-RES for solving the symmetrically preconditioned problem M 1=2 CM 1=2 (M 1=2 y) = M 1=2 z which involves matrix vector multiplies with M 1 rather than M 1=2 (see [5, Section 10.3.1] and <ref> [7] </ref>). <p> In this subsection we present a variant of MIN-RES for solving the symmetrically preconditioned problem M 1=2 CM 1=2 (M 1=2 y) = M 1=2 z which involves matrix vector multiplies with M 1 rather than M 1=2 (see [5, Section 10.3.1] and [7]). A variant of MR-II (see <ref> [8, 7] </ref>) for the symmetrically precon ditioned problem involving only matrix vector multiplies with M 1 can be similarly derived. 13 Algorithm 5: Preconditioned MINRES y 0 = 0; r 0 = z; v 0 = M 1 r 0 ; d 0 = v 0 For k = 0; :
Reference: [8] <author> M. Hanke and J. Nagy, </author> <title> Restoration of atmospherically blurred images by symmetric indefinite conjugate gradient techniques, Inverse Problems, </title> <booktitle> 12 (1996), </booktitle> <pages> pp. 157-173. </pages>
Reference-contexts: However, the displacement rank of the augmented matrix will remain the same as the original matrix ( 4). Iterative Krylov subspace methods can be used as regularization techniques <ref> [16, 15, 8, 7] </ref>. They can be particularly efficient on problems involving Toeplitz matrices since multiplication of a Toeplitz matrix times a vector can be done quickly (see [14]). <p> The approximate solution in the original coordinate system is f = S ~y: We note that we do not address the problem of determining when it is best to stop iterating to get a good solution. The interested reader is referred to <ref> [8] </ref> for a discussion of how the L-curve method can be used to determine an appropriate regularization parameter for MR-II and to [7] for how Morozov's discrepancy principle can be used to find a regularization parameter for MINRES (see also [11]). 7.1. Determining M 1 . <p> In this subsection we present a variant of MIN-RES for solving the symmetrically preconditioned problem M 1=2 CM 1=2 (M 1=2 y) = M 1=2 z which involves matrix vector multiplies with M 1 rather than M 1=2 (see [5, Section 10.3.1] and [7]). A variant of MR-II (see <ref> [8, 7] </ref>) for the symmetrically precon ditioned problem involving only matrix vector multiplies with M 1 can be similarly derived. 13 Algorithm 5: Preconditioned MINRES y 0 = 0; r 0 = z; v 0 = M 1 r 0 ; d 0 = v 0 For k = 0; :
Reference: [9] <author> M. Hanke, J. Nagy, and R. Plemmons, </author> <title> Preconditioned iterative regularization for ill-posed problems, Numerical Linear Algebra and Sci. </title> <booktitle> Computing, </booktitle> <year> (1993), </year> <pages> pp. 141-163. </pages> <editor> L. Reichel, A. Ruttan, and R.S. Varga, editors. </editor> <volume> 20 </volume>
Reference-contexts: The value of m fl = 0 corresponds to no preconditioning. In each example, we also give the results for the preconditioned method of Kilmer and O'Leary [16] and the method of Hanke, et al <ref> [9] </ref>, for various values of m fl . 8.1. Example 1. For this example, we modified the matrix and exact solution of the signal processing example in [16] by dropping the last row and column of T and the last row of ^ f and ^g. <p> Method of [16] Method of <ref> [9] </ref> minimum achieved minimum achieved m fl rel. error at iter. rel. error. at iter. 0 .223 119 47 .224 58 .224 40 61 .238 105 .229 30 Table 2 Convergence comparison of preconditioned CGLS scheme of Kilmer and O'Leary and method of Hanke, et al for various values of m <p> Table 2 gives the convergence results for the preconditioned CGLS scheme of [16] and for the preconditioned scheme of <ref> [9] </ref> for comparison purposes. Note that neither method does as well as the preconditioned MINRES or CGLS schemes mentioned in this paper in terms of reducing the error to a sufficient level within few enough iterations. <p> In general, preconditioned CGLS required more iterations to achieve comparable regularized solutions, and at more work per iteration. The results for the preconditioned scheme of [16] and for the method of <ref> [9] </ref> applied to Example 2 are shown in Table 4. <p> error at iter. rel. error. at iter. 0 .185 36 .112 41 19 .088 7 .107 17 25 .205 5 .121 14 31 .241 8 .116 9 Table 3 Convergence comparison of MINRES and CGLS for various values of m fl , Example 2. 19 Method of [16] Method of <ref> [9] </ref> minimum achieved minimum achieved m fl rel. error at iter. rel. error. at iter. 0 .112 41 19 .109 15 .108 57 25 .123 12 .113 54 31 .136 4 .134 55 Table 4 Convergence comparison of preconditioned CGLS scheme of Kilmer and O'Leary and method of Hanke, et al
Reference: [10] <author> P. C. Hansen, </author> <title> The discrete Picard condition for discrete ill-posed problems, </title> <journal> BIT, </journal> <volume> 30 (1990), </volume> <pages> pp. 658-672. </pages>
Reference-contexts: The matrix T has been normalized so that its largest eigenvalue is of order 1. 2. The uncontaminated data vector ^g satisfies the discrete Picard condition; i.e., the spectral coefficients of ^g decay in absolute value faster than the singular values <ref> [20, 10] </ref>. 3. The additive noise is zero-mean white Gaussian. In this case, the components of the error e are independent random variables normally distributed with mean zero and variance * 2 . 4. The noise level, kek 2 , is strictly less than one.
Reference: [11] <author> P. C. Hansen and D. P. O'Leary, </author> <title> The use of the l-curve in the regularization of discrete ill-posed problems, </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 14 (1993), </volume> <pages> pp. 1487-1503. </pages>
Reference-contexts: The interested reader is referred to [8] for a discussion of how the L-curve method can be used to determine an appropriate regularization parameter for MR-II and to [7] for how Morozov's discrepancy principle can be used to find a regularization parameter for MINRES (see also <ref> [11] </ref>). 7.1. Determining M 1 .
Reference: [12] <author> G. Heinig and A. Bojanczyk, </author> <title> Transformation techniques for Toeplitz and Toeplitz-plus-Hankel matrices. I. Transformations, </title> <journal> Linear Algebra Appl., </journal> <volume> 254 (1997), </volume> <pages> pp. 193-226. </pages>
Reference-contexts: Note that the entries of C are completely characterized in terms of the generators, the n numbers ! i , and the n diagonal entries of the matrix. The following property shows how a Toeplitz matrix can be transformed into a partially reconstructible Cauchy-like matrix <ref> [12, 14] </ref>: Property 1. <p> equation DC CD = ~ A ~ B T ; ~ A = SA; ~ B = SB;(3) where D = diag n + 1 ; cos 2 n + 1 and S is the normalized discrete sine transform matrix S = 2 n + 1 k;j=1 The authors of <ref> [12] </ref> give an explicit formula for computing the diagonal entries of C which are unspecified by (3). Alternately, these entries can be computed by diagonalizing the corresponding T. Chan-type preconditioner described in [14]. <p> Alternately, these entries can be computed by diagonalizing the corresponding T. Chan-type preconditioner described in [14]. Fortunately, since we have assumed N = n +1 is a power of 2, this can be done quickly by means of fast sine (and cosine in the case of <ref> [12] </ref>) transforms in O (N lg N ) operations. Note that the generators of T are readily determined from (2) and the generators of C can be determined with fast sine transforms. The next property gives some insight into how matrix-vector multiplications might be computed. Property 2. <p> : ; n 1 denote the diagonals of T and define s jk = sin (jk=(n + 1)), t n = 0, and r k = t 0 n2 nk+3 n+1 t k+1 ; k &gt; 1 Then c j = sin (j=(n + 1)) k=1 Now Heinig and Bojanczyk <ref> [12] </ref> show that the off-diagonal elements C ij for which i + j is odd are 0 while if i + j is even, we have C ij = cos ( i n+1 ) 2 ) sin ( n + 1 n X s ik t k sin ( n +
Reference: [13] <author> B. J ahne, </author> <title> Digital Image Processing, </title> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference-contexts: But this is, up to a factor 2 p ith coefficient of the discrete Fourier transform of the vector v e , the odd-extension of v about v n+1 = 0 <ref> [13] </ref>. If the kernel of the integral equation is smooth, then the Fourier coefficients tend to decrease in magnitude quickly as i approaches n. Thus, the DST coefficients v i of v tend to decrease in i.
Reference: [14] <author> T. Kailath and V. Olshevsky, </author> <title> Displacement structure approach to discrete-trigonometric-transform based preconditioners of the G. Strang type and of T. Chan type, </title> <month> (Sept. </month> <year> 1996). </year> <note> submitted, </note> <year> 1996. </year>
Reference-contexts: Iterative Krylov subspace methods can be used as regularization techniques [16, 15, 8, 7]. They can be particularly efficient on problems involving Toeplitz matrices since multiplication of a Toeplitz matrix times a vector can be done quickly (see <ref> [14] </ref>). However, algorithms such as CGLS (conjugate gradient for least squares) and MINRES (minimal residual, [17]) can be slow to converge to a regularized solution of (1). Therefore, we look for preconditioners which will speed convergence to a regularized solution while filtering noise in early iterates. <p> To overcome this difficulty, we note that a Toeplitz matrix T is orthogonally related to a partially reconstructible (PR) Cauchy-like matrix C via discrete trigonometric transforms <ref> [14] </ref>. One such relation considered here uses discrete sine transforms and allows us to preserve symmetry and to determine a fast algorithm for applying the preconditioner. <p> Note that the entries of C are completely characterized in terms of the generators, the n numbers ! i , and the n diagonal entries of the matrix. The following property shows how a Toeplitz matrix can be transformed into a partially reconstructible Cauchy-like matrix <ref> [12, 14] </ref>: Property 1. <p> Alternately, these entries can be computed by diagonalizing the corresponding T. Chan-type preconditioner described in <ref> [14] </ref>. Fortunately, since we have assumed N = n +1 is a power of 2, this can be done quickly by means of fast sine (and cosine in the case of [12]) transforms in O (N lg N ) operations. <p> The values c j for a symmetric matrix C are the entries of the diagonal matrix SC S S, where C S is the Chan-type preconditioner in <ref> [14] </ref>. Using this relationship, an exact formula can be determined for computing the c j (see [14]): Let t i ; i = 0; : : : ; n 1 denote the diagonals of T and define s jk = sin (jk=(n + 1)), t n = 0, and r k <p> The values c j for a symmetric matrix C are the entries of the diagonal matrix SC S S, where C S is the Chan-type preconditioner in <ref> [14] </ref>. Using this relationship, an exact formula can be determined for computing the c j (see [14]): Let t i ; i = 0; : : : ; n 1 denote the diagonals of T and define s jk = sin (jk=(n + 1)), t n = 0, and r k = t 0 n2 nk+3 n+1 t k+1 ; k &gt; 1 Then c j = <p> However, this requires that complex arithmetic be used to compute the product when T is real. Rather, we make use of the fast, real-arithmetic approach suggested in <ref> [14] </ref> for computing these products in O (N lg N ) operations. 7.4. Variant of MINRES. <p> We plan to generalize the results in this paper to the two-dimensional problems involving symmetric BTTB matrices. Acknowledgment. The author would like to thank Vadim Olshevsky for suggesting the possibility of a symmetric analogue of the work in [16] and for providing reference to work in <ref> [14] </ref>. The author is also grateful to Dianne O'Leary for her helpful comments.
Reference: [15] <author> M. E. Kilmer, </author> <title> Cauchy-like preconditioners for 2-dimensional ill-posed problems, </title> <type> Tech. Report CS-TR-3776, </type> <institution> University of Maryland, College Park, </institution> <year> 1997. </year>
Reference-contexts: However, the displacement rank of the augmented matrix will remain the same as the original matrix ( 4). Iterative Krylov subspace methods can be used as regularization techniques <ref> [16, 15, 8, 7] </ref>. They can be particularly efficient on problems involving Toeplitz matrices since multiplication of a Toeplitz matrix times a vector can be done quickly (see [14]).
Reference: [16] <author> M. E. Kilmer and D. P. O'Leary, </author> <title> Pivoted Cauchy-like preconditioners for regularized solution of ill-posed problems, </title> <type> Tech. Report CS-TR-3682, </type> <institution> University of Maryland, College Park, </institution> <year> 1996. </year>
Reference-contexts: symmetric and Toeplitz, ^g represents the noise free data, e represents noise, and g is the actual measured data. (We shall assume that T is n fi n, but note that the preconditioning scheme to be introduced in this paper could be adjusted for the rectangular case as described in <ref> [16] </ref>). Given only T and the noisy data g, one would like to approximate the exact solution ^ f to the noise-free problem T ^ f = ^g. However, since the continuous problem is ill-posed, the matrix T is ill-conditioned. <p> However, the displacement rank of the augmented matrix will remain the same as the original matrix ( 4). Iterative Krylov subspace methods can be used as regularization techniques <ref> [16, 15, 8, 7] </ref>. They can be particularly efficient on problems involving Toeplitz matrices since multiplication of a Toeplitz matrix times a vector can be done quickly (see [14]). <p> However, algorithms such as CGLS (conjugate gradient for least squares) and MINRES (minimal residual, [17]) can be slow to converge to a regularized solution of (1). Therefore, we look for preconditioners which will speed convergence to a regularized solution while filtering noise in early iterates. As in <ref> [16] </ref>, the idea is to use the rank revealing properties of a factorization of T to develop a preconditioner. <p> To circumvent this problem, in <ref> [16] </ref> we advocated transforming T to a Cauchy-like matrix C using a particular unitary transformation. However, if T was symmetric, C in that case was no longer symmetric. <p> This implies that no pivoting is needed to partially factor C as it was in <ref> [16] </ref>. Since the m fl fi m fl submatrix is itself a PR Cauchy-like matrix, it is possible to compute the LDU factorization in O ((m fl ) 2 ) operations; thus our method requires less initialization overhead than the method in [16]. This paper is organized as follows. <p> to partially factor C as it was in <ref> [16] </ref>. Since the m fl fi m fl submatrix is itself a PR Cauchy-like matrix, it is possible to compute the LDU factorization in O ((m fl ) 2 ) operations; thus our method requires less initialization overhead than the method in [16]. This paper is organized as follows. In x2, we define a partially reconstructible Cauchy-like matrix and give some of its properties. In x3, we give some background on regularization and preconditioning in the context of regularization. We introduce our preconditioner in x4 and give theoretical results in x5. <p> Further, there exists 0 &lt; m fl m such that for i &gt; m fl it is never the case that j i j j i j. As in <ref> [16] </ref>, we therefore choose to partition the columns of V into bases for the upper, lower, and transition subspaces as follows. We say that the upper subspace is the space spanned by the first m fl columns of V . <p> Since S is an orthogonal matrix, C = SV flV T S T ; where S = S T , so that C and T have the same eigenvalues and there is no mixing of the upper and lower subspaces by changing to the new coordinate system. In <ref> [16] </ref>, in order to determine the preconditioner one first had to perform a partial factorization of the corresponding Cauchy-like matrix in order to permute the largest magnitude components of C to the leading principal submatrix. <p> Therefore we save the cost of performing the partial factorization. Setting z = Sf and g = Sg, the problem T f = g is equivalent to Cy = z: If we desire to use CGLS, we would choose, as in <ref> [16] </ref>, a preconditioner for the left so that M 1 Cy = M 1 z:(8) If T , and hence C, is symmetric, however, we may want to find a symmetric pre--conditioner M and apply MINRES or MR-II to the symmetrically preconditioned system M 1=2 CM 1=2 ^y = M 1=2 <p> Writing C in block form we have C T where C 1 is m fl fi m fl . The permutation ensures that C 1 is well-conditioned having the largest magnitude elements of C. The preconditioner M is then defined as in <ref> [16] </ref>: M = C 1 0 5. Properties of the Preconditioner. Since M is defined in the same way as in [16], the theory in [16] tells us that the left preconditioned matrix has the desired properties; namely, that the largest m fl singular values are clustered around 1, while the <p> The permutation ensures that C 1 is well-conditioned having the largest magnitude elements of C. The preconditioner M is then defined as in <ref> [16] </ref>: M = C 1 0 5. Properties of the Preconditioner. Since M is defined in the same way as in [16], the theory in [16] tells us that the left preconditioned matrix has the desired properties; namely, that the largest m fl singular values are clustered around 1, while the lower subspace, and the small singular values, remain relatively untouched. <p> The permutation ensures that C 1 is well-conditioned having the largest magnitude elements of C. The preconditioner M is then defined as in <ref> [16] </ref>: M = C 1 0 5. Properties of the Preconditioner. Since M is defined in the same way as in [16], the theory in [16] tells us that the left preconditioned matrix has the desired properties; namely, that the largest m fl singular values are clustered around 1, while the lower subspace, and the small singular values, remain relatively untouched. <p> Proof: Proceed as in Theorem 3.1 of <ref> [16] </ref> to deduce that m fl of the singular values of M 1=2 CM 1=2 are bounded below by 1. <p> Then the (m fl + i)th largest mag nitude eigenvalue of M 1=2 CM 1=2 lies in the interval [0; c m fl p (m fl + i)th largest magnitude eigenvalue of C lies in the interval [0; p Proof: Proceeding as in Theorem 3.3 of <ref> [16] </ref>, we can show i+m fl (C 2 ) i (E 2C ); i = 1; : : :; m fl i+m fl ((M 1=2 CM 1=2 ) 2 ) i (E 2M ); i = 1; : : :; m fl : Now E 2M = M 1=2 E 2C <p> Applying the preconditioner. Since we are using a different transformation to Cauchy-like than that used in <ref> [16] </ref>, we need a different method for quickly applying the preconditioner. Let v be a vector of length m fl , and assume that the permutation matrix is the identity. <p> In the experiments, we compare the results of MINRES with CGLS for Cauchy-like pre-conditioners of size m fl defined in this paper. The value of m fl = 0 corresponds to no preconditioning. In each example, we also give the results for the preconditioned method of Kilmer and O'Leary <ref> [16] </ref> and the method of Hanke, et al [9], for various values of m fl . 8.1. Example 1. For this example, we modified the matrix and exact solution of the signal processing example in [16] by dropping the last row and column of T and the last row of ^ <p> each example, we also give the results for the preconditioned method of Kilmer and O'Leary <ref> [16] </ref> and the method of Hanke, et al [9], for various values of m fl . 8.1. Example 1. For this example, we modified the matrix and exact solution of the signal processing example in [16] by dropping the last row and column of T and the last row of ^ f and ^g. The condition number of the new 255 fi 255 matrix T is 4:4 fi 10 5 . <p> Method of <ref> [16] </ref> Method of [9] minimum achieved minimum achieved m fl rel. error at iter. rel. error. at iter. 0 .223 119 47 .224 58 .224 40 61 .238 105 .229 30 Table 2 Convergence comparison of preconditioned CGLS scheme of Kilmer and O'Leary and method of Hanke, et al for various <p> The condition number of C 1 for m fl = 47 is about 87; the condition number of C 1 for m fl = 61 is about 2:5 fi 10 4 . Table 2 gives the convergence results for the preconditioned CGLS scheme of <ref> [16] </ref> and for the preconditioned scheme of [9] for comparison purposes. Note that neither method does as well as the preconditioned MINRES or CGLS schemes mentioned in this paper in terms of reducing the error to a sufficient level within few enough iterations. <p> Also, these methods are more expensive (by a constant factor) per iteration than preconditioned MINRES since they require an additional matrix-vector product with the Cauchy-like matrix or the Toeplitz matrix, respectively, and they compute using complex arithmetic. Further, the initialization cost of the preconditioner of <ref> [16] </ref> is higher. <p> that since n + 1 is a power of 2 and these latter 2 methods use FFT's, it would have been more efficient to augment T by a 1 fi 1 identity and append a number to g, and solve the resulting system (see the footnote in the introduction of <ref> [16] </ref>). 8.2. Example 2. In this example, we used Hansen's Regularization Toolbox to generate a 512 fi512 symmetric Phillips Toeplitz matrix, and set T to be the 511 fi511 leading principal submatrix. <p> On the other hand, for no value of m fl could preconditioned CGLS achieve a relative error of less than :107. In general, preconditioned CGLS required more iterations to achieve comparable regularized solutions, and at more work per iteration. The results for the preconditioned scheme of <ref> [16] </ref> and for the method of [9] applied to Example 2 are shown in Table 4. <p> m fl rel. error at iter. rel. error. at iter. 0 .185 36 .112 41 19 .088 7 .107 17 25 .205 5 .121 14 31 .241 8 .116 9 Table 3 Convergence comparison of MINRES and CGLS for various values of m fl , Example 2. 19 Method of <ref> [16] </ref> Method of [9] minimum achieved minimum achieved m fl rel. error at iter. rel. error. at iter. 0 .112 41 19 .109 15 .108 57 25 .123 12 .113 54 31 .136 4 .134 55 Table 4 Convergence comparison of preconditioned CGLS scheme of Kilmer and O'Leary and method of <p> We plan to generalize the results in this paper to the two-dimensional problems involving symmetric BTTB matrices. Acknowledgment. The author would like to thank Vadim Olshevsky for suggesting the possibility of a symmetric analogue of the work in <ref> [16] </ref> and for providing reference to work in [14]. The author is also grateful to Dianne O'Leary for her helpful comments.
Reference: [17] <author> C. C. Paige and M. A. Saunders, </author> <title> Solution of sparse indefinite systems of linear equations, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 12 (1975), </volume> <pages> pp. 617-629. </pages>
Reference-contexts: They can be particularly efficient on problems involving Toeplitz matrices since multiplication of a Toeplitz matrix times a vector can be done quickly (see [14]). However, algorithms such as CGLS (conjugate gradient for least squares) and MINRES (minimal residual, <ref> [17] </ref>) can be slow to converge to a regularized solution of (1). Therefore, we look for preconditioners which will speed convergence to a regularized solution while filtering noise in early iterates.
Reference: [18] <author> D. R. Sweet, </author> <title> The use of pivoting to improve the numerical performance of Toeplitz matrix algorithms, </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 14 (1993), </volume> <pages> pp. 468-493. </pages>
Reference-contexts: However, when the necessary pivoting is incorporated into the fast or super-fast LDU factorization algorithms for Toeplitz matrices, these methods can become too expensive for our purposes, often requiring O (n 3 ) operations to factor an n fi n Toeplitz matrix <ref> [18, 6, 1] </ref>. To circumvent this problem, in [16] we advocated transforming T to a Cauchy-like matrix C using a particular unitary transformation. However, if T was symmetric, C in that case was no longer symmetric.
Reference: [19] <author> A. van der Sluis and H. van der Vorst, </author> <title> The rate of convergence of conjugate gradients, </title> <journal> Numer. Math., </journal> <volume> 48 (1986), </volume> <pages> pp. 543-560. </pages>
Reference-contexts: To speed convergence to a regularized solution, we must develop a preconditioner which clusters the first m fl eigenvalues (in absolute value) around one (see <ref> [19] </ref>); however, to keep the preconditioner from mixing noise into early iterates, we also want the small singular values, and with them, the lower subspace, to be unchanged. 4. The preconditioner. Let C = ST S be the partially reconstructible (PR) Cauchy-like matrix corresponding to the Toeplitz matrix T .
Reference: [20] <author> J. M. Varah, </author> <title> Pitfalls in the numerical solution of linear ill-posed problems, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 4 (1983), </volume> <pages> pp. 164-176. 21 </pages>
Reference-contexts: The matrix T has been normalized so that its largest eigenvalue is of order 1. 2. The uncontaminated data vector ^g satisfies the discrete Picard condition; i.e., the spectral coefficients of ^g decay in absolute value faster than the singular values <ref> [20, 10] </ref>. 3. The additive noise is zero-mean white Gaussian. In this case, the components of the error e are independent random variables normally distributed with mean zero and variance * 2 . 4. The noise level, kek 2 , is strictly less than one.
References-found: 20

