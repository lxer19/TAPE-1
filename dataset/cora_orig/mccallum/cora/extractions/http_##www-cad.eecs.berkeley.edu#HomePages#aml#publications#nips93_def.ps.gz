URL: http://www-cad.eecs.berkeley.edu/HomePages/aml/publications/nips93_def.ps.gz
Refering-URL: http://www-cad.eecs.berkeley.edu/HomePages/aml/publications/index.html
Root-URL: http://www.cs.berkeley.edu
Title: Learning Complex Boolean Functions: Algorithms and Applications  
Author: Arlindo L. Oliveira and Alberto Sangiovanni-Vincentelli 
Address: Berkeley CA 94720  
Affiliation: Dept. of EECS UC Berkeley  
Abstract: The most commonly used neural network models are not well suited to direct digital implementations because each node needs to perform a large number of operations between floating point values. Fortunately, the ability to learn from examples and to generalize is not restricted to networks of this type. Indeed, networks where each node implements a simple Boolean function (Boolean networks) can be designed in such a way as to exhibit similar properties. Two algorithms that generate Boolean networks from examples are presented. The results show that these algorithms generalize very well in a class of problems that accept compact Boolean network descriptions. The techniques described are general and can be applied to tasks that are not known to have that characteristic. Two examples of applications are presented: image reconstruction and hand-written character recognition.
Abstract-found: 1
Intro-found: 1
Reference: [ Blumer et al., 1987 ] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. Warmuth. </author> <title> Occam's razor. </title> <journal> Information Processing Letters, </journal> <volume> 24 </volume> <pages> 377-380, </pages> <year> 1987. </year>
Reference-contexts: Additionally, many alternatives are available to designers that want to implement Boolean networks, from full-custom design to field programmable gate arrays. This makes the digital alternative more cost effective than solutions based on analog designs. Occam's razor <ref> [ Blumer et al., 1987; Rissanen, 1986 ] </ref> provides the theoretical foundation for the development of algorithms that can be used to obtain Boolean networks that generalize well. According to this paradigm, simpler explanations for the available data have higher predictive power.
Reference: [ Dietterich and Bakiri, 1991 ] <author> T. G. Dietterich and G. Bakiri. </author> <title> Error-correcting output codes: A general method for improving multiclass inductive learning programs. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence (AAAI-91), </booktitle> <pages> pages 572-577. </pages> <publisher> AAAI Press, </publisher> <year> 1991. </year>
Reference-contexts: When the target label can have more than 2 values, some encoding must be used. The prefered solution is to encode the outputs using an error correcting code <ref> [ Dietterich and Bakiri, 1991 ] </ref> . This approach preserves most of the compactness of a digital encoding while beeing much less sensitive to errors in one of the output variables.
Reference: [ Fahlman and Lebiere, 1990 ] <author> S.E. Fahlman and C. Lebiere. </author> <title> The cascade-correlation learning architecture. </title> <editor> In D.S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 524-532, </pages> <address> San Mateo, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The remaining ones accept compact multi-level representations but have large two level descriptions. The algorithms described in sections 3.1 and 3.2 were compared with the cascade-correlation algorithm <ref> [ Fahlman and Lebiere, 1990 ] </ref> and a standard decision tree algorithm analog to ID3 [ Quinlan, 1986 ] .
Reference: [ Garey and Johnson, 1979 ] <author> M.R. Garey and D.S. Johnson. </author> <title> Computers and Intractability: </title>
Reference-contexts: Although this particular way of measuring complexity may prove inappropriate in some cases, we believe the approach proposed can be generalized and used with minor modifications in many other tasks. The problem of finding the smallest Boolean network consistent with the training set is NP-hard <ref> [ Garey and Johnson, 1979 ] </ref> and cannot be solved exactly in most cases. Heuristic approaches like the ones described are therefore required. 2 Definitions We consider the problem of supervised learning in an attribute based description language.
References-found: 4

