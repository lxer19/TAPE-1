URL: http://www.isr.ist.utl.pt/~jasv/vislab/publications/ps/96-RobAutonSys.ps.gz
Refering-URL: http://www.isr.ist.utl.pt/~jasv/vislab/publications/publications.html
Root-URL: 
Email: e-mail: jasv@isr.ist.utl.pt e-mail:giulio@vision.dist.unige.it  
Title: Embedded Visual Behaviors for Navigation long-term goal of building a completely autonomous system, these behaviors,
Author: Jose Santos-Victor Giulio Sandini 
Note: Besides the  
Address: Av. Rovisco Pais, 1 Via Opera Pia, 13 1096 Lisboa Codex Portugal 16145 Genova Italia  
Affiliation: Instituto Superior Tecnico DIST, Lira-Lab Instituto de Sistemas e Robotica University of Genova  
Abstract: The implementation of a set of visually-based behavior for navigation is presented. The approach, which has been inspired by insect's behaviors, is aimed at building a "library" of embedded visually-guided behaviors coping with the most common situations encountered during navigation in an indoor environment. Following this approach, the main goal is no longer how to characterize the environment, but how to embed in each behavior the perceptual processes necessary to understand the aspects of the environment required to generate a purposeful motor output. The approach relies on the purposive definition of the task to be solved by each of the behaviors and it is based on the possibility of computing visual information during the action. All the implemented behaviors share the same input process (partial information of the image flow field) and the same control variables (heading direction and velocity) to demonstrate both the generality of the approach as well as its efficient use of the computational resources. The controlled mobile base is supposed to move on a flat surface but virtually no calibration is required of the intrinsic and extrinsic parameters of the two cameras and no attempt is made at building a 2D or 3D map of the environment: the only output of the perceptual processes is a motor command. The first behavior, the centering-reflex allows a robot to be easily controlled to navigate along corridors or following walls of a given scene structure. The second behavior extends the system capabilities to the detection of obstacles lying on the pavement in front of the mobile robot. Finally docking behaviors to control the robot to a given position in the environment, with controlled speed and orientation, are presented. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Aloimonos. </author> <title> Purposive and qualitative active vision. </title> <booktitle> In Proc. ECCV90 - European Conference of Computer Vision, Antibes, </booktitle> <address> France, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: point of view, the approach has its roots in the paradigm of active perception [3, 4, 2] in the sense that the behaviors are based on active exploration of the environment and take advavantage of both the structure of the environment and the purpose of the task to be solved <ref> [1] </ref>.
Reference: [2] <author> Y. Aloimonos, I. Weiss, and A. Banddophaday. </author> <title> Active vision. </title> <journal> Int. Journal of Computer Vision, </journal> <volume> 1(4) </volume> <pages> 333-356, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: From the perceptual point of view, the approach has its roots in the paradigm of active perception <ref> [3, 4, 2] </ref> in the sense that the behaviors are based on active exploration of the environment and take advavantage of both the structure of the environment and the purpose of the task to be solved [1].
Reference: [3] <editor> R. Bajcsy and C. Tsikos. </editor> <title> Perception via manipulation. </title> <booktitle> In Proc. of the Int. Symp. & Exposition on Robots, </booktitle> <pages> pages 237-244, </pages> <address> Sydney, Australia, </address> <month> November 6-10 </month> <year> 1988. </year>
Reference-contexts: From the perceptual point of view, the approach has its roots in the paradigm of active perception <ref> [3, 4, 2] </ref> in the sense that the behaviors are based on active exploration of the environment and take advavantage of both the structure of the environment and the purpose of the task to be solved [1].
Reference: [4] <author> D.H. Ballard. </author> <title> Animate vision. </title> <journal> Artificial Intelligence, </journal> <volume> 48 </volume> <pages> 57-86, </pages> <year> 1991. </year>
Reference-contexts: From the perceptual point of view, the approach has its roots in the paradigm of active perception <ref> [3, 4, 2] </ref> in the sense that the behaviors are based on active exploration of the environment and take advavantage of both the structure of the environment and the purpose of the task to be solved [1].
Reference: [5] <author> D. Coombs and K. Roberts. </author> <title> Centering behaviour using peripheral vision. In D.P. </title> <editor> Casasent, editor, </editor> <booktitle> Intelligent Robots and Computer Vision XI: Algorithms, Techniques and Active Vision, </booktitle> <pages> pages 714-21. </pages> <booktitle> SPIE, </booktitle> <volume> Vol. 1825, </volume> <month> Nov. </month> <year> 1992. </year> <month> 23 </month>
Reference-contexts: What is worth mentioning here, however, is the fact that, instead of trying to compute the rotation component of the flow field, or to eliminate it by introducing a gaze-stabilization mechanism <ref> [5] </ref>, our approach has been that of designing the system (i.e. the camera's position with respect to the axis of rotation) as well as the controller so that the rotational component 7 of the flow becomes negligible with respect to the component of the optical flow required for the centering reflex.
Reference: [6] <editor> C. Fermuller. Navigational preliminaries. In Y. Aloimonos, editor, </editor> <title> Active Perception. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <year> 1993. </year>
Reference-contexts: Two powerful assumption are made throughout the paper: the first is the possibility of using visual information during the action <ref> [6, 8] </ref>, the second is the possibility of designing a set of closed-loop visually guided behaviors whose goal is solely that of controlling direction of heading and forward velocity of the navigating actor (the goal of perception is to act).
Reference: [7] <author> H. Mallot, H. Bulthoff, J. Little, and S. Bohrer. </author> <title> Inverse perspective mapping simplifies opticla flow computation and obstacle detection. </title> <journal> Biological Cybernetics, </journal> <volume> 64 </volume> <pages> 177-185, </pages> <year> 1991. </year>
Reference-contexts: Orienting the optical axis vertically is not useful for practical reasons, but we can inverse project the optical flow in P c onto the horizontal plane, P h, as suggested in <ref> [7] </ref>, thus simplifying 13 the original problem. The method operates in two steps. Initially, the robot moves with pure translation and the projective transformation between the image and ground planes is estimated, without the need to calibrate the camera.
Reference: [8] <author> G. Sandini, F. Gandolfo, E. Grosso, and M. Tistarelli. </author> <title> Vision during action. </title> <editor> In Y. Aloi-monos, editor, </editor> <title> Active Perception. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <year> 1993. </year>
Reference-contexts: Two powerful assumption are made throughout the paper: the first is the possibility of using visual information during the action <ref> [6, 8] </ref>, the second is the possibility of designing a set of closed-loop visually guided behaviors whose goal is solely that of controlling direction of heading and forward velocity of the navigating actor (the goal of perception is to act). <p> The use of vision during action <ref> [8] </ref>, on the contrary, may be a very powerful extension of the concept of active observer by exploiting the use of dynamic visual information not only at the "reflexive" level of motor control.
Reference: [9] <author> J. Santos-Victor and G. </author> <title> Sandini. Visual behaviors for docking. Submitted to Computer Vision and Image Understanding. </title>
Reference-contexts: However, in the ego--docking, the camera position with respect to the robot is fixed, whereas in the eco-docking it is changing continuously, thus posing new problems for the visuo-motor control loop. By proper formulation of the problem <ref> [9] </ref>, exactly the same control architecture can be used in both cases. As before, this behavior does not require information about the full optical flow field. Again, we use the planar surface assumption (see [10]) to approximate the flow by an affine mapping. <p> Again, we use the planar surface assumption (see [10]) to approximate the flow by an affine mapping. The affine flow parameters are used directly by the robot motion-controllers. The problem of how to segment the docking surface from the global image is analyzed in detail in <ref> [9] </ref>. 5.1 Visual Based Control The coordination of perception and action results in an improved performance as the visual information is continuously being used to monitor the robot behavior.
Reference: [10] <author> J. Santos-Victor and G. </author> <title> Sandini. Docking behaviours via active perception. </title> <booktitle> In Proc. of the 3rd Int. Symposium on Intelligent Autonomous Systems, </booktitle> <address> Pisa, Italy, </address> <year> 1995. </year>
Reference-contexts: By proper formulation of the problem [9], exactly the same control architecture can be used in both cases. As before, this behavior does not require information about the full optical flow field. Again, we use the planar surface assumption (see <ref> [10] </ref>) to approximate the flow by an affine mapping. The affine flow parameters are used directly by the robot motion-controllers.
Reference: [11] <author> J. Santos-Victor and G. </author> <title> Sandini. Visual based obstacle detection : a purposive approach using the normal flow. </title> <booktitle> In Proc. of the Int. Conference on Intelligent Autonomous Systems, </booktitle> <address> Karlsruhe, Germany, </address> <year> 1995. </year>
Reference-contexts: Here, we describe a system for obstacle detection which uses a similar input as Robee to avoid obstacles. It exploits the geometric properties of the vehicle-camera-scene arrangement, and does not depend on the knowledge of the camera parameters or vehicle motion, as described in detail in <ref> [11] </ref>. The basic assumption is that the robot is moving on a ground floor (as it was the case of the centering behavior presented before) and any object not lying on this plane is considered to be an obstacle. <p> As the robot we used has a single forward component of the linear velocity, we have also estimated the rotation component and canceled this term from the overall inverse projected flow and then, the same detection process was applied. In both cases, similar results were obtained (see <ref> [11] </ref> for details). resulting inverse projected flow. Right: detected obstacle. 15 In all the tests performed, the system performed robustly detecting various obstacles and stopping the robot accordingly. The resolution of the overall system determines the minimum obstacle size that can be detected, and strongly depends on the image resolution.
Reference: [12] <author> J. Santos-Victor, G. Sandini, F. Curotto, and S. Garibaldi. </author> <title> Divergent stereo in autonomous navigation : From bees to robots. </title> <journal> Int. Journal of Computer Vision, </journal> <note> Special Issue on Qualitative Vision, </note> <editor> Y. Aloimonos (Ed.), </editor> <volume> 14(2) </volume> <pages> 159-178, </pages> <year> 1995. </year>
Reference-contexts: The qualitative visual measure used is the difference between the image velocities computed over a lateral portion of the left and the right visual fields, as described in detail in <ref> [12] </ref>. The first, and possibly major, driving hypothesis is the use of qualitative depth measurements: no attempt is made to actually compute depth in metric terms. The second guideline is simplicity: whenever possible, the tradeoff between accuracy and simplicity has been biased toward the latter criterion. <p> The centering behavior is based on the comparison between partial information of the flow field acquired by two cameras pointing laterally. All the details regarding the flow estimation and a thorough analysis of the influence of the robot rotation in the behavior is presented in <ref> [12] </ref>. <p> Qualitatively, this corresponds to saying that the size of the environment is scaled by the robot speed. This control approach is also thoroughly described in <ref> [12] </ref>. 3.1.3 "Sustained" behavior The navigation system, described in the previous sections, allows the robot to navigate by balancing the flow measurements on the left and right sides. Therefore, it can only be applied as long as there is texture on both sides of a corridor-like environment.
Reference: [13] <author> Jose Santos-Victor. </author> <title> Visual Perception for Mobile Robots : from Percepts to Behaviors. </title> <type> PhD thesis, </type> <institution> Instituto Superior Tecnico, Lisboa, Portugal, </institution> <year> 1995. </year>
Reference-contexts: Initially, the robot moves with pure translation and the projective transformation between the image and ground planes is estimated, without the need to calibrate the camera. During normal operation, the normal flow field is inverse projected onto the horizontal plane, where obstacles are easily detected (see <ref> [13] </ref> for details). The planarity assumption is used to approximate the flow field of the pavement to an affine mapping. The affine flow parameters are then used to estimate the projective transformation.
Reference: [14] <author> M.V. Srinivasan, M. Lehrer, W.H. Kirchner, and S.W. Zhang. </author> <title> Range perception through apparent image speed in freely flying honeybees. </title> <journal> Visual Neuroscience, </journal> <volume> 6 </volume> <pages> 519-535, </pages> <year> 1991. </year> <month> 24 </month>
Reference-contexts: We describe two situations the ego-docking and eco-docking which open a large number of potential applications. 3 Divergent Stereo the Centering Behavior The basis of the visually guided behavior of Robee is the centering reflex, described in <ref> [14] </ref> to explain the behavior of honeybees flying within two parallel "walls". The qualitative visual measure used is the difference between the image velocities computed over a lateral portion of the left and the right visual fields, as described in detail in [12].
References-found: 14

