URL: ftp://hpsl.cs.umd.edu/pub/papers/irreg95.manolo.ps.Z
Refering-URL: http://www.cs.umd.edu/projects/hpsl/papers.brandnew/LocalResources/tech-10-23.htm
Root-URL: 
Email: fujaldon, ezapatag@atc.ctima.uma.es fshamik, saltzg@cs.umd.edu  
Title: Run-time techniques for parallelizing sparse matrix problems  
Author: M. Ujaldon S. D. Sharma J. Saltz E. L. Zapata 
Address: Plaza El Ejido, s/n. 29013 Malaga, Spain College Park, MD 20742  
Affiliation: Computer Architecture Department Computer Science Department University of Malaga University of Maryland  
Abstract: Sparse matrix problems are difficult to parallelize efficiently on message-passing machines, since they access data through multiple levels of indirection. Inspector/executor strategies, which are typically used to parallelize such problems impose significant preprocessing overheads. This paper describes the runtime support required by new compilation techniques for sparse matrices and evaluates their performance, highlighting optimizations and improvements over previous techniques.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> R. Das, J. Saltz and R. von Hanxleden. </author> <title> Slicing Analysis and Indirect Access to Distributed Arrays, </title> <booktitle> Proceedings of the 6th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> Aug </month> <year> 1993, </year> <pages> pp 152-168. </pages> <note> Also available as University of Maryland Technical Report CS-TR-3076 and UMIACS-TR-93-42. </note>
Reference-contexts: The executor stage uses this information to fetch the data and to perform the computation in the original loop body. Such runtime preprocessing techniques have been fairly well studied and successfully incorporated into compilers <ref> [7, 1] </ref>. ? This work was supported by the Ministry of Education and Science (CICYT) of Spain under project TIC92-0942 and by ONR under contracts No. SC 292-1-22913 and No. N000149410907, by NASA under contract No. NAG-11560 and by ARPA under contract No. NAG-11485. <p> Most of the research on irregular problems in Fortran has concentrated on handling single-level indirections. In practice, irregular application codes have complex access functions that go beyond the scope of current compilation techniques. Das et. al. suggested a technique using program slicing <ref> [1] </ref>, that can deal with multiple levels of indirection by transforming code containing such references into code that contains only a single level of indirection. However, their technique will generate multiple inspector stages for multi-level indirections.
Reference: 2. <author> R. Das, J. Saltz, K. Kennedy, P. Havlak. </author> <title> Index Array Flattening Through Program Transformations. Submitted to Supercomputing'95. </title> <address> San Diego, CA. Dic. </address> <year> 1995. </year>
Reference-contexts: There are six memory references in the loop body: Row (I) , Row (I+1) , Y (I) , Data (Row (I)+J) , Column (Row (I)+J)) and X (Column (Row (I)+J)). Using the terminology of <ref> [2] </ref>, the first three references are classified as level-1 non-local references, the second two as level-2 non-local references and the last one as a level-3 non-local reference. A level-N non-local reference usually requires N preprocessing stages when applying inspector-executor, one for each indirection.
Reference: 3. <institution> High Performance Language Specification. </institution> <note> Version 1.0, Technical Report TR92-225, </note> <institution> Rice University, </institution> <month> May 3, </month> <year> 1993. </year> <note> Also available as Scientific Programming 2(1-2):1-170, Spring and Summer 1993. </note>
Reference: 4. <author> A. Krishnamurthy, D.E. Culler, A. Dusseau, S.C. Goldstein, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel Programming in Split-C, </title> <booktitle> Proceedings Supercomputing'93. </booktitle> <month> Nov. </month> <year> 1993, </year> <pages> pp 262-273. </pages>
Reference-contexts: | this choice would be dictated by the sparsity of input matrices used in the application domain and the relative importance of the inspector and executor costs on the algorithm. 6 Related Work There have been many efforts aimed at providing compile-time and run-time support for irregular problems such as <ref> [5, 4, 7] </ref>. Most of the research on irregular problems in Fortran has concentrated on handling single-level indirections. In practice, irregular application codes have complex access functions that go beyond the scope of current compilation techniques.
Reference: 5. <author> P. Mehrotra and J. Van Rosendale. </author> <title> Programming distributed memory architectures using Kali. </title> <editor> In A. Nicolau, D. Gelernter, T. Gross and D. Padua, editors, </editor> <booktitle> Advances in Languages and Compilers for Parallel Processing, </booktitle> <pages> pp. 364-384. </pages> <address> Pitman/MIT-Press, </address> <year> 1991. </year>
Reference-contexts: | this choice would be dictated by the sparsity of input matrices used in the application domain and the relative importance of the inspector and executor costs on the algorithm. 6 Related Work There have been many efforts aimed at providing compile-time and run-time support for irregular problems such as <ref> [5, 4, 7] </ref>. Most of the research on irregular problems in Fortran has concentrated on handling single-level indirections. In practice, irregular application codes have complex access functions that go beyond the scope of current compilation techniques.
Reference: 6. <author> R. Mirchandaney, J. Saltz, R.M. Smith, D.M. Nicol and Kay Crowley. </author> <title> Principles of run-time support for parallel processors. </title> <booktitle> Proceedings of the 1988 ACM International Conference on Supercomputing, </booktitle> <pages> pages 140-152, </pages> <month> July, </month> <year> 1988. </year>
Reference-contexts: Since these index arrays are read in at runtime, compilers cannot analyse which matrix elements will actually be touched in a given loop, making it impossible to determine communication requirements at compile-time. To parallelize loops that use indirect addressing, compilers typically use an inspector-executor strategy <ref> [6] </ref>. Loops are tranformed so that for each indirect reference in the loop, a preprocessing step, called an inspector is generated. During program execution, the inspector examines the global addresses referenced by the indirection and determines which (if any) non-local elements must be fetched. <p> of the programmer at the expense of losing efficiency, postponing the selection of the sparse storage format until the compilation phase. 7 Conclusions In this paper, we have discussed the sparse array rolling (SAR) preprocessing technique, and have showed how it can significantly reduce the preprocessing overhead introduced by inspectors <ref> [6] </ref> in sparse-matrix codes. We have compared our runtime methods to previous techniques, particularly those implemented by CHAOS, a standard runtime library. Using different input matrices from the Harwell-Boeing sparse matrix collection, we compared the performance of these techniques on SpMxV, a very common sparse kernel.
Reference: 7. <author> S. D. Sharma, R. Ponnusamy, B. Moon, Y. Hwang, R. Das and J. Saltz. </author> <title> Run-time and Compile-time Support for Adaptive Irregular Problems, </title> <booktitle> Proceedings Supercomputing '94, </booktitle> <month> Nov. </month> <year> 1994, </year> <pages> pp. 97-106. </pages>
Reference-contexts: The executor stage uses this information to fetch the data and to perform the computation in the original loop body. Such runtime preprocessing techniques have been fairly well studied and successfully incorporated into compilers <ref> [7, 1] </ref>. ? This work was supported by the Ministry of Education and Science (CICYT) of Spain under project TIC92-0942 and by ONR under contracts No. SC 292-1-22913 and No. N000149410907, by NASA under contract No. NAG-11560 and by ARPA under contract No. NAG-11485. <p> Parallelization of SpMxV using CHAOS and SAR : CHAOS code (left), SAR code (right) 3.1 SpMxV parallelized using CHAOS runtime support Here we provide an overview of how a compiler would use CHAOS support to parallelize the SpMxV kernel. For details about CHAOS, see <ref> [7] </ref>. Figure 5 shows the kernel parallelized using pseudo-calls to the CHAOS runtime library. Partitioning: Standard compilers would parallelize the sparse-matrix vector product loop-nest by distributing iterations of the outer loop using an owner-computes rule. This work distribution prescribes a row-wise distribution for the sparse-matrix data-structures. <p> | this choice would be dictated by the sparsity of input matrices used in the application domain and the relative importance of the inspector and executor costs on the algorithm. 6 Related Work There have been many efforts aimed at providing compile-time and run-time support for irregular problems such as <ref> [5, 4, 7] </ref>. Most of the research on irregular problems in Fortran has concentrated on handling single-level indirections. In practice, irregular application codes have complex access functions that go beyond the scope of current compilation techniques.
Reference: 8. <author> M. Ujaldon and E.L. Zapata, </author> <title> Efficient Resolution of Sparse Indirections in Data-Parallel Compilers. </title> <booktitle> Proceedings of the 9th ACM International Conference on Supercomputing. </booktitle> <address> Barcelona (Spain), </address> <note> July 1995 (to appear). </note>
Reference-contexts: NAG-11560 and by ARPA under contract No. NAG-11485. The authors assume all responsibility for the contents of the paper. The inspector-executor paradigm incurs a runtime overhead for each inspec-tor stage, which is greatly increased when multiple levels of indirection are used. To reduce these costs, Ujaldon et al. <ref> [8] </ref> proposed the sparse array rolling (SAR) technique, in which a data-parallel compiler can treat many common multi-level indirections as single-level indirections, thus greatly decreasing the amount of preprocessing required for a sparse code. <p> For a more formal characterization of types of references that can be optimized using SAR see <ref> [8] </ref>. 2.1 Sparse Matrix Storage Format The CRS format (e.g. <p> level-2 non-local reference Data (Row (I)+J), the compiler knows the J-th non-zero element in the I-th row of the sparse matrix A is being accessed, which eliminates preprocessing for the intermediate Row (I) reference, since using only the indices I and J and a look up to a local MRD-descriptor <ref> [8] </ref> the runtime support can directly determine the owner processor. The same analysis could be resued for the Column (Row (I)+J)) references if Column had appeared as a target array in the loop body (it does not). <p> The key idea behind these optimizations is that the compiler can use the semantic knowledge of the matrix storage format to skip over intermediate references to the sparse matrix index arrays, thereby rolling multi-level indirections into single-level indirections. For more details we refer readers to <ref> [8] </ref>. 3 Run-time support Compilers using SAR techniques can eliminate many levels of indirections using knowledge of the storage format and distribution of the sparse matrix. For the indirections that remain, the preprocessing must still be performed. The runtime support must provide mechanisms for performing such preprocessing.
Reference: 9. <author> M. Ujaldon, E. L. Zapata, B. Chapman and H. Zima. </author> <title> Vienna-Fortran/HPF Extensions for Sparse and Irregular Problems and their Compilation, Submitted to IEEE Transactions on Parallel and Distributed Systems. Also available as Technical Report, This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: The last two sections discuss related work and present the conclusions we drew from this work. 2 Sparse Array Rolling (SAR) This section provides a brief overview of the sparse array rolling (SAR) technique. Among the different data distributions and storage formats over which SAR can be applied <ref> [9] </ref>, we have confined the discussion in this paper to the CRS (Compressed Row Storage) format and the Multiple Recursive Decomposition (MRD) [9]. For a more formal characterization of types of references that can be optimized using SAR see [8]. 2.1 Sparse Matrix Storage Format The CRS format (e.g. <p> Among the different data distributions and storage formats over which SAR can be applied <ref> [9] </ref>, we have confined the discussion in this paper to the CRS (Compressed Row Storage) format and the Multiple Recursive Decomposition (MRD) [9]. For a more formal characterization of types of references that can be optimized using SAR see [8]. 2.1 Sparse Matrix Storage Format The CRS format (e.g. <p> The Multiple Recursive Decomposition (MRD) <ref> [9] </ref> is one of the distribution schemes fulfilling these criteria. MRD recursively decomposes the sparse matrix over P processors performing a certain number of horizontal and vertical partitions until P submatrices have been obtained. <p> Other dense vectors in the program can also be aligned with one of the dimensions of a MRD decomposition, which can cause a limited degree of replication of the elements of the vector. 2.3 Language Support Using user-specified directives <ref> [9] </ref>, a compiler can automatically insert the run-time calls necessary to distribute matrices and vectors as described earlier.
References-found: 9

