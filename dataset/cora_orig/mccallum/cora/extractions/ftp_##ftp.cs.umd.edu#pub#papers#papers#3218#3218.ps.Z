URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3218/3218.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Title: An Improved Shape from Shading Algorithm  
Author: Hemant Singh Rama Chellappa 
Address: College Park, MD 20742-3285  College Park, MD 20742-3275  
Affiliation: Department of Electrical Engineering University of Maryland  Department of Electrical Engineering Center for Automation Research, and Institute for Advanced Computer Studies University of Maryland  
Date: February 1994  
Pubnum: CAR-TR-700 DACA76-92-C-0009  
Abstract: We propose an improved shape from shading (SFS) algorithm which is an extension of the recently published algorithm by Zheng and Chellappa [13]. A markedly more accurate estimate of the azimuth of the illumination source is presented. Depth reconstruction has been improved upon by using a new set of boundary conditions and adapting a more sophisticated technique for hierarchical implementation of the SFS algorithm. Errors at the boundaries of images and in rotation of the reconstructed images have been corrected. Typical results on The support of the Advanced Research Projects Agency (ARPA Order No. 8459) and the U.S. Army Topographic Engineering Center under Contract DACA76-92-C-0009 is gratefully acknowledged, as is the help of Sandy German in preparing this paper. synthetic and real images are presented.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B.K.P. Horn and M.J. Brooks, </author> <title> Shape From Shading. </title> <publisher> Cambridge, MA:MIT Press, </publisher> <year> 1989. </year>
Reference: [2] <author> V.S. Ramachandran, </author> <title> "Perception of shape and shading," </title> <journal> Nature, </journal> <volume> vol. 331, no. 14, </volume> <pages> pp. 163-166, </pages> <month> Jan. </month> <year> 1988. </year>
Reference: [3] <author> B.K.P. Horn, </author> <title> "Shape from shading:A method for obtaining the shape of a smooth opaque object from one view," </title> <type> Ph.D. dissertation, </type> <institution> Dept. of Electrical Engineering, MIT, </institution> <address> Cambridge, MA, </address> <year> 1970. </year> <title> [4] , "Height and gradient from shading," </title> <journal> Int. J. Comput. Vision, </journal> <volume> vol. 5, no. 1, </volume> <pages> pp. 584-597, </pages> <month> Aug. </month> <year> 1990. </year> <title> [5] , "Hill shading and the reflectance map," </title> <journal> Proc. IEEE, </journal> <volume> vol. 69, no. 1, </volume> <pages> pp. 448-455, </pages> <month> Apr. </month> <year> 1982. </year>
Reference: [6] <author> M.J. Brooks and B.K.P. Horn, </author> <title> "Shape and source from shading," </title> <booktitle> in Proc. Int. Joint Conf. Comput. Artificial Intell. </booktitle> <address> (Los Angeles), </address> <pages> pp. 932-936, </pages> <month> Aug. </month> <year> 1985. </year>
Reference-contexts: The notation used in this report is similar to that used in [13]. To estimate the illuminant direction from the images we first need a model relating image intensity to illuminant source. In computer vision research, one of the most commonly used image formation models is the Lambertian model <ref> [6, 7, 8] </ref>.
Reference: [7] <author> C.H. Lee and A. Rosenfeld, </author> <title> "Improved methods of estimating shape from shading using the light source coordinate system," </title> <editor> in B.K.P. Horn and M.J. Brooks, Eds., </editor> <title> Shape from Shading. </title> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <pages> pp. 323-569, </pages> <year> 1989. </year>
Reference-contexts: The notation used in this report is similar to that used in [13]. To estimate the illuminant direction from the images we first need a model relating image intensity to illuminant source. In computer vision research, one of the most commonly used image formation models is the Lambertian model <ref> [6, 7, 8] </ref>.
Reference: [8] <author> T. Simchony, R. Chellappa, and M. Shao, </author> <title> "Direct analytic methods for solving Poisson equations in computer vision problems," </title> <journal> IEEE Trans. Pattern Anal. Machine Intell., </journal> <volume> vol. PAMI-12, no. 5, </volume> <pages> pp. 435-446, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The notation used in this report is similar to that used in [13]. To estimate the illuminant direction from the images we first need a model relating image intensity to illuminant source. In computer vision research, one of the most commonly used image formation models is the Lambertian model <ref> [6, 7, 8] </ref>.
Reference: [9] <author> K. Ikeuchi and B.K.P. Horn, </author> <title> "Numerical shape from shading and occluding boundaries," </title> <journal> Artificial Intell., </journal> <volume> vol. 17, </volume> <pages> pp. 141-184, </pages> <month> Aug. </month> <year> 1981. </year>
Reference: [10] <author> B.K. Horn and M.J. Brooks, </author> <title> "The variantional approach to shape from shading," Com-put. Vision, Graphics, </title> <booktitle> Image Processing,, </booktitle> <volume> vol. 33, </volume> <pages> pp. 174-208, </pages> <month> Nov. </month> <year> 1986. </year>
Reference: [11] <author> R.T. Frankot and R. Chellappa, </author> <title> "A method for enforcing integrability in shape from shading algorithms," </title> <journal> IEEE Trans. Pattern Anal. Machine Intell., </journal> <volume> vol. PAMI-10, no. 4, </volume> <pages> pp. 439-451, </pages> <month> July </month> <year> 1988. </year> <month> 24 </month>
Reference: [12] <author> M. Shao, T. Simchony, and R. Chellappa, </author> <title> "New Algorithms for reconstruction of a 3-D depth map from one or more images," </title> <journal> in Proc. Comput. Vision Pattern Recognition (Ann Arbor, </journal> <volume> MI), </volume> <pages> pp. 513-528, </pages> <month> June </month> <year> 1988. </year>
Reference: [13] <author> Q. Zheng and R. Chellappa, </author> <title> "Estimation of illuminant direction, albedo, and shape from shading," </title> <journal> IEEE Trans. Pattern Anal. Machine Intell., </journal> <volume> vol. PAMI-13, no. 7, </volume> <pages> pp. 680-702, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: 1 Introduction We present an improved version of the SFS algorithm suggested recently by Zheng and Chel-lappa <ref> [13] </ref>. A significantly improved azimuth angle estimator of the illuminant is demonstrated. Some of the reconstruction errors obtained by the authors of [13] have been eliminated by adopting a more sophisticated hierarchical implementation of the SFS algorithm. Essentially our algorithm is the same as that in [13] but by changing boundary <p> 1 Introduction We present an improved version of the SFS algorithm suggested recently by Zheng and Chel-lappa <ref> [13] </ref>. A significantly improved azimuth angle estimator of the illuminant is demonstrated. Some of the reconstruction errors obtained by the authors of [13] have been eliminated by adopting a more sophisticated hierarchical implementation of the SFS algorithm. Essentially our algorithm is the same as that in [13] but by changing boundary conditions and preserving the reflectance map between resolution layers during reconstruction we have been able to remove warping and rotation errors in <p> by Zheng and Chel-lappa <ref> [13] </ref>. A significantly improved azimuth angle estimator of the illuminant is demonstrated. Some of the reconstruction errors obtained by the authors of [13] have been eliminated by adopting a more sophisticated hierarchical implementation of the SFS algorithm. Essentially our algorithm is the same as that in [13] but by changing boundary conditions and preserving the reflectance map between resolution layers during reconstruction we have been able to remove warping and rotation errors in image reconstructions. The notation used in this report is similar to that used in [13]. <p> Essentially our algorithm is the same as that in <ref> [13] </ref> but by changing boundary conditions and preserving the reflectance map between resolution layers during reconstruction we have been able to remove warping and rotation errors in image reconstructions. The notation used in this report is similar to that used in [13]. To estimate the illuminant direction from the images we first need a model relating image intensity to illuminant source. In computer vision research, one of the most commonly used image formation models is the Lambertian model [6, 7, 8]. <p> We have used a different set of boundary conditions for the edge and corner pixels of an image for this azimuth estimation. A good improvement over the azimuth estimates of <ref> [13] </ref> has been achieved for the same images used in [13]. Section 3 presents the changes that we incorporated into the SFS algorithm and its modified implementation. The SFS algorithm essentially remains the same except that now the reflectance map is preserved in transition across the pyramidal layers. <p> We have used a different set of boundary conditions for the edge and corner pixels of an image for this azimuth estimation. A good improvement over the azimuth estimates of <ref> [13] </ref> has been achieved for the same images used in [13]. Section 3 presents the changes that we incorporated into the SFS algorithm and its modified implementation. The SFS algorithm essentially remains the same except that now the reflectance map is preserved in transition across the pyramidal layers. Also the boundary conditions have been changed while reconstructing the height map. <p> Also the boundary conditions have been changed while reconstructing the height map. In Section 4, we present typical examples illustrating the improved results obtained. We first show some simple geometric shaded objects and SFS results on these. A cylinder and hyperbola are used because some doubts were expressed in <ref> [13] </ref> about the implementation of SFS on such objects. Then we show how warping and rotation errors in reconstruction of the Mozart image can be removed using our modified algorithm. Results for some other multiple-object and occluded images are also shown. <p> Results for some other multiple-object and occluded images are also shown. Conclusions are given in Section 5. 2 Improved Estimation of Azimuth Angle of Illuminant The image model that has been used is that of Section II (A) of <ref> [13] </ref>. We have concentrated on the local voting method [13] because it has given more consistent results than the other contour-based method [13] for a variety of images. <p> Results for some other multiple-object and occluded images are also shown. Conclusions are given in Section 5. 2 Improved Estimation of Azimuth Angle of Illuminant The image model that has been used is that of Section II (A) of <ref> [13] </ref>. We have concentrated on the local voting method [13] because it has given more consistent results than the other contour-based method [13] for a variety of images. For completeness we shall first describe the local voting estimator and then describe the changes made to the estimation technique that have led to significantly improved estimation. <p> Conclusions are given in Section 5. 2 Improved Estimation of Azimuth Angle of Illuminant The image model that has been used is that of Section II (A) of <ref> [13] </ref>. We have concentrated on the local voting method [13] because it has given more consistent results than the other contour-based method [13] for a variety of images. For completeness we shall first describe the local voting estimator and then describe the changes made to the estimation technique that have led to significantly improved estimation. <p> We start with the assumption that for any image point (x 0 ; y 0 ; z (x 0 ; y 0 )), its neighbors can be approximated by a spherical patch <ref> [13] </ref> with (a (x 0 ; y 0 ); b (x 0 ; y 0 ); c (x 0 ; y 0 )) being the center of the sphere, and r (x 0 ; y 0 ) the radius of the sphere (here the sphere is a local approximation; the radius <p> for the eight pixels to be used are given as I (i; j) = I (i; j) 2 [I (i + 1; j) I (i; j + 1) I (i + 1; j + 1)] I (i + 2; j) I (i; j + 2) (6) It is shown in <ref> [13] </ref> that E x;y ~x L L + ~x 2 ) and ( ~x 2 L = sin t:F (fl) (8) where F (fl) = fi 1 Z sin fl cos fi cos fl sin fi cos ff sin 2 flcos 2 fi + cos 2 flsin 2 fi 1 2 <p> The same logic is applied to the corner pixels where the weights are higher for boundary pixels rather than for inside pixels. By incorporating such a change in the azimuth estimator in <ref> [13] </ref> we obtained very good estimates of the azimuth. For the Mozart image the t obtained in [13] for a ground truth of t = 45 degrees was 31.03 degrees whereas our estimate is 44.67823 degrees. <p> The same logic is applied to the corner pixels where the weights are higher for boundary pixels rather than for inside pixels. By incorporating such a change in the azimuth estimator in <ref> [13] </ref> we obtained very good estimates of the azimuth. For the Mozart image the t obtained in [13] for a ground truth of t = 45 degrees was 31.03 degrees whereas our estimate is 44.67823 degrees. We 4 have successfully tested this estimation scheme on a number of images and also on a number of azimuth angles of the Mozart image. <p> However, as the local estimates on symmetric parts of a cylinder will cancel each other, the net error caused by a cylindrical patch is usually negligible <ref> [13] </ref>. This was a doubt expressed by Zheng and Chellappa [13] which we have verified by our algorithm to hold true. <p> However, as the local estimates on symmetric parts of a cylinder will cancel each other, the net error caused by a cylindrical patch is usually negligible <ref> [13] </ref>. This was a doubt expressed by Zheng and Chellappa [13] which we have verified by our algorithm to hold true. <p> It will be shown in the next section that the local estimates do indeed cancel out and the algorithm is robust to reconstruction of a cylinder. 3 Modified SFS Algorithm The formulation of the algorithm remains the same as that in Section III (A) of <ref> [13] </ref>. We have modified the hierarchical implementation of the algorithm as described below. In the implementation, the image resolution is reduced by a factor of 2 between adjacent resolution layers. The image size for the lowest resolution layer is 32. <p> The image size for the lowest resolution layer is 32. Let the variables with a tilde (~) stand for the shape descriptors of the higher resolution layer while the variables without a tilde denote those of the lower resolution layer. The transition rules are the same as in <ref> [13] </ref>. For the implementation of the algorithm the more sophisticated pyramidal approach due to Peleg and Ron [14] has been used. A brief description of the technique is made here. <p> As evident from these three figures the azimuth angle has been estimated to a high degree of accuracy for all the test images. It is a significant improvement over the corresponding results in <ref> [13] </ref>. The results for azimuth angle estimation for all images are summarized in Table 1. <p> All the images presented in our examples are 256 fi 256. The 3-D plots of height maps are reduced to 64 fi 64 for clarity of presentation. 7 of <ref> [13] </ref>, especially in Figure 5 (j) which is the reconstructed q map. Their q map and ours are shown in Figure 8 (a) and (b) respectively. It is clear that by preserving the reflectance map and modifying boundary conditions we have been able to remove warping errors in reconstruction. <p> Figures 9 (f) and (g) compare the q map of the the ground truth and that obtained by SFS. A comparison of the reconstructed (Z; p; q) with the ground truth shows that the errors in the background region and a rotation distortion present in results of <ref> [13] </ref> (refer to Figures 5 (e,h,j,l,n) of [13]) have been removed. We attribute this to the preserving of the reflectance map between resolution layers and the change in boundary conditions. by our algorithm are t = 7:74 ffi , fl = 59:52 ffi , = 192:01, and 0 = 3. <p> A comparison of the reconstructed (Z; p; q) with the ground truth shows that the errors in the background region and a rotation distortion present in results of <ref> [13] </ref> (refer to Figures 5 (e,h,j,l,n) of [13]) have been removed. We attribute this to the preserving of the reflectance map between resolution layers and the change in boundary conditions. by our algorithm are t = 7:74 ffi , fl = 59:52 ffi , = 192:01, and 0 = 3. <p> image in (a). (c) The image synthesized with the SFS result. (d) SFS reconstructed height map. 15 (a) (b) map of image in (a). (c) The image synthesized with the SFS result. (d) SFS reconstructed height map. 16 (a) (b) (a) The SFS reconstructed q map by Zheng and Chellappa <ref> [13] </ref>. (b) The SFS reconstructed q obtained by us. 17 (a) (b) (c) (g) generated from the SFS result using the estimated reflectance map parameters as the input image. (c) A 3-D plot of the reconstructed height map. (d) True Z x map. (e) The SFS reconstructed p map. (f) True
Reference: [14] <author> S. Peleg and G. Ron, </author> <title> "Nonlinear multiresolution: A shape-from-shading example," </title> <journal> IEEE Trans. Pattern Anal. Machine Intell., </journal> <volume> vol. PAMI-12, no. 12, </volume> <pages> pp. 1206-1210, </pages> <year> 1990. </year> <month> 25 </month>
Reference-contexts: The transition rules are the same as in [13]. For the implementation of the algorithm the more sophisticated pyramidal approach due to Peleg and Ron <ref> [14] </ref> has been used. A brief description of the technique is made here. For each image pixel the tangent of the elevation angle of the source, defined as T in [14], is computed as T = p 2 + q 2 = tan (ff ~ N s ) (11) Given T <p> For the implementation of the algorithm the more sophisticated pyramidal approach due to Peleg and Ron <ref> [14] </ref> has been used. A brief description of the technique is made here. For each image pixel the tangent of the elevation angle of the source, defined as T in [14], is computed as T = p 2 + q 2 = tan (ff ~ N s ) (11) Given T , R [14] is computed from the image intensities as R = 1 Based on the value T as computed in (12), the suggested algorithm for building the gray level <p> A brief description of the technique is made here. For each image pixel the tangent of the elevation angle of the source, defined as T in <ref> [14] </ref>, is computed as T = p 2 + q 2 = tan (ff ~ N s ) (11) Given T , R [14] is computed from the image intensities as R = 1 Based on the value T as computed in (12), the suggested algorithm for building the gray level pyramid for SFS purposes is as follows [14]: 1. <p> + q 2 = tan (ff ~ N s ) (11) Given T , R <ref> [14] </ref> is computed from the image intensities as R = 1 Based on the value T as computed in (12), the suggested algorithm for building the gray level pyramid for SFS purposes is as follows [14]: 1. T 0 is calculated from the given input image I 0 using (12). 2. A Gaussian pyramid T 0 ; : : : ; T n1 is built, whose base is T 0 .
References-found: 12

