URL: ftp://ftp.cs.washington.edu/tr/1994/10/UW-CSE-94-10-05.PS.Z
Refering-URL: http://www.cs.washington.edu/research/tr/tr-by-title.html
Root-URL: 
Title: Scheduling Memory Constrained Jobs on Distributed Memory Parallel Computers  
Author: Cathy McCann and John Zahorjan c Cathy McCann and John Zahorjan, 
Address: Seattle, Washington, U.S.A. 98195  
Affiliation: Department of Computer Science and Engineering, FR-35 University of Washington  
Date: October 27, 1994  1994  
Pubnum: Technical Report #94-10-05  
Abstract: fl This material is based upon work supported by the ARPA Fellowship for High Performance Computing administered by the Institute for Advanced Computer Studies, University of Maryland (MDA972-92-J-1017), the National Science Foundation (Grants CCR-9123308 and CCR-9200832), the Washington Technology Center, and Digital Equipment Corporation Systems Research Center and External Research Program. Authors' addresses: Department of Computer Science and Engineering FR-35, University of Washington, Seattle, WA 98195; mccann@cs.washington.edu, zahorjan@cs.washington.edu. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R.M. Brown, J.C. Browne, and K.M. Chandy. </author> <title> Memory management and response time. </title> <journal> Communications of the ACM, </journal> <volume> 20(3) </volume> <pages> 153-165, </pages> <month> March </month> <year> 1977. </year>
Reference-contexts: This result is compatible with those of <ref> [1] </ref>, which showed that the penalty of FCFS scheduling is substantially lower on multiprocessors than on sequential machines. ff Opt-Epoch (1) fl Heuristic-Epoch (1) ffi Equi-Epoch Hybrid (1) Buddy | | | | | | | | | | | | | | | | (a) Load Factor = 100% Number
Reference: [2] <author> S.-H. Chiang, R.K. Mansharamani, and M.K. Vernon. </author> <title> Use of application characteristics and limited preemption for run-to-completion parallel processor scheduling policies. </title> <booktitle> In Proceedings of ACM SIGMETRICS Conference, </booktitle> <pages> pages 33-44, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: 1 Introduction Much of the work on scheduling policies for multiprogrammed multiprocessors has focused on how many processors to allocate to each runnable job without considering the memory requirements of those jobs <ref> [7, 14, 15, 6, 17, 18, 13, 8, 2, 9, 16] </ref>. In this paper we consider jobs whose memory requirements imply a lower bound on the amount of machine resource they can be allocated for execution. The interaction of processor scheduling and job memory usage has been considered in [12]. <p> remaining execution time characteristics, as far as the kernel can tell. (For example, they might be the set of jobs currently residing at the highest non-empty priority level in a feedback scheme.) While there has been interest in how to use job-supplied information on execution characteristics in making scheduling decisions <ref> [7, 14, 2] </ref>, this approach runs contrary to what has proven successful in sequential systems, and relies on the user to be both reliable at estimating job characteristics and honest in relating them to the system. <p> While it is certainly possible to design run-to-completion policies that load only a subset of the jobs and run each until it completes <ref> [2] </ref>, this approach violates the principle of equal resource allocation required to approximate shortest-job-first, and there is strong evidence that this results in performance inferior to policies that instead employ some amount of time-sharing [2, 9]. 2.3 Problem Definition The system we consider is defined by three parameters: N , the <p> run-to-completion policies that load only a subset of the jobs and run each until it completes [2], this approach violates the principle of equal resource allocation required to approximate shortest-job-first, and there is strong evidence that this results in performance inferior to policies that instead employ some amount of time-sharing <ref> [2, 9] </ref>. 2.3 Problem Definition The system we consider is defined by three parameters: N , the number of nodes; J , the number of jobs; and ~ M (M 1 ; M 2 ; :::; M J ), the vector of job minimum node requirements. <p> This is the common case in current software. An application is dynamic if it can perform this repartitioning function. This distinction between static and dynamic applications is captured by the speedup functions used for them. Following <ref> [2] </ref>, for dynamic applications we use a synthetic speedup function (taken from [3]) S dynamic (n) = (1 + fi)n=(fi + n). The jobs in our workloads choose speedup parameter fi uniformly from 30 to 300, the range considered in [2]. <p> Following <ref> [2] </ref>, for dynamic applications we use a synthetic speedup function (taken from [3]) S dynamic (n) = (1 + fi)n=(fi + n). The jobs in our workloads choose speedup parameter fi uniformly from 30 to 300, the range considered in [2]. For static partitioning jobs, we assume that the jobs have been partitioned into N threads, so that they can make use of the full machine should it become available [9]. When n divides N , these jobs experience speedups equal to S dynamic (n). <p> fl fl fl fl 16 The other lesson learned from these results is that absolute equality of service allocation is not required to minimize response times. (See Figure 18.) While it has been shown that extreme inequality (running only a subset of the jobs to completion) is detrimental to performance <ref> [2, 9] </ref>, the potential disadvantage of unequal service from our disciplines is outweighed by the smaller allocations (and thus higher efficiencies) possible with them.
Reference: [3] <author> L. Dowdy. </author> <title> On the partitioning of multiprocessor systems. </title> <type> Technical report, </type> <institution> Vanderbilt University, </institution> <month> June </month> <year> 1988. </year>
Reference-contexts: This is the common case in current software. An application is dynamic if it can perform this repartitioning function. This distinction between static and dynamic applications is captured by the speedup functions used for them. Following [2], for dynamic applications we use a synthetic speedup function (taken from <ref> [3] </ref>) S dynamic (n) = (1 + fi)n=(fi + n). The jobs in our workloads choose speedup parameter fi uniformly from 30 to 300, the range considered in [2].
Reference: [4] <author> W. Leland and T. Ott. </author> <title> Load-balancing heuristics and process behavior. </title> <booktitle> In Proceedings of ACM SIGMETRICS Conference, </booktitle> <pages> pages 54-69, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: Because there is only a single processor, this is achieved through time-sharing (i.e., round-robin scheduling). Second, because it has been observed that the remaining time to completion of a job is very strongly correlated with the processing time it has already consumed <ref> [4] </ref>, jobs move down in priority level as they consume resources, and round-robin scheduling is applied only to the set of jobs in the highest non-empty priority level. Similar results have been obtained for parallel machines, primarily in the context of shared-memory systems [15, 6].
Reference: [5] <author> D. Lenoski, J. Laudon, T. Joe, D. Nakahira, L. Stevens, A. Gupta, and J. Hennessy. </author> <title> The DASH prototype: Logic overhead and performance. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(1) </volume> <pages> 41-61, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: The important characteristic of these machines to our work is that associated with each processor is a local memory module that can be accessed much more efficiently than any other memory module in the machine. While not all machines have a one-to-one mapping of memory modules to processors (e.g., <ref> [5] </ref>), the notion of local and remote memories is fundamental to nearly all. d d d & % ' $ m % % e Interconnection Network Node 0 Node N Processor Processor Memory Memory Because access to remote memories is inefficient on shared-memory machines and impossible on message-passing machines, the association
Reference: [6] <author> S. Leutenegger and M. Vernon. </author> <title> The performance of multiprogrammed multiprocessor scheduling policies. </title> <booktitle> In Proceedings of ACM SIGMETRICS Conference, </booktitle> <pages> pages 226-236, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Much of the work on scheduling policies for multiprogrammed multiprocessors has focused on how many processors to allocate to each runnable job without considering the memory requirements of those jobs <ref> [7, 14, 15, 6, 17, 18, 13, 8, 2, 9, 16] </ref>. In this paper we consider jobs whose memory requirements imply a lower bound on the amount of machine resource they can be allocated for execution. The interaction of processor scheduling and job memory usage has been considered in [12]. <p> Similar results have been obtained for parallel machines, primarily in the context of shared-memory systems <ref> [15, 6] </ref>. For parallel machines, however, space-sharing, in which resources are divided equally by partitioning the processors among the jobs, has proven to be more effective than time-sharing the machine by rotating all resources among the jobs in a round-robin fashion [15, 17].
Reference: [7] <author> S. Majumdar, D.L. Eager, and R. Bunt. </author> <title> Scheduling in multiprogrammed parallel systems. </title> <booktitle> In Proceedings of ACM SIGMETRICS Conference, </booktitle> <pages> pages 104-113, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: 1 Introduction Much of the work on scheduling policies for multiprogrammed multiprocessors has focused on how many processors to allocate to each runnable job without considering the memory requirements of those jobs <ref> [7, 14, 15, 6, 17, 18, 13, 8, 2, 9, 16] </ref>. In this paper we consider jobs whose memory requirements imply a lower bound on the amount of machine resource they can be allocated for execution. The interaction of processor scheduling and job memory usage has been considered in [12]. <p> remaining execution time characteristics, as far as the kernel can tell. (For example, they might be the set of jobs currently residing at the highest non-empty priority level in a feedback scheme.) While there has been interest in how to use job-supplied information on execution characteristics in making scheduling decisions <ref> [7, 14, 2] </ref>, this approach runs contrary to what has proven successful in sequential systems, and relies on the user to be both reliable at estimating job characteristics and honest in relating them to the system.
Reference: [8] <author> C. McCann, R. Vaswani, and J. Zahorjan. </author> <title> A dynamic processor allocation policy for multiprogrammed, shared memory multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(2) </volume> <pages> 146-178, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Much of the work on scheduling policies for multiprogrammed multiprocessors has focused on how many processors to allocate to each runnable job without considering the memory requirements of those jobs <ref> [7, 14, 15, 6, 17, 18, 13, 8, 2, 9, 16] </ref>. In this paper we consider jobs whose memory requirements imply a lower bound on the amount of machine resource they can be allocated for execution. The interaction of processor scheduling and job memory usage has been considered in [12].
Reference: [9] <author> C. McCann and J. Zahorjan. </author> <title> Processor allocation policies for message-passing parallel computers. </title> <booktitle> In Proceedings of ACM SIGMETRICS Conference, </booktitle> <pages> pages 19-32, </pages> <month> May </month> <year> 1994. </year> <month> 19 </month>
Reference-contexts: 1 Introduction Much of the work on scheduling policies for multiprogrammed multiprocessors has focused on how many processors to allocate to each runnable job without considering the memory requirements of those jobs <ref> [7, 14, 15, 6, 17, 18, 13, 8, 2, 9, 16] </ref>. In this paper we consider jobs whose memory requirements imply a lower bound on the amount of machine resource they can be allocated for execution. The interaction of processor scheduling and job memory usage has been considered in [12]. <p> In particular, if an application is capable of partitioning of its data and computation into pieces only at load time (or even earlier), allocation of an incompatible number of processors can significantly degrade application efficiency <ref> [15, 9] </ref>. We do consider this problem in detail in examining our policies. 1 In fact, static disciplines also face possibly severe problems at job arrivals. <p> Also, because we consider only how many nodes to allocate, and not which to allocate 2 , our policies should be applicable as long as IO bandwidth is spread reasonably uniformly throughout the machine. 2 Our policies are compatible with the lower level policies in <ref> [9] </ref>, however. Those policies address the question of which nodes to allocate. 2 One assumption we will make about IO hardware, however, is that it is highly parallel. <p> run-to-completion policies that load only a subset of the jobs and run each until it completes [2], this approach violates the principle of equal resource allocation required to approximate shortest-job-first, and there is strong evidence that this results in performance inferior to policies that instead employ some amount of time-sharing <ref> [2, 9] </ref>. 2.3 Problem Definition The system we consider is defined by three parameters: N , the number of nodes; J , the number of jobs; and ~ M (M 1 ; M 2 ; :::; M J ), the vector of job minimum node requirements. <p> The jobs in our workloads choose speedup parameter fi uniformly from 30 to 300, the range considered in [2]. For static partitioning jobs, we assume that the jobs have been partitioned into N threads, so that they can make use of the full machine should it become available <ref> [9] </ref>. When n divides N , these jobs experience speedups equal to S dynamic (n). <p> fl fl fl fl 16 The other lesson learned from these results is that absolute equality of service allocation is not required to minimize response times. (See Figure 18.) While it has been shown that extreme inequality (running only a subset of the jobs to completion) is detrimental to performance <ref> [2, 9] </ref>, the potential disadvantage of unequal service from our disciplines is outweighed by the smaller allocations (and thus higher efficiencies) possible with them.
Reference: [10] <author> Catherine M. McCann. </author> <title> Processor Allocation Policies for Message-Passing Parallel Computers. </title> <type> PhD thesis, </type> <institution> University of Washington, </institution> <year> 1994. </year>
Reference-contexts: Thus, BUDDY executes efficiently. The quality of BUDDY's schedules is capture by Theorem 1, which, due to space limitations, we state here without formal proof. (The reader is referred to <ref> [10] </ref> for the complete proof.) Theorem 1 The BU DDY algorithm produces an optimal schedule under the restrictions that N , J , and allocations A j for all jobs j are powers of two, and that all jobs are allocated equal resources. 5 While the formal proof that BUDDY is <p> We state here the major result regarding the quality of the schedules produced by EQUI-EPOCH. The reader is referred to <ref> [10] </ref> for the details of the proof. Theorem 2 When N = 2 n , for some integer n 0, EQUI-EPOCH is optimal among epoch scheduling policies that guarantee equal resource allocations. <p> Acknowledgements The authors thank Raj Vaswani for his careful reading of and comments on an earlier version of this paper. We also thank Martin Tompa for his assistance in refining the proofs referenced in <ref> [10] </ref>.
Reference: [11] <author> J. Ousterhout. </author> <title> Scheduling techniques for concurrent systems. </title> <booktitle> In 3rd International Conference on Distributed Computing Systems, </booktitle> <pages> pages 22-30, </pages> <month> October </month> <year> 1982. </year>
Reference-contexts: Among the benefits of space-sharing is that it interacts well with co-scheduling, which requires that either all processes of a single parallel application be scheduled at once or that none be scheduled. Co-scheduling has long been known to be critical to parallel machine scheduling <ref> [11] </ref>. Based on these results, we consider only policies that provide co-scheduling and that use space sharing as the primary mechanism to approach equal resource allocation.
Reference: [12] <author> V.G.J. Peris, M.S. Squillante, and V.K. Naik. </author> <title> Analysis of the impact of memory in distributed parallel processing systems. </title> <booktitle> In Proceedings of ACM SIGMETRICS Conference, </booktitle> <pages> pages 5-18, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: In this paper we consider jobs whose memory requirements imply a lower bound on the amount of machine resource they can be allocated for execution. The interaction of processor scheduling and job memory usage has been considered in <ref> [12] </ref>. However, they examined a paging environment, with the intent of exposing the influence that paging traffic has on performance.
Reference: [13] <author> S. Setia, M.S. Squillante, , and S. Tripathi. </author> <title> Processor scheduling on multiprogrammed, </title> <booktitle> distributed memory parallel systems. In Proceedings of ACM SIGMETRICS Conference, </booktitle> <pages> pages 158-170, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Much of the work on scheduling policies for multiprogrammed multiprocessors has focused on how many processors to allocate to each runnable job without considering the memory requirements of those jobs <ref> [7, 14, 15, 6, 17, 18, 13, 8, 2, 9, 16] </ref>. In this paper we consider jobs whose memory requirements imply a lower bound on the amount of machine resource they can be allocated for execution. The interaction of processor scheduling and job memory usage has been considered in [12].
Reference: [14] <author> K.C. Sevcik. </author> <title> Characterizations of parallelism in applications and their use in scheduling. </title> <booktitle> In Proceedings of ACM SIGMETRICS Conference, </booktitle> <pages> pages 171-180, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: 1 Introduction Much of the work on scheduling policies for multiprogrammed multiprocessors has focused on how many processors to allocate to each runnable job without considering the memory requirements of those jobs <ref> [7, 14, 15, 6, 17, 18, 13, 8, 2, 9, 16] </ref>. In this paper we consider jobs whose memory requirements imply a lower bound on the amount of machine resource they can be allocated for execution. The interaction of processor scheduling and job memory usage has been considered in [12]. <p> remaining execution time characteristics, as far as the kernel can tell. (For example, they might be the set of jobs currently residing at the highest non-empty priority level in a feedback scheme.) While there has been interest in how to use job-supplied information on execution characteristics in making scheduling decisions <ref> [7, 14, 2] </ref>, this approach runs contrary to what has proven successful in sequential systems, and relies on the user to be both reliable at estimating job characteristics and honest in relating them to the system.
Reference: [15] <author> A. Tucker and A. Gupta. </author> <title> Process control and scheduling issues for multiprogrammed shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 159-166, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: 1 Introduction Much of the work on scheduling policies for multiprogrammed multiprocessors has focused on how many processors to allocate to each runnable job without considering the memory requirements of those jobs <ref> [7, 14, 15, 6, 17, 18, 13, 8, 2, 9, 16] </ref>. In this paper we consider jobs whose memory requirements imply a lower bound on the amount of machine resource they can be allocated for execution. The interaction of processor scheduling and job memory usage has been considered in [12]. <p> In particular, if an application is capable of partitioning of its data and computation into pieces only at load time (or even earlier), allocation of an incompatible number of processors can significantly degrade application efficiency <ref> [15, 9] </ref>. We do consider this problem in detail in examining our policies. 1 In fact, static disciplines also face possibly severe problems at job arrivals. <p> Similar results have been obtained for parallel machines, primarily in the context of shared-memory systems <ref> [15, 6] </ref>. For parallel machines, however, space-sharing, in which resources are divided equally by partitioning the processors among the jobs, has proven to be more effective than time-sharing the machine by rotating all resources among the jobs in a round-robin fashion [15, 17]. <p> For parallel machines, however, space-sharing, in which resources are divided equally by partitioning the processors among the jobs, has proven to be more effective than time-sharing the machine by rotating all resources among the jobs in a round-robin fashion <ref> [15, 17] </ref>. Among the benefits of space-sharing is that it interacts well with co-scheduling, which requires that either all processes of a single parallel application be scheduled at once or that none be scheduled. Co-scheduling has long been known to be critical to parallel machine scheduling [11].
Reference: [16] <author> J.L. Wolf, J. Turek, M.-S. Chen, and P.S. Yu. </author> <title> Scheduling multiple queries on a parallel machine. </title> <booktitle> In Proceedings of ACM SIGMETRICS Conference, </booktitle> <pages> pages 45-55, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: 1 Introduction Much of the work on scheduling policies for multiprogrammed multiprocessors has focused on how many processors to allocate to each runnable job without considering the memory requirements of those jobs <ref> [7, 14, 15, 6, 17, 18, 13, 8, 2, 9, 16] </ref>. In this paper we consider jobs whose memory requirements imply a lower bound on the amount of machine resource they can be allocated for execution. The interaction of processor scheduling and job memory usage has been considered in [12].
Reference: [17] <author> J. Zahorjan and C. McCann. </author> <title> Processor scheduling in shared memory multiprocessors. </title> <booktitle> In Proceedings of ACM SIGMETRICS Conference, </booktitle> <pages> pages 214-225, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Much of the work on scheduling policies for multiprogrammed multiprocessors has focused on how many processors to allocate to each runnable job without considering the memory requirements of those jobs <ref> [7, 14, 15, 6, 17, 18, 13, 8, 2, 9, 16] </ref>. In this paper we consider jobs whose memory requirements imply a lower bound on the amount of machine resource they can be allocated for execution. The interaction of processor scheduling and job memory usage has been considered in [12]. <p> For parallel machines, however, space-sharing, in which resources are divided equally by partitioning the processors among the jobs, has proven to be more effective than time-sharing the machine by rotating all resources among the jobs in a round-robin fashion <ref> [15, 17] </ref>. Among the benefits of space-sharing is that it interacts well with co-scheduling, which requires that either all processes of a single parallel application be scheduled at once or that none be scheduled. Co-scheduling has long been known to be critical to parallel machine scheduling [11].
Reference: [18] <author> S. Zhou and T. Brecht. </author> <title> Processor-pool-based scheduling for large-scale numa multiprocessors. </title> <booktitle> In Proceedings of ACM SIGMETRICS Conference, </booktitle> <pages> pages 133-142, </pages> <month> May </month> <year> 1991. </year> <month> 20 </month>
Reference-contexts: 1 Introduction Much of the work on scheduling policies for multiprogrammed multiprocessors has focused on how many processors to allocate to each runnable job without considering the memory requirements of those jobs <ref> [7, 14, 15, 6, 17, 18, 13, 8, 2, 9, 16] </ref>. In this paper we consider jobs whose memory requirements imply a lower bound on the amount of machine resource they can be allocated for execution. The interaction of processor scheduling and job memory usage has been considered in [12].
References-found: 18

