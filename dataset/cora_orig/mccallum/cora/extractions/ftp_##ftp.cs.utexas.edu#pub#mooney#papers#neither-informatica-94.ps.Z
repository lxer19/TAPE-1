URL: ftp://ftp.cs.utexas.edu/pub/mooney/papers/neither-informatica-94.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/ml/theory-rev.html
Root-URL: http://www.cs.utexas.edu
Title: Extending Theory Refinement to M-of-N Rules  
Author: Paul T. Baffes and Raymond J. Mooney 
Keyword: artificial intelligence, multistrategy learning, theory refinement  
Date: Received: May 26, 1993 Revised: October 15, 1993 Accepted: October 15, 1993  
Address: Austin, Texas 78712-1188 USA  
Affiliation: Department of Computer Sciences University of Texas  
Note: Informatica 17 page xxx-yyy 1  Edited by: Gheorghe Tecuci  
Abstract: In recent years, machine learning research has started addressing a problem known as theory refinement. The goal of a theory refinement learner is to modify an incomplete or incorrect rule base, representing a domain theory, to make it consistent with a set of input training examples. This paper presents a major revision of the Either propositional theory refinement system. Two issues are discussed. First, we show how run time efficiency can be greatly improved by changing from a exhaustive scheme for computing repairs to an iterative greedy method. Second, we show how to extend Either to refine M-of-N rules. The resulting algorithm, Neither (New Either), is more than an order of magnitude faster and produces significantly more accurate results with theories that fit the M-of-N format. To demonstrate the advantages of Neither, we present experimental results from two real-world domains. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. Baffes and R. J. Mooney. </author> <title> Using theory revision to model students and acquire stereotypical errors. </title> <booktitle> In Proceedings of the Thirteenth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 617-622, </pages> <address> Bloomington, IN, </address> <year> 1992. </year>
Reference-contexts: First, it represents another real-world domain which has an M-of-N flavor (the "shock" concept in the theory is represented using a threshold rule). Second, Neither's ability to refine theories in this domain is the centerpiece of another of our research efforts in student modeling <ref> [1] </ref>. In short, a student modeling algorithm must be able to recover from a variety of deviations from the correct theory in order to be useful to a variety of students.
Reference: [2] <author> G.G. Buchanan and eds. </author> <title> E.H. Shortliffe. Rule-Based Expert Systems:The MYCIN Experiments of the Stanford Heuristic Programming Project. </title> <publisher> Addison-Wesley Publishing Co., </publisher> <address> Reading, MA, </address> <year> 1984. </year>
Reference-contexts: Therefore, Neither's revised theories are less complex and presumably easier to understand. Finally, unlike Kbann, Neither is guaranteed to converge to 100% accuracy on the training data. Rapture [6] uses a combination of symbolic and neural-network learning methods to revise a certainty-factor rule base <ref> [2] </ref>. Consequently, it lies somewhere between Neither and Kbann on the symbolic-connectionist dimension. As illustrated in the results, its accuracy on the promoter problem is only slightly superior to Neither's.
Reference: [3] <author> A. D. Danyluk. </author> <title> Gemini: An integration of analytical and empirical learning. </title> <booktitle> In Proceedings of the International Workshop on Multistrategy Learning, </booktitle> <pages> pages 191-206, </pages> <address> Harper's Ferry, W.Va., </address> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Recently, a number of machine learning systems have been developed that use examples to revise an approximate (incomplete and/or incorrect) domain theory <ref> [4, 11, 18, 3, 21, 7] </ref>. Most of these systems revise theories composed of strict if-then rules (Horn clauses).
Reference: [4] <author> A. Ginsberg. </author> <title> Theory reduction, theory revision, </title> <booktitle> and retranslation. In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 777-782, </pages> <address> Detroit, MI, </address> <month> July </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Recently, a number of machine learning systems have been developed that use examples to revise an approximate (incomplete and/or incorrect) domain theory <ref> [4, 11, 18, 3, 21, 7] </ref>. Most of these systems revise theories composed of strict if-then rules (Horn clauses).
Reference: [5] <author> A. Ginsberg, S. M. Weiss, and P. Politakis. </author> <title> Automatic knowledge based refinement for classification systems. </title> <journal> Artificial Intelligence, </journal> <volume> 35 </volume> <pages> 197-226, </pages> <year> 1988. </year>
Reference-contexts: CRLS [17] learns M-of-N rules and out-performed standard rule induction in several medical domains. ID-2-of-3 [9] incorporates M-of-N tests in decision-tree learning and outperformed standard decision-tree induction in a number of domains. Both projects clearly demonstrate the advantages of M-of-N rules. Seek2 <ref> [5] </ref> includes operators for refining M-of-N rules; however, its revision process is heuristic and it is not guaranteed to produce a revised theory that is consistent with all of the training examples.
Reference: [6] <author> J. J. Mahoney and R. J. Mooney. </author> <title> Combining neural and symbolic learning to revise probabilistic rule bases. </title> <editor> In S.J. Hanson, J.C. Cowan, </editor> <title> and Extending Theory Refinement to M-of-N Rules Informatica 17 page xxx-yyy 9 C.L. </title> <editor> Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> Vol. 5, </volume> <pages> pages 107-114, </pages> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: There has been some work on the induction of M-of-N rules demonstrating the advantages of this representation [17, 9]. Other work has focused on revising rules that have real-valued weights <ref> [19, 6] </ref>. However, revising theories with simple M-of-N rules has not previously been addressed. Since M-of-N rules are more constrained than rules with real-valued weights, they provide a stronger bias and are easier to comprehend. <p> We selected this particular data set because the original Either algorithm was outperformed by other systems on this data due to the M-of-N qualities required to reason correctly in this domain. In addition to testing Either, Neither and Neither-MofN, we ran experiments using Id3 [13], backpropagation [16] and Rapture <ref> [6] </ref> (a revision system based on certainty factors). We also included data on the performance of Kbann on this data set as reported in [20]. The experiments proceeded as follows. Each data set was divided into training and test sets. <p> Therefore, Neither's revised theories are less complex and presumably easier to understand. Finally, unlike Kbann, Neither is guaranteed to converge to 100% accuracy on the training data. Rapture <ref> [6] </ref> uses a combination of symbolic and neural-network learning methods to revise a certainty-factor rule base [2]. Consequently, it lies somewhere between Neither and Kbann on the symbolic-connectionist dimension. As illustrated in the results, its accuracy on the promoter problem is only slightly superior to Neither's.
Reference: [7] <author> S. Matwin and B. Plante. </author> <title> A deductive-inductive method for theory revision. </title> <booktitle> In Proceedings of the International Workshop on Multistrategy Learning, </booktitle> <pages> pages 160-174, </pages> <address> Harper's Ferry, W.Va., </address> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Recently, a number of machine learning systems have been developed that use examples to revise an approximate (incomplete and/or incorrect) domain theory <ref> [4, 11, 18, 3, 21, 7] </ref>. Most of these systems revise theories composed of strict if-then rules (Horn clauses).
Reference: [8] <author> Marilyn A. Murphy and Gayle V. Davidson. </author> <title> Computer-based adaptive instruction: Effects of learner control on concept learning. </title> <journal> Journal of Computer-Based Instruction, </journal> <volume> 18(2) </volume> <pages> 51-56, </pages> <year> 1991. </year>
Reference-contexts: The data for this experiment was borrowed from a separate research project designed to test nursing students retention of concepts for determining if a patient is suffering from shock <ref> [8] </ref>. Each patient can be labeled in one of four ways: as suffering from hypovolemic, cardiogenic, or vascular tone shock, or as not in shock. A theory for diagnosing shock was written using the definitions and examples presented to the students and consultations with a medical expert.
Reference: [9] <author> P. M. Murphy and M. J. Pazzani. ID2-of-3: </author> <title> Constructive induction of M-of-N concepts for discriminators in decision trees. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <pages> pages 183-187, </pages> <address> Evanston, IL, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: There has been some work on the induction of M-of-N rules demonstrating the advantages of this representation <ref> [17, 9] </ref>. Other work has focused on revising rules that have real-valued weights [19, 6]. However, revising theories with simple M-of-N rules has not previously been addressed. Since M-of-N rules are more constrained than rules with real-valued weights, they provide a stronger bias and are easier to comprehend. <p> CRLS [17] learns M-of-N rules and out-performed standard rule induction in several medical domains. ID-2-of-3 <ref> [9] </ref> incorporates M-of-N tests in decision-tree learning and outperformed standard decision-tree induction in a number of domains. Both projects clearly demonstrate the advantages of M-of-N rules.
Reference: [10] <author> D. Ourston. </author> <title> Using Explanation-Based and Empirical Methods in Theory Revision. </title> <type> PhD thesis, </type> <institution> University of Texas, Austin, TX, </institution> <month> August </month> <year> 1991. </year> <note> Also appears as Artificial Intelligence Laboratory Technical Report AI 91-164. </note>
Reference-contexts: We show that Neither is able to restore significantly damaged theories to near perfect accuracy. 2 Informatica 17 page xxx-yyy Baffes and Mooney lines. 2 Theory Revision Algorithm 2.1 The Either Algorithm The original Either theory refinement algorithm has been presented in various levels of detail in <ref> [11, 12, 10] </ref>. It was designed to revise propositional Horn-clause theories. For Either, a theory is a set of propositional Horn-clause rules such as those shown in the top half of Figure 1. <p> Either retracts and generalizes existing antecedents and learns new rules to fix these problems. Unlike other theory revision systems that perform hill-climbing (and are therefore subject to local maxima), Either is guaranteed to fix any arbitrarily incorrect propositional Horn-clause theory <ref> [10] </ref>. The basic algorithm used by Either for both generalization and specialization is shown in the top half of Figure 3. There are three steps. First, all possible repairs for each failing example are computed.
Reference: [11] <author> D. Ourston and R. Mooney. </author> <title> Changing the rules: A comprehensive approach to theory refinement. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 815-820, </pages> <address> Detroit, MI, </address> <month> July </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Recently, a number of machine learning systems have been developed that use examples to revise an approximate (incomplete and/or incorrect) domain theory <ref> [4, 11, 18, 3, 21, 7] </ref>. Most of these systems revise theories composed of strict if-then rules (Horn clauses). <p> However, revising theories with simple M-of-N rules has not previously been addressed. Since M-of-N rules are more constrained than rules with real-valued weights, they provide a stronger bias and are easier to comprehend. This paper presents a major revision of the Either propositional theory refinement system <ref> [11, 12] </ref> that is significantly more efficient and is also capable of revising theories with M-of-N rules. Either is inefficient because it computes a potentially exponential number of repairs for each failing example. <p> We show that Neither is able to restore significantly damaged theories to near perfect accuracy. 2 Informatica 17 page xxx-yyy Baffes and Mooney lines. 2 Theory Revision Algorithm 2.1 The Either Algorithm The original Either theory refinement algorithm has been presented in various levels of detail in <ref> [11, 12, 10] </ref>. It was designed to revise propositional Horn-clause theories. For Either, a theory is a set of propositional Horn-clause rules such as those shown in the top half of Figure 1. <p> Finally, we need to perform a more comprehensive experimental evaluation of the system. 6 Conclusions This paper has presented an efficient propositional theory refinement system that is capable of revising M-of-N rules. The basic framework is a modification of Either <ref> [11] </ref>; however, the construction of partial proofs has been reduced from exponential to linear time and a method for revising the thresholds of M-of-N rules has been incorporated.
Reference: [12] <author> D. Ourston and R. J. Mooney. </author> <title> Theory refinement combining analytical and empirical methods. </title> <journal> Artificial Intelligence, </journal> <note> in press. </note>
Reference-contexts: However, revising theories with simple M-of-N rules has not previously been addressed. Since M-of-N rules are more constrained than rules with real-valued weights, they provide a stronger bias and are easier to comprehend. This paper presents a major revision of the Either propositional theory refinement system <ref> [11, 12] </ref> that is significantly more efficient and is also capable of revising theories with M-of-N rules. Either is inefficient because it computes a potentially exponential number of repairs for each failing example. <p> We show that Neither is able to restore significantly damaged theories to near perfect accuracy. 2 Informatica 17 page xxx-yyy Baffes and Mooney lines. 2 Theory Revision Algorithm 2.1 The Either Algorithm The original Either theory refinement algorithm has been presented in various levels of detail in <ref> [11, 12, 10] </ref>. It was designed to revise propositional Horn-clause theories. For Either, a theory is a set of propositional Horn-clause rules such as those shown in the top half of Figure 1.
Reference: [13] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: Figure 1 illustrates this process for theory generalization where Either is searching for leaf-rule antecedent deletions to correct failing positive examples. A leaf rule is a rule whose antecedents include an observable or an intermediate concept that is not 1 Either uses a version of Id3 <ref> [13] </ref> for its induction. <p> We selected this particular data set because the original Either algorithm was outperformed by other systems on this data due to the M-of-N qualities required to reason correctly in this domain. In addition to testing Either, Neither and Neither-MofN, we ran experiments using Id3 <ref> [13] </ref>, backpropagation [16] and Rapture [6] (a revision system based on certainty factors). We also included data on the performance of Kbann on this data set as reported in [20]. The experiments proceeded as follows. Each data set was divided into training and test sets.
Reference: [14] <author> J.R. Quinlan. </author> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5(3) </volume> <pages> 239-266, </pages> <year> 1990. </year>
Reference-contexts: The theories were tested both before and after refinement using the same 50 test examples. This entire process was repeated 10 times, and the results averaged. For comparison purposes, we also ran the same training data through a propositional version of the Foil inductive learner <ref> [14] </ref> and tested the results using the same test data, averaging the results of the 10 trials. 3.2.2 Results Table 3 shows the results of the recovery experiments. Note that the results for Foil are identical for each experiment since induction does not make use of an input theory.
Reference: [15] <author> B. Richards and R. Mooney. </author> <title> First-order theory revision. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <pages> pages 447-451, </pages> <address> Evanston, IL, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Also, we could extend our methods to handle negation as failure and incorporate the ability to handle M-of-N rules into first-order theory revision <ref> [15] </ref>. The inductive component of Neither should be modified to produce threshold rules directly, rather than symbolic rules. Finally, we need to perform a more comprehensive experimental evaluation of the system. 6 Conclusions This paper has presented an efficient propositional theory refinement system that is capable of revising M-of-N rules.
Reference: [16] <author> D. E. Rumelhart, G. E. Hinton, and J. R. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D. E. Rumelhart and J. L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Vol. I, </volume> <pages> pages 318-362. </pages> <publisher> MIT Press, </publisher> <address> Cam-bridge, MA, </address> <year> 1986. </year>
Reference-contexts: We selected this particular data set because the original Either algorithm was outperformed by other systems on this data due to the M-of-N qualities required to reason correctly in this domain. In addition to testing Either, Neither and Neither-MofN, we ran experiments using Id3 [13], backpropagation <ref> [16] </ref> and Rapture [6] (a revision system based on certainty factors). We also included data on the performance of Kbann on this data set as reported in [20]. The experiments proceeded as follows. Each data set was divided into training and test sets.
Reference: [17] <author> K. A. Spackman. </author> <title> Learning categorical decision criteria in biomedical domains. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <pages> pages 36-46, </pages> <address> Ann Arbor, MI, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: There has been some work on the induction of M-of-N rules demonstrating the advantages of this representation <ref> [17, 9] </ref>. Other work has focused on revising rules that have real-valued weights [19, 6]. However, revising theories with simple M-of-N rules has not previously been addressed. Since M-of-N rules are more constrained than rules with real-valued weights, they provide a stronger bias and are easier to comprehend. <p> Finally, though Neither-MofN was unable to exactly duplicate the original theory in all cases, the refinements made seemed reasonable in light of the alterations made in the modified theories. 4 Related Work Several researchers have developed methods for inducing M-of-N concepts from scratch. CRLS <ref> [17] </ref> learns M-of-N rules and out-performed standard rule induction in several medical domains. ID-2-of-3 [9] incorporates M-of-N tests in decision-tree learning and outperformed standard decision-tree induction in a number of domains. Both projects clearly demonstrate the advantages of M-of-N rules.
Reference: [18] <author> G. Towell and J. Shavlik. </author> <title> Refining symbolic knowledge using neural networks. </title> <booktitle> In Proceedings of the International Workshop on Multistrategy Learning, </booktitle> <pages> pages 257-272, </pages> <address> Harper's Ferry, W.Va., </address> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Recently, a number of machine learning systems have been developed that use examples to revise an approximate (incomplete and/or incorrect) domain theory <ref> [4, 11, 18, 3, 21, 7] </ref>. Most of these systems revise theories composed of strict if-then rules (Horn clauses). <p> Also, because it was restricted to strict Horn-clause theories, the old Either algorithm could not produce as accurate results as a neural-network revision sys tem called Kbann on a domain known as the DNA promoter problem <ref> [18, 19] </ref>. Essentially, this is because some aspects of the promoter concept fit the M-of-N format. Specifically, there are several potential sites where hydrogen bonds can form between the DNA and a protein; if enough of these bonds form, promoter activity can occur.
Reference: [19] <author> G. Towell and J. Shavlik. </author> <title> Interpretation of artificial neural networks: Mapping knowledge-based neural networks into rules. </title> <editor> In R. Lippmann, J. Moody, and D. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 4. </volume> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: There has been some work on the induction of M-of-N rules demonstrating the advantages of this representation [17, 9]. Other work has focused on revising rules that have real-valued weights <ref> [19, 6] </ref>. However, revising theories with simple M-of-N rules has not previously been addressed. Since M-of-N rules are more constrained than rules with real-valued weights, they provide a stronger bias and are easier to comprehend. <p> Also, because it was restricted to strict Horn-clause theories, the old Either algorithm could not produce as accurate results as a neural-network revision sys tem called Kbann on a domain known as the DNA promoter problem <ref> [18, 19] </ref>. Essentially, this is because some aspects of the promoter concept fit the M-of-N format. Specifically, there are several potential sites where hydrogen bonds can form between the DNA and a protein; if enough of these bonds form, promoter activity can occur. <p> Neither uses a greedy covering approach to guarantee that it finds a set of revisions that fix all of the misclassified examples in the training set. Also, unlike Neither, Seek2 cannot learn new rules or add new antecedents to existing rules. Kbann <ref> [19] </ref> revises a theory by translating it into a neural network, using backpropagation to refine the 8 Informatica 17 page xxx-yyy Baffes and Mooney 0.1 modified theory 0.2 modified theory 0.3 modified theory before refinement 68.25 50.25 43.5 after refinement 100.0 94.0 94.0 induction 80.4 80.4 80.4 Table 3: Shock Test
Reference: [20] <author> G. G. Towell. </author> <title> Symbolic Knowledge and Neural Networks: Insertion, Refinement, and Extraction. </title> <type> PhD thesis, </type> <institution> University of Wisconsin, Madison, WI, </institution> <year> 1991. </year>
Reference-contexts: neg. example a 1 of (b,c,d) Y Y split rule pos. example neg. example X 1 of (b,c) Y Y Table 2: Induced Antecedent Addition. 3.1 The DNA Promoter Domain 3.1.1 Experimental Design We tested both Neither and Neither-MofN against other classification algorithms using the DNA promoter sequences data set <ref> [20] </ref>. This data set involves 57 features, 106 examples, and 2 categories. The theory provided with the data set is supposed to recognize promoters in strings of nucleotides. A promoter is a genetic region which initiates the first step in the expression of an adjacent gene transcription. <p> In addition to testing Either, Neither and Neither-MofN, we ran experiments using Id3 [13], backpropagation [16] and Rapture [6] (a revision system based on certainty factors). We also included data on the performance of Kbann on this data set as reported in <ref> [20] </ref>. The experiments proceeded as follows. Each data set was divided into training and test sets. Training sets were further divided into subsets, so that the algorithms could be evaluated with varying amounts of training data. After training, each system's accuracy was recorded on the test set. <p> In short, by adding one more operator to the generalization and specialization processes, Neither-MofN is able to accurately revise a theory known to be difficult for symbolic systems, without having to sacrifice the efficiency of a symbolic approach. Finally, the most comparable learning-curve results from <ref> [20] </ref> would indicate that Kbann's accuracy in the promoter domain is about the same as Neither-MofN's. The most surprising result of the experiments was the difference in accuracy between the original Either algorithm and Neither. <p> Although Kbann's results are referred to as M-of-N rules, they actually contain real-valued antecedent weights and therefore are not strictly M-of-N. In addition, Kbann's revised theories for the promoter problem are also more complex in terms of number of antecedents than the initial theory <ref> [20] </ref>, while Neither actually produces a slight reduction. Therefore, Neither's revised theories are less complex and presumably easier to understand. Finally, unlike Kbann, Neither is guaranteed to converge to 100% accuracy on the training data.
Reference: [21] <author> B. L. Whitehall, S. C. Lu, and R. E. Stepp. </author> <title> Theory completion using knowledge-based learning. </title> <booktitle> In Proceedings of the International Workshop on Multistrategy Learning, </booktitle> <pages> pages 144-159, </pages> <address> Harper's Ferry, W.Va., </address> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Recently, a number of machine learning systems have been developed that use examples to revise an approximate (incomplete and/or incorrect) domain theory <ref> [4, 11, 18, 3, 21, 7] </ref>. Most of these systems revise theories composed of strict if-then rules (Horn clauses).
References-found: 21

