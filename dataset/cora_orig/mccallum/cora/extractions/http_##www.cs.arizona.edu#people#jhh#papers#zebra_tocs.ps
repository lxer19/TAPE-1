URL: http://www.cs.arizona.edu/people/jhh/papers/zebra_tocs.ps
Refering-URL: http://www.cs.arizona.edu/people/jhh/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: The Zebra Striped Network File System  
Author: John H. Hartman John K. Ousterhout 
Abstract: Zebra is a network file system that increases throughput by striping file data across multiple servers. Rather than striping each file separately, Zebra forms all the new data from each client into a single stream, which it then stripes using an approach similar to a log-structured file system. This provides high performance for writes of small files as well as for reads and writes of large files. Zebra also writes parity information in each stripe in the style of RAID disk arrays; this increases storage costs slightly but allows the system to continue operation even while a single storage server is unavailable. A prototype implementation of Zebra, built in the Sprite operating system, provides 4-5 times the throughput of the standard Sprite file system or NFS for large files and a 15% to 300% improvement for writing small files. 
Abstract-found: 1
Intro-found: 1
Reference: [Anderson95] <author> Thomas E. Anderson, David E. Culler, and David A. Patterson, </author> <title> A Case for NOW (Networks of Workstations). </title> <note> To appear in IEEE Micro, </note> <year> 1995. </year>
Reference: [Baker91] <author> Mary G. Baker, John H. Hartman, Michael D. Kupfer, Ken W. Shirriff, and John K. Ousterhout, </author> <title> Measurements of a Distributed File System, </title> <booktitle> Proceedings of the 13th Symposium on Operating Systems Principles (SOSP), Asilomar, </booktitle> <address> CA, </address> <month> October </month> <year> 1991, </year> <pages> 198-212. </pages> <note> Published as ACM SIGOPS Operating Systems Review 25, 5. </note>
Reference-contexts: Zebra is designed to support UNIX workloads as found in office and engineering environments. These workloads are characterized by short file lifetimes, sequential file accesses, infrequent write-sharing of files by different clients, and many small files <ref> [Baker91] </ref>. This environment is also notable because of the behavior it does not exhibit, namely random accesses to files. Zebra is therefore designed to handle sequential file accesses well, perhaps at the expense of random file accesses.
Reference: [Baker92a] <author> Mary Baker and Mark Sullivan, </author> <title> The Recovery Box: Using Fast Recovery to Provide High Availability in the UNIX Environment, </title> <booktitle> Proceedings of the Summer 1992 USENIX Conference, </booktitle> <month> June </month> <year> 1992, </year> <pages> 31-43. </pages>
Reference-contexts: If the file managers machine should break then the file manager can be restarted on another machine. Of course, if the file manager crashes Zebra will be unavailable until the file manager restarts, but it should be possible to restart the file manager quickly <ref> [Baker92a] </ref>. A similar approach has been proposed by Cabrera and Long for the Swift file system [Cabrera91] for making its storage mediator highly available. 7.4 Stripe Cleaner Crashes The technique used to make the stripe cleaner highly available is similar to that used for the file manager. <p> Another approach to highly available file service is to design file servers that can quickly reboot after a software failure <ref> [Baker92a] </ref>. The idea is to reboot the file server so quickly that file service is not interrupted. This alternative does not require redundant copies or parity, but neither does it allow the system to continue operation in the event of a hardware failure.
Reference: [Baker92b] <author> Mary Baker, Satoshi Asami, Etienne Deprit, and John Ousterhout, </author> <title> Non-Volatile Memory for Fast, Reliable File Systems, </title> <booktitle> Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), </booktitle> <address> Boston, MA, </address> <month> October </month> <year> 1992, </year> <pages> 10-22. </pages>
Reference-contexts: The rate at which applications invoke fsync will have a large effect on Zebras performance (or any other file systems) because fsyncs require synchronous disk operations. Baker et. al <ref> [Baker92b] </ref> found that under a transaction processing workload up to 90% of the segments written on an LFS file system were partial segments caused by an fsync. Such a workload would have poor performance on Zebra as well.
Reference: [Bernstein81] <author> Philip A. Bernstein and Nathan Goodman, </author> <title> Concurrency Control in Distributed Database Systems, </title> <journal> ACM Computing Surveys 13, </journal> <month> 2 (June </month> <year> 1981), </year> <pages> 185-222. </pages>
Reference-contexts: In (b) the entire file is placed on one server but the parity requires as much space as the file data. File File Servers Parity (a) File File Servers Parity (b) 5 There exist protocols for ensuring that two writes to two different file servers are carried out atomically <ref> [Bernstein81] </ref> but they are complex and expensive. 3.3 Log-Structured File Systems and Log-Based Striping Zebra uses techniques from log-structured file systems (LFS) [Rosenblum91] to avoid the problems of file-based striping. LFS is a disk management technique that treats the disk like an append-only log.
Reference: [Birrell82] <author> Andrew D. Birrell, Roy Levin, Roger M. Needham, and Michael D. Schroeder, Grapevine: </author> <title> An Exercise in Distributed Computing, </title> <journal> Communications of the ACM 25, </journal> <month> 4 (April </month> <year> 1982), </year> <pages> 260-274. </pages>
Reference-contexts: The create and delete deltas used by Zebra are similar to the active and deleted sublists used in the Grapevine mail system to manage entries in a registration database <ref> [Birrell82] </ref>.
Reference: [Cabrera91] <author> Luis-Felipe Cabrera and Darrell D. E. Long, Swift: </author> <title> Using Distributed Disk Striping to Provide High I/O Data Rates, </title> <booktitle> Computing Systems 4, 4 (Fall 1991), </booktitle> <pages> 405-436. </pages>
Reference-contexts: A striping file system offers the potential to achieve very high performance using collections of inexpensive computers and disks. Several striping file systems have already been built, such as Swift <ref> [Cabrera91] </ref> and Bridge [Dibble88]. These systems are similar in that they stripe data within individual files, so that only large files benefit from the striping. Zebra uses a different approach borrowed from log-structured file systems (LFS) [Rosenblum91]. <p> Of course, if the file manager crashes Zebra will be unavailable until the file manager restarts, but it should be possible to restart the file manager quickly [Baker92a]. A similar approach has been proposed by Cabrera and Long for the Swift file system <ref> [Cabrera91] </ref> for making its storage mediator highly available. 7.4 Stripe Cleaner Crashes The technique used to make the stripe cleaner highly available is similar to that used for the file manager. <p> Several other striping file systems have been built. Most, such as sfs [LoVerso93], Bridge [Dibble88] and CFS [Pierce89], stripe across I/O nodes in a parallel computer; to our knowledge only one, Swift <ref> [Cabrera91] </ref>, stripes across servers in a network file system. All of these systems use file-based striping, so they work best with large files.
Reference: [Cao93] <author> Pei Cao, Swee B. Lim, Shivakumar Venkataraman, and John Wilkes, </author> <title> The TickerTAIP parallel RAID architecture, </title> <booktitle> Proceedings of the 20th Annual International Symposium of Computer Architecture, </booktitle> <month> May </month> <year> 1993, </year> <pages> 52-63. </pages>
Reference-contexts: However, there are many other related projects in the areas of striping and availability. RAID-II [Drapeau94], DataMesh [Wilkes92], and TickerTAIP <ref> [Cao93] </ref> all use RAID technology to build high-performance file servers. RAID-II uses a dedicated high-bandwidth data path between the network and the disk array to bypass the slow memory system of the server host.
Reference: [Chen90] <author> Peter M. Chen and David A. Patterson, </author> <title> Maximizing Performance in a Striped Disk Array, </title> <booktitle> Proceedings of the 17th Annual International Symposium of Computer Architecture, </booktitle> <month> May </month> <year> 1990, </year> <pages> 322-331. </pages>
Reference-contexts: This makes small writes in a RAID about four times as expensive as they would be in a disk array without parity, since they require two reads and two writes to complete. Unfortunately the best size for a striping unit appears to be tens of kilobytes or more <ref> [Chen90] </ref>, which is larger than the average file size in many environments [Baker91][Hartman93], so writes will often be smaller than a full stripe.
Reference: [Chutani92] <author> Sailesh Chutani, Owen T. Anderson, Michael L. Kazar, Bruce W. Leverett, W. Anthony Mason, and Robert N. Sidebotham, </author> <title> The Episode File System, </title> <booktitle> Proceedings of the Winter 1992 USENIX Conference, </booktitle> <month> January </month> <year> 1992, </year> <pages> 43-60. </pages>
Reference-contexts: The combination of these two techniques allows Zebra to recover quickly after crashes. It need not consider any information on disk that is older than the most recent checkpoint. Zebra is similar to other logging file systems such as LFS, Episode <ref> [Chutani92] </ref>, and the Cedar File System [Hagmann87] in this respect.
Reference: [Dibble88] <author> Peter C. Dibble, Michael L. Scott, and Carla Schlatter Ellis, </author> <title> Bridge: A High-Performance File System for Parallel Processors, </title> <booktitle> Proceedings of the 8th International Conference on Distributed Computing Systems (ICDCS), </booktitle> <year> 1988, </year> <pages> 154-161. </pages>
Reference-contexts: A striping file system offers the potential to achieve very high performance using collections of inexpensive computers and disks. Several striping file systems have already been built, such as Swift [Cabrera91] and Bridge <ref> [Dibble88] </ref>. These systems are similar in that they stripe data within individual files, so that only large files benefit from the striping. Zebra uses a different approach borrowed from log-structured file systems (LFS) [Rosenblum91]. <p> Furthermore, RADD does not stripe data; the data stored on each disk are logically independent, thus RADD does not improve the performance of individual data accesses. Several other striping file systems have been built. Most, such as sfs [LoVerso93], Bridge <ref> [Dibble88] </ref> and CFS [Pierce89], stripe across I/O nodes in a parallel computer; to our knowledge only one, Swift [Cabrera91], stripes across servers in a network file system. All of these systems use file-based striping, so they work best with large files.
Reference: [Drapeau94] <author> Ann L. Drapeau, Ken Shirriff, John H. Hartman, Ethan L. Miller, Srinivasan Seshan, Randy H. Katz, Ken Lutz, David A. Patterson, Edward K. Lee, Peter M. Chen, and Garth A. Gibson, </author> <title> RAID-II: A High-Bandwidth Network File Server, </title> <booktitle> Proceedings of the 21st Annual International Symposium of Computer Architecture, </booktitle> <month> April </month> <year> 1994, </year> <pages> 234-244. </pages>
Reference-contexts: On the DECstation 5000/200 machines used for the Zebra prototype these copies to and from the SCSI and network controllers can only proceed at about 6-8 Mbytes/second. The Berkeley RAID project has built a special-purpose memory system with a dedicated high-bandwidth path between the network and the disks <ref> [Drapeau94] </ref> but even this system can support only a few dozen disks at full speed. The fundamental problem with using a disk array to improve server bandwidth is that the server itself becomes a performance bottleneck. <p> However, there are many other related projects in the areas of striping and availability. RAID-II <ref> [Drapeau94] </ref>, DataMesh [Wilkes92], and TickerTAIP [Cao93] all use RAID technology to build high-performance file servers. RAID-II uses a dedicated high-bandwidth data path between the network and the disk array to bypass the slow memory system of the server host.
Reference: [Floyd89] <author> Richard A. Floyd and Carla S. Ellis, </author> <title> Directory Reference Patterns in Hierarchical File Systems, </title> <journal> IEEE Transactions on Knowledge and Data Engineering 1, </journal> <month> 2 (June </month> <year> 1989), </year> <pages> 238-247. </pages>
Reference-contexts: There have been several published studies of the effectiveness of name caching, and they all indicate that a relatively small directory cache can absorb a large fraction of directory accesses. A study of directory reference patterns in a time-shared UNIX system <ref> [Floyd89] </ref> found that a cache of 10 directories, occupying 14 Kbytes of space, would have a hit ratio of 85%. A hit ratio of 95% was attainable with a cache of only 30 directories requiring 41 Kbytes of memory.
Reference: [Freeh94] <author> Vincent W. Freeh, David K. Lowenthal, Gregory R. Andrews, </author> <title> Distributed Filaments: Efficient Fine-Grain Parallelism on a Cluster of Workstations, </title> <booktitle> Proceedings of the First USENIX Symposium on Operating Systems Design and Implementation (OSDI), </booktitle> <month> November </month> <year> 1994, </year> <pages> 201-213. </pages>
Reference: [Guy90] <author> Richard G. Guy, John S. Heidemann, Wai Mak, Thomas W. Page, Jr., Gerald J. Popek, and Dieter Rothmeier, </author> <title> Implementation of the Ficus Replicated File System, </title> <booktitle> Proceedings of the Summer 1990 USENIX Conference, </booktitle> <address> Anaheim, CA, </address> <month> June </month> <year> 1990, </year> <pages> 63-71. 24 </pages>
Reference-contexts: There have also been several recent research efforts to improve the availability of network file systems, such as Locus [Walker83], Coda [Satyanarayanan90], Deceit [Siegel90], Ficus <ref> [Guy90] </ref> and Harp [Liskov91]. All of these and disk, client CPU, storage server (SS) CPU, and the disk during the previous three benchmarks.
Reference: [Hagmann87] <author> Robert Hagmann, </author> <title> Reimplementing the Cedar File System Using Logging and Group Commit, </title> <booktitle> Proceedings of the 131h Symposium on Operating Systems Principles (SOSP), </booktitle> <month> November, </month> <year> 1987, </year> <pages> 155-162. </pages> <note> Published as ACM SIGOPS Operating Systems Review 21, 5. </note>
Reference-contexts: The combination of these two techniques allows Zebra to recover quickly after crashes. It need not consider any information on disk that is older than the most recent checkpoint. Zebra is similar to other logging file systems such as LFS, Episode [Chutani92], and the Cedar File System <ref> [Hagmann87] </ref> in this respect.
Reference: [Hartman93] <author> John H. Hartman and John K. Ousterhout, </author> <title> Letter to the Editor, </title> <booktitle> ACM SIGOPS Operating Systems Review 27, </booktitle> <month> 1 (January </month> <year> 1993), </year> <pages> 7-10. </pages>
Reference: [Hisgen89] <author> Andy Hisgen, Andrew Birrell, Timothy Mann, Michael Schroeder, and Garret Swart, </author> <title> Availability and Consistency Tradeoffs in the Echo Distributed File System, </title> <booktitle> Proceedings of the Second Workshop on Workstation Operating Systems, </booktitle> <month> September </month> <year> 1989, </year> <pages> 49-54. </pages>
Reference-contexts: We believe that this problem can be solved by caching naming information on clients so that the file manager need not be contacted for most opens and closes. Name caching has been used successfully in several network file systems, including AFS [Howard88], LOCUS [Walker83], and Echo <ref> [Hisgen89] </ref>. There have been several published studies of the effectiveness of name caching, and they all indicate that a relatively small directory cache can absorb a large fraction of directory accesses.
Reference: [Howard88] <author> John H. Howard, Michael L. Kazar, Sherri G. Menees, David A. Nichols, M. Satyanarayanan, Robert N. Sidebotham, and Michael J. West, </author> <title> Scale and Performance in a Distributed File System, </title> <journal> ACM Transactions on Computer Systems 6, </journal> <month> 1 (February </month> <year> 1988), </year> <pages> 51-81. </pages>
Reference-contexts: We believe that this problem can be solved by caching naming information on clients so that the file manager need not be contacted for most opens and closes. Name caching has been used successfully in several network file systems, including AFS <ref> [Howard88] </ref>, LOCUS [Walker83], and Echo [Hisgen89]. There have been several published studies of the effectiveness of name caching, and they all indicate that a relatively small directory cache can absorb a large fraction of directory accesses.
Reference: [Liskov91] <author> Barbara Liskov, Sanjay Ghemawat, Robert Gruber, Paul Johnson, Liuba Shrira, and Michael Williams, </author> <title> Replication in the Harp File System, </title> <booktitle> Proceedings of the 13th Symposium on Operating Systems Principles (SOSP), Asilomar, </booktitle> <address> CA, </address> <month> October </month> <year> 1991, </year> <pages> 226-238. </pages> <note> Published as ACM SIGOPS Operating Systems Review 25, 5. </note>
Reference-contexts: There have also been several recent research efforts to improve the availability of network file systems, such as Locus [Walker83], Coda [Satyanarayanan90], Deceit [Siegel90], Ficus [Guy90] and Harp <ref> [Liskov91] </ref>. All of these and disk, client CPU, storage server (SS) CPU, and the disk during the previous three benchmarks.
Reference: [Long94] <author> Darrell D. E. Long, Bruce R. Montague, and Luis-Felipe Cabrera, Swift/RAID: </author> <title> A Distributed RAID System, </title> <booktitle> Computing Systems 7, 3 (Summer 1994), </booktitle> <pages> 333-359. </pages>
Reference-contexts: The Swift prototype has recently been reimplemented to incorporate the reliability mechanisms described in the Swift architecture <ref> [Long94] </ref>. The prototype can now support a variety of parity organizations. Measurements show that the parity computation incurs a significant overhead, so that the performance of a five-server system with parity enabled is only 53% of the original Swift prototype with the same number of servers.
Reference: [Lo Verso93] <author> Susan J. Lo Verso, Marshall Isman, Andy Nanopoulos, William Nesheim, Ewan D. Milne, and Richard Wheeler, sfs: </author> <title> A Parallel File System for the CM-5, </title> <booktitle> Proceedings of the Summer 1993 USENIX Conference, </booktitle> <address> Cincinnati, OH, </address> <month> June </month> <year> 1993, </year> <pages> 291-305. </pages>
Reference: [McKusick84] <author> Marshall K. McKusick, William N. Joy, Samuel J. Leffler, and Robert S. Fabry, </author> <title> A Fast File System for UNIX, </title> <journal> ACM Transactions on Computer Systems 2, </journal> <month> 3 (August </month> <year> 1984), </year> <pages> 181-197. </pages>
Reference-contexts: Zebra is similar to other logging file systems such as LFS, Episode [Chutani92], and the Cedar File System [Hagmann87] in this respect. In contrast, file systems without logs, such as the BSD Fast File System <ref> [McKusick84] </ref>, cannot tell which portions of the disk were being modified at the time of a crash, so they must re-scan all of the metadata in the entire file system during recovery. 6.1 Internal Stripe Consistency When a client crashes it is possible for fragments to be missing from stripes that <p> Zebra borrows its log structure from LFS [Rosenblum91], a high-performance write-optimized file system. A recent paper by Seltzer et. al [Seltzer93] has shown that adding extents to FFS <ref> [McKusick84] </ref> results in a file system (called EFS [McVoy91]) that has comparable performance to LFS on large reads and writes.
Reference: [McVoy91] <author> Larry W. McVoy and Steve R. Kleiman, </author> <title> Extent-like Performance from a UNIX File System, </title> <booktitle> Proceedings of the Winter 1991 USENIX Conference, </booktitle> <address> Dallas, TX, </address> <month> January </month> <year> 1991, </year> <pages> 33-43. </pages>
Reference-contexts: Zebra borrows its log structure from LFS [Rosenblum91], a high-performance write-optimized file system. A recent paper by Seltzer et. al [Seltzer93] has shown that adding extents to FFS [McKusick84] results in a file system (called EFS <ref> [McVoy91] </ref>) that has comparable performance to LFS on large reads and writes. However, EFS does not improve performance for small files as does LFS and therefore Zebra, nor does it address the parity and striping issues presented by a striped network file system.
Reference: [Nelson88] <author> Michael N. Nelson, Brent B. Welch, and John K. Ousterhout, </author> <title> Caching in the Sprite Network File System, </title> <journal> ACM Transactions on Computer Systems 6, </journal> <month> 1 (February </month> <year> 1988), </year> <pages> 134-154. </pages>
Reference-contexts: We chose to use the Sprite approach to consistency, which involves ushing or disabling caches when files are opened <ref> [Nelson88] </ref>, because it was readily available, but any other approach could have been used as well. The only changes for Zebra occur when a client ushes a file from its cache. <p> The file manager must also recover the information that it uses to ensure client cache consistency; for this Zebra uses the same approach as in Sprite, which is to let clients reopen their files to rebuild the client cache consistency state <ref> [Nelson88] </ref>. If a client crashes then the file manager cleans up its data structures by closing all of the clients open files, also in the same manner as Sprite. However, Zebra introduces three consistency problems that are not present in other file systems.
Reference: [Ousterhout88] <author> John Ousterhout, Andrew Cherenson, Fred Douglis, Mike Nelson, and Brent Welch, </author> <title> The Sprite Network Operating System, </title> <booktitle> IEEE Computer 21, </booktitle> <month> 2 (February </month> <year> 1988), </year> <pages> 23-36. </pages>
Reference-contexts: Zebra can continue operation while a server is unavailable. Even if a server is totally destroyed Zebra can reconstruct the lost data. We have constructed a prototype implementation of Zebra as part of the Sprite operating system <ref> [Ousterhout88] </ref>. Although it does not incorporate all of the reliability and recovery aspects of the Zebra architecture, it does 2 demonstrate the performance benefits.
Reference: [Ousterhout95] <author> John Ousterhout, </author> <note> A Critique of Seltzers 1993 USENIX Paper. Available as http://www.smli.com/~ouster/seltzer93.html. </note>
Reference-contexts: In a transaction-processing benchmark on a nearly full disk Seltzer found that cleaning accounted for 60-80% of all write traffic and significantly affected system throughput [Seltzer93]. Unfortunately, that study was unable to fully account for the surprisingly poor LFS performance <ref> [Ousterhout95] </ref>, leading to the publication of a more extensive study [Seltzer95]. The new study found that LFS performance on a transaction-processing benchmark was at most 10% worse than FFS. The reasons for the LFS performance degradation are still not fully explicable, indicating that further study is warranted.
Reference: [Patterson88] <author> David A. Patterson, Garth Gibson, and Randy H. Katz, </author> <title> A Case for Redundant Arrays of Inexpensive Disks (RAID), </title> <booktitle> Proceedings of the 1988 ACM Conference on Management of Data (SIGMOD), </booktitle> <address> Chicago, IL, </address> <month> June </month> <year> 1988, </year> <pages> 109-116. </pages>
Reference-contexts: It also reduces network overhead, simplifies the storage servers, and spreads write traffic uniformly across the servers. Zebras style of striping also makes it easy to use redundancy techniques from RAID disk arrays to improve availability and data integrity <ref> [Patterson88] </ref>. One of the fragments of each stripe stores parity for the rest of the stripe, allowing the stripes data to be reconstructed in the event of a disk or server failure. Zebra can continue operation while a server is unavailable. <p> To do this Zebra borrows from two recent innovations in the management of disk storage systems: RAID technology (Redundant Arrays of Inexpensive Disks) <ref> [Patterson88] </ref>, and log-structured file systems (LFS) [Rosenblum91]. RAID technology allows Zebra to provide scalable file access performance while using parity instead of redundant copies to guard against server failures. <p> a client crash the file manager closes all the open files on the client, thus allowing those files to be cached by other clients. 7.2 Storage Server Crashes Zebras parity mechanism allows it to tolerate the failure of a single storage server using algorithms similar to those described for RAIDs <ref> [Patterson88] </ref>. To read a file while a storage server is down, a client must reconstruct any stripe fragment that was stored on the down server. This is done by computing the parity of all the other fragments in the same stripe; the result is the missing fragment.
Reference: [Pierce89] <author> Paul Pierce, </author> <title> A Concurrent File System for a Highly Parallel Mass Storage Subsystem, </title> <booktitle> Proceedings of the Fourth Conference on Hypercubes, </booktitle> <address> Monterey CA, </address> <month> March </month> <year> 1989, </year> <pages> 155-160. </pages>
Reference-contexts: Furthermore, RADD does not stripe data; the data stored on each disk are logically independent, thus RADD does not improve the performance of individual data accesses. Several other striping file systems have been built. Most, such as sfs [LoVerso93], Bridge [Dibble88] and CFS <ref> [Pierce89] </ref>, stripe across I/O nodes in a parallel computer; to our knowledge only one, Swift [Cabrera91], stripes across servers in a network file system. All of these systems use file-based striping, so they work best with large files.
Reference: [Rosenblum91] <author> Mendel Rosenblum and John K. Ousterhout, </author> <title> The Design and Implementation of a Log-Structured File System, </title> <booktitle> Proceedings of the 13th Symposium on Operating Systems Principles (SOSP), Asilomar, </booktitle> <address> CA, </address> <month> October </month> <year> 1991, </year> <pages> 1-15. </pages> <note> Published as ACM SIGOPS Operating Systems Review 25, 5. </note>
Reference-contexts: Several striping file systems have already been built, such as Swift [Cabrera91] and Bridge [Dibble88]. These systems are similar in that they stripe data within individual files, so that only large files benefit from the striping. Zebra uses a different approach borrowed from log-structured file systems (LFS) <ref> [Rosenblum91] </ref>. Each client forms its new data for all files into a sequential log that it stripes across the storage servers. <p> To do this Zebra borrows from two recent innovations in the management of disk storage systems: RAID technology (Redundant Arrays of Inexpensive Disks) [Patterson88], and log-structured file systems (LFS) <ref> [Rosenblum91] </ref>. RAID technology allows Zebra to provide scalable file access performance while using parity instead of redundant copies to guard against server failures. <p> File File Servers Parity (a) File File Servers Parity (b) 5 There exist protocols for ensuring that two writes to two different file servers are carried out atomically [Bernstein81] but they are complex and expensive. 3.3 Log-Structured File Systems and Log-Based Striping Zebra uses techniques from log-structured file systems (LFS) <ref> [Rosenblum91] </ref> to avoid the problems of file-based striping. LFS is a disk management technique that treats the disk like an append-only log. <p> If there are no empty stripes and more free space is needed then the cleaner chooses one or more stripes to clean. The policy it uses for this is identical to the one described by Rosenblum <ref> [Rosenblum91] </ref>, i.e. a cost-benefit analysis is done for each stripe, which considers both the amount of live data in the stripe and the age of the data. There are two issues in cleaning a stripe: identifying the live blocks, and copying them to a new stripe. <p> Seltzer found LFS cleaning costs to be negligible on a software development benchmark [Seltzer93]. Rosenblum measured production usage of LFS on Sprite for several months and found that only 2-7% of the data in stripes that were cleaned were live and needed to be copied <ref> [Rosenblum91] </ref>. <p> Since the file manager uses the same disk structures as a non-striped file system, it can also use the same recovery mechanism. In the Zebra prototype the metadata is stored in a log-structured file system, so we use the LFS recovery mechanism described by Rosenblum <ref> [Rosenblum91] </ref>. The file manager must also recover the information that it uses to ensure client cache consistency; for this Zebra uses the same approach as in Sprite, which is to let clients reopen their files to rebuild the client cache consistency state [Nelson88]. <p> The idea is to reboot the file server so quickly that file service is not interrupted. This alternative does not require redundant copies or parity, but neither does it allow the system to continue operation in the event of a hardware failure. Zebra borrows its log structure from LFS <ref> [Rosenblum91] </ref>, a high-performance write-optimized file system. A recent paper by Seltzer et. al [Seltzer93] has shown that adding extents to FFS [McKusick84] results in a file system (called EFS [McVoy91]) that has comparable performance to LFS on large reads and writes.
Reference: [Satyanarayanan90] <author> Mahadev Satyanarayanan, James J. Kistler, Puneet Kumar, Maria E. Okasaki, E. H. Siegel, and D. C. Steere, Coda: </author> <title> a highly available file system for a distributed workstation environment, </title> <journal> IEEE Transactions on Computers 39, </journal> <month> 4 (April </month> <year> 1990), </year> <pages> 447-459. </pages>
Reference-contexts: There have also been several recent research efforts to improve the availability of network file systems, such as Locus [Walker83], Coda <ref> [Satyanarayanan90] </ref>, Deceit [Siegel90], Ficus [Guy90] and Harp [Liskov91]. All of these and disk, client CPU, storage server (SS) CPU, and the disk during the previous three benchmarks.
Reference: [Schloss90] <author> Gary A. Schloss and Michael Stonebraker, </author> <title> Highly Redundant Management of Distributed Data, </title> <booktitle> Proceedings of the IEEE Workshop on the Management of Replicated Data, </booktitle> <month> November </month> <year> 1990, </year> <pages> 91-95. </pages>
Reference-contexts: In all of these systems the striping is internal to the server, whereas in Zebra the clients participate in striping files. RADD (Redundant Array of Distributed Disks) <ref> [Schloss90] </ref> is similar to RAID in that it uses parity to withstand the loss of a disk, but it differs by separating the disks geographically to decrease the likelihood of losing multiple disks.
Reference: [Seltzer93] <author> Margo Seltzer, Keith Bostic, Marshall Kirk McKusick, and Carl Staelin, </author> <title> An Implementation of a Log-Structured File System for UNIX, </title> <booktitle> Proceedings of the Winter 1993 USENIX Conference, </booktitle> <address> San Diego, CA, </address> <month> January </month> <year> 1993, </year> <pages> 307-326. </pages>
Reference-contexts: In a transaction-processing benchmark on a nearly full disk Seltzer found that cleaning accounted for 60-80% of all write traffic and significantly affected system throughput <ref> [Seltzer93] </ref>. Unfortunately, that study was unable to fully account for the surprisingly poor LFS performance [Ousterhout95], leading to the publication of a more extensive study [Seltzer95]. The new study found that LFS performance on a transaction-processing benchmark was at most 10% worse than FFS. <p> Despite the controversy surrounding LFS performance on transaction-processing workloads, several studies have shown the cleaning cost to be minimal on more typical workstation workloads. Seltzer found LFS cleaning costs to be negligible on a software development benchmark <ref> [Seltzer93] </ref>. Rosenblum measured production usage of LFS on Sprite for several months and found that only 2-7% of the data in stripes that were cleaned were live and needed to be copied [Rosenblum91]. <p> Unfortunately, this produced lock convoys that effectively halted all normal file accesses during cleaning and resulted in significant pauses. Zebras stripe cleaner uses an optimistic approach similar to that of Seltzer et al. <ref> [Seltzer93] </ref>. It doesnt lock any files during cleaning or invoke any cache consistency actions. Instead the stripe cleaner just copies the block and issues a cleaner delta, assuming optimistically that its information about the block is correct and the block hasnt been updated recently. <p> This alternative does not require redundant copies or parity, but neither does it allow the system to continue operation in the event of a hardware failure. Zebra borrows its log structure from LFS [Rosenblum91], a high-performance write-optimized file system. A recent paper by Seltzer et. al <ref> [Seltzer93] </ref> has shown that adding extents to FFS [McKusick84] results in a file system (called EFS [McVoy91]) that has comparable performance to LFS on large reads and writes.
Reference: [Seltzer95] <author> Margo Seltzer, Keith A. Smith, Hari Balakrishnan, Jacqueline Chang, Sara McMains, and Venkata Padmanabhan, </author> <title> File System Logging Versus Clustering: A Performance Comparison, </title> <booktitle> Proceedings of the Winter 1995 USENIX Conference, </booktitle> <month> January </month> <year> 1995, </year> <pages> 249-264. </pages>
Reference-contexts: Unfortunately, that study was unable to fully account for the surprisingly poor LFS performance [Ousterhout95], leading to the publication of a more extensive study <ref> [Seltzer95] </ref>. The new study found that LFS performance on a transaction-processing benchmark was at most 10% worse than FFS. The reasons for the LFS performance degradation are still not fully explicable, indicating that further study is warranted.
Reference: [Sheltzer86] <author> Alan B. Sheltzer, Robert Lindell, and Gerald J. Popek, </author> <title> Name Service Locality and Cache Design in a Distributed Operating System, </title> <booktitle> Proceedings of the 6th International Conference on Distributed Computing Systems (ICDCS), </booktitle> <month> May </month> <year> 1986, </year> <pages> 515-522. </pages>
Reference-contexts: A hit ratio of 95% was attainable with a cache of only 30 directories requiring 41 Kbytes of memory. Sheltzer et al. <ref> [Sheltzer86] </ref> found that in the LOCUS network file system a 40-directory cache produced a hit ratio of 87% to 96%.
Reference: [Shirriff92] <author> Ken Shirriff and John Ousterhout, </author> <title> A Trace-driven Analysis of Name and Attribute Caching in a Distributed File System, </title> <booktitle> Proceedings of the Winter 1992 USENIX Conference, </booktitle> <month> January </month> <year> 1992, </year> <pages> 315-331. </pages>
Reference-contexts: Sheltzer et al. [Sheltzer86] found that in the LOCUS network file system a 40-directory cache produced a hit ratio of 87% to 96%. A more recent study of directory reference patterns in a network file system by Shirriff <ref> [Shirriff92] </ref> found that a 10-directory cache had a hit ratio of 91%, while a 20-directory cache had a hit ratio 8 of 97%.
Reference: [Siegel90] <author> Alex Siegel, Kenneth Birman, and Keith Marzullo, Deceit: </author> <title> A Flexible Distributed File System, </title> <booktitle> Proceedings of the Summer 1990 USENIX Conference, </booktitle> <address> Anaheim, CA, </address> <month> June </month> <year> 1990, </year> <pages> 51-61. 25 </pages>
Reference-contexts: There have also been several recent research efforts to improve the availability of network file systems, such as Locus [Walker83], Coda [Satyanarayanan90], Deceit <ref> [Siegel90] </ref>, Ficus [Guy90] and Harp [Liskov91]. All of these and disk, client CPU, storage server (SS) CPU, and the disk during the previous three benchmarks.
Reference: [Walker83] <author> Bruce Walker, Gerald Popek, Robert English, Charles Kline, and Greg Thiel, </author> <title> The LOCUS Distributed Operating System, </title> <booktitle> Proceedings of the 9th Symposium on Operating Systems Principles (SOSP), </booktitle> <month> November </month> <year> 1983, </year> <pages> 49-70. </pages> <note> Published as ACM SIGOPS Operating Systems Review 17, 5. </note>
Reference-contexts: We believe that this problem can be solved by caching naming information on clients so that the file manager need not be contacted for most opens and closes. Name caching has been used successfully in several network file systems, including AFS [Howard88], LOCUS <ref> [Walker83] </ref>, and Echo [Hisgen89]. There have been several published studies of the effectiveness of name caching, and they all indicate that a relatively small directory cache can absorb a large fraction of directory accesses. <p> There have also been several recent research efforts to improve the availability of network file systems, such as Locus <ref> [Walker83] </ref>, Coda [Satyanarayanan90], Deceit [Siegel90], Ficus [Guy90] and Harp [Liskov91]. All of these and disk, client CPU, storage server (SS) CPU, and the disk during the previous three benchmarks.
Reference: [Wilkes92] <author> John Wilkes, </author> <title> DataMesh research project, phase 1, </title> <booktitle> Proceedings of the USENIX File Systems Workshop, </booktitle> <month> May </month> <year> 1992, </year> <pages> 63-69. </pages>
Reference-contexts: However, there are many other related projects in the areas of striping and availability. RAID-II [Drapeau94], DataMesh <ref> [Wilkes92] </ref>, and TickerTAIP [Cao93] all use RAID technology to build high-performance file servers. RAID-II uses a dedicated high-bandwidth data path between the network and the disk array to bypass the slow memory system of the server host.
References-found: 39

