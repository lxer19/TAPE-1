URL: http://www.cs.rice.edu/~willy/papers/ieeep97b.ps.gz
Refering-URL: http://www.cs.rice.edu/~willy/TreadMarks/papers.html
Root-URL: 
Title: Combining Compile-Time and Run-Time Support for Efficient Software Distributed Shared Memory  
Author: Sandhya Dwarkadas Honghui Lu Alan L. Cox Ramakrishnan Rajamony and Willy Zwaenepoel 
Affiliation: Department of Computer Science, University of Rochester Department of Electrical and Computer Engineering, Rice University Department of Computer Science, Rice University  
Abstract: We describe an integrated compile-time and run-time system for efficient shared memory parallel computing on distributed memory machines. The combined system presents the user with a shared memory programming model, with its well-known benefits in terms of ease of use. The run-time system implements a consistent shared memory abstraction using memory access detection and automatic data caching. The compiler improves the efficiency of the shared memory implementation by directing the run-time system to exploit the message passing capabilities of the underlying hardware. To do so, the compiler analyzes shared memory accesses, and transforms the code to insert calls to the run-time system that provide it with the access information computed by the compiler. The run-time system is augmented with the appropriate entry points to use this information to implement bulk data transfer and to reduce the overhead of run-time consistency maintenance. In those cases where the compiler analysis succeeds for the entire program, we demonstrate that the combined system achieves performance comparable to that produced by compilers that directly target message passing. If the compiler analysis is successful only for parts of the program, for instance, because of irregular accesses to some of the arrays, the resulting optimizations can be applied to those parts for which the analysis succeeds. If the compiler analysis fails entirely, we rely on the run-time's maintenance of shared memory, and thereby avoid the complexity and the limitations of compilers that directly target message passing. The result is a single system that combines efficient support for both regular and irregular memory access patterns. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Agarwal and J. Saltz. </author> <title> Interprocedural compilation of irregular applications for distributed memory machines. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <month> December </month> <year> 1995. </year>
Reference: [2] <author> C. Amza, A.L. Cox, S. Dwarkadas, P. Keleher, H. Lu, R. Raja mony, and W. Zwaenepoel. TreadMarks: </author> <title> Shared memory computing on networks of workstations. </title> <journal> IEEE Computer, </journal> <volume> 29(2) </volume> <pages> 18-28, </pages> <month> February </month> <year> 1996. </year>
Reference: [3] <author> Applied Parallel Research. </author> <title> FORGE High Performance Fortran User's Guide, </title> <note> version 2.0 edition. </note>
Reference-contexts: The compiler-optimized TreadMarks program executing with the augmented TreadMarks run-time system 2 All measurements for Tomcatv and Grid were made on 120 MHz thin nodes. Opt-Tmk. 3. A message passing version automatically generated by the Forge XHPF compiler <ref> [3] </ref> from Applied Parallel Research, Inc. (APR) - XHPF. The results for the XHPF compiler are provided in order to compare performance against a commercial parallelizing compiler for data-parallel programs. for the compiler-optimized TreadMarks version reflect the gains achieved by the most sophisticated level of analysis possible for each application.
Reference: [4] <author> D. Bailey, J. Barton, T. Lasinski, and H. Simon. </author> <title> The NAS par allel benchmarks. </title> <type> Technical Report 103863, </type> <institution> NASA, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: Our aim is to compare performance against state-of-the-art compiler techniques currently available to optimize performance for these types of applications. A. Overall Results for Regular Applications We used eight Fortran programs: IS and 3D-FFT from the NAS benchmark suite <ref> [4] </ref>, the Shallow benchmark from the National Center for Atmospheric Research, Tomcatv from the SPEC benchmark suite [9], Grid from Applied Parallel Research, Inc., and Jacobi, Gauss, and Modified Gramm-Schmidt (MGS), three locally developed benchmarks.
Reference: [5] <author> B.R. Brooks, R.E. Bruccoleri, B.D. Olafson, D.J. States, S. Swaminathan, and M. Karplus. Charmm: </author> <title> A program for macromolecular energy, minimization, and dynamics calculations. </title> <journal> Journal of Computational Chemistry, </journal> <volume> 4:187, </volume> <year> 1983. </year>
Reference-contexts: However, our methods can also be applied to applications for which the analysis cannot be made precise, or for which only some phases can be analyzed. E.2 Moldyn Moldyn is a molecular dynamics simulation. Its computational structure resembles the non-bonded force calculation in CHARMM <ref> [5] </ref>, which is a well-known molecular dynamics code used at NIH to model macromolecu-lar systems. Non-bonded forces are long-range interactions existing between each pair of molecules. CHARMM approximates the non-bonded calculation by ignoring all pairs which are beyond a certain cutoff radius. <p> Our intent in presenting the CHAOS performance numbers is to compare performance with state-of-the-art compiler technology for irregular applications. The compiler-optimized TreadMarks programs include optimizations for both regular and irregular access patterns. programs, Moldyn from CHARMM <ref> [5] </ref> and NBF from the GROMOS benchmark [12], both molecular dynamics simulation kernels. Table I presents the sequential execution time and data set sizes used. In the case of Moldyn, we vary the frequency with which the indirection array is recomputed.
Reference: [6] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Techniques for reducing consistency-related information in distributed shared memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 13(3) </volume> <pages> 205-243, </pages> <month> August </month> <year> 1995. </year>
Reference: [7] <author> S. Chandra and J. R. Larus. </author> <title> Optimizing communication in hpf programs for fine-grain distributed shared memory. </title> <booktitle> In Proceedings of the 6th Symposium on the Principles and Practice of Parallel Programming, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: Keleher and Tseng [18] describe a run-time interface and compile-time system that couples the compiler and the run-time in a manner similar to our system. Their interface and implementation are, however, more run-time intensive. Chandra and Larus <ref> [7] </ref> also describe a combined compiler and run-time system that is similar in spirit to our system, but in the context of fine-grained software shared memory. VI. Conclusion We have described an integrated compile-time/run-time approach for executing regular and irregular computations on distributed memory machines.
Reference: [8] <author> R. Das, P. Havlak, J. Saltz, and K. Kennedy. </author> <title> Index array flat tening through program transformation. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <month> December </month> <year> 1995. </year>
Reference: [9] <author> K. M. Dixit. </author> <title> The spec benchmarks. </title> <booktitle> Parallel Computing, </booktitle> <pages> pages 1195-1209, </pages> <year> 1991. </year>
Reference-contexts: A. Overall Results for Regular Applications We used eight Fortran programs: IS and 3D-FFT from the NAS benchmark suite [4], the Shallow benchmark from the National Center for Atmospheric Research, Tomcatv from the SPEC benchmark suite <ref> [9] </ref>, Grid from Applied Parallel Research, Inc., and Jacobi, Gauss, and Modified Gramm-Schmidt (MGS), three locally developed benchmarks. For each application, we use two data set sizes to illustrate any effects from changing the computation to communication ratio, as well as due to false sharing.
Reference: [10] <author> S. Dwarkadas, A.L. Cox, and W. Zwaenepoel. </author> <title> An integrated compile-time/run-time software distributed shared memory system. </title> <booktitle> In Proceedings of the 7th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> Octo-ber </month> <year> 1996. </year>
Reference: [11] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference: [12] <author> W.F. van Gunsteren and H.J.C. Berendsen. GROMOS: </author> <title> GROningen MOlecular Simulation software. </title> <type> Technical report, </type> <institution> Laboratory of Physical Chemistry, University of Groningen, </institution> <year> 1988. </year>
Reference-contexts: Our intent in presenting the CHAOS performance numbers is to compare performance with state-of-the-art compiler technology for irregular applications. The compiler-optimized TreadMarks programs include optimizations for both regular and irregular access patterns. programs, Moldyn from CHARMM [5] and NBF from the GROMOS benchmark <ref> [12] </ref>, both molecular dynamics simulation kernels. Table I presents the sequential execution time and data set sizes used. In the case of Moldyn, we vary the frequency with which the indirection array is recomputed. In the case of NBF, we vary the data set size to introduce false sharing.
Reference: [13] <author> P. Havlak and K. Kennedy. </author> <title> An implementation of interproce dural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-360, </pages> <month> July </month> <year> 1991. </year>
Reference: [14] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference: [15] <author> S. Ioannidis and S. Dwarkadas. </author> <title> Compiler and run-time support for adaptive load balancing in software distributed shared memory systems. </title> <booktitle> In Fourth Workshop on Languages, Compilers, and Run-time Systems for Scalable Computers, </booktitle> <month> May </month> <year> 1998. </year>
Reference-contexts: This information can be utilized by the run-time, not only to optimize communication, but also to balance load <ref> [15] </ref>. Several recent proposals for hardware shared memory machines include a message passing subsystem designed in part to allow applications to take advantage of bulk data transfer [20], [21]. Woo et al. [33] evaluate one such design in the context of the Flash system.
Reference: [16] <author> T.E. Jeremiassen and S. Eggers. </author> <title> Computing per-process sum mary side-effect information. </title> <editor> In U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Fifth Workshop on Languages and Compilers for Parallelism, </booktitle> <pages> pages 175-191, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Their prefetching strategy involves fetching data in advance of synchronization operations. Our strategy involves leveraging the program synchronization in order to reduce redundant messages, as well as eliminating consistency overhead where possible. Jeremiassen et al. <ref> [16] </ref> present a static algorithm for computing per-process memory references to shared data in coarse-grained parallel programs. We use a similar analysis in terms of processor identifiers in order to replace a barrier with a Push.
Reference: [17] <author> P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Lazy release consis tency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference: [18] <author> P. Keleher and C. Tseng. </author> <title> Enhancing software DSM for compiler parallelized applications. </title> <booktitle> In Proceedings of the 11th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1997. </year>
Reference-contexts: They conclude that TSM is not competitive with CHAOS, while XSM achieves performance comparable to CHAOS after introducing several special-purpose protocols. In our work, we use a fairly straight-forward compiler to optimize the shared memory programs, rather than relying on hand-coded special-purpose protocols. Keleher and Tseng <ref> [18] </ref> describe a run-time interface and compile-time system that couples the compiler and the run-time in a manner similar to our system. Their interface and implementation are, however, more run-time intensive.
Reference: [19] <author> K. Kennedy, K. S. McKinley, and C. Tseng. </author> <title> Analysis and trans formation in an interactive parallel programming tool. </title> <journal> Concur-rency: Practice and Experience, </journal> <volume> 5(7), </volume> <month> October </month> <year> 1993. </year>
Reference: [20] <author> D. Kranz, K. Johnson, A. Agarwal, J. Kubiatowicz, and B. Lim. </author> <title> Integrating message-passing and shared-memory: Early experience. </title> <booktitle> In Proceedings of the 1993 Conference on the Principles and Practice of Parallel Programming, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: This information can be utilized by the run-time, not only to optimize communication, but also to balance load [15]. Several recent proposals for hardware shared memory machines include a message passing subsystem designed in part to allow applications to take advantage of bulk data transfer <ref> [20] </ref>, [21]. Woo et al. [33] evaluate one such design in the context of the Flash system.
Reference: [21] <author> J. Kuskin and D. Ofelt et al. </author> <title> The Stanford FLASH multiproces sor. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: This information can be utilized by the run-time, not only to optimize communication, but also to balance load [15]. Several recent proposals for hardware shared memory machines include a message passing subsystem designed in part to allow applications to take advantage of bulk data transfer [20], <ref> [21] </ref>. Woo et al. [33] evaluate one such design in the context of the Flash system.
Reference: [22] <author> K. Li and P. Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference: [23] <author> H. Lu, A.L. Cox, S. Dwarkadas, R. Rajamony, and W. Zwaenepoel. </author> <title> Compiler and software distributed shared memory support for irregular applications. </title> <booktitle> In Proceedings of the 6th Symposium on the Principles and Practice of Parallel Programming, </booktitle> <pages> pages 48-56, </pages> <month> June </month> <year> 1997. </year>
Reference: [24] <author> H. Lu, S. Dwarkadas, A.L. Cox, and W. Zwaenepoel. </author> <title> Message passing versus distributed shared memory on networks of workstations. </title> <booktitle> In Proceedings SuperComputing '95, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: IS has a migratory access pattern. The use of diffs in TreadMarks results in extra data communicated due to the diff accumulation <ref> [24] </ref> problem that of multiple overlapping diffs being communicated due to multiple processors successively modifying the same data. With the compiler-based directives, this overhead can be eliminated.
Reference: [25] <author> T.C. Mowry, C.Q.C. Chan, and A.K.W. Lo. </author> <title> Comparative eval uation of latency tolerance techniques for software distributed shared memory. </title> <booktitle> In Proceedings of the Fourth High Performance Computer Architecture Symposium, </booktitle> <month> February </month> <year> 1998. </year>
Reference-contexts: The same access pattern information can be used in a hardware shared memory environment to exploit the bulk transfer features. The information can also be used for optimal page placement and re-mapping in machines such as the Origin-2000. V. Related Work Mowry et al. <ref> [25] </ref> examine the effect of combining prefetching and multithreading in a software DSM system. Their prefetching strategy involves fetching data in advance of synchronization operations. Our strategy involves leveraging the program synchronization in order to reduce redundant messages, as well as eliminating consistency overhead where possible.
Reference: [26] <author> S.S. Mukherjee, S.D. Sharma, M.D. Hill, J.R. Larus, A. Rogers, and J. Saltz. </author> <title> Efficient support for irregular applications on distributed memory machines. </title> <booktitle> In Proceedings of the 5th ACM Symposium on the Principles and Practice of Parallel Programming, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: Jeremiassen et al. [16] present a static algorithm for computing per-process memory references to shared data in coarse-grained parallel programs. We use a similar analysis in terms of processor identifiers in order to replace a barrier with a Push. Mukherjee et al. <ref> [26] </ref> compare the CHAOS inspector-executor system to the TSM (transparent shared memory) and the XSM (extendible shared memory) systems, both implemented on the Tempest interface [27]. They conclude that TSM is not competitive with CHAOS, while XSM achieves performance comparable to CHAOS after introducing several special-purpose protocols.
Reference: [27] <author> Steven K. Reinhardt, James R. Larus, and David A. Wood. Tem pest and typhoon: </author> <title> User-level shared memory. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: We use a similar analysis in terms of processor identifiers in order to replace a barrier with a Push. Mukherjee et al. [26] compare the CHAOS inspector-executor system to the TSM (transparent shared memory) and the XSM (extendible shared memory) systems, both implemented on the Tempest interface <ref> [27] </ref>. They conclude that TSM is not competitive with CHAOS, while XSM achieves performance comparable to CHAOS after introducing several special-purpose protocols. In our work, we use a fairly straight-forward compiler to optimize the shared memory programs, rather than relying on hand-coded special-purpose protocols.
Reference: [28] <author> D.J. Scales, K. Gharachorloo, and C.A. Thekkath. </author> <title> Shasta: A low overhead, software-only approach for supporting fine-grain shared memory. </title> <booktitle> In Proceedings of the 7th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 174-185, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: IV. Applicability to Other Platforms While the experimental results presented here are specific to the TreadMarks SDSM system, the techniques described generalize to other SDSM systems such as Cashmere [30], home-based lazy release consistency (HLRC) [34], or Shasta <ref> [28] </ref>. In these systems, each coherence unit has a home where modifications are collected or where directory information is maintained.
Reference: [29] <author> S. D. Sharma, R. Ponnusamy, B. Moon, Y. Hwang, R. Das, and J. Saltz. </author> <title> Run-time and compile-time support for adaptive irregular problems. </title> <booktitle> In SuperComputing, </booktitle> <year> 1994. </year>
Reference-contexts: B. Overall Results for Irregular Applications In the case of the irregular applications, we compare the compiler-optimized TreadMarks programs (Opt-Tmk) with the hand-coded CHAOS (inspector-executor based <ref> [29] </ref>) programs (CHAOS), as well as the base TreadMarks programs (Tmk). Our intent in presenting the CHAOS performance numbers is to compare performance with state-of-the-art compiler technology for irregular applications.
Reference: [30] <author> R. Stets, S. Dwarkadas, N. Hardavellas, G. Hunt, L. Kon tothanassis, S. Parthasarathy, and M.L. Scott. Cashmere-2l: </author> <title> Software coherent shared memory on a clustered remote-write network. </title> <booktitle> In Proceedings of the 16th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 170-183, </pages> <month> October </month> <year> 1997. </year>
Reference-contexts: Its disadvantage is the potential for false sharing overhead when the data set is small or has poor spatial locality. IV. Applicability to Other Platforms While the experimental results presented here are specific to the TreadMarks SDSM system, the techniques described generalize to other SDSM systems such as Cashmere <ref> [30] </ref>, home-based lazy release consistency (HLRC) [34], or Shasta [28]. In these systems, each coherence unit has a home where modifications are collected or where directory information is maintained.
Reference: [31] <author> R. von Hanxleden and K. Kennedy. </author> <title> Give-N-Take a balanced code placement framework. </title> <booktitle> In Proceedings of the ACM SIG-PLAN 91 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1994. </year>
Reference: [32] <author> R. von Hanxleden, K. Kennedy, C. Koelbel, R. Das, and J. Saltz. </author> <title> Compiler analysis for irregular problems in Fortran D. </title> <booktitle> In Proceedings of the 5th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1992. </year>
Reference: [33] <author> S.C. Woo, J.P. Singh, and J.L. Hennessy. </author> <title> The performance advantages of integrating block data transfer in cache-coherent multiprocessors. </title> <booktitle> In Proceedings of the 6th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 219-231, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Several recent proposals for hardware shared memory machines include a message passing subsystem designed in part to allow applications to take advantage of bulk data transfer [20], [21]. Woo et al. <ref> [33] </ref> evaluate one such design in the context of the Flash system.
Reference: [34] <author> Y. Zhou, L. Iftode, and J.P. Singh. </author> <title> Performance evaluation of two home-based lazy release consistency protocols for shared virtual memory systems. </title> <booktitle> In Proceedings of the Second USENIX Symposium on Operating System Design and Implementation, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: IV. Applicability to Other Platforms While the experimental results presented here are specific to the TreadMarks SDSM system, the techniques described generalize to other SDSM systems such as Cashmere [30], home-based lazy release consistency (HLRC) <ref> [34] </ref>, or Shasta [28]. In these systems, each coherence unit has a home where modifications are collected or where directory information is maintained.
References-found: 34

