URL: http://www-unix.mcs.anl.gov/prism/lib/techsrc/wn6.ps.Z
Refering-URL: http://www-unix.mcs.anl.gov/prism/lib/tech.html
Root-URL: http://www.mcs.anl.gov
Title: A COMPARISON OF ALGORITHMS FOR BANDED MATRIX MULTIPLICATION*  
Author: A. Tsao and T. Turnbull 
Date: 21 April 1993  
Address: 17100 Science Drive Bowie, MD 20715-4300  
Affiliation: Supercomputing Research Center  
Pubnum: SRC-TR-93-092  
Abstract: We present and compare several methods for multiplying banded square matrices. Various storage schemes and their implementations are discussed. Of particular interest is an algorithm for multiplying matrices by diagonals, which always references contiguous matrix elements. Two blocked implementations also are presented. These specialized routines are attractive for multiplying matrices whose bandwidths are known to be small relative to the size of the matrices. Results from tests performed on a Cray-2, Cray Y-MP, and RS/6000 are given. It is shown that, for specialized applications, a substantial savings can be realized over the standard three-loop multiplication algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Anderson, E., Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Hammarling, A. McKenney, S. Ostrouchov, & D. Sorensen, </author> <title> LAPACK Users' Guide, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1992. </year>
Reference-contexts: How one actually performs packing of the matrix also can vary. Consider a 4 fi 4 matrix with bandwidth b = 2: 2 4 a 21 a 22 a 23 a 24 0 a 42 a 43 a 44 7 One of the LAPACK <ref> [1] </ref> storage schemes packs this matrix by diagonals as follows: 2 6 4 0 a 12 a 23 a 34 a 21 a 32 a 43 0 3 7 5 which keeps the column orientation intact. (Here the zeros are really just placeholders in the array and do not get referenced.)
Reference: 2. <author> Golub, G. & C. F. Van Loan, </author> <title> Matrix Computations, 2nd ed., </title> <publisher> Johns Hopkins University Press, </publisher> <address> Baltimore, </address> <year> 1989. </year>
Reference-contexts: This allows us to work mostly with banded matrices and to perform only those portions of the matrix multiplications necessary to compute the elements within the resulting band. (By band or bandwidth, we mean the upper or lower bandwidth of a matrix, as defined in <ref> [2] </ref>. <p> TSAO AND T. TURNBULL performed considerably better than the assembly-coded routine SGEMM for bandwidths less than 100. routine and the actual performance observed when multiplying two 1000 fi 1000 matrices with varying bandwidths together. Following <ref> [2] </ref>, the cost to perform a dense matrix multiplication is 2n 3 , and although an approximate cost for the banded case can be derived, we used the actual number of operations performed during the multiplication. This ratio gives the maximum improvement possible.
Reference: 3. <author> Lederman, S., A. Tsao, & T. Turnbull, </author> <title> A parallelizable eigensolver for real diagonalizable matrices with real eigenvalues, </title> <type> Technical Report TR-91-042, </type> <institution> Supercomputing Research Center (1991). </institution>
Reference-contexts: Results from tests performed on a Cray-2, Cray Y-MP, and RS/6000 are given. It is shown that, for specialized applications, a substantial savings can be realized over the standard three-loop multiplication algorithm. 1. Motivation During the development of the Invariant Subspace Decomposition Algorithm (ISDA) <ref> [3] </ref>, which computes the eigenvalues and eigenvectors of matrices, we began an investigation into the issues relating to banded matrices.
Reference: 4. <author> Madsen, N. K., G H Rodrigue, J Karush, </author> <title> Matrix multiplication by diagonals on a vector/parallel processor, </title> <journal> Info. Proc. Letters. </journal> <volume> 5 (1976), no. 2, </volume> <pages> 41-5. </pages>
Reference-contexts: We found this method particularly attractive since the indexing is much simpler|the beginning of each row is also the beginning of each diagonal. This came in handy for the implementation of an algorithm designed by Madsen, Rodrigue, and Karush <ref> [4] </ref> to perform matrix multiplication by diagonals on vector and parallel computers. For ease of identification, we will refer to this as the MRK algorithm. Additionally, for performance considerations, there are differences between storage schemes for FORTRAN and C. <p> This alleviates the problem of irregular access of column elements, if the matrix is stored row-wise, or irregular access of row elements, if the matrix is stored column-wise. Following the notation in <ref> [4] </ref>, the main diagonal of A is denoted a 0 , the super-diagonals are denoted a 1 , a 2 , etc., and the sub-diagonals as a 1 , a 2 , etc.
Reference: 5. <author> Tsao, A. & T. Turnbull, </author> <title> A study of the Invariant Subspace Decomposition Algorithm for banded symmetric matrices; (in preparation). </title>
Reference-contexts: In particular, since the ISDA relies heavily on matrix multiplication to decompose the matrix into smaller subproblems, we are exploring ways to decrease the amount of time spent performing matrix multiplication. One way to accomplish this is to perform periodic reductions to banded form <ref> [5] </ref>. This allows us to work mostly with banded matrices and to perform only those portions of the matrix multiplications necessary to compute the elements within the resulting band. (By band or bandwidth, we mean the upper or lower bandwidth of a matrix, as defined in [2].
References-found: 5

