URL: ftp://ftp.cs.orst.edu/pub/tgd/papers/nips91-grbf.ps.gz
Refering-URL: http://www.cs.orst.edu/~tgd/cv/pubs.html
Root-URL: 
Title: Improving the Performance of Radial Basis Function Networks by Learning Center Locations  
Author: Dietrich Wettschereck and Thomas Dietterich 
Date: 1992  
Note: Advances in Neural Information Processing Systems 4 Edited by J.E.Moody, S.J.Hanson, and R.P.Lippmann, Morgan Kaufmann Publishers,  
Address: Corvallis, OR 97331-3202  San Mateo, CA,  
Affiliation: Department of Computer Science Oregon State University  
Abstract-found: 0
Intro-found: 1
Reference: <author> D. W. Aha, D. Kibler & M. K. Albert. </author> <title> (1991) Instance-based learning algorithms. </title> <booktitle> Machine Learning 6(1) </booktitle> <pages> 37-66. </pages>
Reference: <author> E. Barnard & R. A. Cole. </author> <title> (1989) A neural-net training program based on conjugate-gradient optimization. </title> <type> Rep. No. CSE 89-014. </type> <institution> Oregon Graduate Institute, </institution> <address> Beaver-ton, OR. </address>
Reference: <author> T. G. Dietterich & G. Bakiri. </author> <title> (1991) Error-correcting output codes: A general method for improving multiclass inductive learning programs. </title> <booktitle> Proceedings of the Ninth National Conference on Artificial Intelligence (AAAI-91), </booktitle> <address> Anaheim, CA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: the 1000-word test set in comparison with several other algorithms: nearest neighbor, the decision tree algorithm ID3 (Quinlan, 1986), sigmoid networks trained via backpropagation (160 hidden units, cross-validation training, learning rate 0.25, momentum 0.9), Wolpert's (1990) HERBIE algorithm (with weights set via mutual information), and ID3 with error-correcting output codes <ref> (ECC, Dietterich & Bakiri, 1991) </ref>.
Reference: <author> T. G. Dietterich, H. Hild, & G. Bakiri. </author> <title> (1990) A comparative study of ID3 and backpropagation for English text-to-speech mapping. </title> <booktitle> Proceedings of the 1990 Machine Learning Conference, </booktitle> <address> Austin, TX. </address> <pages> 24-31. </pages>
Reference-contexts: The purpose of this paper is twofold. First, we carefully tested the performance of RBF networks on the well-known NETtalk task (Sejnowski & Rosenberg, 1987) and compared it to the performance of a wide variety of algorithms that we have previously tested on this task <ref> (Dietterich, Hild, & Bakiri, 1990) </ref>. The results confirm that there is a substantial gap between RBF generalization and other methods.
Reference: <author> K. J. Lang, A. H. Waibel & G. E. Hinton. </author> <title> (1990) A time-delay neural network architecture for isolated word recognition. </title> <booktitle> Neural Networks 3 </booktitle> <pages> 33-43. </pages>
Reference: <author> J. MacQueen. </author> <title> (1967) Some methods of classification and analysis of multivariate observations. </title> <editor> In LeCam, L. M. & Neyman, J. (Eds.), </editor> <booktitle> Proceedings of the 5th Berkeley Symposium on Mathematics, Statistics, and Probability (p. 281). </booktitle> <address> Berkeley, CA: </address> <publisher> University of California Press. </publisher>
Reference: <author> J. Moody & C. J. Darken. </author> <title> (1989) Fast learning in networks of locally-tuned processing units. </title> <booktitle> Neural Computation 1(2) </booktitle> <pages> 281-294. </pages>
Reference-contexts: excellent approximations to smooth functions (Poggio & Girosi, 1989), (b) their "centers" are interpretable as "prototypes", and (c) they can be learned very quickly, because the center locations (x ff ) can be determined by unsupervised learning algorithms and the weights (c ff ) can be computed by pseudo-inverse methods <ref> (Moody and Darken, 1989) </ref>. Although the application of unsupervised methods to learn the center locations does yield very efficient training, there is some evidence that the generalization performance of RBF networks is inferior to sigmoid networks.
Reference: <author> R. Penrose. </author> <title> (1955) A generalized inverse for matrices. </title> <booktitle> Proceedings of Cambridge Philosophical Society 51 </booktitle> <pages> 406-413. </pages>
Reference: <author> T. Poggio & F. Girosi. </author> <title> (1989) A theory of networks for approximation and learning. Report Number AI-1140. </title> <institution> MIT Artificial Intelligence Laboratory, </institution> <address> Cambridge, MA. </address>
Reference-contexts: These distances are then passed through Gaussians (with variance 2 and zero mean), weighted by c ff , and summed. Radial basis function networks (RBF networks) provide an attractive alternative to sigmoid networks for learning real-valued mappings: (a) they provide excellent approximations to smooth functions <ref> (Poggio & Girosi, 1989) </ref>, (b) their "centers" are interpretable as "prototypes", and (c) they can be learned very quickly, because the center locations (x ff ) can be determined by unsupervised learning algorithms and the weights (c ff ) can be computed by pseudo-inverse methods (Moody and Darken, 1989).
Reference: <author> J. R. Quinlan. </author> <title> (1986) Induction of decision trees. </title> <booktitle> Machine Learning 1(1) </booktitle> <pages> 81-106. </pages>
Reference-contexts: Table 1 shows the performance of RBF on the 1000-word test set in comparison with several other algorithms: nearest neighbor, the decision tree algorithm ID3 <ref> (Quinlan, 1986) </ref>, sigmoid networks trained via backpropagation (160 hidden units, cross-validation training, learning rate 0.25, momentum 0.9), Wolpert's (1990) HERBIE algorithm (with weights set via mutual information), and ID3 with error-correcting output codes (ECC, Dietterich & Bakiri, 1991).
Reference: <author> T. J. Sejnowski & C. R. Rosenberg. </author> <title> (1987) Parallel networks that learn to pronounce English text. </title> <booktitle> Complex Systems 1 </booktitle> <pages> 145-168. </pages>
Reference-contexts: In many applications, this assumption is known to be false, so this could yield poor results. The purpose of this paper is twofold. First, we carefully tested the performance of RBF networks on the well-known NETtalk task <ref> (Sejnowski & Rosenberg, 1987) </ref> and compared it to the performance of a wide variety of algorithms that we have previously tested on this task (Dietterich, Hild, & Bakiri, 1990). The results confirm that there is a substantial gap between RBF generalization and other methods. <p> To perform supervised learning of center locations, feature weights, and variances, we applied conjugate-gradient optimization. We modified the conjugate-gradient implementation of backpropagation supplied by Barnard & Cole (1989). 3 The NETtalk Domain We tested all networks on the NETtalk task <ref> (Sejnowski & Rosenberg, 1987) </ref>, in which the goal is to learn to pronounce English words by studying a dictionary of correct pronunciations.
Reference: <author> D. Wolpert. </author> <title> (1990) Constructing a generalizer superior to NETtalk via a mathematical theory of generalization. </title> <booktitle> Neural Networks 3 </booktitle> <pages> 445-452. </pages>
References-found: 12

