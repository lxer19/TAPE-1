URL: http://www.cs.duke.edu/~jsv/Papers/ViK96.prefetching.ps.gz
Refering-URL: http://www.cs.duke.edu/~jsv/Papers/catalog/node48.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: CS-1995-25 Optimal Prefetching via Data Compression 1  
Author: Jeffrey Scott Vitter P. Krishnan 
Date: November 13, 1995  
Address: 27708-0129  
Affiliation: Department of Computer Science Duke University Durham, North Carolina  
Abstract: 1 An extended abstract of this work appears in Proceedings of the 32nd Annual IEEE Symposium on Foundations of Computer Science, Puerto Rico, October 1991. The work was performed while the authors were at Brown University. Support was provided in part by an National Science Foundation Presidential Young Investigator Award CCR-9047466 with matching funds from IBM, by NSF research grant CCR-9007851, by Army Research Office grant DAAL03-91-G-0035, and by the Office of Naval Research and the Defense Advanced Research Projects Agency under contract N00014-83-K-0146 and ARPA order 6320, amendment 1. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N. Abe and M. Warmuth, </author> <title> "On the Computational Complexity of Approximating Distributions by Probabilistic Automata," </title> <type> UCSC, </type> <institution> UCSC-CRL-90-63, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: p; z 0 ), where S is a finite set of states with jSj = s, A is a finite alphabet of size ff, g is a deterministic "next state" function that maps S fi A into S, p is a "probability assignment function" that maps S fi A into <ref> [0; 1] </ref> with the restriction that P all z 2 S, and z 0 2 S is the start state. A probabilistic FSA when used to generate strings is called a Markov source. <p> Then we have Compression M (s);n ( n 1 ) n c Proof : The working of an arithmetic coder using a probabilistic FSA as its model can be explained in the following way: The arithmetic coder associates a unit interval <ref> [0; 1] </ref> with each state of the model. This interval is partitioned into distinct subintervals, one per character, the size of the subinterval associated with a character proportional to the probability of its transition out of the state. <p> Any string that takes the coder from state x to state x 0 of the model defines implicitly a subinterval of the original <ref> [0; 1] </ref> interval at x; also, this subinterval uniquely characterizes the string. The size of this subinterval is clearly the product of the probabilities of the transitions taken by the arithmetic coder while processing the string and all the arithmetic coder's output has to do is to identify this subinterval. <p> As an example, consider the probabilistic FSA of Figure 1 being used as a model by an arithmetic coder. Starting at node x, the string "ba" would cause the interval to shrink by a factor of 3=5 from <ref> [0; 1] </ref> to [0:2; 0:8] and then by a factor of 1=3 from [0:2; 0:8] to [0:2; 0:4]. To identify "ba," which is associated with the interval [0:2; 0:4], the arithmetic coder needs to output at least lg (0:4 0:2) bits (which is the same as lg ((3=5)(1=3))). <p> It is clear that if there are c 0 distinct substrings processed in which M starts from state x and ends in state x 0 , these c 0 substrings define c 0 distinct subintervals of the original <ref> [0; 1] </ref> interval at x. If these c 0 substrings are such that no one is a prefix of another, the subintervals corresponding to them are non-overlapping, and the sum of the lengths of these subintervals is at most 1. <p> The prefetch requests are issued much in advance of their anticipated use. This introduces a number of interesting timing issues dealing with when to initiate the prefetch request, and provides another possible model to study prefetching analytically. The framework of Abe and Warmuth <ref> [1] </ref>, who investigated a quite different learning problem related to FSAs, has led us to propose a static PAC-learning framework for prefetching, in which the prefetcher is trained on several independently generated sequences of a particular length generated by a source, and the prefetcher should converge sufficiently fast.
Reference: [2] <author> Y. Amit and M. Miller, </author> <title> "Large Deviations for Coding Markov Chains and Gibbs Random Fields," </title> <institution> Washington University, </institution> <type> Technical Report, </type> <year> 1990. </year>
Reference-contexts: Hence f P f M = p AC p BC * 1 + * 2 j* 1 j + j* 2 j *: The following lemma is well known; however, we reproduce it and its proof from <ref> [2] </ref> here for the sake of completeness. The summation on the right-hand side of the lemma is the Kullback-Leibler divergence of (r 1 ; : : : ; r ff ) with respect to (p 1 ; : : : ; p ff ).
Reference: [3] <author> T. C. Bell, J. C. Cleary, and I. H. Witten, </author> <title> Text Compression, </title> <publisher> Prentice Hall Advanced Reference Series, </publisher> <year> 1990. </year> <month> 19 </month>
Reference-contexts: The Ziv-Lempel encoder can be converted from a word-based method to a character-based algorithm E by building a probabilistic model that feeds probability information to an arithmetic coder <ref> [3, 24] </ref>, as explained in the example below. It has been shown that the coding length obtained in this character-based approach is at least as good as that obtained using the word-based approach [3,17,24]. Hence, the optimality results in [38] hold without change for the character-based approach.
Reference: [4] <author> D. Blackwell, </author> <title> "An Analog to the Minimax Theorem for Vector Payoffs," </title> <journal> Pacific Journal of Mathematics 6 (1956), </journal> <pages> 1-8. </pages>
Reference: [5] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth, </author> <note> "Occam's Razor," Information Processing Letters 24 (1987). </note>
Reference: [6] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth, </author> <title> "Learnability and the Vapnik Chervonenkis Dimension," </title> <note> Journal of the ACM (October 1989). </note>
Reference: [7] <author> R. Board and L. Pitt, </author> <title> "On the Necessity of Occam Algorithms," </title> <booktitle> Proceedings of the 22nd Annual ACM Symposium on Theory of Computation (May 1990), </booktitle> <pages> 54-63. </pages>
Reference: [8] <author> A. Borodin, S. Irani, P. Raghavan, and B. Schieber, </author> <title> "Competitive Paging with Locality of Reference," </title> <booktitle> Proceedings of the 23rd Annual ACM Symposium on Theory of Computation (May 1991). </booktitle>
Reference: [9] <author> J. T. Brady, </author> <title> "A theory of productivity in the creative process," </title> <journal> IEEE CG&A (May 1986), </journal> <pages> 25-34. </pages>
Reference-contexts: Current database systems perform prefetching using such sequential prefetching techniques derived from older virtual memory prefetchers. The I/O bottleneck is seriously impeding performance in large-scale databases, and the demand for improving response time performance is growing <ref> [9] </ref>. The older virtual memory-based prefetchers are inadequate for newer object-oriented and hypertext applications, and this has stimulated renewed interest in developing improved algorithms for prefetching [10,23,28,29,31]. In this paper, we give the first provable theoretical bounds on the performance of prediction algorithms for prefetching.
Reference: [10] <author> T. F. Chen and J. L. Baer, </author> <title> "Reducing Memory Latency via Non-blocking and Prefetching Caches," </title> <booktitle> Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (October 1992). </booktitle>
Reference: [11] <author> T. M. Cover and A. Shenhar, </author> <title> "Compound Bayes Predictors with Apparent Markov Structure," </title> <journal> IEEE Transactions on System Man and Cybernetics SMC-7 (June 1977), </journal> <pages> 421-424. </pages>
Reference: [12] <author> K. Curewitz, P. Krishnan, and J. S. Vitter, </author> <title> "Practical Prefetching via Data Compression," </title> <booktitle> Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data (May 1993), </booktitle> <pages> 257-266. </pages>
Reference-contexts: Pure prefetching is mathematically elegant since it isolates the prediction component of prefetching from the issue of cache replacement. At the same time, pure prefetchers can be converted into general prefetchers using techniques described in <ref> [12] </ref>. We refer to this issue again in Sections 6 and 7. Prior work on prefetching has been primarily empirical. <p> A natural question at this stage is to understand how the algorithms perform in practice. Practical issues that arise in implementing the prefetcher P (or any data compression-based prefetcher) are extensive and are discussed in a separate publication <ref> [12] </ref> and patent application [36]. In particular, it shown in [12,36] how to tackle the issues arising from the limited space available for storing the data structures of the prefetcher, and from the limited time available to do the prefetching. <p> The impact of melding good cache replacement strategies with good pure prefetchers in order to get general non-pure prefetchers, and how to deal with situations when the user's page request preempts prefetching are also studied. In <ref> [12] </ref>, significant reductions in fault rate are demonstrated (for example, a 15-70% reduction in fault rate over using the least recently used heuristic for cache replacement) using CAD application traces and traces obtained from the OO1 and OO7 benchmarks. <p> It would be interesting to see the effect of such data compression-based prefetchers for hypertext-based applications, such as Internet browsers where the caching is done at the client. An interesting observation from <ref> [12] </ref> is that for the prefetchers built from data compressors, the first prediction is always the most significant; in other words, there is little difference in practice between prefetching only one page between any two user page requests and pure prefetching.
Reference: [13] <author> M. Feder, N. Merhav, and M. Gutman, </author> <title> "Universal Prediction of Individual Sequences," </title> <journal> IEEE Transactions on Information Theory IT-38 (July 1992), </journal> <pages> 1258-1270. </pages>
Reference-contexts: The performance of the proposed prefetcher is shown to be optimal in the worst case with respect to the optimal finite-state prefetcher. The time per prediction is also optimal. The model is along the lines of that proposed for the ff = 2, k = 1 case discussed in <ref> [13] </ref>. The approach in [22] is necessarily different from those of this paper and [13] in order to handle the general case and additionally to perform the prediction in constant time per prediction. <p> The time per prediction is also optimal. The model is along the lines of that proposed for the ff = 2, k = 1 case discussed in <ref> [13] </ref>. The approach in [22] is necessarily different from those of this paper and [13] in order to handle the general case and additionally to perform the prediction in constant time per prediction. The prefetching algorithms we have considered in this paper are adaptive and based only on the page request sequence.
Reference: [14] <author> A. Fiat, R. M. Karp, M. Luby, L. A. McGeoch, D. D. Sleator, and N. E. Young, </author> <title> "On Competitive Algorithms for Paging Problems," </title> <booktitle> Journal of Algorithms 12 (1991), </booktitle> <pages> 685-699. </pages>
Reference-contexts: We could combine our prefetcher with such special-purpose techniques so as to get the best of both worlds. Methods for combining predictions for caching appear in <ref> [14] </ref>. Under the compiler-directed prefetching paradigm used for scientific programs [10,28,31], the compiler reorders instructions in application code and introduces explicit prefetching instructions to reduce the effect of cache misses. The prefetch requests are issued much in advance of their anticipated use.
Reference: [15] <author> R. G. Gallager, </author> <title> Information Theory and Reliable Communication, </title> <publisher> Wiley, </publisher> <year> 1968. </year>
Reference-contexts: The logarithm of x to the base 2 is denoted by lg x, the natural logarithm of x is denoted by ln x, and the empty string is denoted by . 3 Definition 1 <ref> [15] </ref> We define a probabilistic finite state automaton (probabilistic FSA) as a quintuple (S; A; g; p; z 0 ), where S is a finite set of states with jSj = s, A is a finite alphabet of size ff, g is a deterministic "next state" function that maps S fi <p> Definition 2 <ref> [15] </ref> Let M be a Markov source. <p> Since we throw away our data structures at the end of each block, each of the b random variables Fault P;n , for 1 i b, depends only on the start state for each block, and our result follows by the ergodic theorem <ref> [15] </ref>. The proof of Theorem 2 essentially deals with showing that F converges to F M for almost all as n ! 1.
Reference: [16] <author> J. F. Hannan, </author> <title> "Approximation to Bayes Risk in Repeated Plays," Contributions to the Theory of Games, </title> <booktitle> Vol 3, Annals of Mathematical Studies (1957), </booktitle> <pages> 97-139. </pages>
Reference: [17] <author> P. G. Howard and J. S. Vitter, </author> <title> "Analysis of Arithmetic Coding for Data Compression," </title> <booktitle> Information Processing and Management 28 (1992), </booktitle> <pages> 749-763, </pages> <note> invited paper in Special Issue on Data Compression for Images and Texts. </note>
Reference: [18] <author> S. Irani, A. R. Karlin, and S. Phillips, </author> <title> "Strongly Competitive Algorithms for Paging with Locality of Reference," </title> <booktitle> Proceedings of the 3rd Annual ACM-SIAM Symposium of Discrete Algorithms (January 1992). </booktitle>
Reference: [19] <author> A. R. Karlin, S. J. Phillips, and P. Raghavan, </author> <title> "Markov Paging," </title> <booktitle> Proceedings of the 33rd Annual IEEE Conference on Foundations of Computer Science (October 1992), </booktitle> <pages> 208-217. </pages>
Reference: [20] <author> S. Karlin and H. M. Taylor, </author> <title> A First Course in Stochastic Processes, </title> <publisher> Academic Press, 2nd edition. </publisher>
Reference-contexts: The prefetcher M estimates the probability of each transition to be the frequency that it is taken. Algorithm M prefetches the pages with the top k estimated probabilities at the current state. From the probability theory of Markov chains and renewal processes <ref> [20] </ref>, we know that at time instant , E (jp z b p z j) = O (1= p p ). Using this in conjunction with Lemma 5, we see that the expected difference in fault between M and the source for time is O (1= p ).
Reference: [21] <author> M. J. Kearns and R. E. Schapire, </author> <title> "Efficient Distribution-Free Learning of Probabilistic Concepts," </title> <booktitle> Proceedings of the 31st Annual IEEE Symposium on Foundations of Computer Science (October 1990), </booktitle> <pages> 382-391. </pages>
Reference-contexts: A harder model is to assume that the prefetcher is trained on one sufficiently long sequence generated by a source. For certain special cases of sources, like mth order Markov sources, we expect that the optimal prefetcher is PAC-learnable. An interesting related model is that of probabilistic concepts <ref> [21] </ref>.
Reference: [22] <author> P. Krishnan and J. S. Vitter, </author> <title> "Optimal Prediction for Prefetching in the Worst Case," </title> <journal> SIAM Journal on Computing, </journal> <note> to appear pending minor revisions. A shortened version appeared in Proceedings of the Fifth Annual ACM-SIAM Symposium on Discrete Algorithms, </note> <month> January </month> <year> 1994, </year> <pages> 392-401. 20 7. </pages> <month> CONCLUSIONS </month>
Reference-contexts: CONCLUSIONS Markov source. Some practical issues regarding prefetching are addressed in Section 6. In follow-on work, an alternative prefetching model analyzed in <ref> [22] </ref> allows the source to be worst-case, that is, determined by an adversary. The performance of the proposed prefetcher is shown to be optimal in the worst case with respect to the optimal finite-state prefetcher. The time per prediction is also optimal. <p> The time per prediction is also optimal. The model is along the lines of that proposed for the ff = 2, k = 1 case discussed in [13]. The approach in <ref> [22] </ref> is necessarily different from those of this paper and [13] in order to handle the general case and additionally to perform the prediction in constant time per prediction. The prefetching algorithms we have considered in this paper are adaptive and based only on the page request sequence.
Reference: [23] <author> P. Laird, </author> <title> "Discrete Sequence Prediction and its Applications," </title> <institution> AI Research Branch, NASA Ames Research Center, </institution> <type> manuscript, </type> <year> 1992. </year>
Reference: [24] <author> G. G. Langdon, </author> <title> "A note on the Ziv-Lempel model for compressing individual sequences," </title> <journal> IEEE Transactions on Information Theory 29 (March 1983), </journal> <pages> 284-287. </pages>
Reference-contexts: The Ziv-Lempel encoder can be converted from a word-based method to a character-based algorithm E by building a probabilistic model that feeds probability information to an arithmetic coder <ref> [3, 24] </ref>, as explained in the example below. It has been shown that the coding length obtained in this character-based approach is at least as good as that obtained using the word-based approach [3,17,24]. Hence, the optimality results in [38] hold without change for the character-based approach.
Reference: [25] <author> G. G. Langdon, </author> <title> "An Introduction to Arithmetic Coding," </title> <institution> IBM J. Res. Develop. </institution> <month> 28 (March </month> <year> 1984), </year> <pages> 135-149. </pages>
Reference: [26] <author> A. Lempel and J. Ziv, </author> <title> "On the Complexity of Finite Sequences," </title> <note> IEEE Transactions on Information Theory 22 (January 1976). </note>
Reference-contexts: It is shown in <ref> [26] </ref> that 0 c ( n n lg ff ; where lim n!1 Theorem 4 is clearly true when c ( n 1 ) = o (n= lg n) since Compression E;n ( n 1 ) ~ 0 as n ! 1.
Reference: [27] <author> L. A. McGeoch and D. D. Sleator, </author> <title> "A Strongly Competitive Randomized Paging Algorithm," </title> <institution> Carnegie-Mellon University, CS-89-122, </institution> <month> March </month> <year> 1989. </year>
Reference: [28] <author> T. C. Mowry, M. S. Lam, and A. Gupta, </author> <title> "Design and Evaluation of a Compiler Algorithm for Prefetching," </title> <booktitle> Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (October 1992). </booktitle>
Reference: [29] <author> M. Palmer and S. Zdonik, </author> <title> "Fido: A Cache that Learns to Fetch," </title> <booktitle> Proceedings of the 1991 International Conference on Very Large Databases (September 1991). </booktitle>
Reference: [30] <author> R. H. Patterson, G. A. Gibson, and M. Satyanarayanan, </author> <title> "A Status Report on Research in Transparent Informed Prefetching," </title> <booktitle> ACM Operating Systems Review 27 (April 1993), </booktitle> <pages> 21-34. </pages>
Reference: [31] <author> A. Rogers and K. Li, </author> <title> "Software Support for Speculative Loads," </title> <booktitle> Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (October 1992). </booktitle>
Reference: [32] <author> A. Shwartz and A. Weiss, </author> <title> Large Deviations for Performance Analysis, </title> <publisher> Chapman & Hall, </publisher> <year> 1995. </year>
Reference-contexts: From large deviation theory <ref> [32] </ref>, we know that Pr (j b p z p z j &gt; ffi) is exponentially small in n for ffi &gt; 0, where n is the length of the sequence. Similarly, from [32], we see that Pr (j b p z;i p z;i j &gt; ffi) is exponentially small in <p> From large deviation theory <ref> [32] </ref>, we know that Pr (j b p z p z j &gt; ffi) is exponentially small in n for ffi &gt; 0, where n is the length of the sequence. Similarly, from [32], we see that Pr (j b p z;i p z;i j &gt; ffi) is exponentially small in n. (The exponentially small probability is of the form O (e affi 2 n ), where a depends on the Markov source.) Using Lemmas 5 and 6, we have E (Fault X;n )
Reference: [33] <author> D. D. Sleator and R. E. Tarjan, </author> <title> "Amortized Efficiency of List Update and Paging Rules," </title> <booktitle> Communications of the ACM 28 (February 1985), </booktitle> <pages> 202-208. </pages>
Reference-contexts: We say that an algorithm is online if it must make its decisions based only on the past history. An o*ine algorithm can use the knowledge of the future. Any implementable algorithm for caching or prefetching must clearly be online. The notion of competitiveness introduced by Sleator and Tarjan <ref> [33] </ref> determines the goodness of an online algorithm by comparing its performance to that of o*ine algorithms. An 2 2.
Reference: [34] <author> K. S. Trivedi, </author> <title> "An Analysis of Prepaging," </title> <booktitle> Computing 22 (1979), </booktitle> <pages> 191-210. </pages>
Reference: [35] <author> V. Vapnik, </author> <title> Estimation of Dependencies Based on Empirical Data, </title> <publisher> Springer-Verlag, </publisher> <year> 1982. </year>
Reference: [36] <author> J. S. Vitter, K. Curewitz, and P. Krishnan, </author> <title> "Online Background Predictors and Prefetch-ers," </title> <institution> Duke University, United States Patent No. 5,485,609, </institution> <month> January 16, </month> <year> 1996. </year>
Reference-contexts: A natural question at this stage is to understand how the algorithms perform in practice. Practical issues that arise in implementing the prefetcher P (or any data compression-based prefetcher) are extensive and are discussed in a separate publication [12] and patent application <ref> [36] </ref>. In particular, it shown in [12,36] how to tackle the issues arising from the limited space available for storing the data structures of the prefetcher, and from the limited time available to do the prefetching.
Reference: [37] <author> I. H. Witten, R. M. Neal, and J. G. Cleary, </author> <title> "Arithmetic Coding for Data Compression," </title> <journal> Communications of the ACM 30 (June 1987), </journal> <pages> 520-540. </pages>
Reference-contexts: To identify a subinterval of length u, an arithmetic coder has to output at least lg (1=u) = lg u bits; for more details see <ref> [17,25, 37] </ref>. As an example, consider the probabilistic FSA of Figure 1 being used as a model by an arithmetic coder.
Reference: [38] <author> J. Ziv and A. Lempel, </author> <title> "Compression of Individual Sequences via Variable-Rate Coding," </title> <journal> IEEE Transactions on Information Theory 24 (September 1978), </journal> <pages> 530-536. </pages>
Reference-contexts: Our models and results are summarized in the next section. In Section 3, for our main Markov source model we apply a character-by-character version of the Ziv-Lempel data compression algorithm <ref> [38] </ref>. In Section 4, we compare our online algorithm to the best algorithm that has full knowledge of the Markov source. We show that the page fault rate of our algorithm converges for almost all page request sequences to this best algorithm's page fault rate. <p> This minimum fault probability, weighted by the probability of being in state z, summed over all states z, gives us F M . This is formalized later in Definition 4. We adapt a character-by-character version of the Ziv-Lempel <ref> [38] </ref> data compressor to get our optimal prefetcher P. Theorems 1 and 2 below are our main results. Theorem 1 Let M be a Markov source. <p> The original Ziv-Lempel algorithm <ref> [38] </ref> is a word-based data compression algorithm. The Ziv-Lempel encoder breaks the input string into blocks of relatively large length n, and it encodes these blocks using a block-to-variable code. Let x 0 be the empty string . <p> It has been shown that the coding length obtained in this character-based approach is at least as good as that obtained using the word-based approach [3,17,24]. Hence, the optimality results in <ref> [38] </ref> hold without change for the character-based approach. Example 1 Assume for simplicity that our alphabet is fa; bg. <p> n 1 and also to en code n 1 (via arithmetic coding), the average compression rate achieved is equal to the entropy of M ; that is, E (Compression M;n ) = H M (n): (2) The definitions in Definition 3 above are similar to those of Ziv and Lempel <ref> [38] </ref>, except that they define M (s) to be a class of "information lossless" non-probabilistic FSA encoders, 4.1 Bounds on Compression 7 program P; begin while not end of user session do begin initialize data structure tree to be a single node (the root); set current node to be the root; <p> Pseudocode for pure prefetching. 8 4. ANALYSIS OF OUR PREFETCHING ALGORITHM use in place of Compression , and use n lg ff in place of n in (1) to get a ratio of output length to input length. We generalize Ziv and Lempel's main result <ref> [38] </ref> to our model M (s) of probabilistic FSAs, using an iterative analysis based on arithmetic coding, to get the following theorem: Theorem 4 The compression rate of E on n 1 is no worse than the best probabilistic FSA in the limit as n ! 1. <p> We are now ready to present the proof of Theorem 4. Proof of Theorem 4: It has been shown in <ref> [38] </ref> that Compression E;n ( n 1 ) 1 ) + 1 lg (2ff (c ( n where c ( n 1 ) is the maximum number of nodes in any parse tree 1 for n 1 . <p> defined by F M (n) = n z2S v=0 X p z;i : If we take the limit of F M (n) as n ! 1, we get the expected fault rate F M of M . 1 This is not the definition of c ( n 1 ) in <ref> [38] </ref> but it is easy to verify that the proofs in [38] also hold under this definition of c ( n 1 ). 12 4. ANALYSIS OF OUR PREFETCHING ALGORITHM We now come to our goal: to show optimality of our prefetcher P. <p> z;i : If we take the limit of F M (n) as n ! 1, we get the expected fault rate F M of M . 1 This is not the definition of c ( n 1 ) in <ref> [38] </ref> but it is easy to verify that the proofs in [38] also hold under this definition of c ( n 1 ). 12 4. ANALYSIS OF OUR PREFETCHING ALGORITHM We now come to our goal: to show optimality of our prefetcher P.
References-found: 38

