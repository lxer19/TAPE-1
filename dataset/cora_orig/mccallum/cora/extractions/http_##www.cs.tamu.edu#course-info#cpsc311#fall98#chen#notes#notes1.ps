URL: http://www.cs.tamu.edu/course-info/cpsc311/fall98/chen/notes/notes1.ps
Refering-URL: http://www.cs.tamu.edu/course-info/cpsc311/fall98/chen/notes/
Root-URL: http://www.cs.tamu.edu
Title: Chapter 17 Information Structures and Access Algorithms 701 17.3 SORTING AND SEARCHING  
Abstract-found: 0
Intro-found: 1
Reference: [AhEtal74] <author> A. V. Aho, J. E. Hopcroft, and J. D. Ullman, </author> <title> The Design and Analysis of Computer Algorithms, </title> <publisher> Addison-Wesley, </publisher> <year> 1974. </year>
Reference-contexts: This implies that the procedure BUILDHEAP takes time O (n log n). A more careful analysis shows that the procedure BUILDHEAP actually runs in linear time <ref> [AhEtal74] </ref>. Therefore, the time complexity of HeapSort is O (n log n). Here is an intuitive explanation why HeapSort is faster than BubbleSort.
Reference: [CoEtal90] <author> T. H. Cormen, C. E. Leiserson, and R. L. Rivest, </author> <title> Introduction to Algorithms, </title> <publisher> McGraw-Hill, </publisher> <year> 1990. </year>
Reference-contexts: The above algorithm can be modified so that it sorts in linear time even when the input elements are not all distinct <ref> [CoEtal90] </ref>.
Reference: [Kn73] <author> D. E. Knuth, </author> <booktitle> The Art of Computer Programming. </booktitle> <volume> Vol. </volume> <month> 3: </month> <title> Sorting and Searching, </title> <publisher> Addison-Wesley, </publisher> <year> 1973. </year>
Reference-contexts: The analysis of ShellSort poses some very difficult mathematical problems, many of which have not yet been solved. In particular, it is not known which choice of increments yields the best result. It is known that they should not be multiples of each other. Knuth <ref> [Kn73] </ref> shows the evidence that the increment list given by h i1 = 3h i + 1 is a reasonable choice. <p> It can be proved <ref> [Kn73] </ref> that if we first distribute the input by the least significant digit (the 1st digit), then by the second least significant digit (the 2nd digit), and so on, then we will finally sort the input correctly. This sorting process is called RadixSort. <p> Formal algorithms and more detailed discussions can be found in <ref> [Kn73] </ref>. The general scheme of the external sorting algorithms has two phases: 1. Arrange the elements in runs in two or more files (a run is a nondecreasing subsequence of certain length); and 2. Repeatedly merge the runs until there is only one. <p> The number of times each element is read from or written to the external memory is log (n=m) + 1. Improvements on the construction of runs as well as on the merging process are possible <ref> [Kn73] </ref>. 17.3.7 Searching in Static Structures Searching in a static structure means that we search a particular element in a set stored in either an organized or an unorganized structure which is not dynamically changed. <p> A rotation on a node of the tree can be performed in constant time. Therefore, each of the operations, Search, Max, Min, Insert, and Delete, on an AVL tree can be done in time O (log n) <ref> [Kn73] </ref>. Hashing: Hashing is an approach to the searching problem completely different from those using comparison-based tree structures. Hashing references elements in a table by do 712 Handbook of Discrete and Combinatorial Mathematics ing arithmetic transformations (hashing) on elements into table addresses. <p> Another hashing scheme, hashing with open addressing, lets elements be stored in the entries of the hash table itself. Hashing collisions are resolved by the method of rehashing. Several rehashing strategies have been proposed <ref> [Kn73] </ref>. To reduce the number of hashing collisions, the hash function should be carefully designed. <p> Other hash functions have also been proposed <ref> [Kn73] </ref>.
References-found: 3

