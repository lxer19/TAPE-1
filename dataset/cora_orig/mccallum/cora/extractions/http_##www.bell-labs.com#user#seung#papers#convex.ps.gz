URL: http://www.bell-labs.com/user/seung/papers/convex.ps.gz
Refering-URL: http://www.bell-labs.com/user/seung/papers/index.html
Root-URL: 
Email: fddlee|seungg@bell-labs.com  
Title: Unsupervised Learning by Convex and Conic Coding  
Author: D. D. Lee and H. S. Seung 
Address: Murray Hill, NJ 07974  
Affiliation: Bell Laboratories, Lucent Technologies  
Abstract: Unsupervised learning algorithms based on convex and conic encoders are proposed. The encoders find the closest convex or conic combination of basis vectors to the input. The learning algorithms produce basis vectors that minimize the reconstruction error of the encoders. The convex algorithm develops locally linear models of the input, while the conic algorithm discovers features. Both algorithms are used to model handwritten digits and compared with vector quantization and principal component analysis. The neural network implementations involve feedback connections that project a reconstruction back to the input layer.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Hinton, GE & Zemel, </author> <title> RS (1994). Autoencoders, minimum description length and Helmholtz free energy. </title> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> 3-10. </pages>
Reference-contexts: These encodings should be contrasted with computationally inefficient ones. A natural modification of the point encoder with combinatorial expressiveness can be obtained by allowing ~v to be any vector of zeros and ones <ref> [1, 2] </ref>. Unfortunately, with this constraint the optimization of Eq. (1) becomes an integer programming problem and is quite inefficient to solve. The convex and conic encodings of an input generally contain coefficients v a that vanish, due to the nonnegativity constraints in the optimization of Eq. (1). <p> Assuming that the input vectors in X have been scaled to the range <ref> [0; 1] </ref>, the constraints on the optimizations in Eq. (2) are given by: Affine: 0 W ia 1, P Convex: 0 W ia 1, V a 0, P Conic: 0 W ia 1, V a 0. <p> Each of the 7291 training and 2007 test images were normalized to a 16 fi 16 grid with pixel intensities in the range <ref> [0; 1] </ref>. There were noticeable segmentation errors resulting in unrecognizable digits, but these images were left in both the training and test sets. The training examples were segregated by digit class and separate basis vectors were trained for each of the classes using the four encodings.
Reference: [2] <author> Ghahramani, </author> <title> Z (1995). Factorial learning and the EM algorithm. </title> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <pages> 617-624. </pages>
Reference-contexts: These encodings should be contrasted with computationally inefficient ones. A natural modification of the point encoder with combinatorial expressiveness can be obtained by allowing ~v to be any vector of zeros and ones <ref> [1, 2] </ref>. Unfortunately, with this constraint the optimization of Eq. (1) becomes an integer programming problem and is quite inefficient to solve. The convex and conic encodings of an input generally contain coefficients v a that vanish, due to the nonnegativity constraints in the optimization of Eq. (1).
Reference: [3] <author> Olshausen, </author> <title> BA & Field, DJ (1996). Emergence of simple-cell receptive field properties by learning a sparse code for natural images. </title> <booktitle> Nature 381, </booktitle> <pages> 607-609. </pages>
Reference-contexts: The convex and conic encodings of an input generally contain coefficients v a that vanish, due to the nonnegativity constraints in the optimization of Eq. (1). This method of obtaining sparse encodings is distinct from the method of simply truncating a linear combination by discarding small coefficients <ref> [3] </ref>. 3 Learning There correspond learning algorithms for each of the encoders described above that minimize the average reconstruction error over an ensemble of inputs.
Reference: [4] <author> Le Cun, Y et al. </author> <year> (1989). </year> <title> Backpropagation applied to handwritten zip code recognition. </title> <journal> Neural Comput. </journal> <volume> 1, </volume> <pages> 541-551. </pages>
Reference-contexts: The issue of local minima is discussed in the following example. 4 Example: modeling handwritten digits We applied Affine, Convex, Conic, and VQ learning to the USPS database <ref> [4] </ref>, which consists of examples of handwritten digits segmented from actual zip codes. Each of the 7291 training and 2007 test images were normalized to a 16 fi 16 grid with pixel intensities in the range [0; 1].
Reference: [5] <author> Scholkopf, B, Burges, C, & Vapnik, </author> <title> V (1995). Extracting support data for a given task. </title> <booktitle> KDD-95 Proceedings, </booktitle> <pages> 252-257. </pages>
Reference-contexts: This improvement arises because the larger convex hulls can better represent the overall nonlinear nature of the input distributions. This is good performance relative to other methods that do not use prior knowledge of invariances, such as the support vector machine (4.0% <ref> [5] </ref>). However, it is not as good as methods that do use prior knowledge, such as nearest neighbor with tangent distance (2.6% [6]). On the other hand, Conic coding with r = 25 results in an error rate of 6.8% (138 errors).
Reference: [6] <author> Simard, </author> <title> P, Le Cun Y & Denker J (1993). Efficient pattern recognition using a new transformation distance. </title> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> 50-58. </pages>
Reference-contexts: This is good performance relative to other methods that do not use prior knowledge of invariances, such as the support vector machine (4.0% [5]). However, it is not as good as methods that do use prior knowledge, such as nearest neighbor with tangent distance (2.6% <ref> [6] </ref>). On the other hand, Conic coding with r = 25 results in an error rate of 6.8% (138 errors). With larger basis sets r &gt; 50, Conic shows worse performance as the features shrink to small spots.
Reference: [7] <author> Tank, DW & Hopfield, </author> <month> JJ </month> <year> (1986). </year> <title> Simple neural optimization networks: an A/D converter, signal decision circuit, and a linear programming circuit. </title> <journal> IEEE Trans. Circ. Syst. </journal> <volume> CAS-33, </volume> <pages> 533-541. </pages>
Reference-contexts: Alternatively, the encoding can be performed by a neural network dynamics <ref> [7] </ref> and the learning by a synaptic update rule. We describe here the implementation for the Conic network; the Convex network is similar. The Conic network has a layer of N error neurons e i and a layer of r encoding neurons v a .
Reference: [8] <author> Bezdek, </author> <title> JC, Coray, C, Gunderson, R & Watson J (1981). Detection and characterization of cluster substructure. </title> <journal> SIAM J. Appl. Math. </journal> <volume> 40, </volume> <pages> 339-357; 358-372. </pages>
Reference-contexts: This performs stochastic gradient descent on the ensemble reconstruction error with learning rate . 6 Discussion Convex coding is similar to other locally linear models <ref> [8, 9, 10, 11] </ref>. Distance to a convex hull was previously used in nearest neighbor classification [12], though no learning algorithm was proposed. Conic coding is similar to the noisy OR [13, 14] and harmonium [15] models.
Reference: [9] <author> Bregler, C & Omohundro, </author> <title> SM (1995). Nonlinear image interpolation using manifold learning. </title> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <pages> 973-980. </pages>
Reference-contexts: This performs stochastic gradient descent on the ensemble reconstruction error with learning rate . 6 Discussion Convex coding is similar to other locally linear models <ref> [8, 9, 10, 11] </ref>. Distance to a convex hull was previously used in nearest neighbor classification [12], though no learning algorithm was proposed. Conic coding is similar to the noisy OR [13, 14] and harmonium [15] models.
Reference: [10] <author> Hinton, GE, Dayan, </author> <title> P & Revow M (1996). Modeling the manifolds of images of handwritten digits. </title> <journal> IEEE Trans. Neural Networks, </journal> <note> submitted. </note>
Reference-contexts: This performs stochastic gradient descent on the ensemble reconstruction error with learning rate . 6 Discussion Convex coding is similar to other locally linear models <ref> [8, 9, 10, 11] </ref>. Distance to a convex hull was previously used in nearest neighbor classification [12], though no learning algorithm was proposed. Conic coding is similar to the noisy OR [13, 14] and harmonium [15] models.
Reference: [11] <author> Hastie, T, Simard, </author> <title> P & Sackinger E (1995). Learning prototype models for tangent distance. </title> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <pages> 999-1006. </pages>
Reference-contexts: This performs stochastic gradient descent on the ensemble reconstruction error with learning rate . 6 Discussion Convex coding is similar to other locally linear models <ref> [8, 9, 10, 11] </ref>. Distance to a convex hull was previously used in nearest neighbor classification [12], though no learning algorithm was proposed. Conic coding is similar to the noisy OR [13, 14] and harmonium [15] models.
Reference: [12] <author> Haas, HPA, Backer, E & Boxma, </author> <title> I (1980). Convex hull nearest neighbor rule. </title> <booktitle> Fifth Intl. Conf. on Pattern Recognition Proceedings, </booktitle> <pages> 87-90. </pages>
Reference-contexts: This performs stochastic gradient descent on the ensemble reconstruction error with learning rate . 6 Discussion Convex coding is similar to other locally linear models [8, 9, 10, 11]. Distance to a convex hull was previously used in nearest neighbor classification <ref> [12] </ref>, though no learning algorithm was proposed. Conic coding is similar to the noisy OR [13, 14] and harmonium [15] models. The main difference is that these previous models contain discrete binary variables, whereas Conic uses continuous ones.
Reference: [13] <author> Dayan, P & Zemel, </author> <title> RS (1995). Competition and multiple cause models. </title> <journal> Neural Com-put. </journal> <volume> 7, </volume> <pages> 565-579. </pages>
Reference-contexts: Distance to a convex hull was previously used in nearest neighbor classification [12], though no learning algorithm was proposed. Conic coding is similar to the noisy OR <ref> [13, 14] </ref> and harmonium [15] models. The main difference is that these previous models contain discrete binary variables, whereas Conic uses continuous ones. The use of analog rather than binary variables makes the encoding computationally tractable and allows for interpolation between basis vectors.
Reference: [14] <author> Saund, </author> <title> E (1995). A multiple cause mixture model for unsupervised learning. </title> <journal> Neural Comput. </journal> <volume> 7, </volume> <pages> 51-71. </pages>
Reference-contexts: Distance to a convex hull was previously used in nearest neighbor classification [12], though no learning algorithm was proposed. Conic coding is similar to the noisy OR <ref> [13, 14] </ref> and harmonium [15] models. The main difference is that these previous models contain discrete binary variables, whereas Conic uses continuous ones. The use of analog rather than binary variables makes the encoding computationally tractable and allows for interpolation between basis vectors.
Reference: [15] <author> Freund, Y & Haussler, </author> <title> D (1992). Unsupervised learning of distributions on binary vectors using two layer networks. </title> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> 912-919. </pages>
Reference-contexts: Distance to a convex hull was previously used in nearest neighbor classification [12], though no learning algorithm was proposed. Conic coding is similar to the noisy OR [13, 14] and harmonium <ref> [15] </ref> models. The main difference is that these previous models contain discrete binary variables, whereas Conic uses continuous ones. The use of analog rather than binary variables makes the encoding computationally tractable and allows for interpolation between basis vectors.
References-found: 15

