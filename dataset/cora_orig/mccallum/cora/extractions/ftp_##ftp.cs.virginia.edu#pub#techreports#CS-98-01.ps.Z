URL: ftp://ftp.cs.virginia.edu/pub/techreports/CS-98-01.ps.Z
Refering-URL: ftp://ftp.cs.virginia.edu/pub/techreports/README.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Abstract  
Abstract: The Counterflow Pipeline (CFP) organization may be a good target for synthesis of applicationspecific microprocessors because it has a regular and simple structure. This paper describes early work using CFPs to improve overall application performance by tailoring a CFP to the kernel loop of an application. A CFP is customized for an application using the kernel loops data dependency graph to determine processor functionality and interconnection. Our technique builds the design space for a given a data dependency graph and explores the space to find the design with the best performance. Preliminary results indicate that speedup for several small graphs range from 1.3 to 2.0 and that our design space traversal heuristics find designs that are within 10% of optimal. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Benitez M. E. and Davidson, J. W., </author> <title> A Portable Global Optimizer and Linker, </title> <booktitle> Proc. of the SIG-PLAN Notices 1988 Symposium on Programming Language Design and Implementation , pp. 329 338, </booktitle> <address> Atlanta, Georgia, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: The behavior of a stage is dependent only on the adjacent stage in the pipeline, which permits local control of stages and avoids the complexity of conventional pipeline synchronization. 1.3. Experimental Framework customizing a CFP processor. The loop is compiled by the optimizer vpo <ref> [1] </ref> into instructions for the SPARC architecture [5]. During optimization, vpo builds the data dependency graph for the kernel loop. A separate synthesis module uses the graph to determine computational devices and emits a CFP specification indicating those elements.
Reference: [2] <author> Childers B. R., Davidson J. W., and Wulf W. A., </author> <title> Synthesis of ApplicationSpecific Counterflow pipelines, Workshop on the Interaction between Compilers and Computer Architecture, held during ACM HPCA-2, </title> <address> San Jose, Ca., </address> <month> February 37, </month> <year> 1996. </year>
Reference-contexts: It is especially useful for understanding where instructions stall in the pipeline. 2. Synthesis Methodology We are exploring techniques for synthesizing a Counterflow Pipeline customized to a kernel loop. Our first experiments used an iterative refinement methodology to derive a pipeline layout <ref> [2] </ref>. The iterative approach picks a pipeline layout, executes the kernel loop using the layout, collects an execution trace, and refines the layout using the trace. This is repeated iteratively until there are no further performance gains.
Reference: [3] <author> Hennessy J. L. and Patterson D. A., </author> <title> Computer Architecture: A Quantitative Approach , 2nd edition, </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Francisco, California, </address> <year> 1996. </year>
Reference-contexts: This document summarizes our Counterflow Pipeline synthesis technique and preliminary results. 1.1. Synthesis Strategy Most high-performance embedded applications have two parts: a control and a computation-intensive part. The computation part is typically a kernel loop that accounts for the majority of execution time. According to Amdahls Law <ref> [3] </ref>, increasing the performance of the most frequently executed portion of an application increases overall performance. Thus, synthesizing custom hardware for the computation-intensive portion may be an effective technique to increase performance.
Reference: [4] <author> Rau B. R. and Fisher J. A., </author> <title> Instruction-level parallel processing: History, overview, and perspective, </title> <editor> J. </editor> <booktitle> of Supercomputing , Vol 7, </booktitle> <pages> pp. 950, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: It is not practical to exhaustively search the entire design space for most dependency graphs. Although the space could be small, it is not likely with aggressive instruction-level parallelism transformations such as speculative and predicated execution, software pipelin-ing, if-conversion, etc. are applied <ref> [4] </ref>. Result flow distance and critical path stage order are important factors in obtaining good performance from a CFP design.
Reference: [5] <author> SPARC International, Inc., </author> <title> The SPARC Architecture Manual, Version 8, </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Engle-wood Cliffs, New Jersey, </address> <year> 1992. </year>
Reference-contexts: Experimental Framework customizing a CFP processor. The loop is compiled by the optimizer vpo [1] into instructions for the SPARC architecture <ref> [5] </ref>. During optimization, vpo builds the data dependency graph for the kernel loop. A separate synthesis module uses the graph to determine computational devices and emits a CFP specification indicating those elements.
Reference: [6] <author> Sproull R. F., Sutherland I. E., and Molnar C. E., </author> <title> The Counterflow Pipeline Processor Architecture, </title> <journal> IEEE Design and Test of Computers, pp. </journal> <volume> 48 59, Vol. 11, No. 3, </volume> <booktitle> Fall 1994. </booktitle> <volume> Graph Total Designs Heuristic 1 Heuristic 2 1 72 12 6 3 480 20 8 5 360 15 6 7 10,800 450 60 9 17,280 720 96 Table 1: Number of design points Graph 0 1 2 1 2 3 4 5 6 7 8 9 10 p e -u Opt H 1 </volume>
Reference-contexts: This is especially useful for embedded systems (e.g., automobile control systems, avionics, cellular phones, etc.) where a small increase in performance and decrease in cost can have a large impact on a products viability. A new computer organization called the Counterflow Pipeline (CFP), proposed by Sproull, Suther-land, and Molnar <ref> [6] </ref>, has several characteristics that make it a possible target organization for the synthesis of applicationspecific microprocessors. The CFP has a simple and regular structure, local control, high degree of modularity, asynchronous implementations, and inherent handling of complex structures such as result forwarding and speculative execution.
References-found: 6

