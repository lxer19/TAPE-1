URL: http://www.santafe.edu/~evca/DynCompEdge.ps
Refering-URL: http://www.santafe.edu/~evca/evabstracts.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Dynamics, Computation, and the "Edge of Chaos": A Re-Examination  
Author: Melanie Mitchell James P. Crutchfield and Peter T. Hraber 
Note: Working Paper 93-06-040 To appear in G. Cowan, D. Pines, and D. Melzner (editors), Integrative Themes. Santa Fe Institute Stuides in the Sciences of Complexity, Proceedings Volume 19. Reading, MA: Addison-Wesley.  
Affiliation: Santa Fe Institute  
Abstract: In this paper we review previous work and present new work concerning the relationship between dynamical systems theory and computation. In particular, we review work by Langton [21] and Packard [29] on the relationship between dynamical behavior and computational capability in cellular automata (CAs). We present results from an experiment similar to the one described by Packard [29], which was cited as evidence for the hypothesis that rules capable of performing complex computations are most likely to be found at a phase transition between ordered and chaotic behavioral regimes for CAs (the "edge of chaos"). Our experiment produced very different results from the original experiment, and we suggest that the interpretation of the original results is not correct. We conclude by discussing general issues related to dynamics, computation, and the "edge of chaos" in cellular automata. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Berlekamp, J. H. Conway, and R. Guy. </author> <title> Winning ways for your mathematical plays. </title> <publisher> Academic Press, </publisher> <address> New York, NY, </address> <year> 1982. </year>
Reference-contexts: Aside from performing particular computational tasks, it has been known for some time that some CAs, e.g., the Game of Life CA, are capable in principle of universal computation; this was proved for the Game of Life by explicit construction <ref> [1] </ref>. The Game of Life has c .
Reference: [2] <author> L. Blum, M. Shub, and S. Smale. </author> <title> On a theory of computation over the real numbers. </title> <journal> Bull. AMS, </journal> <volume> 21:1, </volume> <year> 1989. </year>
Reference-contexts: Rather than considering intrinsic computational structure, a number of "engineering" suggestions have been made that there exist physically plausible dynamical systems implementing Turing machines <ref> [2, 25, 26] </ref>. These studies provided explicit constructions for several types of dynamical systems. At this point, it is unclear whether the resulting computational systems are generic|i.e., likely to be constructible in other dynamical systems|or whether they are robust and reliable in information processing.
Reference: [3] <author> A. A. Brudno. </author> <title> Entropy and the complexity of the trajectories of a dynamical system. </title> <journal> Trans. Moscow Math. Soc., </journal> <volume> 44:127, </volume> <year> 1983. </year>
Reference-contexts: The relationship between the difficulty of prediction and dynamical randomness is simply summarized by the statement that the growth rate of the descriptive complexity is equal to the information production rate <ref> [3] </ref>. These results give a view of deterministic chaos that emphasizes the production of randomness and the resulting unpredictability. They are probably the earliest connections between dynamics and computation. The question of what structures underlie information production in dynamical systems has received attention only more recently.
Reference: [4] <author> G. Chaitin. </author> <title> On the length of programs for computing finite binary sequences. </title> <journal> J. ACM, </journal> <volume> 13:145, </volume> <year> 1966. </year>
Reference-contexts: One result is that a deterministic chaotic system can be viewed as a generator of information [32]. Another is that the complexity of predicting a chaotic system's behavior grows exponentially with time [7]. The complexity metric here, called the Kolmogorov-Chaitin complexity <ref> [4, 20] </ref>, uses a 2 universal Turing machine as the deterministic prediction machine. The relationship between the difficulty of prediction and dynamical randomness is simply summarized by the statement that the growth rate of the descriptive complexity is equal to the information production rate [3].
Reference: [5] <author> M. Creutz. </author> <title> Deterministic Ising dynamics. </title> <journal> Ann. Phys., </journal> <volume> 167:62, </volume> <year> 1986. </year>
Reference-contexts: For example, individual CAs have been known for some time to exhibit phase transitions <ref> [5] </ref> with the requisite divergence of correlation length required for infinite memory capacity. 13 long space-time distances and (2) particle-particle interactions that perform logical opera-tions [6]. A summary of this type of analysis of the GKL rule in terms of particles is given in [23].
Reference: [6] <author> J. P. Crutchfield and J. E. Hanson. </author> <title> Turbulent pattern bases for cellular automata. </title> <journal> Physica D, </journal> <volume> 69 </volume> <pages> 279-301, </pages> <year> 1993. </year>
Reference-contexts: This has led to a reevaluation of CA behavior classification and, in particular, to a redefinition of the chaos and complexity apparent in the spatial patterns that CAs generate <ref> [6] </ref>. Subsequent to Wolfram's work, Langton studied the relationship between the "average" dynamical behavior of cellular automata and a particular statistic () of a CA rule table [21]. <p> However, there is to date no theoretical or experimental evidence for such a relationship, and an alternative framework for analyzing computation in CAs suggests that there is no such relationship <ref> [16, 6] </ref>. Moreover, it is likely that, like -classification, any particular non-trivial computational task will have aspects that (1) require certain ranges of , not necessarily those close to c , or (2) are not reflected in at all. <p> The first steps have been taken in this direction by delineating various structural elements in CA|periodic and positive-entropy domains, intervening walls, particles, and particle interactions <ref> [16, 6] </ref>. Employing this approach, one can determine the intrinsic computational capability in CA behavior. For example, this approach gives a method for constructing nonlinear filters that remove periodic and "chaotic" domains from space-time data produced by a CA. <p> For example, individual CAs have been known for some time to exhibit phase transitions [5] with the requisite divergence of correlation length required for infinite memory capacity. 13 long space-time distances and (2) particle-particle interactions that perform logical opera-tions <ref> [6] </ref>. A summary of this type of analysis of the GKL rule in terms of particles is given in [23]. Let us close by re-emphasizing that our studies do not preclude a future rigorous and useful definition of the phrase "edge of chaos" in the context of cellular automata.
Reference: [7] <author> J. P. Crutchfield and N. H. Packard. </author> <title> Symbolic dynamics of one-dimensional maps: Entropies, finite precision, and noise. </title> <journal> Intl. J. Theo. Phys., </journal> <volume> 21:433, </volume> <year> 1982. </year>
Reference-contexts: One result is that a deterministic chaotic system can be viewed as a generator of information [32]. Another is that the complexity of predicting a chaotic system's behavior grows exponentially with time <ref> [7] </ref>. The complexity metric here, called the Kolmogorov-Chaitin complexity [4, 20], uses a 2 universal Turing machine as the deterministic prediction machine.
Reference: [8] <author> J. P. Crutchfield and N. H. Packard. </author> <title> Symbolic dynamics of noisy chaos. </title> <journal> Physica D, </journal> <volume> 7:201, </volume> <year> 1983. </year>
Reference-contexts: They are probably the earliest connections between dynamics and computation. The question of what structures underlie information production in dynamical systems has received attention only more recently. The first and crudest property considered is the amount of memory a system employs in producing apparent randomness <ref> [8, 14] </ref>. The idea is that an ideal random process uses no memory to produce its information|it simply flips a coin as needed. Similarly, a simple periodic process requires memory only in proportion to the length of the pattern it repeats.
Reference: [9] <author> J. P. Crutchfield and K. Young. </author> <title> Inferring statistical complexity. </title> <journal> Physical Review Letters, </journal> <volume> 63:105, </volume> <year> 1989. </year>
Reference-contexts: of motion on information processing: can a given complex system be designed to emulate a universal Turing machine? In contrast to this sort of engineering question, one is also interested in the intrinsic computational capability of a given complex system; that is, what information-processing structures are intrinsic in its behavior? <ref> [9, 16] </ref> Dynamical systems theory and computation theory have historically been applied independently, but there have been some efforts to understand the relationship between the two|that is, the relationship between a system's ability for information processing and other measures of the system's dynamical behavior. <p> Such processes are more complex to describe statistically than are ideal random or simple periodic processes. The trade-off between structure and randomness is common to much of science. The notion of statistical complexity <ref> [9] </ref> was introduced to measure this trade-off. Computation theory is concerned with more than information and its production and storage. These elements are taken as given and, instead, the focus is on how their combinations yield more or less computational power. <p> Several connections in this vein have been made recently. In the realm of continuous-state dynamical systems, Crutchfield and Young looked at the relationship between the dynamics and computational structure of discrete time series generated by the logistic map at different parameter settings <ref> [9, 10] </ref>. They found that at the onset of chaos there is an abrupt jump in computational class of the time series, as measured by the formal language class required to describe the time series.
Reference: [10] <author> J. P. Crutchfield and K. Young. </author> <title> Computation at the onset of chaos. </title> <editor> In W. H. Zurek, ed-itor, </editor> <title> Complexity, Entropy, </title> <journal> and the Physics of Information, </journal> <pages> pages 223-269. </pages> <publisher> Addison-Wesley, </publisher> <address> Redwood City, CA, </address> <year> 1990. </year>
Reference-contexts: Several connections in this vein have been made recently. In the realm of continuous-state dynamical systems, Crutchfield and Young looked at the relationship between the dynamics and computational structure of discrete time series generated by the logistic map at different parameter settings <ref> [9, 10] </ref>. They found that at the onset of chaos there is an abrupt jump in computational class of the time series, as measured by the formal language class required to describe the time series. <p> The results presented here do not disprove the hypothesis that computational capability can be correlated with phase transitions in CA rule space. 4 Indeed, this general phenomena has already been noted for other dynamical systems, as noted in the introduction <ref> [10] </ref>. More generally, the computational capacity of evolving systems may very well require dynamical properties characteristic of phase transitions if such systems are to increase their complexity. We have shown only that the published experimental support cited for hypotheses relating c and computational capability in CAs was not reproduced.
Reference: [11] <editor> D. Farmer, T. Toffoli, and S. Wolfram, editors. </editor> <booktitle> Cellular Automata: Proceedings of an Interdisciplinary Workshop. </booktitle> <publisher> North Holland, </publisher> <address> Amsterdam, </address> <year> 1984. </year>
Reference-contexts: Cellular Automata and the "Edge of Chaos" Cellular automata are one of the simplest frameworks in which issues related to complex systems, dynamics, and computation can be studied. CAs have been used extensively as models of physical processes and as computational devices <ref> [11, 15, 30, 34, 36] </ref>. In its simplest form, a CA consists of a spatial lattice of cells, each of which, at time t, can be in one of k states. We denote the lattice size (i.e., number of cells) as N.
Reference: [12] <author> M. J. Feigenbaum. </author> <title> Universal behavior in nonlinear systems. </title> <journal> Physica D, </journal> <volume> 7:16, </volume> <year> 1983. </year>
Reference-contexts: They found that at the onset of chaos there is an abrupt jump in computational class of the time series, as measured by the formal language class required to describe the time series. In concert with Feigenbaum's renormalization group analysis of the onset of chaos <ref> [12] </ref>, this result demonstrated that a dynamical system's computational capability|in terms of the richness of behavior it produces|is qualitatively increased at a phase transition.
Reference: [13] <author> P. Gacs, G. L. Kurdyumov, and L. A. Levin. </author> <title> One-dimensional uniform arrays that wash out finite islands. Probl. </title> <journal> Peredachi. Inform., </journal> <volume> 14 </volume> <pages> 92-98, </pages> <year> 1978. </year>
Reference-contexts: The final pattern is interpreted as the "output." An example of this is using CAs to perform image-processing tasks [31]. Packard [29] discussed a particular k = 2; r = 3 rule, invented by Gacs, Kurdyumov, and Levin (GKL) <ref> [13] </ref> as part of their studies of reliable computation in CAs.
Reference: [14] <author> P. Grassberger. </author> <title> Toward a quantitative theory of self-generated complexity. </title> <journal> Intl. J. Theo. Phys., </journal> <volume> 25:907, </volume> <year> 1986. </year>
Reference-contexts: They are probably the earliest connections between dynamics and computation. The question of what structures underlie information production in dynamical systems has received attention only more recently. The first and crudest property considered is the amount of memory a system employs in producing apparent randomness <ref> [8, 14] </ref>. The idea is that an ideal random process uses no memory to produce its information|it simply flips a coin as needed. Similarly, a simple periodic process requires memory only in proportion to the length of the pattern it repeats.
Reference: [15] <author> H. A. Gutowitz, </author> <title> editor. Cellular Automata. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: Cellular Automata and the "Edge of Chaos" Cellular automata are one of the simplest frameworks in which issues related to complex systems, dynamics, and computation can be studied. CAs have been used extensively as models of physical processes and as computational devices <ref> [11, 15, 30, 34, 36] </ref>. In its simplest form, a CA consists of a spatial lattice of cells, each of which, at time t, can be in one of k states. We denote the lattice size (i.e., number of cells) as N.
Reference: [16] <author> J. E. Hanson and J. P. Crutchfield. </author> <title> The attractor-basin portrait of a cellular automaton. </title> <journal> Journal of Statistical Physics, </journal> 66(5/6):1415-1462, 1992. 
Reference-contexts: of motion on information processing: can a given complex system be designed to emulate a universal Turing machine? In contrast to this sort of engineering question, one is also interested in the intrinsic computational capability of a given complex system; that is, what information-processing structures are intrinsic in its behavior? <ref> [9, 16] </ref> Dynamical systems theory and computation theory have historically been applied independently, but there have been some efforts to understand the relationship between the two|that is, the relationship between a system's ability for information processing and other measures of the system's dynamical behavior. <p> He speculated that one of his four classes supports universal computation [35]. It is only recently, however, that CA behavior has been directly related to the basic elements of qualitative dynamics|the attractor-basin portrait <ref> [16] </ref>. This has led to a reevaluation of CA behavior classification and, in particular, to a redefinition of the chaos and complexity apparent in the spatial patterns that CAs generate [6]. <p> However, there is to date no theoretical or experimental evidence for such a relationship, and an alternative framework for analyzing computation in CAs suggests that there is no such relationship <ref> [16, 6] </ref>. Moreover, it is likely that, like -classification, any particular non-trivial computational task will have aspects that (1) require certain ranges of , not necessarily those close to c , or (2) are not reflected in at all. <p> The first steps have been taken in this direction by delineating various structural elements in CA|periodic and positive-entropy domains, intervening walls, particles, and particle interactions <ref> [16, 6] </ref>. Employing this approach, one can determine the intrinsic computational capability in CA behavior. For example, this approach gives a method for constructing nonlinear filters that remove periodic and "chaotic" domains from space-time data produced by a CA.
Reference: [17] <author> J. H. Holland. </author> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1992. </year> <note> Second edition (First edition, </note> <year> 1975). </year>
Reference-contexts: The Original Experiment Langton's empirical CA studies recounted above addressed only the relationship between and the dynamical behavior of CAs as revealed by several statistics. Those studies did not correlate or behavior with an independent measure of computation. Packard [29] addressed this issue by using a genetic algorithm (GA) <ref> [17] </ref> to evolve CA rules to perform a particular computation.
Reference: [18] <author> J. E. Hopcroft and J. D. Ullman. </author> <title> Introduction to Automata Theory, Languages, and Computation. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, </address> <year> 1979. </year>
Reference-contexts: In dynamical terms, complex computation in a small-radius, binary-state CA requires significantly long transients and space-time correlation lengths. Langton hypothesized that such effects are most likely to be seen in a certain region of CA rule space as parameterized 3 See Hopcroft and Ullman <ref> [18] </ref> for an introduction to formal-language classes in computation theory. 5 by [21]. For binary-state CAs, is simply the fraction of 1's in the output bits of the rule table.
Reference: [19] <author> A. N. </author> <title> Kolmogorov. Entropy per unit time as a metric invariant of automorphisms. </title> <journal> Dokl. Akad. Nauk. SSSR, 124:754, 1959. (Russian) Math. Rev. </journal> <volume> vol. 21, no. </volume> <year> 2035b. </year>
Reference-contexts: Beginning with Kolmogorov's and Sinai's adaptation of Shannon's communication theory to mechanics in the late 1950s <ref> [19, 33] </ref>, there has been a continuing effort to relate a nonlinear system's information-processing capability and its temporal behavior. One result is that a deterministic chaotic system can be viewed as a generator of information [32].
Reference: [20] <author> A. N. </author> <title> Kolmogorov. Three approaches to the concept of the amount of information. </title> <journal> Prob. Info. Trans., </journal> <volume> 1:1, </volume> <year> 1965. </year>
Reference-contexts: One result is that a deterministic chaotic system can be viewed as a generator of information [32]. Another is that the complexity of predicting a chaotic system's behavior grows exponentially with time [7]. The complexity metric here, called the Kolmogorov-Chaitin complexity <ref> [4, 20] </ref>, uses a 2 universal Turing machine as the deterministic prediction machine. The relationship between the difficulty of prediction and dynamical randomness is simply summarized by the statement that the growth rate of the descriptive complexity is equal to the information production rate [3].
Reference: [21] <author> C. G. Langton. </author> <title> Computation at the edge of chaos: Phase transitions and emergent computation. </title> <journal> Physica D, </journal> <volume> 42 </volume> <pages> 12-37, </pages> <year> 1990. </year>
Reference-contexts: Subsequent to Wolfram's work, Langton studied the relationship between the "average" dynamical behavior of cellular automata and a particular statistic () of a CA rule table <ref> [21] </ref>. He then hypothesized that "computationally capable" CAs and, in particular, CAs capable of universal computation will have "critical" values corresponding to a phase transition between ordered and chaotic behavior. <p> Langton hypothesized that such effects are most likely to be seen in a certain region of CA rule space as parameterized 3 See Hopcroft and Ullman [18] for an introduction to formal-language classes in computation theory. 5 by <ref> [21] </ref>. For binary-state CAs, is simply the fraction of 1's in the output bits of the rule table. <p> For CAs with k &gt; 2, is defined as the fraction of "nonquiescent" states in the rule table, where one state is arbitrarily chosen to be "quiescent," and all states obey a "strong quiescence" requirement <ref> [21] </ref>. Langton performed a number of Monte Carlo samples of two-dimensional CAs, starting with = 0 and gradually increasing to 1 1=k (i.e., the most homogeneous to the most heterogeneous rule tables). <p> We have shown only that the published experimental support cited for hypotheses relating c and computational capability in CAs was not reproduced. One problem is that these hypotheses have not been unambiguously formulated. If the hypotheses put forth by Langton <ref> [21] </ref> and Packard [29] are interpreted to mean that any rule performing complex computation (as exemplified by the c = 1=2 task) must be close to c , then we have shown it to be false with our argument that correct performance on the c = 1=2 task requires = 1=2.
Reference: [22] <author> C. G. Langton. </author> <title> Computation at the edge of chaos: Phase transitions and emergent computation. </title> <type> PhD thesis, </type> <institution> The University of Michigan, </institution> <address> Ann Arbor, MI, </address> <year> 1991. </year>
Reference-contexts: Aside from performing particular computational tasks, it has been known for some time that some CAs, e.g., the Game of Life CA, are capable in principle of universal computation; this was proved for the Game of Life by explicit construction [1]. The Game of Life has c . Langton <ref> [22] </ref> sketched a construction of some components of a universal computer (similar to those used in the construction for the Game of Life) in another particular two-dimensional CA in the "complex regime." However, these particular constructions do not establish any generic relationship between c and the ability for complex, or even
Reference: [23] <author> M. Mitchell, J. P. Crutchfield, and P. T. Hraber. </author> <title> Evolving cellular automata to perform computations: Mechanisms and impediments. </title> <journal> Physica D, </journal> <note> in press, </note> <year> 1994. </year>
Reference-contexts: Our experiments, however, show some interesting phenomena with respect to the GA evolution of CAs, which we summarize here. Longer, more detailed descriptions of our experiments and results are given in <ref> [24, 23] </ref>. 2. Cellular Automata and the "Edge of Chaos" Cellular automata are one of the simplest frameworks in which issues related to complex systems, dynamics, and computation can be studied. CAs have been used extensively as models of physical processes and as computational devices [11, 15, 30, 34, 36]. <p> To test the robustness of our results, we have performed a wide range of additional experiments. Not only have the results held up, but these experiments have pointed to a number of novel mechanisms that control the interaction of evolution and computation <ref> [23] </ref>. What Causes the Dip at = 1=2? Aside from the many differences between Figure 1 (b) and Figure 1 (c), there is one rough similarity: the histogram shows two symmetrical peaks surrounding a central dip. <p> results from the combination of a number of forces: the selection and combinatorial drift forces described above push the population toward = 1=2, and the error-resisting forces just described push the population away from = 1=2. (Details of the epochs the GA undergoes in developing these strategies are described in <ref> [24, 23] </ref>.) It is important to understand how in general such symmetry breaking can impede an evolutionary process from finding optimal strategies. This is a subject we are currently investigating. 6. <p> A summary of this type of analysis of the GKL rule in terms of particles is given in <ref> [23] </ref>. Let us close by re-emphasizing that our studies do not preclude a future rigorous and useful definition of the phrase "edge of chaos" in the context of cellular automata. Nor do they preclude the discovery that it is associated with a CA's increased computational capability.
Reference: [24] <author> M. Mitchell, P. T. Hraber, and J. P. Crutchfield. </author> <title> Revisiting the edge of chaos: Evolving cellular automata to perform computations. </title> <journal> Complex Systems, </journal> <volume> 7 </volume> <pages> 89-130, </pages> <year> 1993. </year>
Reference-contexts: Our experiments, however, show some interesting phenomena with respect to the GA evolution of CAs, which we summarize here. Longer, more detailed descriptions of our experiments and results are given in <ref> [24, 23] </ref>. 2. Cellular Automata and the "Edge of Chaos" Cellular automata are one of the simplest frameworks in which issues related to complex systems, dynamics, and computation can be studied. CAs have been used extensively as models of physical processes and as computational devices [11, 15, 30, 34, 36]. <p> The notion of "computation" in CAs can have several possible meanings <ref> [24] </ref>, but the most common meaning is that the CA performs some "useful" computational task. <p> The rule thus approximately computes whether the density of 1's in the initial configuration (which we denote as ) is above the threshold c = 1=2. When initial configurations are close to = 1=2, the rule makes a significant number of classification errors <ref> [24] </ref>. Packard was inspired by the GKL rule to use a GA to evolve a rule table to perform this " c = 1=2" task. If &lt; 1=2, then the CA should relax to a configuration of all 0's; otherwise, it should relax to a configuration of all 1's. <p> other words, the required computation is spatially global and corresponds to the recognition of a nonregular language. 3 The global nature of the computation means that information must be transmitted over significant space-time distances (on the order of N ) and this requires the cooperation of many local neighborhood operations <ref> [24] </ref>. In dynamical terms, complex computation in a small-radius, binary-state CA requires significantly long transients and space-time correlation lengths. <p> Thus, gets partial credit for getting some of the bits correct. A rule generating random strings would therefore get an average score of approximately 0.5. 's fitness is its average score over all 300 initial configurations. For more details and for justifications for these parameters, see Mitchell et al. <ref> [24] </ref>. The results of our experiment are given in Figure 1 (c). This histogram displays the observed frequency of rules in the population at generation 100 as a function of , merged from 30 different runs with identical parameters but different random number seeds. <p> Under this fitness function the GKL rule has fitness 0:98 on all 9 lattice sizes; the GA never found a rule with fitness above 0.95 on lattice size 149, and the measured fitness of the best evolved rules was much worse on larger lattice sizes <ref> [24] </ref>. The fitnesses of the rules in Figure 1 (b) were not given by Packard [29], though none of those rules achieved the fitness of the GKL rule [28]. 5. <p> The results of experiments measuring the relative effects of this force and the selection force in our experiment are given elsewhere <ref> [24] </ref>. <p> Such strategies are vulnerable to a number of classification errors. For example, a rule might create a sufficiently sized block of 1's that was not present in an initial configuration with &lt; 1=2 and increase its size to yield an incorrect final configuration. But, as is explained in <ref> [24] </ref>, rules with &lt; 1=2 (for Strategy 1) and rules with &gt; 1=2 (for Strategy 2) are less vulnerable to such errors than are rules with = 1=2. <p> results from the combination of a number of forces: the selection and combinatorial drift forces described above push the population toward = 1=2, and the error-resisting forces just described push the population away from = 1=2. (Details of the epochs the GA undergoes in developing these strategies are described in <ref> [24, 23] </ref>.) It is important to understand how in general such symmetry breaking can impede an evolutionary process from finding optimal strategies. This is a subject we are currently investigating. 6.
Reference: [25] <author> C. Moore. </author> <title> Unpredictability and undecidability in dynamical systems. </title> <journal> Phys. Rev. Lett., </journal> <volume> 64:2354, </volume> <year> 1990. </year>
Reference-contexts: Rather than considering intrinsic computational structure, a number of "engineering" suggestions have been made that there exist physically plausible dynamical systems implementing Turing machines <ref> [2, 25, 26] </ref>. These studies provided explicit constructions for several types of dynamical systems. At this point, it is unclear whether the resulting computational systems are generic|i.e., likely to be constructible in other dynamical systems|or whether they are robust and reliable in information processing.
Reference: [26] <author> S. Omohundro. </author> <title> Modelling cellular automata with partial differential equations. </title> <journal> Physica D, </journal> <volume> 10:128, </volume> <year> 1984. </year>
Reference-contexts: Rather than considering intrinsic computational structure, a number of "engineering" suggestions have been made that there exist physically plausible dynamical systems implementing Turing machines <ref> [2, 25, 26] </ref>. These studies provided explicit constructions for several types of dynamical systems. At this point, it is unclear whether the resulting computational systems are generic|i.e., likely to be constructible in other dynamical systems|or whether they are robust and reliable in information processing.
Reference: [27] <author> N. Packard. </author> <title> Complexity of growing patterns in cellular automata. </title> <editor> In J. Demongeot, E. Goles, and M. Tchuente, editors, </editor> <title> Dynamical Behavior of Automata: Theory and Applications. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1984. </year> <month> 15 </month>
Reference-contexts: The spreading rate fl is a measure of unpredictability in spatio-temporal patterns and so is one possible measure of chaotic behavior <ref> [27, 35] </ref>. It is analogous to, but not the same as, the Lyapunov exponent for continuous-state dynamical systems. In the case of CAs it indicates the average propagation speed of information through space-time, though not the production rate of local information.
Reference: [28] <author> N. H. Packard. </author> <type> Personal communication. </type>
Reference-contexts: The rules cluster close to the two c regions, as can be seen by comparison with the difference-pattern spreading rate plot (a). Note that each individual run produced rules at one or the other peak in graph (b), so when the runs were merged together, both peaks appear <ref> [28] </ref>. Packard interpreted these results as evidence for the hypothesis that, when an ability for complex computation is required, evolution tends to select rules near the transition to chaos. <p> The fitnesses of the rules in Figure 1 (b) were not given by Packard [29], though none of those rules achieved the fitness of the GKL rule <ref> [28] </ref>. 5. <p> For example, the original experiment included a number of additional sources of randomness, such as the regular injection of new random rules at various values and a much higher mutation rate than that in our experiment <ref> [28] </ref>. These sources of randomness may have slowed the GA's search for high-fitness rules and prevented it from converging on rules close to = 1=2. The key observable for this, the fitness of the evolved CAs, was not reported by Packard [29].
Reference: [29] <author> N. H. Packard. </author> <title> Adaptation toward the edge of chaos. </title> <editor> In J. A. S. Kelso, A. J. Mandell, and M. F. Shlesinger, editors, </editor> <booktitle> Dynamic Patterns in Complex Systems, </booktitle> <pages> pages 293-301, </pages> <address> Singapore, 1988. </address> <publisher> World Scientific. </publisher>
Reference-contexts: He then hypothesized that "computationally capable" CAs and, in particular, CAs capable of universal computation will have "critical" values corresponding to a phase transition between ordered and chaotic behavior. Packard experimentally tested this hypothesis by using a genetic algorithm (GA) to evolve CAs to perform a particular complex computation <ref> [29] </ref>. He interpreted the results as showing that the GA tends to select CAs close to "critical" regions|i.e., the "edge of chaos." We now turn our discussion more specifically to issues related to , dynamical-behavior classes, and computation in CAs. <p> We then present experimental results and a theoretical discussion that suggest the interpretation given of the results by Packard <ref> [29] </ref> is not correct. Our experiments, however, show some interesting phenomena with respect to the GA evolution of CAs, which we summarize here. Longer, more detailed descriptions of our experiments and results are given in [24, 23]. 2. <p> The final pattern is interpreted as the "output." An example of this is using CAs to perform image-processing tasks [31]. Packard <ref> [29] </ref> discussed a particular k = 2; r = 3 rule, invented by Gacs, Kurdyumov, and Levin (GKL) [13] as part of their studies of reliable computation in CAs. <p> Perhaps most problematic is the assumption that the selected statistics are uniquely associated with mechanisms that support useful computation. Packard empirically determined rough values of c for one-dimensional k = 2; r = 3 CAs by looking at the difference-pattern spreading rate fl as a function of <ref> [29] </ref>. The spreading rate fl is a measure of unpredictability in spatio-temporal patterns and so is one possible measure of chaotic behavior [27, 35]. It is analogous to, but not the same as, the Lyapunov exponent for continuous-state dynamical systems. <p> The Original Experiment Langton's empirical CA studies recounted above addressed only the relationship between and the dynamical behavior of CAs as revealed by several statistics. Those studies did not correlate or behavior with an independent measure of computation. Packard <ref> [29] </ref> addressed this issue by using a genetic algorithm (GA) [17] to evolve CA rules to perform a particular computation. <p> He argues, like Langton, that this result intuitively makes sense because "rules near the transition to chaos have the capability to selectively communicate information with complex structures in space-time, thus enabling computation." <ref> [29] </ref>. 4. Our Experiment We performed an experiment similar to Packard's. The rules in the population are represented as bit strings, each encoding the output bits of a rule table for (k; r) = (2; 3). Thus, the length of each string is 128 = 2 2r+1 . <p> These populations evolved from initial populations uniformly distributed in . The histogram consists of 16 bins of width 0.0667. The bin above = 1:0 contains just those rules with = 1:0. Graphs (a) and (b) are adapted from <ref> [29] </ref>, with the author's permission. No vertical scale was provided there. (c): Results from our experiment. The histogram plots the frequencies of rules merged from the final generations (generation 100) of 30 runs. These populations evolved from initial populations uniformly distributed in . Following [29] the -axis is divided into 15 <p> (a) and (b) are adapted from <ref> [29] </ref>, with the author's permission. No vertical scale was provided there. (c): Results from our experiment. The histogram plots the frequencies of rules merged from the final generations (generation 100) of 30 runs. These populations evolved from initial populations uniformly distributed in . Following [29] the -axis is divided into 15 bins of length 0.0667 each. The rules with = 1:0 are included in the rightmost bin. <p> The fitnesses of the rules in Figure 1 (b) were not given by Packard <ref> [29] </ref>, though none of those rules achieved the fitness of the GKL rule [28]. 5. <p> This is an important conclusion, since Packard's work <ref> [29] </ref> is the only published experimental study directly linking with computational ability in CAs. Since -classification is only one particular class of tasks, this conclusion does not directly invalidate speculations about a generic relationship between and computational ability in CA. <p> We do not know for certain what accounted for the differences between our experimental results and those obtained by Packard. We speculate that the differences are due to additional mechanisms in the GA used in the original experiment that were not reported by Packard <ref> [29] </ref>. For example, the original experiment included a number of additional sources of randomness, such as the regular injection of new random rules at various values and a much higher mutation rate than that in our experiment [28]. <p> These sources of randomness may have slowed the GA's search for high-fitness rules and prevented it from converging on rules close to = 1=2. The key observable for this, the fitness of the evolved CAs, was not reported by Packard <ref> [29] </ref>. Our experimental results and theoretical analysis indicate that the clustering close to c seen in Figure 1 (b) is almost certainly an artifact of mechanisms in the particular GA that was used rather than a result of any computational advantage conferred by the c regions. <p> We have presented theoretical arguments and results from an experiment similar to Packard's. From these we conclude that Packard's interpretation of his results was not correct. We believe that those original results were due to mechanisms in the particular GA used in that experiment <ref> [29] </ref> rather than to intrinsic computational properties of c CAs. In addition, as we have noted, specific properties of the c = 1=2 task invalidate it as an evolutionary goal for testing the "evolution to c " hypothesis. <p> We have shown only that the published experimental support cited for hypotheses relating c and computational capability in CAs was not reproduced. One problem is that these hypotheses have not been unambiguously formulated. If the hypotheses put forth by Langton [21] and Packard <ref> [29] </ref> are interpreted to mean that any rule performing complex computation (as exemplified by the c = 1=2 task) must be close to c , then we have shown it to be false with our argument that correct performance on the c = 1=2 task requires = 1=2.
Reference: [30] <author> K. Preston and M. Duff. </author> <title> Modern Cellular Automata. </title> <publisher> Plenum, </publisher> <address> New York, </address> <year> 1984. </year>
Reference-contexts: Cellular Automata and the "Edge of Chaos" Cellular automata are one of the simplest frameworks in which issues related to complex systems, dynamics, and computation can be studied. CAs have been used extensively as models of physical processes and as computational devices <ref> [11, 15, 30, 34, 36] </ref>. In its simplest form, a CA consists of a spatial lattice of cells, each of which, at time t, can be in one of k states. We denote the lattice size (i.e., number of cells) as N.
Reference: [31] <author> A. Rosenfeld. </author> <title> Parallel image processing using cellular arrays. </title> <booktitle> Computer, </booktitle> <address> 16:14, </address> <year> 1983. </year>
Reference-contexts: The final pattern is interpreted as the "output." An example of this is using CAs to perform image-processing tasks <ref> [31] </ref>. Packard [29] discussed a particular k = 2; r = 3 rule, invented by Gacs, Kurdyumov, and Levin (GKL) [13] as part of their studies of reliable computation in CAs.
Reference: [32] <author> R. Shaw. </author> <title> Strange attractors, chaotic behavior, and information flow. </title> <editor> Z. Naturforsh., 36a:80, </editor> <year> 1981. </year>
Reference-contexts: One result is that a deterministic chaotic system can be viewed as a generator of information <ref> [32] </ref>. Another is that the complexity of predicting a chaotic system's behavior grows exponentially with time [7]. The complexity metric here, called the Kolmogorov-Chaitin complexity [4, 20], uses a 2 universal Turing machine as the deterministic prediction machine.
Reference: [33] <author> Ja. G. Sinai. </author> <title> On the notion of entropy of a dynamical system. </title> <journal> Dokl. Akad. Nauk. SSSR, </journal> <volume> 124:768, </volume> <year> 1959. </year>
Reference-contexts: Beginning with Kolmogorov's and Sinai's adaptation of Shannon's communication theory to mechanics in the late 1950s <ref> [19, 33] </ref>, there has been a continuing effort to relate a nonlinear system's information-processing capability and its temporal behavior. One result is that a deterministic chaotic system can be viewed as a generator of information [32].
Reference: [34] <author> T. Toffoli and N. Margolus. </author> <title> Cellular Automata Machines: A new environment for modeling. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1987. </year>
Reference-contexts: Cellular Automata and the "Edge of Chaos" Cellular automata are one of the simplest frameworks in which issues related to complex systems, dynamics, and computation can be studied. CAs have been used extensively as models of physical processes and as computational devices <ref> [11, 15, 30, 34, 36] </ref>. In its simplest form, a CA consists of a spatial lattice of cells, each of which, at time t, can be in one of k states. We denote the lattice size (i.e., number of cells) as N.
Reference: [35] <author> S. Wolfram. </author> <title> Universality and complexity in cellular automata. </title> <journal> Physica D, </journal> <volume> 10 </volume> <pages> 1-35, </pages> <year> 1984. </year>
Reference-contexts: Nonetheless, Wolfram introduced a dynamical classification of CA behavior closely allied to that of dynamical systems theory. He speculated that one of his four classes supports universal computation <ref> [35] </ref>. It is only recently, however, that CA behavior has been directly related to the basic elements of qualitative dynamics|the attractor-basin portrait [16]. <p> As reaches a "critical value" c , the claim is that rules tend to have longer and longer transient phases. Additionally, Langton claimed that CAs close to c tend to exhibit long-lived, "complex"|nonperiodic, but nonrandom|patterns. Langton proposed that the c regime roughly corresponds to Wolfram's Class 4 CAs <ref> [35] </ref>, and hypothesized that CAs capable of performing complex computations will most likely be found in this regime. Analysis based on is one possible first step in understanding the structure of CA rule space and the relationship between dynamics and computation in CAs. <p> The spreading rate fl is a measure of unpredictability in spatio-temporal patterns and so is one possible measure of chaotic behavior <ref> [27, 35] </ref>. It is analogous to, but not the same as, the Lyapunov exponent for continuous-state dynamical systems. In the case of CAs it indicates the average propagation speed of information through space-time, though not the production rate of local information.
Reference: [36] <author> S. Wolfram, </author> <title> editor. Theory and applications of cellular automata. </title> <publisher> World Scientific, </publisher> <address> Singapore, </address> <year> 1986. </year>
Reference-contexts: Cellular Automata and the "Edge of Chaos" Cellular automata are one of the simplest frameworks in which issues related to complex systems, dynamics, and computation can be studied. CAs have been used extensively as models of physical processes and as computational devices <ref> [11, 15, 30, 34, 36] </ref>. In its simplest form, a CA consists of a spatial lattice of cells, each of which, at time t, can be in one of k states. We denote the lattice size (i.e., number of cells) as N.
References-found: 36

