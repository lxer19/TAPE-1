URL: file://ftp.cis.ohio-state.edu/pub/tech-report/1994/TR40.ps.gz
Refering-URL: ftp://ftp.cis.ohio-state.edu/pub/tech-report/TRList.html
Root-URL: 
Title: On Mapping Data and Computation for Parallel Sparse Cholesky Factorization  
Author: Kalluri Eswar C.-H. Huang P. Sadayappan 
Address: Columbus Ohio 43210  
Affiliation: Department of Computer and Information Science The Ohio State University  
Abstract: When performing the Cholesky factorization of a sparse matrix on a distributed-memory multiprocessor, the methods used for mapping the elements of the matrix and the operations constituting the factorization to the processors can have a significant impact on the communication overhead incurred. This paper explores how two techniques, one used when mapping dense Cholesky factorization and the other used when mapping sparse Cholesky factorization, can be integrated to achieve a communication-efficient parallel sparse Cholesky factorization. Two localizing techniques to further reduce the communication overhead are also described. The mapping strategies proposed here, as well as other previously proposed strategies fit into the unifying framework developed in this paper. Communication statistics for sample sparse matrices are included. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Ashcraft, </author> <title> A taxonomy of distributed dense LU factorization methods, Boeing Computer Services, </title> <type> Technical Report No. </type> <institution> ECA-TR-161, </institution> <year> 1991. </year> <month> 37 </month>
Reference-contexts: Table 3: Communication statistics for LDLC Comm. type Mesg. count Mesg. volume Factor 1721 102992 Source 890 75276 Local aggr. 146 9158 Global aggr. 3083 77815 Total 5840 265241 7 Related work Various previous works have addressed the use of index-based mapping strategies for dense matrix factorization <ref> [14, 9, 1] </ref>. Several researchers have also proposed nested mapping strategies for sparse matrix factorization [12, 10, 16, 7]. The combination of index-based mapping and nested mapping strategies has been studied in 34 dependently by other researchers in specific contexts.
Reference: [2] <author> C. Ashcraft, </author> <title> The fan-both family of column-based distributed Cholesky factorization algo-rithms, Boeing Computer Services, </title> <type> Technical Report No. </type> <institution> MEA-TR-208, </institution> <year> 1992. </year>
Reference-contexts: The block fan-out algorithm presented in [17] corresponds to an index-based mapping scheme where P A is unity, i.e., it is a F fi S scheme, using the notation of this paper. The fan-both algorithm presented in <ref> [2] </ref> corresponds to a mapping scheme where P F is unity. Nested mapping is not used in [2], but can easily be used as shown here. In a multifrontal algorithm, each node in the elimination tree is associated with a dense lower triangular matrix called its front. <p> The fan-both algorithm presented in <ref> [2] </ref> corresponds to a mapping scheme where P F is unity. Nested mapping is not used in [2], but can easily be used as shown here. In a multifrontal algorithm, each node in the elimination tree is associated with a dense lower triangular matrix called its front. The front consists of the column itself and one column for each of its target columns.
Reference: [3] <author> C. Ashcraft, S. Eisenstat, and J. W.-H. Liu, </author> <title> A fan-in algorithm for distributed sparse numerical factorization, </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> Vol. 11, </volume> <pages> pp. 593-599, </pages> <year> 1990. </year>
Reference-contexts: The combination of a column-level data mapping and the target computes rule for the computation mapping results in a parallel factorization scheme called the column fan-out algorithm [11]. Similarly, the use of the source computes rule for the computation mapping results in the column fan-in algorithm <ref> [3] </ref>. This section reviews the nested mapping approach for data mapping and compares the effect it has on these two algorithms. 3.1 Nested mapping basics The column-level dependence structure of a sparse Cholesky factorization can be captured succinctly by a tree called the elimination tree [18].
Reference: [4] <author> A. J. Cleary, </author> <title> A new torus-like mapping for parallel sparse matrix factorization, Advanced Computation Group, </title> <institution> Australian National University, </institution> <note> Technical Report No. ACTR 13 09 92, </note> <year> 1992. </year>
Reference-contexts: The combination of index-based mapping and nested mapping strategies has been studied in 34 dependently by other researchers in specific contexts. In [13], a mapping strategy for a multifrontal algorithm [5, 15] is presented that combines a two-dimensional partitioning of the matrix with a subtree-to-subcube mapping [12]. In <ref> [4] </ref>, a mapping strategy is presented that integrates the torus wrap mapping and a nested mapping based on the elimination tree. However, the present work is the first attempt at a comprehensive characterization of the communication implications of integrating index-based mappings with nested mappings.
Reference: [5] <author> I. Duff and J. Reid, </author> <title> The multifrontal solution of indefinite sparse symmetric linear equations, </title> <journal> ACM Trans. Math. Software, </journal> <volume> Vol. 9, </volume> <pages> pp. 302-325, </pages> <year> 1983. </year>
Reference-contexts: Several researchers have also proposed nested mapping strategies for sparse matrix factorization [12, 10, 16, 7]. The combination of index-based mapping and nested mapping strategies has been studied in 34 dependently by other researchers in specific contexts. In [13], a mapping strategy for a multifrontal algorithm <ref> [5, 15] </ref> is presented that combines a two-dimensional partitioning of the matrix with a subtree-to-subcube mapping [12]. In [4], a mapping strategy is presented that integrates the torus wrap mapping and a nested mapping based on the elimination tree.
Reference: [6] <author> K. Eswar, C.-H. Huang, and P. Sadayappan, </author> <title> On mapping data and computation for parallel sparse Cholesky factorization, </title> <institution> Department of Computer and Information Science, The Ohio State University, </institution> <type> Technical Report No. </type> <institution> OSU-CISRC-7/94-TR40, </institution> <year> 1994. </year>
Reference: [7] <author> K. Eswar, P. Sadayappan, and V. Visvanathan, </author> <title> Multifrontal factorization of sparse matrices on shared-memory multiprocessors, </title> <booktitle> Proceedings of the Twentieth International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <booktitle> Vol. III, </booktitle> <pages> pp. 159-166, </pages> <year> 1991. </year>
Reference-contexts: When this occurs, the algorithm is recursively applied to each of the t sets of subtrees using the corresponding subtree in the composite processor tree. The recursive partitioning mapping algorithm <ref> [7] </ref> is a special case of the above nested mapping algorithm in which the composite processor tree is a complete binary tree, and the the group of processors associated with each node is equally split between its two daughters. <p> Several researchers have also proposed nested mapping strategies for sparse matrix factorization <ref> [12, 10, 16, 7] </ref>. The combination of index-based mapping and nested mapping strategies has been studied in 34 dependently by other researchers in specific contexts.
Reference: [8] <author> K. Eswar, P. Sadayappan, and V. Visvanathan, </author> <title> Parallel direct solution of sparse linear systems, in Parallel Computing on Distributed Memory Multiprocessors, </title> <editor> F. Ozg uner and F. Ercal (eds.), </editor> <publisher> Springer Verlag, </publisher> <address> Berlin Heidelberg New York, </address> <year> 1993. </year>
Reference-contexts: The effect may be expected to be much more substantial than for the column fan-out algorithm. These expectations are borne out by experimental results <ref> [8] </ref>. 3.3 Grouping of processors The manner in which processor groups are formed may be specified by means of a tree called the composite processor tree. The root of this tree is a node associated with the set of all processors.
Reference: [9] <editor> G. Fox, et al., </editor> <booktitle> Solving problems on concurrent processors: </booktitle> <volume> Volume 1, </volume> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: Table 3: Communication statistics for LDLC Comm. type Mesg. count Mesg. volume Factor 1721 102992 Source 890 75276 Local aggr. 146 9158 Global aggr. 3083 77815 Total 5840 265241 7 Related work Various previous works have addressed the use of index-based mapping strategies for dense matrix factorization <ref> [14, 9, 1] </ref>. Several researchers have also proposed nested mapping strategies for sparse matrix factorization [12, 10, 16, 7]. The combination of index-based mapping and nested mapping strategies has been studied in 34 dependently by other researchers in specific contexts.
Reference: [10] <author> G. Geist and E. G.-Y. Ng, </author> <title> Task scheduling for parallel sparse Cholesky factorization, </title> <journal> Internat. J. Parallel Programming, </journal> <volume> Vol. 18, </volume> <pages> pp. 291-314, </pages> <year> 1989. </year> <month> 38 </month>
Reference-contexts: Several researchers have also proposed nested mapping strategies for sparse matrix factorization <ref> [12, 10, 16, 7] </ref>. The combination of index-based mapping and nested mapping strategies has been studied in 34 dependently by other researchers in specific contexts.
Reference: [11] <author> A. George, M. Heath, J. W.-H. Liu, and E. G.-Y. Ng, </author> <title> Sparse Cholesky factorization on a local--memory multiprocessor, </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> Vol. 9, </volume> <pages> pp. 327-340, </pages> <year> 1988. </year>
Reference-contexts: The nested mapping approach is significantly superior to an index-based approach to the data mapping, presented in Section 2.1.1. The combination of a column-level data mapping and the target computes rule for the computation mapping results in a parallel factorization scheme called the column fan-out algorithm <ref> [11] </ref>. Similarly, the use of the source computes rule for the computation mapping results in the column fan-in algorithm [3].
Reference: [12] <author> A. George, J. W.-H. Liu, and E. G.-Y. Ng, </author> <title> Communication results for parallel sparse Cholesky factorization on a hypercube, </title> <journal> Parallel Comput., </journal> <volume> Vol. 10, </volume> <pages> pp. 287-298, </pages> <year> 1989. </year>
Reference-contexts: Several researchers have also proposed nested mapping strategies for sparse matrix factorization <ref> [12, 10, 16, 7] </ref>. The combination of index-based mapping and nested mapping strategies has been studied in 34 dependently by other researchers in specific contexts. <p> The combination of index-based mapping and nested mapping strategies has been studied in 34 dependently by other researchers in specific contexts. In [13], a mapping strategy for a multifrontal algorithm [5, 15] is presented that combines a two-dimensional partitioning of the matrix with a subtree-to-subcube mapping <ref> [12] </ref>. In [4], a mapping strategy is presented that integrates the torus wrap mapping and a nested mapping based on the elimination tree. However, the present work is the first attempt at a comprehensive characterization of the communication implications of integrating index-based mappings with nested mappings.
Reference: [13] <author> A. Gupta and V. Kumar, </author> <title> A scalable parallel algorithm for sparse matrix factorization, </title> <institution> Department of Computer Science, University of Minnesota, </institution> <type> Technical Report No. TR 94-19, </type> <year> 1994. </year>
Reference-contexts: Several researchers have also proposed nested mapping strategies for sparse matrix factorization [12, 10, 16, 7]. The combination of index-based mapping and nested mapping strategies has been studied in 34 dependently by other researchers in specific contexts. In <ref> [13] </ref>, a mapping strategy for a multifrontal algorithm [5, 15] is presented that combines a two-dimensional partitioning of the matrix with a subtree-to-subcube mapping [12]. In [4], a mapping strategy is presented that integrates the torus wrap mapping and a nested mapping based on the elimination tree.
Reference: [14] <author> B. Hendrickson and D. Womble, </author> <title> The torus-wrap mapping for dense matrix calculations on massively parallel computers, </title> <institution> Sandia National Laboratories, </institution> <type> SAND Report No. SAND 92-0792, </type> <year> 1992. </year>
Reference-contexts: Table 3: Communication statistics for LDLC Comm. type Mesg. count Mesg. volume Factor 1721 102992 Source 890 75276 Local aggr. 146 9158 Global aggr. 3083 77815 Total 5840 265241 7 Related work Various previous works have addressed the use of index-based mapping strategies for dense matrix factorization <ref> [14, 9, 1] </ref>. Several researchers have also proposed nested mapping strategies for sparse matrix factorization [12, 10, 16, 7]. The combination of index-based mapping and nested mapping strategies has been studied in 34 dependently by other researchers in specific contexts.
Reference: [15] <author> J. W.-H. Liu, </author> <title> The multifrontal method for sparse matrix solution: theory and practice, </title> <institution> Dept. of Computer Science, York University, Tech. Report CS-90-04, </institution> <year> 1990. </year>
Reference-contexts: Several researchers have also proposed nested mapping strategies for sparse matrix factorization [12, 10, 16, 7]. The combination of index-based mapping and nested mapping strategies has been studied in 34 dependently by other researchers in specific contexts. In [13], a mapping strategy for a multifrontal algorithm <ref> [5, 15] </ref> is presented that combines a two-dimensional partitioning of the matrix with a subtree-to-subcube mapping [12]. In [4], a mapping strategy is presented that integrates the torus wrap mapping and a nested mapping based on the elimination tree.
Reference: [16] <author> A. Pothen and C. Sun, </author> <title> A distributed multifrontal algorithm using clique trees, </title> <institution> Computer Science, Pennsylvania State University, </institution> <type> Technical Report No. </type> <pages> 91-24, </pages> <year> 1991. </year>
Reference-contexts: Several researchers have also proposed nested mapping strategies for sparse matrix factorization <ref> [12, 10, 16, 7] </ref>. The combination of index-based mapping and nested mapping strategies has been studied in 34 dependently by other researchers in specific contexts.
Reference: [17] <author> E. Rothberg and A. Gupta, </author> <title> An efficient block-oriented approach to parallel sparse Cholesky factorization, </title> <booktitle> Supercomputing '92 Proceedings, </booktitle> <year> 1992. </year>
Reference-contexts: This provides for a clean integration of mapping strategies for the multifrontal algorithm into the unifying framework as described below. The block fan-out algorithm presented in <ref> [17] </ref> corresponds to an index-based mapping scheme where P A is unity, i.e., it is a F fi S scheme, using the notation of this paper. The fan-both algorithm presented in [2] corresponds to a mapping scheme where P F is unity.
Reference: [18] <author> R. Schreiber, </author> <title> A new implementation of sparse Gaussian elimination, </title> <journal> ACM Trans. Math. Software, </journal> <volume> Vol. 8, </volume> <pages> pp. 256-276, </pages> <year> 1982. </year> <month> 39 </month>
Reference-contexts: This section reviews the nested mapping approach for data mapping and compares the effect it has on these two algorithms. 3.1 Nested mapping basics The column-level dependence structure of a sparse Cholesky factorization can be captured succinctly by a tree called the elimination tree <ref> [18] </ref>. Each matrix column has a node in this tree corresponding to it. The terms column and node will be used interchangeably in the following.
References-found: 18

