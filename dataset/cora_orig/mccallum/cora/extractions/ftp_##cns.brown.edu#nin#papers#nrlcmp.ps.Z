URL: ftp://cns.brown.edu/nin/papers/nrlcmp.ps.Z
Refering-URL: http://www.math.tau.ac.il/~nin/research.html
Root-URL: 
Email: Email: nin@brownvm.brown.edu  
Title: Feature Extraction Using an Unsupervised Neural Network  
Author: Nathan Intrator 
Address: Providence, RI 02912  
Affiliation: Center for Neural Science Brown University  
Abstract: A novel unsupervised neural network for dimensionality reduction that seeks directions emphasizing multimodality is presented, and its connection to exploratory projection pursuit methods is discussed. This leads to a new statistical insight into the synaptic modification equations governing learning in Bienenstock, Cooper, and Munro (BCM) neurons (1982). The importance of a dimensionality reduction principle based solely on distinguishing features is demonstrated using a phoneme recognition experiment. The extracted features are compared with features extracted using a back-propagation network.
Abstract-found: 1
Intro-found: 1
Reference: <author> Barron, A. R. and Barron, R. L. </author> <year> (1988). </year> <title> Statistical learining networks: A unifying view. </title> <editor> In Weg man, E., editor, </editor> <booktitle> Computing Science and Statistics: Proc. 20th Symp. Interface, </booktitle> <pages> pages 192-203. </pages> <publisher> American Statistical Association, </publisher> <address> Washington, DC. </address>
Reference-contexts: Performing supervised feature extraction using the class labels, is sensitive to the dimensionality in a similar manner to a high dimensional classifier, and may result in a strong bias to the training data leading to poor generalization properties of the resulting classifier <ref> (Barron and Barron, 1988) </ref>. a fl Research was supported by the National Science Foundation, the Army Research Office, and the Office of Naval Research. 1 A general class of unsupervised dimensionality reduction methods, called exploratory projection pursuit is based on seeking interesting projections of high dimensional data points (Kruskal, 1972; Friedman
Reference: <author> Bear, M. F. and Cooper, L. N. </author> <year> (1988). </year> <title> Molecular mechanisms for synaptic modification in the visual cortex: Interaction between theory and experiment. </title> <editor> In Gluck, M. and Rumelhart, D., editors, </editor> <booktitle> Neuroscience and Connectionist Theory, </booktitle> <pages> pages 65-94. </pages> <publisher> Lawrence Erlbaum, </publisher> <address> Hillsdale, New Jersey. </address>
Reference: <author> Bear, M. F., Cooper, L. N., and Ebner, F. F. </author> <year> (1987). </year> <title> A physiological basis for a theory of synapse modification. </title> <journal> Science, </journal> <volume> 237 </volume> <pages> 42-48. </pages>
Reference: <author> Bellman, R. E. </author> <year> (1961). </year> <title> Adaptive Control Processes. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ. </address>
Reference-contexts: 1 Introduction When a classification of high dimensional vectors is sought, the curse of dimensionality <ref> (Bellman, 1961) </ref> becomes the main factor affecting the classification performance.
Reference: <author> Bienenstock, E. L., Cooper, L. N., and Munro, P. W. </author> <year> (1982). </year> <title> Theory for the development of neuron selectivity: orientation specificity and binocular interaction in visual cortex. </title> <journal> Journal Neuroscience, </journal> <volume> 2 </volume> <pages> 32-48. </pages>
Reference-contexts: The function has been suggested as a biologically plausible synaptic modification 2 function to explain visual cortical plasticity <ref> (Bienenstock, Cooper and Munro, 1982) </ref>. fi m is a dynamic threshold which will be shown later to have an effect on the sign of the synaptic modification. The input x, which is a stochastic process, is assumed to be of Type II ' mixing 1 , bounded, and piecewise constant. <p> achieved via a gradient descent method with respect to m, namely: dm i a = a R m = E [(x m; fi m )x i ]: The resulting differential equations give a modified version of the law governing synaptic weight modification in the BCM theory for learning and memory <ref> (Bienenstock, Cooper and Munro, 1982) </ref>. This theory was presented to account for various experimental results in visual cortical plasticity. The modification lies in the way the threshold fi m is calculated.
Reference: <author> Clothiaux, E. E., Cooper, L. N., and Bear, M. F. </author> <year> (1991). </year> <title> Synaptic plasticity in visual cortex: Comparison of theory with experiment. </title> <journal> Journal of Neurophysiology. </journal> <note> To appear. </note>
Reference-contexts: The biological relevance of the theory has been extensively studied (Bear et al., 1987; Bear and Cooper, 1988) and it was shown that the theory is in agreement with the classical deprivation experiments <ref> (Clothiaux et al., 1991) </ref>. The fact that the distribution has part of its mass on both sides of fi m makes this loss a plausible projection index that seeks multimodalities.
Reference: <author> Cooper, L. N. and Scofield, C. L. </author> <year> (1988). </year> <title> Mean-field theory of a neural network. </title> <booktitle> Proceedings of the National Academy of Science, </booktitle> <volume> 85 </volume> <pages> 1973-1977. </pages>
Reference: <author> Diaconis, P. and Freedman, D. </author> <year> (1984). </year> <title> Asymptotics of graphical projection pursuit. </title> <journal> Annals of Statistics, </journal> <volume> 12 </volume> <pages> 793-815. </pages>
Reference: <author> Duda, R. O. and Hart, P. E. </author> <year> (1973). </year> <title> Pattern Classification and Scene Analysis. </title> <publisher> John Wiley, </publisher> <address> New York. </address>
Reference: <author> Edelman, S. and Bulthoff, H. H. </author> <year> (1991). </year> <title> Orientation dependence in the recognition of familiar and novel views of 3D objects. </title> <journal> Vision Research. </journal> <note> submitted. 8 Friedman, </note> <author> J. H. </author> <year> (1987). </year> <title> Exploratory projection pursuit. </title> <journal> Journal of the American Statistical Asso ciation, </journal> <volume> 82 </volume> <pages> 249-266. </pages>
Reference: <author> Friedman, J. H. and Tukey, J. W. </author> <year> (1974). </year> <title> A projection pursuit algorithm for exploratory data analysis. </title> <journal> IEEE Transactions on Computers, C(23):881-889. </journal>
Reference: <author> Geman, S., Bienenstock, E., and Doursat, R. </author> <year> (1991). </year> <title> Neural networks and the bias-variance dilemma. </title> <note> To appear. </note>
Reference: <author> Hinton, G. E. and Nowlan, S. J. </author> <year> (1990). </year> <title> The bootstrap widrow-hoff rule as a cluster-formation algorithm. </title> <journal> Neural Computation, </journal> <volume> 2(3) </volume> <pages> 355-362. </pages>
Reference-contexts: In some special cases, where the data is known in advance to be bi-modal, it is relatively straightforward to define a good projection index <ref> (Hinton and Nowlan, 1990) </ref>, however, when the structure is not known in advance, defining a general multimodal measure of the projected data is not straight forward, and will be discussed in this paper.
Reference: <author> Huber, P. J. </author> <year> (1985). </year> <title> Projection pursuit. (with discussion). </title> <journal> The Annals of Statistics, </journal> <volume> 13 </volume> <pages> 435-475. </pages>
Reference-contexts: Various projection indices differ on the assumptions about the nature of deviation from normality, and in their computational efficiency. Friedman (1987) argues that the most computationally efficient measures are based on polynomial moments. However polynomial moments heavily emphasize departure from normality in the tails of the distribution <ref> (Huber, 1985) </ref>.
Reference: <author> Intrator, N. </author> <year> (1990). </year> <title> Feature extraction using an unsupervised neural network. </title> <editor> In Touretzky, D. S., Ellman, J. L., Sejnowski, T. J., and Hinton, G. E., editors, </editor> <booktitle> Proceedings of the 1990 Connectionist Models Summer School, </booktitle> <pages> pages 310-318. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: This is done using a loss function which has an expected value that leads to the desired projection index. Mathematical details are given in <ref> (Intrator, 1990) </ref> Before presenting our version of the loss function, we review some necessary notation and assumptions. <p> Consider the six stop consonants [p,k,t,b,g,d], which have been a subject of recent research in evaluating neural networks for phoneme recognition (see review in Lippmann, 1989). According to phonetic feature theory, these stops posses several common features, but only two distinguishing a 2 See <ref> (Intrator, 1990) </ref> for comparison with principal components feature extraction and with k-NN as a classifier 5 Labels Classifier Low Dim Unsupervised Feature Extraction Stop Training Criterion 20 Time 22 Frequency drops below a predetermined threshold on either the same training data (cross validatory test) or on different testing data. phonetic features,
Reference: <author> Intrator, N. and Cooper, L. N. </author> <year> (1991). </year> <title> Objective function formulation of the BCM theory of visual cortical plasticity: Statistical connections, stability conditions. Neural Networks. </title> <note> To appear. </note>
Reference-contexts: The ' mixing property allows for some time dependency in the presentation of the training patterns. These assumptions are needed for the approximation of the resulting deterministic gradient descent by a stochastic one <ref> (Intrator and Cooper, 1991) </ref>. For this reason we use a learning rate that has to decay in time so that this approximation is valid. <p> All the neurons in this network receive the same input and inhibit each other, so as to extract several features in parallel. The relation between this netwrok and the network studied by Cooper and Scofield (1988) is discussed in <ref> (Intrator and Cooper, 1991) </ref>. The activity of neuron k in the network is defined as c k = (x m k ff k ), where m k is the synaptic weight vector of neuron k, and ff k is its threshold. <p> feature extraction method has been applied so far to various high dimensional classification problems: extracting rotation invariant features from 3D wire-like objects (In-trator and Gold, 1991) based on a set of sophisticated psychophysical experiments (Edel-man and Bulthoff, 1991); feature extraction from the TIMIT speech data base using Lyon's Cochlea model <ref> (Intrator and Tajchman, 1991) </ref>. The dimensionality of the feature extraction problem for these experiments was 3969 and 5500 dimensions respectively. It is surprising that a very moderate amount of training data was needed for extracting robust features as will be shown below.
Reference: <author> Intrator, N. and Gold, J. I. </author> <year> (1991). </year> <title> Three-dimensional object recognition of gray level images: The usefulness of distinguishing features. </title> <note> Submitted. </note>
Reference-contexts: The ' mixing property allows for some time dependency in the presentation of the training patterns. These assumptions are needed for the approximation of the resulting deterministic gradient descent by a stochastic one <ref> (Intrator and Cooper, 1991) </ref>. For this reason we use a learning rate that has to decay in time so that this approximation is valid. <p> All the neurons in this network receive the same input and inhibit each other, so as to extract several features in parallel. The relation between this netwrok and the network studied by Cooper and Scofield (1988) is discussed in <ref> (Intrator and Cooper, 1991) </ref>. The activity of neuron k in the network is defined as c k = (x m k ff k ), where m k is the synaptic weight vector of neuron k, and ff k is its threshold. <p> feature extraction method has been applied so far to various high dimensional classification problems: extracting rotation invariant features from 3D wire-like objects (In-trator and Gold, 1991) based on a set of sophisticated psychophysical experiments (Edel-man and Bulthoff, 1991); feature extraction from the TIMIT speech data base using Lyon's Cochlea model <ref> (Intrator and Tajchman, 1991) </ref>. The dimensionality of the feature extraction problem for these experiments was 3969 and 5500 dimensions respectively. It is surprising that a very moderate amount of training data was needed for extracting robust features as will be shown below.
Reference: <author> Intrator, N. and Tajchman, G. </author> <year> (1991). </year> <title> Supervised and unsupervised feature extraction from a cochlear model for speech recognition. </title> <editor> In Juang, B. H., Kung, S. Y., and Kamm, C. A., editors, </editor> <booktitle> Neural Networks for Signal Processing Proceedings of the 1991 IEEE Workshop, </booktitle> <pages> pages 460-469. </pages>
Reference-contexts: The ' mixing property allows for some time dependency in the presentation of the training patterns. These assumptions are needed for the approximation of the resulting deterministic gradient descent by a stochastic one <ref> (Intrator and Cooper, 1991) </ref>. For this reason we use a learning rate that has to decay in time so that this approximation is valid. <p> All the neurons in this network receive the same input and inhibit each other, so as to extract several features in parallel. The relation between this netwrok and the network studied by Cooper and Scofield (1988) is discussed in <ref> (Intrator and Cooper, 1991) </ref>. The activity of neuron k in the network is defined as c k = (x m k ff k ), where m k is the synaptic weight vector of neuron k, and ff k is its threshold. <p> feature extraction method has been applied so far to various high dimensional classification problems: extracting rotation invariant features from 3D wire-like objects (In-trator and Gold, 1991) based on a set of sophisticated psychophysical experiments (Edel-man and Bulthoff, 1991); feature extraction from the TIMIT speech data base using Lyon's Cochlea model <ref> (Intrator and Tajchman, 1991) </ref>. The dimensionality of the feature extraction problem for these experiments was 3969 and 5500 dimensions respectively. It is surprising that a very moderate amount of training data was needed for extracting robust features as will be shown below.
Reference: <author> Kruskal, J. B. </author> <year> (1972). </year> <title> Linear transformation of multivariate data to reveal clustering. In Shepard, </title> <editor> R. N., Romney, A. K., and Nerlove, S. B., editors, </editor> <title> Multidimensional Scaling: Theory and Application in the Behavioral Sciences, I, </title> <booktitle> Theory, </booktitle> <pages> pages 179-191. </pages> <publisher> Seminar Press, </publisher> <address> New York and London. </address>
Reference: <author> Lieberman, P. and Blumstein, S. E. </author> <year> (1988). </year> <title> Speech Physiology, Speech Perception, and Acoustic Phonetics. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge MA. </address>
Reference: <author> Linsker, R. </author> <year> (1988). </year> <title> Self-organization in a perceptual network. </title> <journal> IEEE. Computer, </journal> <volume> 88 </volume> <pages> 105-117. </pages>
Reference: <author> Lippmann, R. P. </author> <year> (1989). </year> <title> Review of neural networks for speech recognition. </title> <journal> Neural Computation, </journal> <volume> 1(1) </volume> <pages> 1-38. </pages>
Reference-contexts: Similar approaches using the RCE and back-propagation network have been carried out by (Reilly et al., 1988). The following describes the linguistic motivation of the experiment. Consider the six stop consonants [p,k,t,b,g,d], which have been a subject of recent research in evaluating neural networks for phoneme recognition <ref> (see review in Lippmann, 1989) </ref>.
Reference: <author> Miller, K. D. </author> <year> (1988). </year> <title> Correlation-based models of neural development. </title> <editor> In Gluck, M. and Rumel hart, D., editors, </editor> <booktitle> Neuroscience and Connectionist Theory, </booktitle> <pages> pages 267-353. </pages> <publisher> Lawrence Erlbaum, </publisher> <address> Hillsdale, New Jersey. </address>
Reference: <author> Oja, E. </author> <year> (1982). </year> <title> A simplified neuron model as a principal component analyzer. </title> <journal> Math. Biology, </journal> <volume> 15 </volume> <pages> 267-273. </pages>
Reference: <author> Reilly, D. L., Scofield, C. L., Cooper, L. N., and Elbaum, C. </author> <year> (1988). </year> <title> Gensep: a multiple neural network with modifiable network topology. </title> <booktitle> In INNS Conference on Neural Networks. </booktitle>
Reference-contexts: Classification on the new feature space was done using back-propagation. 2 The unsupervised feature extraction/classification method is presented in Figure 2. The pixel images corresponding to speech data, are shown in Figure 3. Similar approaches using the RCE and back-propagation network have been carried out by <ref> (Reilly et al., 1988) </ref>. The following describes the linguistic motivation of the experiment. Consider the six stop consonants [p,k,t,b,g,d], which have been a subject of recent research in evaluating neural networks for phoneme recognition (see review in Lippmann, 1989).
Reference: <author> Seebach, B. S. </author> <year> (1990). </year> <title> Evidence for the development of phonetic property detectors in a neural net without innate knowledge of linguistic structure. </title> <type> Ph.D. dissertation, </type> <institution> Brown University. </institution>
Reference: <author> Sejnowski, T. J. </author> <year> (1977). </year> <title> Storing covariance with nonlinearly interacting neurons. </title> <journal> Journal of Math ematical Biology, </journal> <volume> 4 </volume> <pages> 303-321. </pages> <editor> von der Malsburg, C. </editor> <year> (1973). </year> <title> Self-organization of orientation sensitivity cells in the striate cortex. </title> <journal> Kybernetik, </journal> 14:85-100. 
Reference: <author> Zwicker, E. </author> <year> (1961). </year> <title> Subdivision of the audible frequency range into critical bands (frequenzgruppen). </title> <journal> Journal of the Acoustical Society of America, 33(2):248. </journal> <volume> 9 </volume>
Reference-contexts: The speech data consists of 20 consecutive time windows of 32msec with 30msec overlap, aligned to the beginning of the burst. In each time window, a set of 22 energy levels is computed. These energy levels correspond to Zwicker critical band filters <ref> (Zwicker, 1961) </ref>. The consonant-vowel (CV) pairs were pronounced in isolation by native American speakers (two male BSS and LTN, and one female JES.) Additional details on biological motivation for the preprocessing, and linguistic motivation related to child language acquisition can be found in Seebach (1990), .
References-found: 28

