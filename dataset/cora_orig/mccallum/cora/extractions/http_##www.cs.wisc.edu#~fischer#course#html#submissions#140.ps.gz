URL: http://www.cs.wisc.edu/~fischer/course/html/submissions/140.ps.gz
Refering-URL: http://www.cs.wisc.edu/~fischer/course/html/submissions/
Root-URL: http://www.cs.wisc.edu
Email: soumen@cs.berkeley.edu  mgupta,jdchoi@watson.ibm.com  soumen@cs.berkeley.edu  
Phone: 510-642-9583  
Title: Global Communication Analysis and Optimization (Extended Abstract)  
Author: Soumen Chakrabarti Manish Gupta Jong-Deok Choi Soumen Chakrabarti 
Note: Corresponding author  
Address: Berkeley  Yorktown Heights, New York  387 Soda Hall  Berkeley, CA 94720  
Affiliation: Computer Science Division University of California at  IBM T. J. Watson Research Center  Computer Science Division  University of California  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> G. Agarwal, J. Saltz, and R. Das. </author> <title> Interprocedural partial redundancy elimination and its application to distributed memory compilation. </title> <booktitle> In Proc. ACM SIGPLAN '95 Conference on Programming Language Design and Implementation, </booktitle> <address> La Jolla, CA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: These include dataflow analysis over array sections for regular computations [8, 12, 15, 16] and over entire arrays for irregular computations <ref> [25, 1] </ref>. Typically, redundancy elimination is achieved by moving communication to the earliest possible point dictated by data dependency and control flow. Superficially, this also appears to give the fl Corresponding author.
Reference: [2] <author> S. P. Amarasinghe and M. S. Lam. </author> <title> Communication optimization and code generation for distributed memory machines. </title> <booktitle> In Proc. ACM SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <address> Albuquerque, New Mexico, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: The compilers for these languages are responsible for partitioning the computation, and generating the communication necessary to fetch values of non-local data referenced by a processor <ref> [13, 27, 3, 2, 4, 10] </ref>. Accessing remote data is usually orders of magnitude slower than accessing local data. This gap is growing because CPU performance is out-growing network performance, CPU's are running relatively independent multiprogrammed operating systems, and commodity networks are being found more cost-effective. <p> Local analysis of individual communication based on dependence testing alone often retains redundant communication. Naturally, the next development was the use of dataflow analysis. There have been efforts using precise array dataflow analysis to detect redundant communication within a loop nest <ref> [2] </ref>, and those using global dataflow analysis for redundancy elimination across loop nests as well. These include dataflow analysis over array sections for regular computations [8, 12, 15, 16] and over entire arrays for irregular computations [25, 1].
Reference: [3] <author> Z. Bozkus, A. Choudhary, G. Fox, T. Haupt, and S. Ranka. </author> <title> A compilation approach for Fortran 90D/HPF compilers on distributed memory MIMD computers. </title> <booktitle> In Proc. Sixth Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, Oregon, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: The compilers for these languages are responsible for partitioning the computation, and generating the communication necessary to fetch values of non-local data referenced by a processor <ref> [13, 27, 3, 2, 4, 10] </ref>. Accessing remote data is usually orders of magnitude slower than accessing local data. This gap is growing because CPU performance is out-growing network performance, CPU's are running relatively independent multiprogrammed operating systems, and commodity networks are being found more cost-effective.
Reference: [4] <author> T. Brandes. </author> <title> ADAPTOR: A compilation system for data-parallel Fortran programs. </title> <editor> In C. W. Kessler, editor, </editor> <title> Automatic parallelization new approaches to code generation, data distribution, and performance prediction. </title> <booktitle> Vieweg Advanced Studies in Computer Science, </booktitle> <publisher> Vieweg, Wiesbaden, </publisher> <month> January </month> <year> 1994. </year> <month> 12 </month>
Reference-contexts: The compilers for these languages are responsible for partitioning the computation, and generating the communication necessary to fetch values of non-local data referenced by a processor <ref> [13, 27, 3, 2, 4, 10] </ref>. Accessing remote data is usually orders of magnitude slower than accessing local data. This gap is growing because CPU performance is out-growing network performance, CPU's are running relatively independent multiprogrammed operating systems, and commodity networks are being found more cost-effective.
Reference: [5] <author> J.-D. Choi, R. Cytron, and J. Ferrante. </author> <title> On the efficient engineering of ambitious program analysis. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 20(2) </volume> <pages> 105-114, </pages> <month> February </month> <year> 1994. </year>
Reference-contexts: Earliest location is typically identified using a backward and forward dataflow approach using array section descriptors or bit-vectors. In the pHPF compiler, we use the SSA def-use information already computed in an earlier phase. In what follows, we will assume familiarity with SSA terminology <ref> [5] </ref>. While the SSA is built treating each array reference as one to potentially any part of the array (each array definition is therefore viewed as a preserving definition), we use data dependence testing [26] to refine our analysis. The concept of preserving definitions, used in our implementation of SSA [5], <p> <ref> [5] </ref>. While the SSA is built treating each array reference as one to potentially any part of the array (each array definition is therefore viewed as a preserving definition), we use data dependence testing [26] to refine our analysis. The concept of preserving definitions, used in our implementation of SSA [5], allows for this data dependence testing for array references. The pseudocode for computing Earliest (u) for a use u is shown in Figure 7 (a).
Reference: [6] <author> C. Click. </author> <title> Global code motion global value numbering. </title> <booktitle> In PLDI, </booktitle> <pages> pages 246-257. </pages> <booktitle> ACM SIGPLAN, </booktitle> <year> 1995. </year>
Reference-contexts: A simple greedy heuristic is shown in Figure 7 (e). It is similar to Click's global code motion heuristic <ref> [6] </ref>: consider the most constrained (i.e., minimal choice) communication entry next, and put it where it is compatible in communication pattern (as shown by the test below) with the largest number of other candidate communication.
Reference: [7] <author> H. P. F. Forum. </author> <title> High Performance Fortran language specification, version 1.0. </title> <type> Technical Report CRPC-TR92225, </type> <institution> Rice University, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Distributed memory architectures provide a cost-effective method of building massively parallel computers. However, the absence of global address space, and the resulting need for explicit message passing makes these machines difficult to program. This has motivated the design of languages like High Performance Fortran (HPF) <ref> [7] </ref>, which allow the programmer to write sequential or shared-memory parallel programs that are annotated with directives specifying data decomposition.
Reference: [8] <author> E. Granston and A. Veidenbaum. </author> <title> Detecting redundant accesses to array data. </title> <booktitle> In Proc. Supercomputing '91, </booktitle> <pages> pages 854-965, </pages> <year> 1991. </year>
Reference-contexts: There have been efforts using precise array dataflow analysis to detect redundant communication within a loop nest [2], and those using global dataflow analysis for redundancy elimination across loop nests as well. These include dataflow analysis over array sections for regular computations <ref> [8, 12, 15, 16] </ref> and over entire arrays for irregular computations [25, 1]. Typically, redundancy elimination is achieved by moving communication to the earliest possible point dictated by data dependency and control flow. Superficially, this also appears to give the fl Corresponding author.
Reference: [9] <author> M. Gupta and P. Banerjee. </author> <title> A methodology for high-level synthesis of communication on multicomputers. </title> <booktitle> In Proc. 6th ACM International Conference on Supercomputing, </booktitle> <address> Washington D.C., </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Thus compilers must reduce the number as well as the volume of messages. (This can improve performance on shared memory machines as well, because fewer messages translate into fewer synchronization events [24, 19, 11].) The most common optimizations to reduce communication overhead include message vectorization [13, 27], using collective communication <ref> [9, 17] </ref>, message coalescing [13], and exploiting pipelined communication [13, 10], all within the scope of a single loop nest. Local analysis of individual communication based on dependence testing alone often retains redundant communication. Naturally, the next development was the use of dataflow analysis.
Reference: [10] <author> M. Gupta, S. Midkiff, E. Schonberg, V. Seshadri, K. Wang, D. Shields, W.-M. Ching, and T. Ngo. </author> <title> An HPF compiler for the IBM SP2. </title> <booktitle> In Proc. Supercomputing '95, </booktitle> <address> San Diego, CA, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: The compilers for these languages are responsible for partitioning the computation, and generating the communication necessary to fetch values of non-local data referenced by a processor <ref> [13, 27, 3, 2, 4, 10] </ref>. Accessing remote data is usually orders of magnitude slower than accessing local data. This gap is growing because CPU performance is out-growing network performance, CPU's are running relatively independent multiprogrammed operating systems, and commodity networks are being found more cost-effective. <p> the volume of messages. (This can improve performance on shared memory machines as well, because fewer messages translate into fewer synchronization events [24, 19, 11].) The most common optimizations to reduce communication overhead include message vectorization [13, 27], using collective communication [9, 17], message coalescing [13], and exploiting pipelined communication <ref> [13, 10] </ref>, all within the scope of a single loop nest. Local analysis of individual communication based on dependence testing alone often retains redundant communication. Naturally, the next development was the use of dataflow analysis. <p> The algorithm achieves both redundancy elimination and message combining globally, and is able to reduce the number of messages to an extent that is not achievable with any previous approach. Our algorithm for placement of communication code has been implemented in the IBM pHPF prototype compiler <ref> [10] </ref>. We report results from a preliminary study of some well-known HPF programs. The performance gains are impressive. Reduction in static message count can be up to a factor of almost nine. Time spent in communication is reduced in many cases by a factor of two or more. <p> In particular, redundancy elimination via earliest placement can prevent the combining possibilities from being exploited. To demonstrate this, we study the NCAR shallow water code, which has NNC message pattern. As discussed in <ref> [10] </ref>, the message coalescing optimization implemented in the pHPF compiler allows the diagonal communication to be subsumed by an augmented form of the NNC along the two axes. A simplified form of the original code is shown in Figure 2. <p> Even if the programmer were careful enough to write the code in the first column, intermediate passes of compilation may destroy the interval containment. In fact, the current IBM HPF scalarizer <ref> [10] </ref> will translate the F90-style source to the scalarized form in the second column (performing loop fusion before this analysis would solve the problem in some cases, but not always). Thus, limited communication analysis at a single loop-nest level or a rigid placement policy may not work well. <p> This analysis is done after the compiler has performed transformations like loop distribution to increase opportunities for moving communication outside loops <ref> [10] </ref>. 1. For each RHS expression that may need communication, identify the earliest safe position to place that communication. We show how to derive this information as a by-product of the translation to Static Single Assignment (SSA) form (x4.1), with refinements made using dependence-testing. 2. <p> This follows from standard communication analysis in which communication is placed just before the outermost loop in which there is no true dependence on u, and is placed just before the statement containing u if no such loop exists <ref> [27, 13, 10] </ref>. The single dominating earliest choice now pays off: the set of single safe positions has a very simple characterization in terms of the following claims.
Reference: [11] <author> M. Gupta and E. Schonberg. </author> <title> Static analysis to reduce synchronization costs in data-parallel programs. </title> <booktitle> In Proc. 23rd Annual ACM Symposium on Principles of Programming Languages, </booktitle> <address> St. Petersburg Beach, FL, </address> <month> January </month> <year> 1996. </year> <note> (To appear). </note>
Reference-contexts: Thus compilers must reduce the number as well as the volume of messages. (This can improve performance on shared memory machines as well, because fewer messages translate into fewer synchronization events <ref> [24, 19, 11] </ref>.) The most common optimizations to reduce communication overhead include message vectorization [13, 27], using collective communication [9, 17], message coalescing [13], and exploiting pipelined communication [13, 10], all within the scope of a single loop nest.
Reference: [12] <author> M. Gupta, E. Schonberg, and H. Srinivasan. </author> <title> A unified data-flow framework for optimizing communication. </title> <booktitle> In Proc. 7th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Ithaca, NY, </address> <month> August </month> <year> 1994. </year> <note> Springer-Verlag. </note>
Reference-contexts: There have been efforts using precise array dataflow analysis to detect redundant communication within a loop nest [2], and those using global dataflow analysis for redundancy elimination across loop nests as well. These include dataflow analysis over array sections for regular computations <ref> [8, 12, 15, 16] </ref> and over entire arrays for irregular computations [25, 1]. Typically, redundancy elimination is achieved by moving communication to the earliest possible point dictated by data dependency and control flow. Superficially, this also appears to give the fl Corresponding author. <p> There is thus a clear need for global scheduling of communication. In this paper we present a novel compiler algorithm for all the above mentioned optimizations. Our algorithm derives from static single assignment analysis, array dependence analysis, and the recently introduced data availability analysis <ref> [12] </ref>, which is extended to detect compatibility of communication patterns in addition to redundancy. We differ significantly from existing research in that the position of communication code for each remote access is not decided independent of other remote accesses; instead, the positions are decided in an interdependent and global manner. <p> as a running example to illustrate the operation of the steps of the algorithm. 4.1 Identifying the earliest position We differ in one way from traditional earliest analysis, in which a set of control flow graph (CFG) nodes are identified such that each is safe, and no predecessor is safe <ref> [12] </ref>. Communication has to be placed at each of these safe positions. Such replication of communication code in different control flow paths is difficult to support when exploring different candidate placements, mainly due to the complexity of code generation. <p> In our example, Earliest (a 1 ) = Earliest (a 2 ) = 8 (by our convention, the statement identified by communication placement is the one before which communication is to be placed). Traditional array dataflow analysis, which does not insist on dominating defs <ref> [12] </ref>, would lead to Earliest 0 (a 1 ) = Earliest 0 (a 2 ) = f5; 7g. <p> This test is based on the Available Section Descriptor (ASD) representation of communication, described in <ref> [12] </ref>. Briefly, an ASD consists of a pair hD; M i, where D represents the data (scalar variable or an array section) being communicated, and M is a mapping function that maps data to the processors which receive that data. <p> A communication hD 1 ; M 1 i is made redundant by another communication hD 2 ; M 2 i if D 1 D 2 , and M 1 (D 1 ) M 2 (D 1 ) <ref> [12] </ref>. In our case, since there can be many candidate positions for the same reference, we have to propagate the redundancy information globally. The pseudocode for eliminating redundant communication in the context of our current framework is shown in Figure 7 (d). <p> Thus, by choosing a later (than the earliest possible) placement for b 1 , we are able to eliminate that communication completely. In contrast, the solution proposed in <ref> [12] </ref> would move each communication to the earliest point, and reduce the communication for b 2 to ASD (b 2 ) ASD (b 1 ), while the communication for b 1 would remain unchanged. <p> The check for M 1 M 2 is done in the virtual processor space of template positions, as described in <ref> [12] </ref>. However, we have incorporated extensions to check for equality of mappings in physical processor space for nearest-neighbor communication and for mappings to a constant processor position [12]. 4.6 Code generation As shown in Figure 6, the step after communication analysis and optimization is to insert communication code, which is in <p> The check for M 1 M 2 is done in the virtual processor space of template positions, as described in <ref> [12] </ref>. However, we have incorporated extensions to check for equality of mappings in physical processor space for nearest-neighbor communication and for mappings to a constant processor position [12]. 4.6 Code generation As shown in Figure 6, the step after communication analysis and optimization is to insert communication code, which is in the form of subroutine calls to the pHPF runtime library routines, which in turn invoke MPL/MPI.
Reference: [13] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: The compilers for these languages are responsible for partitioning the computation, and generating the communication necessary to fetch values of non-local data referenced by a processor <ref> [13, 27, 3, 2, 4, 10] </ref>. Accessing remote data is usually orders of magnitude slower than accessing local data. This gap is growing because CPU performance is out-growing network performance, CPU's are running relatively independent multiprogrammed operating systems, and commodity networks are being found more cost-effective. <p> Thus compilers must reduce the number as well as the volume of messages. (This can improve performance on shared memory machines as well, because fewer messages translate into fewer synchronization events [24, 19, 11].) The most common optimizations to reduce communication overhead include message vectorization <ref> [13, 27] </ref>, using collective communication [9, 17], message coalescing [13], and exploiting pipelined communication [13, 10], all within the scope of a single loop nest. Local analysis of individual communication based on dependence testing alone often retains redundant communication. Naturally, the next development was the use of dataflow analysis. <p> the number as well as the volume of messages. (This can improve performance on shared memory machines as well, because fewer messages translate into fewer synchronization events [24, 19, 11].) The most common optimizations to reduce communication overhead include message vectorization [13, 27], using collective communication [9, 17], message coalescing <ref> [13] </ref>, and exploiting pipelined communication [13, 10], all within the scope of a single loop nest. Local analysis of individual communication based on dependence testing alone often retains redundant communication. Naturally, the next development was the use of dataflow analysis. <p> the volume of messages. (This can improve performance on shared memory machines as well, because fewer messages translate into fewer synchronization events [24, 19, 11].) The most common optimizations to reduce communication overhead include message vectorization [13, 27], using collective communication [9, 17], message coalescing [13], and exploiting pipelined communication <ref> [13, 10] </ref>, all within the scope of a single loop nest. Local analysis of individual communication based on dependence testing alone often retains redundant communication. Naturally, the next development was the use of dataflow analysis. <p> This follows from standard communication analysis in which communication is placed just before the outermost loop in which there is no true dependence on u, and is placed just before the statement containing u if no such loop exists <ref> [27, 13, 10] </ref>. The single dominating earliest choice now pays off: the set of single safe positions has a very simple characterization in terms of the following claims.
Reference: [14] <author> K. Keeton, T. Anderson, and D. Patterson. </author> <title> LogP quantified: The case for low-overhead local area networks. </title> <booktitle> In Proceedings of Hot Interconnects III: A Symposium on High Performance Interconnects, </booktitle> <address> Stanford, CA, </address> <month> August </month> <year> 1995. </year> <note> URL http://http.cs.berkeley.edu/~kkeeton/Papers/papers.html. </note>
Reference-contexts: The message-passing software used are MPL and MPICH 2 respectively. The SP2 also has an i860-based communication co-processor. Further details of architecture can be found in <ref> [23, 21, 14] </ref>. We want to quantify the following: benefits of combining messages, cost of buffer copy as affected by cache size, and scope of overlap. Figure 4 shows the profiling code and performance results. 1.
Reference: [15] <author> K. Kennedy and N. Nedeljkovic. </author> <title> Combining dependence and data-flow analyses to optimize communication. </title> <booktitle> In International Parallel Processing Symposium. IEEE, </booktitle> <year> 1995. </year>
Reference-contexts: There have been efforts using precise array dataflow analysis to detect redundant communication within a loop nest [2], and those using global dataflow analysis for redundancy elimination across loop nests as well. These include dataflow analysis over array sections for regular computations <ref> [8, 12, 15, 16] </ref> and over entire arrays for irregular computations [25, 1]. Typically, redundancy elimination is achieved by moving communication to the earliest possible point dictated by data dependency and control flow. Superficially, this also appears to give the fl Corresponding author. <p> For this formulation, we will consider all array data being communicated in the program as a set A = fa i g of disjoint array sections (if sections in A intersect, we replace A by the set of smallest intersections <ref> [15] </ref>). Let R = fr j g be the set of remote accesses; each r j is a reference to some a i j 2 A.
Reference: [16] <author> K. Kennedy and A. Sethi. </author> <title> A constraint-based communication placement framework. </title> <type> Technical Report CRPC-TR95515-S, CRPC, </type> <institution> Rice University, </institution> <year> 1995. </year>
Reference-contexts: There have been efforts using precise array dataflow analysis to detect redundant communication within a loop nest [2], and those using global dataflow analysis for redundancy elimination across loop nests as well. These include dataflow analysis over array sections for regular computations <ref> [8, 12, 15, 16] </ref> and over entire arrays for irregular computations [25, 1]. Typically, redundancy elimination is achieved by moving communication to the earliest possible point dictated by data dependency and control flow. Superficially, this also appears to give the fl Corresponding author. <p> Recently, it has been pointed out that communication that is scheduled too eagerly can lead to problems like contention (which reduce effective network bandwidth) and excessive buffer requirement (which upsets the computation's cache and thereby degrades performance) <ref> [16, 18] </ref>. A more striking fact we point out is that earliest placement can also lead to valuable opportunities being missed for reducing the number of messages or eliminating partial redundancy, making it a sub-optimal strategy even in the absence of resource constraints.
Reference: [17] <author> J. Li and M. Chen. </author> <title> Compiling communication-efficient programs for massively parallel machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 361-376, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Thus compilers must reduce the number as well as the volume of messages. (This can improve performance on shared memory machines as well, because fewer messages translate into fewer synchronization events [24, 19, 11].) The most common optimizations to reduce communication overhead include message vectorization [13, 27], using collective communication <ref> [9, 17] </ref>, message coalescing [13], and exploiting pipelined communication [13, 10], all within the scope of a single loop nest. Local analysis of individual communication based on dependence testing alone often retains redundant communication. Naturally, the next development was the use of dataflow analysis.
Reference: [18] <author> T. Mowry, M. Lam, and A. Gupta. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> In Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), </booktitle> <pages> pages 62-73. </pages> <booktitle> ACM SIGPLAN, </booktitle> <year> 1992. </year>
Reference-contexts: Recently, it has been pointed out that communication that is scheduled too eagerly can lead to problems like contention (which reduce effective network bandwidth) and excessive buffer requirement (which upsets the computation's cache and thereby degrades performance) <ref> [16, 18] </ref>. A more striking fact we point out is that earliest placement can also lead to valuable opportunities being missed for reducing the number of messages or eliminating partial redundancy, making it a sub-optimal strategy even in the absence of resource constraints.
Reference: [19] <author> M. O'Boyle and F. Bodin. </author> <title> Compiler reduction of synchronization in shared virtual memory systems. </title> <booktitle> In Proc. 9th ACM International Conference on Supercomputing, </booktitle> <address> Barcelona, Spain, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Thus compilers must reduce the number as well as the volume of messages. (This can improve performance on shared memory machines as well, because fewer messages translate into fewer synchronization events <ref> [24, 19, 11] </ref>.) The most common optimizations to reduce communication overhead include message vectorization [13, 27], using collective communication [9, 17], message coalescing [13], and exploiting pipelined communication [13, 10], all within the scope of a single loop nest.
Reference: [20] <author> S. Olariu. </author> <title> An optimal greedy heuristic to color interval graphs. </title> <journal> Information Processing Letters, </journal> <volume> 37(1) </volume> <pages> 21-25, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Claim 6.1 Graph coloring reduces to the above assignment problem. We defer the proof to the full paper, but note that the proof is not similar to register allocation results; indeed, our candidate sets are similar to intervals on the real line, and coloring such intervals is easy <ref> [20] </ref>. Once we are reconciled to intractability, we can formulate the complete problem, including cache or buffer 11 constraints and CPU-network overlap, using a mixed integer linear program (ILP).
Reference: [21] <author> M. Snir et al. </author> <title> The communication software and parallel environment of the IBM SP2. </title> <journal> IBM Systems Journal, </journal> <volume> 34(2) </volume> <pages> 205-221, </pages> <year> 1995. </year>
Reference-contexts: As a result, communication startup overheads tend to be astronomical on most distributed memory machines, although reasonable bandwidth can be supported for sufficiently large messages <ref> [23, 21] </ref>. <p> The message-passing software used are MPL and MPICH 2 respectively. The SP2 also has an i860-based communication co-processor. Further details of architecture can be found in <ref> [23, 21, 14] </ref>. We want to quantify the following: benefits of combining messages, cost of buffer copy as affected by cache size, and scope of overlap. Figure 4 shows the profiling code and performance results. 1. <p> The gains also depend on the allocation of responsibilities to the CPU and communication co-processor, if any. In fact, the implementors of MPL minimize co-processor assistance because it is much slower than the CPU, and the channel between the CPU and the co-processor is slow <ref> [21] </ref>. 4 Compiler algorithms Our compiler goes through the steps described below for placing communication code. This analysis is done after the compiler has performed transformations like loop distribution to increase opportunities for moving communication outside loops [10]. 1.
Reference: [22] <author> A. Srinivasan. </author> <title> Improved approximation guarantees for packing and covering integer programs. </title> <booktitle> In STOC, </booktitle> <address> Las Vegas, </address> <year> 1995. </year> <institution> ACM SIGACT. </institution>
Reference-contexts: Buffer or cache capacity constraints and CPU-network overlap can be incorporated simply by adding some more linear constraints. While getting an exact solution to the ILP is still NP-complete, there is a large body of literature on approximately solving ILP's in polynomial time <ref> [22] </ref>. We defer the details to the full paper. 6.2 Special communication patterns Reduction communication is dealt with in a special way in the compiler since communication requirement is inverted, in a sense, for reductions.
Reference: [23] <author> C. Stunkel et al. </author> <title> The SP2 high performance switch. </title> <journal> IBM Systems Journal, </journal> <volume> 34(2) </volume> <pages> 185-204, </pages> <year> 1995. </year>
Reference-contexts: As a result, communication startup overheads tend to be astronomical on most distributed memory machines, although reasonable bandwidth can be supported for sufficiently large messages <ref> [23, 21] </ref>. <p> The message-passing software used are MPL and MPICH 2 respectively. The SP2 also has an i860-based communication co-processor. Further details of architecture can be found in <ref> [23, 21, 14] </ref>. We want to quantify the following: benefits of combining messages, cost of buffer copy as affected by cache size, and scope of overlap. Figure 4 shows the profiling code and performance results. 1.
Reference: [24] <author> C.-W. Tseng. </author> <title> Compiler optimizations for eliminating barrier synchronization. </title> <booktitle> In Proc. 5th ACM Symposium on Principles and Practices of Parallel Programming, </booktitle> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Thus compilers must reduce the number as well as the volume of messages. (This can improve performance on shared memory machines as well, because fewer messages translate into fewer synchronization events <ref> [24, 19, 11] </ref>.) The most common optimizations to reduce communication overhead include message vectorization [13, 27], using collective communication [9, 17], message coalescing [13], and exploiting pipelined communication [13, 10], all within the scope of a single loop nest.
Reference: [25] <author> R. v. Hanxleden and K. Kennedy. </author> <title> Give-n-take a balanced code placement framework. </title> <booktitle> In Proc. ACM SIGPLAN '94 Conference on Programming Language Design and Implementation, </booktitle> <address> Orlando, Florida, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: These include dataflow analysis over array sections for regular computations [8, 12, 15, 16] and over entire arrays for irregular computations <ref> [25, 1] </ref>. Typically, redundancy elimination is achieved by moving communication to the earliest possible point dictated by data dependency and control flow. Superficially, this also appears to give the fl Corresponding author.
Reference: [26] <author> M. Wolfe and U. Banerjee. </author> <title> Data dependence and its application to parallel processing. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 16(2) </volume> <pages> 137-178, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: In what follows, we will assume familiarity with SSA terminology [5]. While the SSA is built treating each array reference as one to potentially any part of the array (each array definition is therefore viewed as a preserving definition), we use data dependence testing <ref> [26] </ref> to refine our analysis. The concept of preserving definitions, used in our implementation of SSA [5], allows for this data dependence testing for array references. The pseudocode for computing Earliest (u) for a use u is shown in Figure 7 (a).
Reference: [27] <author> H. Zima, H. Bast, and M. Gerndt. </author> <title> SUPERB: A tool for semi-automatic MIMD/SIMD parallelization. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 1-18, </pages> <year> 1988. </year> <month> 13 </month>
Reference-contexts: The compilers for these languages are responsible for partitioning the computation, and generating the communication necessary to fetch values of non-local data referenced by a processor <ref> [13, 27, 3, 2, 4, 10] </ref>. Accessing remote data is usually orders of magnitude slower than accessing local data. This gap is growing because CPU performance is out-growing network performance, CPU's are running relatively independent multiprogrammed operating systems, and commodity networks are being found more cost-effective. <p> Thus compilers must reduce the number as well as the volume of messages. (This can improve performance on shared memory machines as well, because fewer messages translate into fewer synchronization events [24, 19, 11].) The most common optimizations to reduce communication overhead include message vectorization <ref> [13, 27] </ref>, using collective communication [9, 17], message coalescing [13], and exploiting pipelined communication [13, 10], all within the scope of a single loop nest. Local analysis of individual communication based on dependence testing alone often retains redundant communication. Naturally, the next development was the use of dataflow analysis. <p> This follows from standard communication analysis in which communication is placed just before the outermost loop in which there is no true dependence on u, and is placed just before the statement containing u if no such loop exists <ref> [27, 13, 10] </ref>. The single dominating earliest choice now pays off: the set of single safe positions has a very simple characterization in terms of the following claims. <p> The runtime library provides a high-level interface through which the compiler specifies the data being communicated in the form of array sections, and the runtime system takes care of packing and unpacking of data. For NNC, data is received in overlap regions <ref> [27] </ref> surrounding the local portion of the arrays. For other kinds of communication involving arrays, data is received into a buffer that is allocated dynamically, and the array reference that led to this communication is replaced by a reference to the buffer.
References-found: 27

