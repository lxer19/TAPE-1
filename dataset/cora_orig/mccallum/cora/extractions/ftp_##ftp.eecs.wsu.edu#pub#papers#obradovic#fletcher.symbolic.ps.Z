URL: ftp://ftp.eecs.wsu.edu/pub/papers/obradovic/fletcher.symbolic.ps.Z
Refering-URL: http://www.lehigh.edu/~ob00/integrated/references-new.html
Root-URL: 
Email: jfletche@eecs.wsu.edu zoran@eecs.wsu.edu  zoran@eecs.wsu.edu  
Phone: (509) 335-6601  
Title: COMBINING PRIOR SYMBOLIC KNOWLEDGE AND CONSTRUCTIVE NEURAL NETWORK LEARNING  
Author: Justin Fletcher Zoran Obradovic Dr. Zoran Obradovic 
Note: Contact:  
Address: Pullman WA 99164-2752  Pullman, WA 99164-2752  
Affiliation: School of Electrical Engineering and Computer Science Washington State University,  School of Electrical Engineering and Computer Science Washington State University  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> J. Anderson and E. Rosenfeld, editors. Neurocomputing: </editor> <booktitle> Foundations of Research. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1988. </year>
Reference: [2] <author> E. B. Baum. </author> <title> Neural net algorithms that learn in polynomial time from examples and queries. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2(1) </volume> <pages> 5-19, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Examples include the tiling algorithm of Mezard and Nadal [12] and the cascade-correlation algorithm of Fahlman and Lebiere [5]. An iterative construction of hidden units in a feed-forward neural network with a single hidden layer is proposed by Baum <ref> [2] </ref>. This algorithm constructs a two layer neural network in polynomial time given examples and the ability to query for the classification of specific points within the problem domain. In a neural network, a hidden unit with fan-in k is a representation of a k-1 dimensional hyperplane.
Reference: [3] <author> J. L. Bentley. </author> <title> Multidimensional binary search tree used for associative searching. </title> <journal> Communications of the ACM, </journal> <volume> 18(9) </volume> <pages> 509-517, </pages> <month> September </month> <year> 1975. </year>
Reference-contexts: O (log N ) interpolations are required to search for a point on the hyperplane. In each interpolation step, finding the closest point to the midpoint can be determined in time O (log N) through use of the k-d tree of Bentley <ref> [3] </ref>. Thus, the time required to search for one point on the hyperplane is O (log 2 N ).
Reference: [4] <author> J. Cendrowska. </author> <title> PRISM: An algorithm for inducing modular rules. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 26,27(1,2,4;2,3,4), </volume> <year> 1987. </year>
Reference-contexts: The first problem is in disjunctive normal form. The second is similar to parity problems, and the third is similar to the first with the addition of five percent noise. The rule base, representing good but imperfect prior knowledge, was generated by the PRISM algorithm of Cendrowska <ref> [4] </ref> and is based on Quinlan's induction algorithm ID3 [14]. Hyperplane determination from examples (HDE) is again used to combine the prior knowledge with the knowledge available from the example data.
Reference: [5] <author> S. Fahlman and C. Lebiere. </author> <title> The cascade-correlation learning architecture. </title> <editor> In D. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 524-532, </pages> <address> Denver 1989, 1990. </address> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo. </address>
Reference-contexts: In contrast to learning on a pre-specified topology, a constructive algorithm also learns the topology in a manner specific to the problem. Examples include the tiling algorithm of Mezard and Nadal [12] and the cascade-correlation algorithm of Fahlman and Lebiere <ref> [5] </ref>. An iterative construction of hidden units in a feed-forward neural network with a single hidden layer is proposed by Baum [2]. This algorithm constructs a two layer neural network in polynomial time given examples and the ability to query for the classification of specific points within the problem domain.
Reference: [6] <author> J. Fletcher and Z. Obradovic. </author> <title> Creation of neural networks by hyperplane generation from examples alone. In Neural Networks for Learning, Recognition and Control, page 23, </title> <address> Boston, </address> <year> 1992. </year>
Reference-contexts: In practice, the oracle required by Baum's algorithm may either be too expensive or not available. Here we propose a modified algorithm to construct the hidden units from examples alone (preliminary results appeared in <ref> [6] </ref>). In our algorithm, an approximation of the points on the separating hyperplane are found by repeatedly interpolating between points of the different classes in the training set T . The interpolation begins by selecting positive and negative examples m; n 2 T .
Reference: [7] <author> S. I. Gallant. </author> <title> Perceptron-based learning algorithms. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 1(2) </volume> <pages> 179-191, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: In generated networks there are no hidden connections to learn and so one can use any number of algorithms (including backpropagation) to train the output layer weights. A natural choice is the pocket algorithm <ref> [7] </ref>, which is a single-layer neural network algorithm that finds the optimal separation even for non-linearly separable problems. The algorithm keeps the best set of weights in the "pocket" while the perceptron is trained 4 incrementally.
Reference: [8] <author> J. C. Giarratano. </author> <title> CLIPS User's Guide. </title> <address> Athens, GA, </address> <year> 1991. </year>
Reference-contexts: Such a system is also static. Without modification to the rule base, it cannot learn from the sample data presented. As a sample expert system we used the C Language Integrated Production System (CLIPS) developed by the Software Technology Branch of NASA at the Lyndon B. Johnson Space Center <ref> [8] </ref>. Through the use of CLIPS, it was possible to embed an expert system as a starting point for the iterative construction of a neural network. As shown in Figure 5, the prior knowledge from the rule base is extended by incrementally adding additional hidden units as necessary.
Reference: [9] <editor> F. Hayes-Roth, D. A. Waterman, and D. B. Lenat, editors. </editor> <title> Building Expert Systems. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1983. </year>
Reference-contexts: 1 Introduction Classification systems in artificial intelligence are primarily based on two concepts. The first is that of representing existing human knowledge in a form that can be interpreted by a machine. In particular, knowledge in an expert system is represented by the rule base extracted from a domain expert <ref> [9] </ref>. The second concept is the extraction of the knowledge from the sample data which is the subject of machine learning research [18].
Reference: [10] <author> G. F. Luger and W. A. Stubblefield. </author> <booktitle> Artificial Intelligence and the Design of Expert Systems. </booktitle> <address> Benjamin/Cummings, Redwood City, CA, </address> <year> 1989. </year>
Reference-contexts: In our hybrid system the domain theory is transformed into an initial network through an extended version of KBANN's symbolic knowledge encoding. As an example, consider the rule base in Table 1 which is a modified version of the simple financial advisor from <ref> [10] </ref>. if (savings adequate and income adequate) then invest stocks if dependent savings adequate then savings adequate if assets high then savings adequate if (dependent income adequate and earnings steady) then income adequate if debt low then income adequate if (savings dependents fi 5000) then dependent savings adequate if (income 25000
Reference: [11] <author> W. McCulloch and W. Pitts. </author> <title> A logical calculus of ideas immanent in nervous activity. </title> <journal> Bulletin of Mathematical Biophysics, </journal> <volume> 5, </volume> <year> 1943. </year> <note> Reprinted in [1]. </note>
Reference-contexts: concept are studied separately and as well as combined performing experiments using the MONK's problems [16]. 2 A Constructive Neural Network Learning Algo rithm A neural network is a highly parallel machine consisting of computational units based originally upon the simple binary model of a neuron of McCulloch and Pitts <ref> [11] </ref>. Since the development of the backpropagation algorithm by Rumelhart [15] (among others), there has been increased interest in their usage for applications where traditional computation has performed poorly (e.g., ambiguous data or large contextual influence).
Reference: [12] <author> M. Mezard and J.-P. Nadal. </author> <title> Learning in feedforward layered networks: The tiling algorithm. </title> <journal> Journal of Physics A, </journal> <volume> 22 </volume> <pages> 2191-2204, </pages> <year> 1989. </year>
Reference-contexts: In contrast to learning on a pre-specified topology, a constructive algorithm also learns the topology in a manner specific to the problem. Examples include the tiling algorithm of Mezard and Nadal <ref> [12] </ref> and the cascade-correlation algorithm of Fahlman and Lebiere [5]. An iterative construction of hidden units in a feed-forward neural network with a single hidden layer is proposed by Baum [2].
Reference: [13] <author> Z. Obradovic and R. Srikumar. </author> <title> Dynamic evaluation of a backup hypothesis. In Neural Networks for Learning, Recognition and Control, page 71, </title> <address> Boston, </address> <year> 1992. </year> <month> 12 </month>
Reference-contexts: The algorithm keeps the best set of weights in the "pocket" while the perceptron is trained 4 incrementally. A practical modification of the pocket learning algorithm is proposed in <ref> [13] </ref> which is faster and still has the same guarantee for convergence to the optimal separating hyperplane. We use this parallel dynamic algorithm to determine the output layer weights in the constructed network.
Reference: [14] <author> J. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: The second is similar to parity problems, and the third is similar to the first with the addition of five percent noise. The rule base, representing good but imperfect prior knowledge, was generated by the PRISM algorithm of Cendrowska [4] and is based on Quinlan's induction algorithm ID3 <ref> [14] </ref>. Hyperplane determination from examples (HDE) is again used to combine the prior knowledge with the knowledge available from the example data.
Reference: [15] <author> D. Rumelhart, G. Hinton, and R. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D. Rumelhart and J. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> volume 1, chapter 8, </volume> <pages> pages 318-362. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1986. </year> <note> Reprinted in [1]. </note>
Reference-contexts: Since the development of the backpropagation algorithm by Rumelhart <ref> [15] </ref> (among others), there has been increased interest in their usage for applications where traditional computation has performed poorly (e.g., ambiguous data or large contextual influence). In selecting an appropriate neural network topology for a classification problem, there are two opposing objectives.
Reference: [16] <author> S. B. Thrun et al. </author> <title> The MONK's problems: A performance comparison of different learning algorithms. </title> <type> Technical Report CMU-CS-91-197, </type> <institution> Department of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1991. </year>
Reference-contexts: Section 3 combines this new algorithm with existing methods for representing prior symbolic knowledge. In section 4, the generalization abilities of each concept are studied separately and as well as combined performing experiments using the MONK's problems <ref> [16] </ref>. 2 A Constructive Neural Network Learning Algo rithm A neural network is a highly parallel machine consisting of computational units based originally upon the simple binary model of a neuron of McCulloch and Pitts [11]. <p> In order to evaluate our combined system generalization abilities, experiments were performed using the financial advisor previously described and the MONK's problems <ref> [16] </ref>. Integration with KBANN was tested by performing experiments based on the financial advisor rule base presented earlier.
Reference: [17] <author> G. G. Towell, J. W. Shavlik, and M. O. Noordwier. </author> <title> Refinement of approximate domain theories by knowledge-based neural networks. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 861-866, </pages> <address> Boston, </address> <month> July </month> <year> 1990. </year>
Reference-contexts: The learning algorithm described in the Section 2 is then used to expand the hypothesis. KBANN, a method of generating knowledge-based artificial neural networks from hierarchically-structured rules, was developed by Towell et. al. <ref> [17] </ref>. Although KBANN performs well when much of the domain theory is known, it is restricted to refining the existing rules rather than discovering new rules. The principal reason is that the network's representational power is bounded by a fixed topology determined only by pre-existing set of rules. <p> Predictive quality of 61.76% from rules alone increased to 97.84% for rules, examples and queries and to 84.72% for rules and examples. In comparison, the knowledge refinement technique of the extended KBANN rule-based network using additional connections and backpropagation as in <ref> [17] </ref> provided an increase to 64.64%. Integration with an expert system was tested on the MONK's problems. The MONK's problems were developed in order to challenge a variety of machine learning approaches.
Reference: [18] <author> S. M. Weiss and C. A. </author> <title> Kulikowski, editors. Computer Systems That Learn. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, </address> <year> 1991. </year> <month> 13 </month>
Reference-contexts: In particular, knowledge in an expert system is represented by the rule base extracted from a domain expert [9]. The second concept is the extraction of the knowledge from the sample data which is the subject of machine learning research <ref> [18] </ref>. The knowledge represented by a rule base may be either incomplete or inconsistent requiring techniques such as the conflict resolution heuristics implemented by the inference engine of the expert system. In a machine learning system, the knowledge is usually incomplete as an incomplete set of examples is practically available.
References-found: 18

