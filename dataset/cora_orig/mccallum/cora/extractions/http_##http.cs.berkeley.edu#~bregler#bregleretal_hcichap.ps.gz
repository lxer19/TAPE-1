URL: http://http.cs.berkeley.edu/~bregler/bregleretal_hcichap.ps.gz
Refering-URL: http://http.cs.berkeley.edu/~bregler/pubs.html
Root-URL: 
Title: 7 Probabilistic Models of Verbal and Body Gestures  
Author: CHRISTOPH BREGLER STEPHEN M. OMOHUNDRO MICHELE COVELL, MALCOLM SLANEY, SUBUTAI AHMAD DAVID A. FORSYTH, JEROME A. FELDMAN Cipolla and Pentland. 
Note: To appear as chapter in Computer Vision in Man-Machine Interfaces, Cambridge University Press 1996, edited by  7.1 Introduction  
Address: Berkeley  
Affiliation: U.C. Berkeley Interval Research Corp.  NEC Research Institute  Interval Research Corp.  U.C.  
Abstract: This chapter describes several probabilistic techniques for representing, recognizing, and generating spatiotemporal configuration sequences. We first describe how such techniques can be used to visually track and recognize lip movements to augment a speech recognition system. We then demonstrate additional techniques that can be used to animate video footage of talking faces and synchronize it to different sentences of an audio track. Finally we outline alternative low-level representations that are needed to apply these techniques to articulated body gestures. Gestures can be described as characteristic configurations over time. While uttering a sentence we express very fine grained verbal gestures as complex lip configurations over time, and while performing body actions, we generate articulated configuration sequences of jointed arm and leg segments. Such configurations lie in constrained subspaces and different y Part of this report is published at ICCV'95 [9]
Abstract-found: 1
Intro-found: 0
Reference: [1] <author> A. Adjoudani and C. Benoit. </author> <title> On the integration of auditory and visual parameters in an hmm-based asr. </title> <booktitle> In NATO Advanced Study Institute on Speechreading by Man and Machine, </booktitle> <year> 1995. </year>
Reference-contexts: This was also true of the crosstalk experiment which showed the largest improvement. 7.4.4 Related Computer Lipreading Approaches One of the earliest successful attempts to improve speech recognition by combining acoustic recognition and lipreading was done by Petajan in 1984 [33]. More recent experiments include <ref> [26, 41, 8, 39, 14, 35, 28, 30, 25, 1, 19] </ref>. All approaches attempt to show that computer lip reading is able to improve speech recognition, especially in noisy environments. The systems were applied to phoneme classification, isolated words, or to small continuous word recognition problems.
Reference: [2] <author> Serge Ayer and Harpreet S. Sawhney. </author> <title> Layered representation of mo tion video using robust maximum-likelihood estimation of mixture models and mdl encoding. </title> <booktitle> In Int. Conf. Computer Vision, </booktitle> <pages> pages 777-784, </pages> <address> Cambridge, MA., </address> <year> 1995. </year>
Reference-contexts: Part of the covariance describes the spatial distribution, and one variance describes the graylevel deviation of the motion prediction given the previous image frame. Without the spatial parameters, this approach is similar to layered motion estimation using EM search <ref> [16, 2] </ref>. Each pixel has a hidden random variable that assigns this pixel to one of the motion blob models or a background model.
Reference: [3] <author> T. Beier and S. Neely. </author> <title> Feature-based image metamorphosis. </title> <journal> Com puter Graphics, </journal> <volume> 26(2) </volume> <pages> 35-42, </pages> <year> 1992. </year>
Reference-contexts: We integrate the new lips into the original background sequence using the tracked contours of lips, chin, and neck. We call this "stitching". Figure 7.10 shows the flow chart with example images. Figure 7.10 (a) and (d) are two example key-frames. The images are spatially warped <ref> [3] </ref> in such a way that pixels along source contours are mapped to pixels along target contours. The two key frames have two different sets of source contours, but are mapped to the same set of target contours.
Reference: [4] <author> D. Beymer, A. Shashua, and T. Poggio. </author> <title> Example based image analysis and synthesis. M.I.T. </title> <journal> A.I. </journal> <volume> Memo No. 1431, </volume> <month> Nov </month> <year> 1993. </year>
Reference-contexts: The system described in [12] models this finding. 18 Fig. 7.10 Flow Chart for morphing and stitching new lip images into the original movie sequence morphing was demonstrated for view interpolation of human faces by <ref> [4] </ref>. 7.6.4 Experiments We applied VideoRewrite to stock-footage of a 30 minute sequence of CNN Headline News, and a short sequence of a Marilyn Monroe movie. In the case of the CNN Headline News we had enough data to build the Full Bi-Viseme Database.
Reference: [5] <author> A. Blake, M. Isard, and D. Reynard. </author> <title> Learning to track the visual motion of contours. </title> <editor> In J. </editor> <booktitle> Artificial Intelligence, </booktitle> <year> 1995. </year>
Reference-contexts: We have demonstrated the excellent performance of this approach on synthetic examples [10]. A related mixture model approach applied to input-output mappings appears in [17]. 7.3 Constrained Tracking To track the position of the lips we integrate the manifold representation with an "Active Contour" technique <ref> [18, 42, 21, 5] </ref>. In each image, a contour shape is matched to the boundary of the lips. The space of contours that represent lips is represented by a learned lip-contour-manifold. <p> We applied these techniques to the domain of lip and facial features. The representations of geometric and appearances that we used are related to many other techniques applied to the same domain <ref> [38, 5, 43, 21] </ref>. Although faces and lips span up a very complex set of configurations, the features generated lie on a relatively small constrained subspace. This is different for the domain of articulated objects like human bodies or hands.
Reference: [6] <author> H.A. Bourlard and Morgan N. </author> <title> Connectionist Speech Recognition, </title>
Reference-contexts: Our training set consists of 2955 connected letters (uttered by the 6 speakers). We used an additional cross-validation set of 364 letters to avoid overfitting. In this set of experiments the HMM emission probabilities were estimated by a multi-layer-perceptron (MLP) <ref> [6] </ref>. The same MLP/HMM architecture has achieved state-of-the-art recognition performance on standard acoustic databases like the ARPA resource management task. We have trained 3 different versions of the system: one based purely on acoustic signals using 9-dimensional RASTA-PLP features, and two that combine visual and acoustic features.
References-found: 6

