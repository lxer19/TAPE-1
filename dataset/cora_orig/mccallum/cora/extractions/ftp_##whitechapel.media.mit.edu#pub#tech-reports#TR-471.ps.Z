URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-471.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Email: fclarkson,sandyg@media.mit.edu  
Title: UNSUPERVISED CLUSTERING OF AMBULATORY AUDIO AND VIDEO  
Author: Brian Clarkson and Alex Pentland 
Address: Cambridge, MA 02139  
Affiliation: MIT Media Lab  
Pubnum: Perceptual Computing  
Abstract: A truly personal and reactive computer system should have access to the same information as its user, including the ambient sights and sounds. To this end, we have developed a system for extracting events and scenes from natural audio/visual input. We find our system can (without any prior labeling of data) cluster the audio/visual data into events, such as passing through doors and crossing the street. Also, we hierarchically cluster these events into scenes and get clusters that correlate with visiting the supermarket, or walking down a busy street. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Albert S. Bregman. </author> <title> Auditory Scene Analysis: The Perceptual Organization of Sound. </title> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: THE PERSONAL AUDIO-VISUAL TASK In contrast to subject-oriented video or audio [5], such as TV [4], movies, and video recordings of meetings [3], our goal is to use video to monitor an individual's environment. Literally, the camera and microphone become an extra set of senses for the user. <ref> [1, 2] </ref> 2.1. Data Collection In order to adequately sample the visual and aural environment of a mobile person, the sensors should be small and have a wide field of reception. <p> Abstract: Construct a new feature space from these likelihoods, F (t) = 6 L 1 (t) L N (t) 7 3. Cluster: Now cluster the new feature space into scene HMMs using the algorithm from Section 3. test <ref> [1] </ref> 4. RESULTS We evaluated our performance by noting the correlation between our emergent models and a human-generated transcription. Each cluster plays the role of a hypothesis. A hypothesis is verified when its indexing correlates highly with a ground truth labeling.
Reference: [2] <author> G. J. Brown. </author> <title> Computational Auditory Scene Analysis: A representational approach. </title> <type> PhD thesis, </type> <institution> University of Sheffield, </institution> <year> 1992. </year>
Reference-contexts: THE PERSONAL AUDIO-VISUAL TASK In contrast to subject-oriented video or audio [5], such as TV [4], movies, and video recordings of meetings [3], our goal is to use video to monitor an individual's environment. Literally, the camera and microphone become an extra set of senses for the user. <ref> [1, 2] </ref> 2.1. Data Collection In order to adequately sample the visual and aural environment of a mobile person, the sensors should be small and have a wide field of reception.
Reference: [3] <author> Bernhard Feiten and Stefan Gunzel. </author> <title> Automatic indexing of a sound database using self-organizing neural nets. </title> <journal> Computer Music Journal, </journal> <year> 1994. </year>
Reference-contexts: THE PERSONAL AUDIO-VISUAL TASK In contrast to subject-oriented video or audio [5], such as TV [4], movies, and video recordings of meetings <ref> [3] </ref>, our goal is to use video to monitor an individual's environment. Literally, the camera and microphone become an extra set of senses for the user. [1, 2] 2.1.
Reference: [4] <author> Liu, Wang, , and Chen. </author> <title> Audio feature extraction and analysis for multimedia content classification. </title> <journal> Journal of VLSI Signal Processing Systems, </journal> <year> 1998. </year>
Reference-contexts: THE PERSONAL AUDIO-VISUAL TASK In contrast to subject-oriented video or audio [5], such as TV <ref> [4] </ref>, movies, and video recordings of meetings [3], our goal is to use video to monitor an individual's environment. Literally, the camera and microphone become an extra set of senses for the user. [1, 2] 2.1.
Reference: [5] <author> Silvia Pfeiffer, Stephan Fischer, and Wolfgang Effelsberg. </author> <title> Automatic audio content analysis. </title> <type> Technical report, </type> <institution> University of Mannheim, </institution> <year> 1997. </year>
Reference-contexts: THE PERSONAL AUDIO-VISUAL TASK In contrast to subject-oriented video or audio <ref> [5] </ref>, such as TV [4], movies, and video recordings of meetings [3], our goal is to use video to monitor an individual's environment. Literally, the camera and microphone become an extra set of senses for the user. [1, 2] 2.1.
Reference: [6] <author> Lawrence R. Rabiner. </author> <title> A tutorial on hidden markov models and selected applications in speech recognition. </title> <booktitle> Proceedings of the IEEE, </booktitle> <year> 1989. </year>
Reference-contexts: Both the video and the audio features were calcu lated at a rate of 10Hz. 3. TIME SERIES CLUSTERING The algorithm we used to cluster time series data is a variation on the Segmental K-Means algorithm <ref> [6] </ref>. The procedure is as follows: 1. Given: N , the number of models, T the number of samples allocated to a state, S, the number of states per model, f the expected rate of class changes. 2.
Reference: [7] <editor> Dan Siewiorek, editor. </editor> <booktitle> The First International Symposium on Wearable Computers, </booktitle> <year> 1997. </year>
Reference-contexts: 1. INTRODUCTION Computers have evolved into miniature and wearable systems <ref> [7] </ref>. As a result there is a desire for these computers to be tightly coupled with their user's day-today activities. A popular analogy for this integration equates the wearable computer to an intelligent observer and assistant for its user.
Reference: [8] <author> T. Starner, B. Schiele, and A. Pentland. </author> <title> Visual contextual awareness in wearable computing. </title> <booktitle> In Second International Symposium on Wearable Computers, </booktitle> <month> Oct </month> <year> 1998. </year>
Reference-contexts: A popular analogy for this integration equates the wearable computer to an intelligent observer and assistant for its user. To fill this role effectively, the wearable computer needs to live in the same sensory world as its human user. <ref> [8] </ref> Thus, a system is required that can take this natural and personal audio/video and find the coherent segments, the points of major activity, and recurring events. The field of multimedia indexing has wrestled with many of the problems that such a system creates.
References-found: 8

