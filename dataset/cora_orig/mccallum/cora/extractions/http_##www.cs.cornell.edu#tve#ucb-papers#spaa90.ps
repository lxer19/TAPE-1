URL: http://www.cs.cornell.edu/tve/ucb-papers/spaa90.ps
Refering-URL: 
Root-URL: 
Title: Analysis of Multithreaded Architectures for Parallel Computing  
Author: Rafael H. Saavedra-Barrera David E. Culler Thorsten von Eicken 
Address: Berkeley, California 94720  
Affiliation: Computer Science Division University of California  
Abstract: Multithreading has been proposed as an architectural strategy for tolerating latency in multiprocessors and, through limited empirical studies, shown to offer promise. This paper develops an analytical model of multithreaded processor behavior based on a small set of architectural and program parameters. The model gives rise to a large Markov chain, which is solved to obtain a formula for processor efficiency in terms of the number of threads per processor, the remote reference rate, the latency, and the cost of switching between threads. It is shown that a multithreaded processor exhibits three operating regimes: linear (efficiency is proportional to the number of threads), transition, and saturation (efficiency depends only on the remote reference rate and switch cost). Formulae for regime boundaries are derived. The model is embellished to reflect cache degradation due to multithreading, using an analytical model of cache behavior, demonstrating that returns diminish as the number threads becomes large. Predictions from the embellished model correlate well with published empirical measurements. Prescriptive use of the model under various scenarios indicates that multithreading is effective, but the number of useful threads per processor is fairly small. 
Abstract-found: 1
Intro-found: 1
Reference: [Ande67] <author> Anderson, D.W., Sparacio, F.J., and Tomasulo, </author> <title> R.M., ``The IBM System/360 Model 91: </title> <journal> Machine Philosophy and Instruction-Handling''. IBM Journal, Vol.9, </journal> <volume> No.25, </volume> <month> January </month> <year> 1967, </year> <pages> pp. 8-24. </pages>
Reference-contexts: There are two obvious alternatives: avoid latency or tolerate it. In other words, we may attempt to reduce the number of such requests or perform other useful work while requests are outstanding. In uniprocessors, the former is realized by caches [Smit82] and the latter by dynamic hazard resolution logic <ref> [Ande67, Russ78] </ref>. Unfortunately, neither of these approaches extends trivially to large-scale multiprocessors [Arvi87]. The latency avoidance strategy embodied in caches is attractive for multiprocessors because local copies of shared data are produced on demand.
Reference: [Arga88] <author> Agarwal, A., Simoni, R. Hennessy, J., and Horowitz, M., </author> <title> ``An Evaluation of Directory Schemes for Cache Coherency''. </title> <booktitle> Proc. of the 15th Annual Int. Symp. on Comp. Arch., </booktitle> <address> Honolulu, Hawaii, </address> <month> June </month> <year> 1988, </year> <pages> pp. 280-289. </pages>
Reference-contexts: Unfortunately, neither of these approaches extends trivially to large-scale multiprocessors [Arvi87]. The latency avoidance strategy embodied in caches is attractive for multiprocessors because local copies of shared data are produced on demand. However, this replication leads to a thorny problem of maintaining a coherent image of the memory <ref> [Good83, Arga88] </ref>. Concrete solutions exist for small bus-based multiprocessors, but even these involve considerable complexity and observed miss rates on shared data are fairly high [Egge88]. Approaches that do not provide automatic replication can be expected to make a larger number of long-latency requests.
Reference: [Arga89] <author> Agarwal, A., Horowitz, M., and Hennessy, J., </author> <title> ``An Analytical Cache Model''. </title> <journal> ACM Trans. on Comp. Sys., Vol.7, no.2, </journal> <month> May </month> <year> 1989, </year> <pages> pp. 184-215. </pages>
Reference: [Arvi87] <author> Arvind, and Ianucci, R.A., </author> <title> ``Two Fundamental Issues in Multiprocessing''. </title> <booktitle> Proc. of DFVLR - Conf. 1987 on Parallel Proc. in Sc. </booktitle> <institution> and Eng., West Germany, </institution> <month> June </month> <year> 1987, </year> <pages> pp. 61-88. </pages>
Reference-contexts: In uniprocessors, the former is realized by caches [Smit82] and the latter by dynamic hazard resolution logic [Ande67, Russ78]. Unfortunately, neither of these approaches extends trivially to large-scale multiprocessors <ref> [Arvi87] </ref>. The latency avoidance strategy embodied in caches is attractive for multiprocessors because local copies of shared data are produced on demand. However, this replication leads to a thorny problem of maintaining a coherent image of the memory [Good83, Arga88]. <p> Multithreaded Processor Model In this section, we present the architectural model and definitions used throughout the paper. We focus on one processor in a multiprocessor configuration and ignore issues of synchronization <ref> [Arvi87] </ref>. In addition, we assume the latency in processing a request is determined primarily by the machine structure and minimally affected by program behavior, so it is considered constant. We also assume that many outstanding requests can be issued and will be processed in a pipelined fashion.
Reference: [Arvi88] <author> Arvind, Culler, D.E., and Maa, G.K., </author> <title> ``Assessing the Benefits of Fine-Grain Parallelism in Dataflow Programs''. </title> <journal> The International Journal of Supercomputer Applications, Vol.2, </journal> <volume> No.3, </volume> <month> November </month> <year> 1988. </year>
Reference-contexts: While the approach is conceptually appealing, the available evidence for its effectiveness is drawn from a small sample of programs under hypothetical implementations <ref> [Arvi88, Ianu88, Webe89] </ref>. No coherent framework for evaluating these architectures has emerged. In this paper, we develop a simple analytical model of multithreaded architectures in order to understand the potential gains offered by the approach and its fundamental limitations.
Reference: [Clar83] <author> Clark, D.W., </author> <title> ``Cache Performance in the VAX-11/780''. </title> <journal> ACM Trans. Comp. Sys., Vol.1, </journal> <volume> No.1, </volume> <month> February </month> <year> 1983, </year> <pages> pp. 24-37. </pages>
Reference-contexts: Hence, we only have to consider how the intrinsic interference is affected 2 . Many different techniques have been used to obtain the performance of caches: trace-driven simulation, hardware measurement, and analytical models <ref> [Smit82, Clar83, Agar89] </ref>.
Reference: [Egge88] <author> Eggers, S.J., and Katz, </author> <title> R.H., ``A Characterization of Sharing in Parallel Programs and its Application to Coherency Protocol Evaluation''. </title> <booktitle> Proc. of the 15th Annual Int. Symp. on Comp. Arch., </booktitle> <address> Honolulu, Hawaii, </address> <month> June </month> <year> 1988, </year> <pages> pp. 373-383. </pages>
Reference-contexts: However, this replication leads to a thorny problem of maintaining a coherent image of the memory [Good83, Arga88]. Concrete solutions exist for small bus-based multiprocessors, but even these involve considerable complexity and observed miss rates on shared data are fairly high <ref> [Egge88] </ref>. Approaches that do not provide automatic replication can be expected to make a larger number of long-latency requests. Several recent architectures adopt the alternative strategy of tolerating latency. These can be loosely classified as mul-tithreaded architectures, because each processor supports several active threads of computation.
Reference: [Good83] <author> Goodman, J.R., </author> <title> ``Using Cache Memory to Reduce processor-Memory Traffic''. </title> <booktitle> Proc. of the 10th Annual Int. Symp. on Comp. Arch., </booktitle> <address> Stockholm, Sweden, </address> <year> 1983. </year>
Reference-contexts: Unfortunately, neither of these approaches extends trivially to large-scale multiprocessors [Arvi87]. The latency avoidance strategy embodied in caches is attractive for multiprocessors because local copies of shared data are produced on demand. However, this replication leads to a thorny problem of maintaining a coherent image of the memory <ref> [Good83, Arga88] </ref>. Concrete solutions exist for small bus-based multiprocessors, but even these involve considerable complexity and observed miss rates on shared data are fairly high [Egge88]. Approaches that do not provide automatic replication can be expected to make a larger number of long-latency requests.
Reference: [Hals88] <author> Halstead, R.H., Jr., and Fujita, T., ``MASA: </author> <title> A Mul-tithreaded Processor Architecture for Parallel Symbolic Computing''. </title> <booktitle> Proc. of the 15th Annual Int. Symp. on Comp. Arch., </booktitle> <address> Honolulu, Hawaii, </address> <month> June </month> <year> 1988, </year> <pages> pp. 443-451. </pages>
Reference-contexts: A number of more conservative architectures have been proposed that maintain the state of several conventional instruction streams and switch among these based on some event, either every instruction <ref> [Hals88, Smit78, This88] </ref>, every load instruction [Ianu88, Nikh89], or every cache miss [Webe89]. While the approach is conceptually appealing, the available evidence for its effectiveness is drawn from a small sample of programs under hypothetical implementations [Arvi88, Ianu88, Webe89]. No coherent framework for evaluating these architectures has emerged.
Reference: [Ianu88] <author> Ianucci, R.A., </author> <title> ``Toward a Dataflow / von Neumann Hybrid Architecture''. </title> <booktitle> Proc. of the 15th Annual Int. Symp. on Comp. Arch., </booktitle> <address> Honolulu, Hawaii, </address> <month> June </month> <year> 1988, </year> <pages> pp. 131-140. </pages>
Reference-contexts: A number of more conservative architectures have been proposed that maintain the state of several conventional instruction streams and switch among these based on some event, either every instruction [Hals88, Smit78, This88], every load instruction <ref> [Ianu88, Nikh89] </ref>, or every cache miss [Webe89]. While the approach is conceptually appealing, the available evidence for its effectiveness is drawn from a small sample of programs under hypothetical implementations [Arvi88, Ianu88, Webe89]. No coherent framework for evaluating these architectures has emerged. <p> While the approach is conceptually appealing, the available evidence for its effectiveness is drawn from a small sample of programs under hypothetical implementations <ref> [Arvi88, Ianu88, Webe89] </ref>. No coherent framework for evaluating these architectures has emerged. In this paper, we develop a simple analytical model of multithreaded architectures in order to understand the potential gains offered by the approach and its fundamental limitations.
Reference: [Nikh88] <author> Nikhil, </author> <title> R.S. and Arvind, ``Can Dataflow Subsume von Neumann Computing?''. </title> <booktitle> Proc. of the 16th Annual Int. Symp. on Comp. Arch., </booktitle> <address> Jerusalem, Israel, </address> <month> June </month> <year> 1989. </year>
Reference: [Russ78] <author> Russell, </author> <title> R.M., ``The CRAY-1 Computer System''. </title> <journal> Comm. of the ACM, Vol.21, </journal> <volume> No.1, </volume> <month> January </month> <year> 1978. </year> <pages> pp. 63-72. </pages>
Reference-contexts: There are two obvious alternatives: avoid latency or tolerate it. In other words, we may attempt to reduce the number of such requests or perform other useful work while requests are outstanding. In uniprocessors, the former is realized by caches [Smit82] and the latter by dynamic hazard resolution logic <ref> [Ande67, Russ78] </ref>. Unfortunately, neither of these approaches extends trivially to large-scale multiprocessors [Arvi87]. The latency avoidance strategy embodied in caches is attractive for multiprocessors because local copies of shared data are produced on demand.
Reference: [Saav90] <author> Saavedra-Barrera, R.H., and Culler, D., </author> <title> ``An Analytical Solution for a Markov Chain Modeling Mul-tithread Execution'', </title> <institution> University of California, Berke-ley, </institution> <note> technical report in preparation. </note>
Reference-contexts: Fortunately, we have found the exact solution to the limiting probabilities. The main steps in the derivation are sketched below; details can be found in <ref> [Saav90] </ref>. Although the number of equations in the transition matrix for the Markov chain is very large, there are several patterns present in this particular system that makes it possible to reduce all equations to a new system having only one unknown. <p> Essentially, we must determine the number of contexts required to reduce the probability that the processor is idle below some threshold. Knowing that a geometric random variable with mean p has variance (1 - p )/p 2 = R (R - 1)) and making some simplifications <ref> [Saav90] </ref>, we obtain a quadratic equation for the new approximate saturation point (N s ).
Reference: [Smit82] <author> Smith, A.J., </author> <title> ``Cache Memories''. </title> <journal> ACM Computing Surveys, Vol.14, </journal> <volume> No.3, </volume> <month> September </month> <year> 1982, </year> <pages> pp. 473-530. </pages>
Reference-contexts: There are two obvious alternatives: avoid latency or tolerate it. In other words, we may attempt to reduce the number of such requests or perform other useful work while requests are outstanding. In uniprocessors, the former is realized by caches <ref> [Smit82] </ref> and the latter by dynamic hazard resolution logic [Ande67, Russ78]. Unfortunately, neither of these approaches extends trivially to large-scale multiprocessors [Arvi87]. The latency avoidance strategy embodied in caches is attractive for multiprocessors because local copies of shared data are produced on demand. <p> Hence, we only have to consider how the intrinsic interference is affected 2 . Many different techniques have been used to obtain the performance of caches: trace-driven simulation, hardware measurement, and analytical models <ref> [Smit82, Clar83, Agar89] </ref>.
Reference: [Smit78] <author> Smith, B.J., </author> <title> ``A Pipelined, Shared Resource MIMD Computer''. </title> <booktitle> 1978 Int. Conf. on Parallel Proc., </booktitle> <year> 1978, </year> <pages> pp. 6-8. </pages>
Reference-contexts: A number of more conservative architectures have been proposed that maintain the state of several conventional instruction streams and switch among these based on some event, either every instruction <ref> [Hals88, Smit78, This88] </ref>, every load instruction [Ianu88, Nikh89], or every cache miss [Webe89]. While the approach is conceptually appealing, the available evidence for its effectiveness is drawn from a small sample of programs under hypothetical implementations [Arvi88, Ianu88, Webe89]. No coherent framework for evaluating these architectures has emerged.
Reference: [Thie89] <author> Thiebaut, D., </author> <title> ``On the Fractal Dimension of Computer Programs and its Application to the Prediction of the Cache Miss Ratio''. </title> <journal> IEEE Trans. on Computers, Vol.38, </journal> <volume> No.7, </volume> <month> July </month> <year> 1989, </year> <pages> pp. 1012-1026. </pages>
Reference-contexts: For, example, if the total cache size is 256K hhhhhhhhhhhhhhhhhhhhh 2 This does not reflects that there may be some minimum miss rate, regardless of cache size, due to communication of data between processors, but could be easily extended to do so. 3 D. Thiebaut argues in <ref> [Thie89] </ref> that this relationship can be explained by considering the program's execution as a fractal random walk over the address space.
Reference: [This88] <author> Thistle, M.R., and Smith, B.J., </author> <title> ``A Processor Architecture for Horizon''. </title> <booktitle> Supercomputing '88, </booktitle> <address> Florida, </address> <month> October </month> <year> 1988, </year> <pages> pp. 35-40. </pages>
Reference-contexts: A number of more conservative architectures have been proposed that maintain the state of several conventional instruction streams and switch among these based on some event, either every instruction <ref> [Hals88, Smit78, This88] </ref>, every load instruction [Ianu88, Nikh89], or every cache miss [Webe89]. While the approach is conceptually appealing, the available evidence for its effectiveness is drawn from a small sample of programs under hypothetical implementations [Arvi88, Ianu88, Webe89]. No coherent framework for evaluating these architectures has emerged.
Reference: [Webe89] <author> Weber, W., and Gupta A., </author> <title> ``Exploring the Benefits of Multiple Hardware Contexts in a Multiprocessor Architecture: Preliminary Results''. </title> <booktitle> Proc. of the 16th Annual Int. Symp. on Comp. Arch., </booktitle> <address> Jerusalem, Israel, </address> <month> June </month> <year> 1989, </year> <pages> pp. 273-280. </pages>
Reference-contexts: A number of more conservative architectures have been proposed that maintain the state of several conventional instruction streams and switch among these based on some event, either every instruction [Hals88, Smit78, This88], every load instruction [Ianu88, Nikh89], or every cache miss <ref> [Webe89] </ref>. While the approach is conceptually appealing, the available evidence for its effectiveness is drawn from a small sample of programs under hypothetical implementations [Arvi88, Ianu88, Webe89]. No coherent framework for evaluating these architectures has emerged. <p> While the approach is conceptually appealing, the available evidence for its effectiveness is drawn from a small sample of programs under hypothetical implementations <ref> [Arvi88, Ianu88, Webe89] </ref>. No coherent framework for evaluating these architectures has emerged. In this paper, we develop a simple analytical model of multithreaded architectures in order to understand the potential gains offered by the approach and its fundamental limitations. <p> We now compare predictions from our model against trace-driven simulation results for a multiprocessor currently being studied at Stanford <ref> [Webe89] </ref>. <p> The caches are maintained consistent by a directory-based cache coherency protocol. A context executes until it requires information not available in its processor cache or attempts to write data marked "read-shared". Restating measurements presented in <ref> [Webe89] </ref> in terms of our model we obtain values for the relevant parameters shown in the top portion of table 1. These are used as input for our model and the comparison with reported simulation results is presented in the remainder of table 1.
References-found: 18

