URL: ftp://ftp.cs.washington.edu/tr/1997/12/UW-CSE-97-12-02.PS.Z
Refering-URL: http://www.cs.washington.edu/research/tr/tr-by-title.html
Root-URL: 
Email: zpl-info@cs.washington.edu  
Title: ZPL's WYSIWYG Performance Model  
Author: Bradford L. Chamberlain Sung-Eun Choi E Christopher Lewis Calvin Lin Lawrence Snyder W. Derrick Weathersby 
Address: Seattle, WA 98195-2350 USA  Austin, TX 78712 USA  
Affiliation: University of Washington,  University of Texas,  
Abstract: ZPL is an array language designed for high performance scientific and engineering computations. Unlike other parallel languages, ZPL is founded on a machine model (CTA) distinct from any implementing hardware. The machine model, which abstracts contemporary parallel computers, makes it possible to correlate ZPL programs with machine behavior. Using this association, programmers can know approximately how code will perform on a typical parallel machine, allowing them to make informed decisions between alternative programming solutions. This paper describes ZPL's syntactic cues to the programmer which convey performance information. The what-you-see-is-what-you-get (WYSIWYG) characteristics of ZPL operations are illustrated on four machines: the Cray T3E, IBM SP-2, SGI Power Challenge and Intel Paragon. Additionally, the WYSIWYG performance model is used to evaluate two algorithms for matrix multiplication, one of which is considered to be the most scalable of portable parallel solutions. Experiments show that the performance model correctly predicts the faster solution on all four platforms for a range of problem sizes.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Guy E. Blelloch. </author> <title> Programming parallel algorithms. </title> <journal> Communications of the ACM, </journal> <volume> 39(3):85 97, </volume> <month> March </month> <year> 1996. </year>
Reference-contexts: However, coding at this low level is tedious and error-prone, motivating the existence of higher-level parallel programming languages. In the arena of higher-level parallel languages, NESL is a language with a well-defined performance model <ref> [1] </ref>. It uses a work/depth model to calculate asymptotic bounds for the execution time of NESL programs on parallel computers.
Reference: [2] <author> L. F. Cannon. </author> <title> A Cellular Computer to Implement the Kalman Filter Algorithm. </title> <type> PhD thesis, </type> <institution> Montana State University, </institution> <year> 1969. </year> <month> 16 </month>
Reference-contexts: In Figure 2, we show two algorithms for matrix-matrix multiplication, SUMMA [16] and Cannon's Algorithm <ref> [2] </ref>. SUMMA is considered to be the most scalable of portable parallel matrix multiplication algorithms. It iteratively floods a column of matrix A and a row of matrix B, accumulating their product in C.
Reference: [3] <author> Marios D. Dikaiakos, Calvin Lin, Daphne Manoussaki, and Diana E. Woodward. </author> <title> The portable parallel implementation of two novel mathematical biology algorithms in ZPL. </title> <booktitle> In Ninth International Conference on Supercomputing, </booktitle> <year> 1995. </year>
Reference-contexts: ZPL generally outperforms HPF and has proven to be competitive with hand-coded C and message passing [9, 7]. Applications from a variety of disciplines have been written using ZPL <ref> [6, 3, 13] </ref>, and the language was released for widespread use in July 1997. Supported platforms include the Cray T3D/T3E, Intel Paragon, IBM SP-2, SGI Power Challenge, clusters of workstations using PVM or MPI, and sequential workstations.
Reference: [4] <author> Richard Friedman, John Levesque, and Gene Wagenbreth. </author> <title> Fortran Parallelization Handbook, </title> <note> Preliminary Edition. Applied Parallel Research, </note> <month> April </month> <year> 1995. </year>
Reference-contexts: One of the biggest causes of ambiguity in the performance of HPF programs is the fact that communication is completely hidden from the user, making it difficult to evaluate different implementation options <ref> [4] </ref>. As an example, Ngo compares matrix multiply algorithms written in HPF, demonstrating that there is neither any source-level indication of how they will perform, nor does either algorithm consistently outperform the other. [10].
Reference: [5] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Specification Version 2.0. </title> <month> Jan-uary </month> <year> 1997. </year>
Reference-contexts: As an example, the cost of interprocessor communication is considered negligible in the NESL model and is therefore ignored entirely. The most prevalent parallel language, High Performance Fortran <ref> [5] </ref>, suffers a complete lack of a performance model. As a result, programmers must re-tune their program for each compiler and platform that they use, neutralizing any notion of portable performance.
Reference: [6] <author> E Christopher Lewis, Calvin Lin, Lawrence Snyder, and George Turkiyyah. </author> <title> A portable parallel n-body solver. </title> <editor> In D. Bailey, P. Bjorstad, J. Gilbert, M. Mascagni, R. Schreiber, H. Simon, V. Torczon, and L. Watson, editors, </editor> <booktitle> Proceedings of the Seventh SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 331336. </pages> <publisher> SIAM, </publisher> <year> 1995. </year>
Reference-contexts: ZPL generally outperforms HPF and has proven to be competitive with hand-coded C and message passing [9, 7]. Applications from a variety of disciplines have been written using ZPL <ref> [6, 3, 13] </ref>, and the language was released for widespread use in July 1997. Supported platforms include the Cray T3D/T3E, Intel Paragon, IBM SP-2, SGI Power Challenge, clusters of workstations using PVM or MPI, and sequential workstations.
Reference: [7] <author> C. Lin, L. Snyder, R. Anderson, B. Chamberlain, S. Choi, G. Forman, E. Lewis, and W. D. Weathersby. </author> <title> ZPL vs. HPF: A comparison of performance and programming style. </title> <type> Technical Report 951105, </type> <institution> Department of Computer Science and Engineering, University of Wash-ington, </institution> <year> 1995. </year>
Reference-contexts: ZPL has sequential semantics that allow programs to be written and debugged on sequential workstations and then effortlessly recompiled for execution on parallel architectures. ZPL generally outperforms HPF and has proven to be competitive with hand-coded C and message passing <ref> [9, 7] </ref>. Applications from a variety of disciplines have been written using ZPL [6, 3, 13], and the language was released for widespread use in July 1997. Supported platforms include the Cray T3D/T3E, Intel Paragon, IBM SP-2, SGI Power Challenge, clusters of workstations using PVM or MPI, and sequential workstations.
Reference: [8] <author> Calvin Lin. </author> <title> ZPL language reference manual. </title> <type> Technical Report 941006, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <year> 1994. </year>
Reference-contexts: In this section, we give a brief introduction to ZPL concepts that are required to understand this paper. A more complete presentation of the language is available in the ZPL Programmer's Guide and Reference Manual <ref> [15, 8] </ref>. 3.1 Regions and Arrays The region is ZPL's most fundamental concept. Regions are index sets through which a program's parallelism is expressed. In their most basic form, regions are simply dense rectangular set of 5 indices similar to those used to define arrays in traditional languages.
Reference: [9] <author> Calvin Lin and Lawrence Snyder. </author> <title> SIMPLE performance results in ZPL. </title> <editor> In Keshav Pingali, Uptal Banerjee, David Gelernter, Alexandru Nicolau, and David Padua, editors, </editor> <booktitle> Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 361375. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: ZPL has sequential semantics that allow programs to be written and debugged on sequential workstations and then effortlessly recompiled for execution on parallel architectures. ZPL generally outperforms HPF and has proven to be competitive with hand-coded C and message passing <ref> [9, 7] </ref>. Applications from a variety of disciplines have been written using ZPL [6, 3, 13], and the language was released for widespread use in July 1997. Supported platforms include the Cray T3D/T3E, Intel Paragon, IBM SP-2, SGI Power Challenge, clusters of workstations using PVM or MPI, and sequential workstations.
Reference: [10] <author> Ton A. Ngo. </author> <title> The Role of Performance Models in Parallel Programming and Languages. </title> <type> PhD thesis, </type> <institution> University of Washington, Department of Computer Science and Engineering, </institution> <year> 1997. </year>
Reference-contexts: As an example, Ngo compares matrix multiply algorithms written in HPF, demonstrating that there is neither any source-level indication of how they will perform, nor does either algorithm consistently outperform the other. <ref> [10] </ref>. By defining a formal performance model to which all HPF compilers must adhere, this problem could be alleviated. <p> Performing the same experiment in HPF, Ngo demonstrated that not only is it virtually impossible to predict the performance of these algorithms by looking at the HPF source, but also that neither algorithm consistently outperforms the other across all compilers <ref> [10] </ref>. ZPL's WYSIWYG performance model makes both source-level evaluation and portable performance a reality. 15 5.3 Discussion It should be noted that, as in the sequential realm, ZPL's performance model does not yield exact information about a program's running time.
Reference: [11] <author> Ton A. Ngo, Lawrence Snyder, and Bradford L. Chamberlain. </author> <title> Portable performance of data parallel languages. </title> <note> to appear in Supercomputing 1997, </note> <month> November </month> <year> 1997. </year>
Reference-contexts: As a result, programmers must re-tune their program for each compiler and platform that they use, neutralizing any notion of portable performance. Ngo demonstrates that this lack of a performance model results in erratic execution times when compiling HPF programs using different compilers on the IBM SP-2 <ref> [11] </ref>. One of the biggest causes of ambiguity in the performance of HPF programs is the fact that communication is completely hidden from the user, making it difficult to evaluate different implementation options [4].
Reference: [12] <author> Robert W. Numrich and Jon L. Steidel. </author> <title> Simple parallel extensions to Fortran 90. </title> <booktitle> In 8th SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <month> March </month> <year> 1997. </year>
Reference-contexts: In response to HPF's hidden and underspecified communication model, F was developed to make communication explicit and highly visible to the programmer using a simple and natural syntax extension to Fortran 90 <ref> [12] </ref>. This results in a better performance model than HPF, but not without some cost. The user is forced to program at a local per-processor level, thereby forfeiting 4 some of the benefits of higher-level languages, such as sequential semantics and deterministic execution.
Reference: [13] <author> Wilkey Richardson, Mary Bailey, and William H. Sanders. </author> <title> Using ZPL to develop a parallel Chaos router simulator. </title> <booktitle> In 1996 Winter Simulation Conference, </booktitle> <month> December </month> <year> 1996. </year>
Reference-contexts: ZPL generally outperforms HPF and has proven to be competitive with hand-coded C and message passing [9, 7]. Applications from a variety of disciplines have been written using ZPL <ref> [6, 3, 13] </ref>, and the language was released for widespread use in July 1997. Supported platforms include the Cray T3D/T3E, Intel Paragon, IBM SP-2, SGI Power Challenge, clusters of workstations using PVM or MPI, and sequential workstations.
Reference: [14] <author> Lawrence Snyder. </author> <title> Experimental validation of models of parallel computation. </title> <editor> In A. Hofmann and J. van Leeuwen, editors, </editor> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> Special Volume 1000, </volume> <pages> pages 78100. </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: In addition to the standard concerns inherited from the sequential domain, parallel language performance models need to emphasize the cost of interpro-cessor data transfer, since communication can have a significant impact on the performance of parallel computations. ZPL achieves this by using the CTA parallel machine model <ref> [14] </ref> as the basis for its performance model. The CTA empahsizes data locality and accurately models contemporary parallel machines. ZPL's performance model has the additional benefit of ensuring that every operation which requires communication is visible to the programmer at the source level.
Reference: [15] <author> Lawrence Snyder. </author> <title> The ZPL Programmer's Guide. </title> <month> May </month> <year> 1996. </year>
Reference-contexts: In this section, we give a brief introduction to ZPL concepts that are required to understand this paper. A more complete presentation of the language is available in the ZPL Programmer's Guide and Reference Manual <ref> [15, 8] </ref>. 3.1 Regions and Arrays The region is ZPL's most fundamental concept. Regions are index sets through which a program's parallelism is expressed. In their most basic form, regions are simply dense rectangular set of 5 indices similar to those used to define arrays in traditional languages.
Reference: [16] <author> Robert van de Geijn and Jerrell Watts. SUMMA: </author> <title> Scalable universal matrix multiplication algorithm. </title> <type> Technical Report TR-95-13, </type> <institution> University of Texas, Austin, Texas, </institution> <month> April </month> <year> 1995. </year> <month> 17 </month>
Reference-contexts: In Figure 2, we show two algorithms for matrix-matrix multiplication, SUMMA <ref> [16] </ref> and Cannon's Algorithm [2]. SUMMA is considered to be the most scalable of portable parallel matrix multiplication algorithms. It iteratively floods a column of matrix A and a row of matrix B, accumulating their product in C.
References-found: 16

