URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P687.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts97.htm
Root-URL: http://www.mcs.anl.gov
Title: Automatic Differentiation and Navier-Stokes Computations  
Author: Paul Hovland Bijan Mohammadi Christian Bischof 
Abstract: We describe the use of automatic differentiation (AD) in, and its application to, a compressible Navier-Stokes model. Within the solver, AD is used to accelerate convergence by more than an order of magnitude. Outside the solver, AD is used to compute the derivatives needed for optimization. We emphasize the potential for performance gains if the programmer does not treat AD as a black box, but instead utilizes high-level knowledge about the nature of the application. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Albada and B. V. Leer. </author> <title> Flux vector splitting and runge-kutta methods for the euler equations. </title> <type> Technical Report 84-27, </type> <institution> ICASE, </institution> <year> 1984. </year>
Reference-contexts: The numerical solution is based on a finite volume-Galerkin method for unstructured triangular meshes. The convective part of the equations can be solved using either a Roe [17] or Osher [16] approximate Riemann solver. Second order accuracy is obtained by using a MUSCL-like extension with Van Albada type limiters <ref> [1] </ref>. Applying this approach to the k" equations will not guarantee the positivity of aek and ae". Therefore, the convective fluxes for k and " are computed using the PSI fluctuation splitting scheme [20]. The viscous term is discretized using a classical piecewise linear finite element method.
Reference: [2] <author> B. Averick, J. More, C. Bischof, A. Carle, and A. Griewank. </author> <title> Computing large sparse Jacobian matrices using automatic differentiation. </title> <journal> SIAM Journal on Scientific Computing, </journal> <volume> 15(2) </volume> <pages> 285-294, </pages> <year> 1994. </year> <title> Automatic Differentiation and Navier-Stokes Computations 21 </title>
Reference-contexts: However, there is room for improvement in the implicit method. The computation of the Jacobian can be accelerated by employing the technique of Averick et al <ref> [2] </ref> to compute a compressed Jacobian using dense derivative vectors. An additional small improvement can be achieved through the use of the interface contraction technique described in [11]. <p> SparsLinC offers considerably better performance than would a dense Jacobian computation. However, because of the overhead associated with manipulating the sparse derivative vectors, the cost is considerably more than it would be for a computation involving dense vectors of the same length. In <ref> [2] </ref>, Averick et al describe a technique for compressing the Jacobian based on a coloring algorithm that identifies structurally orthogonal columns of the Jacobian. We adapted this technique to our method.
Reference: [3] <author> Y. Y. Azmy. </author> <title> Post-convergence automatic differentiation of iterative schemes. </title> <journal> Nuclear Science and Engineering, </journal> <volume> 125 </volume> <pages> 12-18, </pages> <year> 1997. </year>
Reference-contexts: Hovland, B. Mohammadi, and C. Bischof than differentiating through the PETSc nonlinear equation solver, we can first compute the steady state solution u 1 (ff), then use the incremental iterative method described by Sherman et al [13] (See also <ref> [3] </ref>) to compute the derivative d u=d ff. 7. Conclusions AD tools can enhance Navier-Stokes calculations in a variety of ways. They can be used to generate code for computing the Jacobian matrix used in an implicit solver.
Reference: [4] <author> S. Balay, W. Gropp, L. C. McInnes, and B. Smith. </author> <note> PETSc 2.0 users manual. Technical Report ANL-95/11 Revision 2.0.17, </note> <institution> Argonne National Laboratory, </institution> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: We can solve (3) using GMRES [19] with ILU preconditioning. We used the linear equation solver provided by the PETSc toolkit for scientific computing <ref> [4] </ref>. Note that without preconditioning, the implicit solver may converge very slowly, if at all. 5.1.
Reference: [5] <author> C. Bischof, A. Carle, P. Khademi, and A. Mauer. ADIFOR 2.0: </author> <title> Automatic differentiation of Fortran 77 programs. </title> <journal> IEEE Computational Science & Engineering, </journal> <volume> 3(3) </volume> <pages> 18-32, </pages> <year> 1996. </year>
Reference-contexts: Source transformation AD tools work by converting source code for computing a function into source code for computing that function and its derivatives. For example, the ADIFOR tool <ref> [5] </ref> for AD of Fortran 77 converts the subroutine in Figure 1 into the subroutine in Figure 2. The derivative objects g_x, g_temp and g_y contain the derivatives of x, temp, and y with respect to x.
Reference: [6] <author> C. Bischof, A. Carle, P. Khademi, A. Mauer, and P. Hovland. </author> <title> ADIFOR 2.0 user's guide (Revision C). </title> <type> Technical Memorandum ANL/MCS-TM-192, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1995. </year>
Reference-contexts: Finally, the nonlinearity of the function (u) Automatic Differentiation and Navier-Stokes Computations 15 can be handled better if we solve the system of nonlinear equations directly rather than first linearizing the system. 5.1.1. Compressed Jacobian Since the Jacobian being computed is very sparse, we use the SparsLinC library <ref> [6] </ref>, which provides support for sparse derivative vectors. SparsLinC offers considerably better performance than would a dense Jacobian computation. However, because of the overhead associated with manipulating the sparse derivative vectors, the cost is considerably more than it would be for a computation involving dense vectors of the same length.
Reference: [7] <author> C. Faure. </author> <title> Splitting of algebraic expressions for automatic differentiation. </title> <editor> In M. Berz, C. Bischof, G. Corliss, and A. Griewank, editors, </editor> <title> Computational Differentiation: Techniques, </title> <booktitle> Applications, and Tools, </booktitle> <pages> pages 117-128, </pages> <address> Philadelphia, PA, </address> <year> 1996. </year> <note> SIAM. </note>
Reference-contexts: This reduces the memory requirements by a factor k (the number of time steps). This technique would not apply in the case of unsteady flows. The second optimization relies on a technique called inter-procedural differentiation <ref> [7] </ref>. This method can be viewed as a heuristic for checkpointing, 10 P. Hovland, B. Mohammadi, and C. Bischof and relies on the way in which Odyssee and other source transformation tools implement the reverse mode for subroutines.
Reference: [8] <author> A. Griewank. </author> <title> On automatic differentiation. </title> <booktitle> In Mathematical Programming: Recent Developments and Applications, </booktitle> <pages> pages 83-108, </pages> <address> Amster-dam, 1989. </address> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: Partial derivatives for these elementary functions can be obtained by table lookup, and total derivatives propagated using the chain rule. A thorough description of AD is beyond the scope of this paper. In the following paragraphs, we briefly discuss one method for implementing AD; see <ref> [8] </ref> for an explanation of the theoretical foundations of AD and [12] for a survey of other methods. Source transformation AD tools work by converting source code for computing a function into source code for computing that function and its derivatives. <p> In general, the reverse mode has much greater memory requirements than the forward mode, but can offer better performance when the number of scalar independent variables is significantly greater than the number of scalar dependent variables. For our example, this occurs when m &lt;< n. See <ref> [8, 9] </ref> for more details. 4. <p> The reverse mode of AD is at one extreme, achieving a time complexity bounded by a constant multiple of that for the function evaluation, at the cost of a potentially exponential increase in storage complexity <ref> [8] </ref>. Griewank has demonstrated that it is possible to achieve logarithmic growth in time and space through checkpointing and recomputation of intermediate values [9]. For a large problem that takes many time iterations to reach a steady state solution, the storage requirements for the basic reverse mode can be enormous.
Reference: [9] <author> A. Griewank. </author> <title> Achieving logarithmic growth of temporal and spatial complexity in reverse automatic differentiation. </title> <journal> Optimization Methods and Software, </journal> <volume> 1(1) </volume> <pages> 35-54, </pages> <year> 1992. </year>
Reference-contexts: In general, the reverse mode has much greater memory requirements than the forward mode, but can offer better performance when the number of scalar independent variables is significantly greater than the number of scalar dependent variables. For our example, this occurs when m &lt;< n. See <ref> [8, 9] </ref> for more details. 4. <p> Griewank has demonstrated that it is possible to achieve logarithmic growth in time and space through checkpointing and recomputation of intermediate values <ref> [9] </ref>. For a large problem that takes many time iterations to reach a steady state solution, the storage requirements for the basic reverse mode can be enormous.
Reference: [10] <author> A. Griewank, C. Bischof, G. Corliss, A. Carle, and K. Williamson. </author> <title> Derivative convergence of iterative equation solvers. </title> <journal> Optimization Methods and Software, </journal> <volume> 2 </volume> <pages> 321-355, </pages> <year> 1993. </year>
Reference-contexts: The first optimization takes advantage of the fact that our cost function depends only on the flow at the steady state. If our convergence criterion is sufficiently strict <ref> [10, 15] </ref>, it is sufficient to only differentiate through one time step, if we start with an initial state corresponding to the steady state for a given shape. This reduces the memory requirements by a factor k (the number of time steps).
Reference: [11] <author> P. Hovland, C. Bischof, D. Spiegelman, and M. Casella. </author> <title> Efficient derivative codes through automatic differentiation and interface contraction: An application in biostatistics. </title> <journal> SIAM Journal on Scientific Computing, </journal> <volume> 18(4) </volume> <pages> 1056-1066, </pages> <month> July </month> <year> 1997. </year>
Reference-contexts: The computation of the Jacobian can be accelerated by employing the technique of Averick et al [2] to compute a compressed Jacobian using dense derivative vectors. An additional small improvement can be achieved through the use of the interface contraction technique described in <ref> [11] </ref>. Finally, the nonlinearity of the function (u) Automatic Differentiation and Navier-Stokes Computations 15 can be handled better if we solve the system of nonlinear equations directly rather than first linearizing the system. 5.1.1. <p> Because the mesh does not change, subsequent iterations have the same sparsity pattern. By coloring the Jacobian matrix from the first iteration, we are able to compute a compressed Jacobian for subsequent iterations. 5.1.2. Interface Contraction In <ref> [11] </ref>, Hovland et al describe a heuristic for reducing the cost of computing derivatives based on the observation that the number of parameters passed to a subroutine is usually quite small.
Reference: [12] <author> D. Juedes. </author> <title> A taxonomy of automatic differentiation tools. </title> <editor> In A. Griewank and G. Corliss, editors, </editor> <booktitle> Proceedings of the Workshop on Automatic Differentiation of Algorithms: Theory, Implementation, and Application, </booktitle> <pages> pages 315-330, </pages> <address> Philadelphia, PA, </address> <year> 1991. </year> <note> SIAM. </note>
Reference-contexts: A thorough description of AD is beyond the scope of this paper. In the following paragraphs, we briefly discuss one method for implementing AD; see [8] for an explanation of the theoretical foundations of AD and <ref> [12] </ref> for a survey of other methods. Source transformation AD tools work by converting source code for computing a function into source code for computing that function and its derivatives.
Reference: [13] <author> V. Korivi, L. Sherman, A. Taylor, G. Hou, L. Green, and P. New-man. </author> <title> First- and second-order aerodynamic sensitivity derivatives via 22 P. Hovland, </title> <editor> B. Mohammadi, and C. </editor> <title> Bischof automatic differentiation with incremental iterative methods. </title> <booktitle> In Proceedings of the 5th AIAA/NASA/USAF/ISSMO Symposium on Multidisciplinary Analysis and Optimization, </booktitle> <pages> AIAA 94-4262, pages 87-120. </pages> <institution> American Institute of Aeronautics and Astronautics, </institution> <year> 1994. </year>
Reference-contexts: Hovland, B. Mohammadi, and C. Bischof than differentiating through the PETSc nonlinear equation solver, we can first compute the steady state solution u 1 (ff), then use the incremental iterative method described by Sherman et al <ref> [13] </ref> (See also [3]) to compute the derivative d u=d ff. 7. Conclusions AD tools can enhance Navier-Stokes calculations in a variety of ways. They can be used to generate code for computing the Jacobian matrix used in an implicit solver.
Reference: [14] <author> B. Mohammadi. </author> <title> CFD with NSC2KE: an user guide. </title> <type> Technical Report 164, </type> <institution> INRIA, </institution> <year> 1994. </year>
Reference-contexts: We close the system using the equation of state p = p (ae; T ). Note that this model does not take into account turbulent contributions to the pressure and total energy. In this paper, we consider the numerical solution of this model using the NSC2KE program <ref> [14] </ref>. The numerical solution is based on a finite volume-Galerkin method for unstructured triangular meshes. The convective part of the equations can be solved using either a Roe [17] or Osher [16] approximate Riemann solver. <p> Therefore, the convective fluxes for k and " are computed using the PSI fluctuation splitting scheme [20]. The viscous term is discretized using a classical piecewise linear finite element method. Time discretization is based on a four-stage Runge-Kutta scheme. See <ref> [14, 15] </ref> for more details. 3. Automatic Differentiation via Source Transformation Automatic differentiation (AD) works by exploiting the fact that all algorithms compute functions through the composition of elementary functions provided by the programming language.
Reference: [15] <author> B. Mohammadi. </author> <title> A new optimal shape design procedure for inviscid and viscous turbulent flows. International Journal for Numerical Methods in Fluids, </title> <booktitle> 25 </booktitle> <pages> 183-203, </pages> <year> 1997. </year>
Reference-contexts: Therefore, the convective fluxes for k and " are computed using the PSI fluctuation splitting scheme [20]. The viscous term is discretized using a classical piecewise linear finite element method. Time discretization is based on a four-stage Runge-Kutta scheme. See <ref> [14, 15] </ref> for more details. 3. Automatic Differentiation via Source Transformation Automatic differentiation (AD) works by exploiting the fact that all algorithms compute functions through the composition of elementary functions provided by the programming language. <p> The first optimization takes advantage of the fact that our cost function depends only on the flow at the steady state. If our convergence criterion is sufficiently strict <ref> [10, 15] </ref>, it is sufficient to only differentiate through one time step, if we start with an initial state corresponding to the steady state for a given shape. This reduces the memory requirements by a factor k (the number of time steps).
Reference: [16] <author> S. Osher and F. Solomon. </author> <title> Upwind difference schemes for the hyperbolic systems of conservation laws. </title> <journal> Mathematics of Computation, </journal> <volume> 38(158) </volume> <pages> 339-374, </pages> <year> 1982. </year>
Reference-contexts: In this paper, we consider the numerical solution of this model using the NSC2KE program [14]. The numerical solution is based on a finite volume-Galerkin method for unstructured triangular meshes. The convective part of the equations can be solved using either a Roe [17] or Osher <ref> [16] </ref> approximate Riemann solver. Second order accuracy is obtained by using a MUSCL-like extension with Van Albada type limiters [1]. Applying this approach to the k" equations will not guarantee the positivity of aek and ae".
Reference: [17] <author> P.L.Roe. </author> <title> Approximate Riemann solvers, parameters vectors and difference schemes,. </title> <journal> Journal of Computational Physics, </journal> <volume> 43, </volume> <year> 1981. </year>
Reference-contexts: In this paper, we consider the numerical solution of this model using the NSC2KE program [14]. The numerical solution is based on a finite volume-Galerkin method for unstructured triangular meshes. The convective part of the equations can be solved using either a Roe <ref> [17] </ref> or Osher [16] approximate Riemann solver. Second order accuracy is obtained by using a MUSCL-like extension with Van Albada type limiters [1]. Applying this approach to the k" equations will not guarantee the positivity of aek and ae".
Reference: [18] <author> N. Rostaing, S. Dalmas, and A. Galligo. </author> <title> Automatic differentiation in Odyssee. </title> <address> Tellus, 45a(5):558-568, </address> <month> October </month> <year> 1993. </year>
Reference-contexts: The derivative objects g_x, g_temp and g_y contain the derivatives of x, temp, and y with respect to x. Typically, on entry g_x will contain the n fi n identity matrix and on exit g_y will contain the Jacobian matrix d y=d x. Similarly, the Odyssee tool <ref> [18] </ref> converts the same subroutine into the one in Figure 3. Variables yccl and xccl contain the derivatives of y with respect to y and x. The save* variables are used to save intermediate values. <p> We could also implement the adjoint code by hand, but this can be extremely tedious and hence error-prone. Instead, we use the reverse mode of automatic differentiation to do this. For our problem, we used the Odyssee automatic differentiation tool <ref> [18] </ref>. 4.2. Adjusting the Shape Once we have computed dJ dx w , we use steepest descent to compute a perturbation to x w , ffix w . To avoid oscillations, a "local" smoothing operator is defined over the shape.
Reference: [19] <author> Y. Saad and M. H. Schultz. </author> <title> GMRES: a generalized minimal residual algorithm for solving nonsymmetric linear systems. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 7 </volume> <pages> 856-869, </pages> <year> 1986. </year>
Reference-contexts: Then, given iterate u n and a timestep t, we can find u n+1 by solving the linear system 1 I + u (u n ) u = (u n ) (3) and letting u n+1 = u n + u. We can solve (3) using GMRES <ref> [19] </ref> with ILU preconditioning. We used the linear equation solver provided by the PETSc toolkit for scientific computing [4]. Note that without preconditioning, the implicit solver may converge very slowly, if at all. 5.1.
Reference: [20] <author> R. Struijs, H. Deconinck, P. de Palma, P. Roe, and G.G.Powel. </author> <title> Progress on multidimensional upwind euler solvers for unstructured grids. </title> <type> AIAA paper 91-1550, </type> <year> 1991. </year>
Reference-contexts: Second order accuracy is obtained by using a MUSCL-like extension with Van Albada type limiters [1]. Applying this approach to the k" equations will not guarantee the positivity of aek and ae". Therefore, the convective fluxes for k and " are computed using the PSI fluctuation splitting scheme <ref> [20] </ref>. The viscous term is discretized using a classical piecewise linear finite element method. Time discretization is based on a four-stage Runge-Kutta scheme. See [14, 15] for more details. 3.
References-found: 20

