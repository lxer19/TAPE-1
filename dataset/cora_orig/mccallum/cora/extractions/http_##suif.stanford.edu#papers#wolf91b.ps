URL: http://suif.stanford.edu/papers/wolf91b.ps
Refering-URL: http://suif.stanford.edu/papers/papers.html
Root-URL: 
Title: A Loop Transformation Theory and an Algorithm to Maximize Parallelismyz  
Author: Michael E. Wolf and Monica S. Lam 
Address: CA 94305  
Affiliation: Computer Systems Laboratory Stanford University,  
Abstract: This paper proposes a new approach to transformations for general loop nests. In this approach, we unify all combinations of loop interchange, skewing and reversal as unimodular transformations. The use of matrices to model transformations has previously been applied only to those loop nests whose dependences can be summarized by distance vectors. Our technique is applicable to general loop nests where the dependences include both distances and directions. This theory provides the foundation for solving an open question in compilation for parallel machines: Which loop transformations, and in what order, should be applied to achieve a particular goal, such as maximizing parallelism or data locality. This paper presents an efficient loop transformation algorithm based on this theory to maximize the degree of parallelism in a loop nest.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Allen and K. Kennedy. </author> <title> Automatic translation of FORTRAN programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <year> 1987. </year>
Reference-contexts: 1 Introduction Loop transformations, such as interchange, reversal, skewing and tiling (or blocking) <ref> [1, 4, 26] </ref> have been shown to be useful for two important goals: parallelism and efficient use of the memory hierarchy. <p> We use the notation ` + ' as shorthand for <ref> [1; 1] </ref>, ` ' as shorthand for [1; 1], and `' as shorthand for [1; 1]. They correspond to the directions ` &lt; ', ` &gt; ', and `fl' respectively. All the properties discussed in Sections 2 and 3 hold for the distance vector sets of the dependence vectors. <p> We use the notation ` + ' as shorthand for <ref> [1; 1] </ref>, ` ' as shorthand for [1; 1], and `' as shorthand for [1; 1]. They correspond to the directions ` &lt; ', ` &gt; ', and `fl' respectively. All the properties discussed in Sections 2 and 3 hold for the distance vector sets of the dependence vectors. <p> We use the notation ` + ' as shorthand for <ref> [1; 1] </ref>, ` ' as shorthand for [1; 1], and `' as shorthand for [1; 1]. They correspond to the directions ` &lt; ', ` &gt; ', and `fl' respectively. All the properties discussed in Sections 2 and 3 hold for the distance vector sets of the dependence vectors. <p> Thus 2 + <ref> [3; 1] </ref> = [2; 2] + [3; 1] = [1; 1]. <p> Thus 2 + <ref> [3; 1] </ref> = [2; 2] + [3; 1] = [1; 1]. <p> Thus 2 + [3; 1] = [2; 2] + [3; 1] = <ref> [1; 1] </ref>. <p> we need a floor it is a simple matter to adjust l j j i;0 and replace the ceiling with the floor, and likewise if a floor occurs where we need a ceiling.) If any loop increments is not one, then it must first be made so via loop normalization <ref> [1] </ref>. If the bounds are not of the proper form, then the given loop cannot be involved in any transformations, and the loop nest is effectively divided into two: those outside the loop and those nested in the loop.
Reference: [2] <author> U. Banerjee. </author> <title> Data dependence in ordinary programs. </title> <type> Technical Report 76-837, </type> <institution> University of Illinios at Urbana-Champaign, </institution> <month> Nov </month> <year> 1976. </year>
Reference-contexts: Distance vectors cannot represent the dependences of general loop nests, where two or more loops must execute sequentially. A commonly used notation for representing general dependences is direction vectors <ref> [2, 26] </ref>. This research combines the advantages of both approaches. We combine the mathematical rigor in the matrix transformation model with the generality of the vectorizing and concurrentizing compiler approach. <p> but the dependences cannot be represented as distance vectors: for I 1 := 0 to N do a [I 1 ,I 2 ] := a [I 1 + 1,b [I 2 ]]; To represent this type of information, previous research on vectorizing and parallelizing compilers introduced the concept of directions <ref> [2, 26] </ref>. The dependence vector for the first example above would have been (` fl '; ` fl '), indicating that all the iterations are using the same data b, and must be serialized. <p> Thus 2 + [3; 1] = <ref> [2; 2] </ref> + [3; 1] = [1; 1].
Reference: [3] <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer Academic, </publisher> <year> 1988. </year>
Reference-contexts: For example, the transformation from Figure 1 (a) to Figure 1 (b) is a skew of the inner loop with respect to the outer loop by a factor of one, which can be represented as 1 1 . All these elementary transformation matrices are unimodular matrices <ref> [3] </ref>. A unimodular matrix has three important properties. First, it is square, meaning that it maps an n-dimensional iteration space to an n-dimensional iteration space. Second, it has all integral components, so it maps integer vectors to integer vectors. Third, the absolute value of its determinant is one. <p> Thus 2 + <ref> [3; 1] </ref> = [2; 2] + [3; 1] = [1; 1]. <p> Thus 2 + <ref> [3; 1] </ref> = [2; 2] + [3; 1] = [1; 1].
Reference: [4] <author> U. Banerjee. </author> <title> A theory of loop permutations. </title> <booktitle> In 2nd Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> Aug </month> <year> 1989. </year>
Reference-contexts: 1 Introduction Loop transformations, such as interchange, reversal, skewing and tiling (or blocking) <ref> [1, 4, 26] </ref> have been shown to be useful for two important goals: parallelism and efficient use of the memory hierarchy.
Reference: [5] <author> U. Banerjee. </author> <title> Unimodular transformations of double loops. </title> <booktitle> In 3rd Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> Aug </month> <year> 1990. </year>
Reference-contexts: Since t 1 and t 2 are relatively prime, an x and y pair can be easily found using the 2 Banerjee discusses a method for choosing the direction that maximizes the number of iterations that can be performed in parallel <ref> [5] </ref>. 22 Extended Euclidean algorithm.
Reference: [6] <author> R. Cytron. </author> <title> Compile-time Scheduling and Optimization for Asynchronous Machines. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1984. </year>
Reference-contexts: In this way, the synchronization cost is reduced by the size of the tile. An example of a wavefront of tiles is highlighted in Figure 5. II 2 To further reduce the synchronization cost, we can apply the concept of a DOACROSS loop <ref> [6] </ref> to the tile level 12 [25]. After tiling, instead of skewing the loops statically to form DOALL loops, the computation is allowed to skew dynamically by explicit synchronization between data dependent tiles.
Reference: [7] <author> J.-M. Delosme and I. C. F. Ipsen. </author> <title> Efficient systolic arrays for the solution of Toeplitz Systems: an illustration of a methodology for the construction of systolic architectures in VLSI. </title> <type> Technical Report 370, </type> <institution> Yale University, </institution> <year> 1985. </year>
Reference-contexts: They have the characteristic that their dependences can be represented by a set of integer vectors, known as distance vectors. Loop interchange, reversal, and skewing transformations are modeled as unimodular transformations in the iteration space <ref> [7, 8, 12, 13, 19, 21] </ref>. A compound transformation is just another linear transformation, being a product of several elementary transformations. This model makes it possible to determine the compound transformation directly in maximizing some objective function.
Reference: [8] <author> J. A. B. Fortes and D. I. Moldovan. </author> <title> Parallelism detection and transformation techniques useful for VLSI algorithms. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 2 </volume> <pages> 277-301, </pages> <year> 1985. </year>
Reference-contexts: They have the characteristic that their dependences can be represented by a set of integer vectors, known as distance vectors. Loop interchange, reversal, and skewing transformations are modeled as unimodular transformations in the iteration space <ref> [7, 8, 12, 13, 19, 21] </ref>. A compound transformation is just another linear transformation, being a product of several elementary transformations. This model makes it possible to determine the compound transformation directly in maximizing some objective function.
Reference: [9] <author> K. Gallivan, W. Jalby, U. Meier, and A. Sameh. </author> <title> The impact of hierarchical memory systems on linear algebra algorithm design. </title> <type> Technical report, </type> <institution> University of Illinios, </institution> <year> 1987. </year>
Reference-contexts: As the processor speed improves and the gap between processor and memory speeds widens, data locality becomes more important. Even with very simple machine models (for example, uniprocessors with data caches), complex loop transformations may be necessary <ref> [9, 10, 17] </ref>. The consideration of data locality makes it more important to be able to combine primitive loop transformations in a systematic manner.
Reference: [10] <author> D. Gannon, W. Jalby, and K. Gallivan. </author> <title> Strategies for cache and local memory management by global program transformation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5 </volume> <pages> 587-616, </pages> <year> 1988. </year>
Reference-contexts: As the processor speed improves and the gap between processor and memory speeds widens, data locality becomes more important. Even with very simple machine models (for example, uniprocessors with data caches), complex loop transformations may be necessary <ref> [9, 10, 17] </ref>. The consideration of data locality makes it more important to be able to combine primitive loop transformations in a systematic manner. <p> First, within each tile, fine-grain parallelism can easily be obtained by skewing the loops within the tile and moving the DOALL loop innermost. In this way, we can obtain both coarse- and fine-grain parallelism. Second, tiling can improve data locality if there is data reuse across several loops <ref> [10, 24] </ref>. 3.3 Summary Using the loop transformation theory, we have shown a simple algorithm in exploiting both coarse- and fine-grain parallelism for loops with distance vectors. The algorithm consists of two steps: the first is to transform the code into the canonical form of a fully permutable loop nest.
Reference: [11] <author> F. Irigoin. </author> <title> Partitionnement Des Boucles Imbeiquees: Une Technique D'optimisation four les Programmes Scientifiques. </title> <type> PhD thesis, </type> <institution> Universite Paris-VI, </institution> <month> June </month> <year> 1987. </year>
Reference-contexts: Neither of these problems occur in the body of the innermost loop, and thus the extra computation cost should be negligible. These problems can be removed via well known integer linear system algorithms <ref> [11] </ref>. 7.2 Tiling Transformation The technique we apply for tiling does not require any changes to the loop body. We thus discuss only the changes to the loop nest and bounds. <p> The time wasted in determining that the tile is empty should be negligible when compared to the execution of the large number of non-empty tiles in the loop. Once again, these problems can be removed via well known integer linear system algorithms <ref> [11] </ref>.
Reference: [12] <author> F. Irigoin and R. Triolet. </author> <title> Computing dependence direction vectors and dependence cones. </title> <type> Technical Report E94, </type> <institution> Centre D'Automatique et Informatique, </institution> <year> 1988. </year> <month> 29 </month>
Reference-contexts: They have the characteristic that their dependences can be represented by a set of integer vectors, known as distance vectors. Loop interchange, reversal, and skewing transformations are modeled as unimodular transformations in the iteration space <ref> [7, 8, 12, 13, 19, 21] </ref>. A compound transformation is just another linear transformation, being a product of several elementary transformations. This model makes it possible to determine the compound transformation directly in maximizing some objective function. <p> The discussion in this section is limited to distance vectors, that is, d i 2 Z. Techniques for extracting distance vectors are discussed in <ref> [12, 16, 22] </ref>. General loop dependences may not be representable with a finite set of distance vectors; extensions to include directions are necessary and will be discussed in Section 4. Each dependence vector defines a set of edges on pairs of nodes in the iteration space. <p> A related but simpler problem, referred to here as the time cone problem, is to find a linear transformation matrix for a finite set of distance vectors such that the first components of all transformed distance vectors are all positive <ref> [12, 13, 18, 21] </ref>. This means that the rest of the loops are DOALL loops. We have shown that if all the distance vectors are lexicographically positive, this can be achieved by first skewing to make the loops fully permutable, and wavefronting to generate the DOALL loops.
Reference: [13] <author> F. Irigoin and R. Triolet. </author> <title> Supernode partitioning. </title> <booktitle> In Proc. 15th Annual ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages, </booktitle> <pages> pages 319-329, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: They have the characteristic that their dependences can be represented by a set of integer vectors, known as distance vectors. Loop interchange, reversal, and skewing transformations are modeled as unimodular transformations in the iteration space <ref> [7, 8, 12, 13, 19, 21] </ref>. A compound transformation is just another linear transformation, being a product of several elementary transformations. This model makes it possible to determine the compound transformation directly in maximizing some objective function. <p> This model makes it possible to determine the compound transformation directly in maximizing some objective function. Loop nests whose dependences can be represented as distance vectors have the property that an n-deep loop nest has at least n 1 degrees of parallelism <ref> [13] </ref>, and can exploit data locality in all possible loop dimensions [24]. Distance vectors cannot represent the dependences of general loop nests, where two or more loops must execute sequentially. A commonly used notation for representing general dependences is direction vectors [2, 26]. <p> We are interested in running the code on machines supporting fine-grain parallelism, machines supporting coarse-grain parallelism and also machines supporting both levels of parallelism. We will show that n-deep loops whose dependences can be represented with distance vectors, have at least n 1 degrees of parallelism <ref> [13] </ref>, exploitable at both fine and coarse granularity. The algorithm consists of two steps: it first transforms the original loop nest into a canonical form, namely a fully permutable loop nest. <p> We then show how to obtain the same degree of parallelism a lower synchronization cost, and how to produce both fine- and coarse-grain parallelism. 3.2.1 Fine Grain Parallelism A nest of n fully permutable loops can be transformed to code containing at least n 1 degrees of parallelism <ref> [13] </ref>. In the degenerate case when no dependences are carried by these n loops, the degree of parallelism is n. <p> A related but simpler problem, referred to here as the time cone problem, is to find a linear transformation matrix for a finite set of distance vectors such that the first components of all transformed distance vectors are all positive <ref> [12, 13, 18, 21] </ref>. This means that the rest of the loops are DOALL loops. We have shown that if all the distance vectors are lexicographically positive, this can be achieved by first skewing to make the loops fully permutable, and wavefronting to generate the DOALL loops.
Reference: [14] <author> F. Irigoin and R. Triolet. </author> <title> Dependence approximation and global parallel code generation for nested loops. </title> <booktitle> In Parallel and Distributed Algorithms, </booktitle> <year> 1989. </year>
Reference-contexts: In this example, the dimensionality of the iteration space is two, but the dimensionality of the space spanned by the dependence vectors is only one. When the dependence vectors do not span the entire iteration space, it is possible to perform a transformation that makes outermost DOALL loops <ref> [14] </ref>. By choosing the transformation matrix T with first row ~ t 1 such that ~ t 1 ~ d = 0 for all dependence vectors ~ d , the transformation produces an outermost DOALL.
Reference: [15] <author> M. S. Lam. </author> <title> Software pipelining: An effective scheduling technique for VLIW machines. </title> <booktitle> In Proc. ACM SIGPLAN 88 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 318-328, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Coalescing multiple DOALL loops prevents the pitfall of parallelizing only a loop with a small iteration count. It can reduce further the overhead of starting and finishing a parallel loop if code scheduling techniques such as software pipelining <ref> [15] </ref> are used. 9 for I 0 doall I 0 2 := max (0,d (I 0 1 =2c) do 1 2I 0 1 2I 0 1 2I 0 + a [I 0 2 + 2]); 1 0 1 0 1 0 D 0 = T D = f (1; 0); (2;
Reference: [16] <author> D. E. Maydan, J. L. Hennessy, and M. S. Lam. </author> <title> Efficient and exact data dependence analysis. </title> <booktitle> In Proc. ACM SIGPLAN 91 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1991. </year>
Reference-contexts: The discussion in this section is limited to distance vectors, that is, d i 2 Z. Techniques for extracting distance vectors are discussed in <ref> [12, 16, 22] </ref>. General loop dependences may not be representable with a finite set of distance vectors; extensions to include directions are necessary and will be discussed in Section 4. Each dependence vector defines a set of edges on pairs of nodes in the iteration space.
Reference: [17] <author> A. Porterfield. </author> <title> Software Methods for Improvement of Cache Performance on Supercomputer Applications. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: As the processor speed improves and the gap between processor and memory speeds widens, data locality becomes more important. Even with very simple machine models (for example, uniprocessors with data caches), complex loop transformations may be necessary <ref> [9, 10, 17] </ref>. The consideration of data locality makes it more important to be able to combine primitive loop transformations in a systematic manner.
Reference: [18] <author> P. </author> <title> Quinton. The systematic design of systolic arrays. </title> <type> Technical Report 193, </type> <institution> Centre National de la Recherche Scientifique, </institution> <year> 1983. </year>
Reference-contexts: A related but simpler problem, referred to here as the time cone problem, is to find a linear transformation matrix for a finite set of distance vectors such that the first components of all transformed distance vectors are all positive <ref> [12, 13, 18, 21] </ref>. This means that the rest of the loops are DOALL loops. We have shown that if all the distance vectors are lexicographically positive, this can be achieved by first skewing to make the loops fully permutable, and wavefronting to generate the DOALL loops.
Reference: [19] <author> P. </author> <title> Quinton. Automatic synthesis of systolic arrays from uniform recurrent equations. </title> <booktitle> In Proc. 11th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1984. </year>
Reference-contexts: They have the characteristic that their dependences can be represented by a set of integer vectors, known as distance vectors. Loop interchange, reversal, and skewing transformations are modeled as unimodular transformations in the iteration space <ref> [7, 8, 12, 13, 19, 21] </ref>. A compound transformation is just another linear transformation, being a product of several elementary transformations. This model makes it possible to determine the compound transformation directly in maximizing some objective function.
Reference: [20] <author> H. B. Ribas. </author> <title> Automatic Generation of Systolic Programs From Nested Loops. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <month> June </month> <year> 1990. </year>
Reference-contexts: The approach has been adopted by numerous researchers particularly in the context of automatic systolic algorithm generation, a discussion of which can be found in Ribas' dissertation <ref> [20] </ref>. 2.1 Loop Nest Representation In this model, a loop nest of depth n is represented as a finite convex polyhedron in the iteration space Z n bounded by the loop bounds.
Reference: [21] <author> R. Schreiber and J. Dongarra. </author> <title> Automatic blocking of nested loops. </title> <year> 1990. </year>
Reference-contexts: They have the characteristic that their dependences can be represented by a set of integer vectors, known as distance vectors. Loop interchange, reversal, and skewing transformations are modeled as unimodular transformations in the iteration space <ref> [7, 8, 12, 13, 19, 21] </ref>. A compound transformation is just another linear transformation, being a product of several elementary transformations. This model makes it possible to determine the compound transformation directly in maximizing some objective function. <p> A related but simpler problem, referred to here as the time cone problem, is to find a linear transformation matrix for a finite set of distance vectors such that the first components of all transformed distance vectors are all positive <ref> [12, 13, 18, 21] </ref>. This means that the rest of the loops are DOALL loops. We have shown that if all the distance vectors are lexicographically positive, this can be achieved by first skewing to make the loops fully permutable, and wavefronting to generate the DOALL loops. <p> However, if the distances are not lexicographically positive, the complexity of typical methods to find such a transformation assuming one exists is at least O (d n1 ), where d is the number of dependences and n is the loop nest depth <ref> [21] </ref>. The problem of finding the largest fully permutable loop nest is even harder, since we need to find the largest nest for which such a transformation exists.
Reference: [22] <author> C.-W. Tseng and M. J. Wolfe. </author> <title> The power test for data dependence. </title> <institution> Technical Report Rice COMP TR90-145, Rice University, </institution> <month> Dec </month> <year> 1990. </year>
Reference-contexts: The discussion in this section is limited to distance vectors, that is, d i 2 Z. Techniques for extracting distance vectors are discussed in <ref> [12, 16, 22] </ref>. General loop dependences may not be representable with a finite set of distance vectors; extensions to include directions are necessary and will be discussed in Section 4. Each dependence vector defines a set of edges on pairs of nodes in the iteration space.
Reference: [23] <author> M. E. Wolf. </author> <title> Improving Parallelism and Data Locality in Nested Loops. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1991. </year> <note> In preparation. </note>
Reference-contexts: It returns the transformation performed T , the new dependences D, and the loops that SRP failed to place. The SRP transformation has several important properties, proofs of which can be found in <ref> [23] </ref>. First, although the entire transformation is constructed as a series of a permutation, reversal and skew combination, the complete transformation can be expressed as the application of one permutation transformation, followed by a reversal of zero or more of the loops, followed by a skew.
Reference: [24] <author> M. E. Wolf and M. S. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Proc. ACM SIGPLAN 91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 30-44, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Loop nests whose dependences can be represented as distance vectors have the property that an n-deep loop nest has at least n 1 degrees of parallelism [13], and can exploit data locality in all possible loop dimensions <ref> [24] </ref>. Distance vectors cannot represent the dependences of general loop nests, where two or more loops must execute sequentially. A commonly used notation for representing general dependences is direction vectors [2, 26]. This research combines the advantages of both approaches. <p> Using the same theoretical framework presented in this paper, we have developed a locality optimization that applies compound unimodular loop transforms and tiling to use the memory hierarchy efficiently <ref> [24] </ref>. This paper introduces our model of loop dependences and transformations. We describe how the model facilitates the application of compound transformation, using parallelism as our target. The model is important in that it enables the choice of an optimal transformation without an exhaustive search. <p> All the components of the dependences are non-negative. This implies that any arbitrary loop permutation would render the transformed dependences lexicographically positive and is thus legal. We say such loop nests are fully permutable. Full permutability is an important property both for parallelization, discussed below, and for locality optimizations <ref> [24] </ref>. 7 Theorem 2.2 Loops I i through I j of a legal computation with dependence vectors D are fully permutable if and only if 8 ~ d 2 D : (d 1 ; . . . ; d i1 ) ~ 0 or (8i k j : d k 0) <p> These controlling loops are themselves fully permutable and so easily parallelizable. However, each iteration of the outer n loops is a tile of iterations of an individual iteration. Tiling can therefore increase the granularity of synchronization [25] and data is often reused within a tile <ref> [24] </ref>. Without tiling, when a DOALL loop is nested within a non-DOALL loop, all processors must be synchronized at the end of each DOALL loop with a barrier. Using tiling, we can reduce the synchronization cost in the two ways described below. <p> First, within each tile, fine-grain parallelism can easily be obtained by skewing the loops within the tile and moving the DOALL loop innermost. In this way, we can obtain both coarse- and fine-grain parallelism. Second, tiling can improve data locality if there is data reuse across several loops <ref> [10, 24] </ref>. 3.3 Summary Using the loop transformation theory, we have shown a simple algorithm in exploiting both coarse- and fine-grain parallelism for loops with distance vectors. The algorithm consists of two steps: the first is to transform the code into the canonical form of a fully permutable loop nest. <p> Besides applying the theory to maximizing parallelism, we have successfully used this model in developing an efficient algorithm that applies unimodular and tiling transforms to improve the data locality of loop nests <ref> [24] </ref>. The elegance of this theory helps reduce the complexity of the implementation. Once the dependences are extracted, the derivation of the compound transform simply consists of matrix and vector operations.
Reference: [25] <author> M. J. Wolfe. </author> <title> More iteration space tiling. </title> <booktitle> In Supercomputing '89, </booktitle> <month> Nov </month> <year> 1989. </year>
Reference-contexts: Those loop can be made outermost DOALLs. The remaining loops in the tile can be wavefronted to get the remaining parallelism. 3.2.3 Tiling to Reduce Synchronization It is possible to reduce the synchronization cost and improve the data locality of parallelized loops via an optimization known as tiling <ref> [25, 26] </ref>. Tiling is not a unimodular transformation. In general, tiling maps an n-deep loop nest into a 2n-deep loop nest where the inner n loops include only a small fixed number of iterations. <p> These controlling loops are themselves fully permutable and so easily parallelizable. However, each iteration of the outer n loops is a tile of iterations of an individual iteration. Tiling can therefore increase the granularity of synchronization <ref> [25] </ref> and data is often reused within a tile [24]. Without tiling, when a DOALL loop is nested within a non-DOALL loop, all processors must be synchronized at the end of each DOALL loop with a barrier. <p> In this way, the synchronization cost is reduced by the size of the tile. An example of a wavefront of tiles is highlighted in Figure 5. II 2 To further reduce the synchronization cost, we can apply the concept of a DOACROSS loop [6] to the tile level 12 <ref> [25] </ref>. After tiling, instead of skewing the loops statically to form DOALL loops, the computation is allowed to skew dynamically by explicit synchronization between data dependent tiles.
Reference: [26] <author> M. J. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> MIT Press, </publisher> <year> 1989. </year> <month> 30 </month>
Reference-contexts: 1 Introduction Loop transformations, such as interchange, reversal, skewing and tiling (or blocking) <ref> [1, 4, 26] </ref> have been shown to be useful for two important goals: parallelism and efficient use of the memory hierarchy. <p> Distance vectors cannot represent the dependences of general loop nests, where two or more loops must execute sequentially. A commonly used notation for representing general dependences is direction vectors <ref> [2, 26] </ref>. This research combines the advantages of both approaches. We combine the mathematical rigor in the matrix transformation model with the generality of the vectorizing and concurrentizing compiler approach. <p> For example, the matrix representing loop reversal of the outermost loop of a two-deep loop nest is 0 1 . * Skewing: Skewing loop I j by an integer factor f with respect to loop I i <ref> [26] </ref> maps iteration (p 1 ; . . . ; p i1 ; p i ; p i+1 ; . . . ; p j1 ; p j ; p j+1 ; . . . ; p n ) (p 1 ; . . . ; p i1 ; p i <p> Those loop can be made outermost DOALLs. The remaining loops in the tile can be wavefronted to get the remaining parallelism. 3.2.3 Tiling to Reduce Synchronization It is possible to reduce the synchronization cost and improve the data locality of parallelized loops via an optimization known as tiling <ref> [25, 26] </ref>. Tiling is not a unimodular transformation. In general, tiling maps an n-deep loop nest into a 2n-deep loop nest where the inner n loops include only a small fixed number of iterations. <p> but the dependences cannot be represented as distance vectors: for I 1 := 0 to N do a [I 1 ,I 2 ] := a [I 1 + 1,b [I 2 ]]; To represent this type of information, previous research on vectorizing and parallelizing compilers introduced the concept of directions <ref> [2, 26] </ref>. The dependence vector for the first example above would have been (` fl '; ` fl '), indicating that all the iterations are using the same data b, and must be serialized. <p> We thus discuss only the changes to the loop nest and bounds. While it has been suggested that strip-mining and interchanging be applied to determine the bounds of a tiled loop, this approach is not straightforward when the iteration space is not rectangular <ref> [26] </ref>. A more direct method is as follows. When tiling, we partition the iteration space, whatever the shape of the bounds, as in Figure 8. Each rectangle represents a computation performed by a tile; some tiles may contain little or even no work.
References-found: 26

