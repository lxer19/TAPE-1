URL: ftp://ftp.cc.gatech.edu/pub/coc/tech_reports/1994/GIT-CC-94-37.ps.Z
Refering-URL: http://www.cs.gatech.edu/tech_reports/index.94.html
Root-URL: 
Email: (kaushik@cc.gatech.edu)  
Title: Fast Message Passing via the ALLCACHE Memory on KSR Computers  
Author: Kaushik Ghosh 
Note: This work was performed while Kaushik Ghosh was on an internship at Kendall Square Research  '93), and was supervised by Dr. Stephen R. Breit. Dr. Breit also provided useful comments on the paper. Dennis Marsa implemented an earlier version of TCGMSG on KSR.  
Address: Atlanta, Georgia 30332-0280  
Affiliation: College of Computing Georgia Institute of Technology  
Date: September 24, 1993  (July September  
Pubnum: GIT-CC-94-37  
Abstract: A large body of applications have been built which use a message-passing style of inter-process communication. Thus, it is important to be able to support efficient message-passing even on shared-memory computers. Unfortunately, `direct' porting of message-passing packages to shared-memory computers invariably produces unacceptably poor performance. In this paper, we discuss schemes for efficiently implementing the primitives of two commonly-used message-passing packages - PVM and TCGMSG through the ALLCACHE memory of KSR computers. First, we describe a generic interface for message-passing and buffering, and map the specific calls of these two packages on to this generic interface. We then derive analytical results about the achievable bandwidth for message-passing via the shared ALLCACHE memory on KSR machines. Further, we describe a simple but widely-used benchmark called ping-pong, and report the performance of this benchmark on our implementation of each of the two message-passing packages. Finally, we suggest some new features to the system software on KSR machines which might support such packages more efficiently, and point out some drawbacks in the interfaces of the packages which hinder their efficient implementation on multiprocessors. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Scientific Computing Associates. </institution> <note> C-linda reference manual. </note> <year> 1992. </year>
Reference-contexts: Message-passing is a low level of communication software on a network of computers. While powerful high-level abstractions like Distributed Shared Memory can be built up over a layer of message passing software, raw, low-level message-passing produces the best performance. Several message-passing libraries are available today: CPS [2], Linda <ref> [1] </ref>, TCGMSG [4] and PVM [5], to name a few. Most of these libraries are complete parallel-programming environments, rather than mere low-level communication software. There is now an increasing number of application programs that have been parallelized using these "standard" message-passing interfaces.
Reference: [2] <author> M. Fausey, F. Rinaldok, S. Wolbers, D. Potter, and B. Yeager. </author> <title> Cps and cps batch reference guide. </title> <institution> Fermi National Accelerator Laboratory, </institution> <year> 1992. </year>
Reference-contexts: Message-passing is a low level of communication software on a network of computers. While powerful high-level abstractions like Distributed Shared Memory can be built up over a layer of message passing software, raw, low-level message-passing produces the best performance. Several message-passing libraries are available today: CPS <ref> [2] </ref>, Linda [1], TCGMSG [4] and PVM [5], to name a few. Most of these libraries are complete parallel-programming environments, rather than mere low-level communication software. There is now an increasing number of application programs that have been parallelized using these "standard" message-passing interfaces.
Reference: [3] <author> Kaushik Ghosh, Bodhisattwa Mukherjee, and Karsten Schwan. </author> <title> Experimentation with Configurable, Lightweight Threads on a KSR Multiprocessor. </title> <booktitle> Proceedings of the First International Workshop on Parallel Processing, </booktitle> <address> Bangalore, India., </address> <month> December </month> <year> 1994. </year>
Reference-contexts: Access to non-local memory results in the corresponding cache line being migrated to the local cache, so that future accesses to that memory element are relatively cheap <ref> [3] </ref>. A subcache miss produces a 2 Most of the communication software libraries mentioned run on Unix. 2 delay of 23 cycles, a cache miss produces a delay of 150 cycles, and a ring miss gives rise to a delay of 600 cycles (one level) 3 .
Reference: [4] <author> R. J. Harrison. </author> <title> Portable tools and applications for parallel computers. </title> <journal> International Journal of Quantum Chemistry, </journal> <volume> 40 </volume> <pages> 847-863, </pages> <year> 1991. </year>
Reference-contexts: While powerful high-level abstractions like Distributed Shared Memory can be built up over a layer of message passing software, raw, low-level message-passing produces the best performance. Several message-passing libraries are available today: CPS [2], Linda [1], TCGMSG <ref> [4] </ref> and PVM [5], to name a few. Most of these libraries are complete parallel-programming environments, rather than mere low-level communication software. There is now an increasing number of application programs that have been parallelized using these "standard" message-passing interfaces.
Reference: [5] <author> V. S. Sunderam. </author> <title> High-speed multiprocessor and compilation techniques. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2 </volume> <pages> 315-339, </pages> <year> 1990. </year> <month> 10 </month>
Reference-contexts: While powerful high-level abstractions like Distributed Shared Memory can be built up over a layer of message passing software, raw, low-level message-passing produces the best performance. Several message-passing libraries are available today: CPS [2], Linda [1], TCGMSG [4] and PVM <ref> [5] </ref>, to name a few. Most of these libraries are complete parallel-programming environments, rather than mere low-level communication software. There is now an increasing number of application programs that have been parallelized using these "standard" message-passing interfaces.
Reference: [6] <author> M. H. Schultz T. G. Mattson, C. D. Douglas. </author> <title> A comparison of cps, linda, p4, posybyl, pvm and tcgmsg: Two node communiation times. </title> <type> Technical Report YALEU/DCS/TR-975, </type> <institution> Department of Computer Science, Yale University, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: We first mention the salient points about the application, which has been described in detail elsewhere <ref> [6, 7] </ref>. The ping-pong benchmark repeatedly moves data between two processes. In each cycle of communication, the sender process sends to the recipient process. The roles of the processes are interchanged in alternate cycles.
Reference: [7] <author> M. H. Schultz T. G. Mattson, C. D. Douglas. </author> <title> Parallel programming systems for workstation clusters. </title> <type> Technical Report YALEU/DCS/TR-975, </type> <institution> Department of Computer Science, Yale University, </institution> <month> August </month> <year> 1993. </year> <month> 11 </month>
Reference-contexts: We first mention the salient points about the application, which has been described in detail elsewhere <ref> [6, 7] </ref>. The ping-pong benchmark repeatedly moves data between two processes. In each cycle of communication, the sender process sends to the recipient process. The roles of the processes are interchanged in alternate cycles.
References-found: 7

