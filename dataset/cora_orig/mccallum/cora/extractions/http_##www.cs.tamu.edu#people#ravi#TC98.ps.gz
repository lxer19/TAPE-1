URL: http://www.cs.tamu.edu/people/ravi/TC98.ps.gz
Refering-URL: http://www.cs.tamu.edu/people/ravi/
Root-URL: http://www.cs.tamu.edu
Email: E-mail: fbhuyan, ravi, hwangg@cs.tamu.edu  
Title: Evaluating Switch Architectures and Memory Management Policies for Cache-Coherent Multiprocessors: An Application-Based Study  
Author: L. Bhuyan, R. Iyer, H. Wang, and A. Kumar 
Keyword: Switch design, wormhole routing, virtual channels, memory management, execution-based simulation, scientific applications, shared-memory multiprocessor.  
Note: This research has been supported by NSF grant MIP 9622740. Currently with Intel Corporation, 2200  
Address: College Station, TX 77843-3112, USA.  Mission College Blvd, Santa Clara, CA 95052-8119, USA  
Affiliation: Department of Computer Science Texas A&M University  
Abstract: In this paper, the effect of interconnection network switch designs on the application performance of cache-coherent non-uniform memory access (CC-NUMA) multiprocessors is studied in detail. Wormhole routing and cut-through switching have been evaluated for these shared-memory multiprocessors that employ a full map directory-based cache coherence protocol. The switch design considers virtual channels and varying number of input buffers per switch. The effect of various memory management or data placement policies on the traffic interference in the interconnection network is also studied. The evaluation is based on execution-driven simulation using five different scientific applications to capture the random bursty nature of the network traffic arrival. The memory traces for different memory management techniques, namely buddy, round-robin & first-touch policies, are analyzed and their performance is compared. We show that the use of cut-through switching with buffers and virtual channels improves the average message latency with all the above policies. The waiting times of messages at various stages of the network are also presented. Finally, we show the variation of stall times and execution times for these applications by varying several switch design parameters, cache block size and memory page size. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L.N. Bhuyan, Q. Yang and D.P. Agrawal, </author> <title> "Performance of Multiprocessor Interconnection Networks," </title> <journal> Computer, </journal> <volume> vol. 22, no. 2, pp.25-37, </volume> <month> Feb. </month> <year> 1989. </year>
Reference-contexts: Examples include SGI Origin, Convex Exemplar, and Sequent NUMA-Q machines. Design issues for faster communication time in shared memory multiprocessors include the efficient use of caches, interconnection networks and memory management. Performance evaluation of interconnection networks (INs) has been an active area of research for a long time <ref> [1, 2, 3, 4] </ref>. All these studies consider the network in isolation and are based on analytical models that assume simple traffic situations. These models do not consider different types of messages, bulky requests and invalidation traffic associated with cache coherence and synchronization in a shared memory environment.
Reference: [2] <author> H. Yoon, K.Y.Lee, and M.T .Liu, </author> <title> "Performance Analysis of Multibuffered Packet-Switching Networks in Multiprocessor Systems," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 39, no. 3, </volume> <pages> pp. 319-327, </pages> <month> Mar. </month> <year> 1990. </year>
Reference-contexts: Examples include SGI Origin, Convex Exemplar, and Sequent NUMA-Q machines. Design issues for faster communication time in shared memory multiprocessors include the efficient use of caches, interconnection networks and memory management. Performance evaluation of interconnection networks (INs) has been an active area of research for a long time <ref> [1, 2, 3, 4] </ref>. All these studies consider the network in isolation and are based on analytical models that assume simple traffic situations. These models do not consider different types of messages, bulky requests and invalidation traffic associated with cache coherence and synchronization in a shared memory environment.
Reference: [3] <author> W. J. Dally, </author> <title> "Virtual-Channel Flow Control," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 3, no. 2, </volume> <pages> pp. 194-205, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Examples include SGI Origin, Convex Exemplar, and Sequent NUMA-Q machines. Design issues for faster communication time in shared memory multiprocessors include the efficient use of caches, interconnection networks and memory management. Performance evaluation of interconnection networks (INs) has been an active area of research for a long time <ref> [1, 2, 3, 4] </ref>. All these studies consider the network in isolation and are based on analytical models that assume simple traffic situations. These models do not consider different types of messages, bulky requests and invalidation traffic associated with cache coherence and synchronization in a shared memory environment. <p> The IN has become quite advanced with the introduction of techniques such as virtual channel <ref> [3] </ref> and adaptive routing [7]. The effect of all these advances in the INs has to be judged from the dynamic changes during the execution of applications. Execution-based evaluations for mesh interconnection networks have been reported recently [8, 9] to test the effectiveness of virtual channels and adaptive routing.
Reference: [4] <author> L. N. Bhuyan, et al., </author> <title> "Performance of Multistage Bus Networks for a Distributed Shared Memory Multiprocessor," </title> <journal> IEEE Transactions on Prallel and Distributed Systems, </journal> <volume> vol. 8, no. 1, </volume> <pages> pp. 82-95, </pages> <month> Jan. </month> <year> 1997. </year>
Reference-contexts: Examples include SGI Origin, Convex Exemplar, and Sequent NUMA-Q machines. Design issues for faster communication time in shared memory multiprocessors include the efficient use of caches, interconnection networks and memory management. Performance evaluation of interconnection networks (INs) has been an active area of research for a long time <ref> [1, 2, 3, 4] </ref>. All these studies consider the network in isolation and are based on analytical models that assume simple traffic situations. These models do not consider different types of messages, bulky requests and invalidation traffic associated with cache coherence and synchronization in a shared memory environment.
Reference: [5] <author> D. Chaiken, et al., </author> <title> "Directory-Based Cache Coherence in Large Scale Multiprocessors," </title> <journal> Computer, </journal> <volume> vol. 23, no. 6, </volume> <pages> pp. 49-58, </pages> <month> June </month> <year> 1990. </year> <month> 29 </month>
Reference-contexts: On the other hand, numerous cache coherence studies of CC-NUMA architectures based on execution-driven simulation do not model the IN switches explicitly, rather they assume a fixed delay in the IN even ignoring the possible interference among various messages <ref> [5, 6] </ref>. The IN has become quite advanced with the introduction of techniques such as virtual channel [3] and adaptive routing [7]. The effect of all these advances in the INs has to be judged from the dynamic changes during the execution of applications.
Reference: [6] <author> D.J. Lilja, </author> <title> "Cache Coherence in Large Scale Shared Memory Multiprocessors," </title> <journal> ACM Computing Surveys, </journal> <volume> vol. 25, no.3, </volume> <pages> pp. 303-338, </pages> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: On the other hand, numerous cache coherence studies of CC-NUMA architectures based on execution-driven simulation do not model the IN switches explicitly, rather they assume a fixed delay in the IN even ignoring the possible interference among various messages <ref> [5, 6] </ref>. The IN has become quite advanced with the introduction of techniques such as virtual channel [3] and adaptive routing [7]. The effect of all these advances in the INs has to be judged from the dynamic changes during the execution of applications.
Reference: [7] <author> P. Gawghan and S. Yalamanchi, </author> <title> "Adaptive Routing Protocols for Hypercube Interconnection Networks," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 26, no. 5, </volume> <pages> pp. 12-23, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The IN has become quite advanced with the introduction of techniques such as virtual channel [3] and adaptive routing <ref> [7] </ref>. The effect of all these advances in the INs has to be judged from the dynamic changes during the execution of applications. Execution-based evaluations for mesh interconnection networks have been reported recently [8, 9] to test the effectiveness of virtual channels and adaptive routing.
Reference: [8] <author> A. Kumar and L.N. Bhuyan, </author> <title> "Evaluating virtual channels for cache coherent shared memory multiprocessors," </title> <booktitle> ACM International Conference on Supercomuting, </booktitle> <address> Philadel-phia, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: The effect of all these advances in the INs has to be judged from the dynamic changes during the execution of applications. Execution-based evaluations for mesh interconnection networks have been reported recently <ref> [8, 9] </ref> to test the effectiveness of virtual channels and adaptive routing. These studies evaluate the multiprocessors at the network level and do not reflect the effect of design details of switch hardware. <p> Results showed that buddy and round-robin memory management techniques greatly improve their performance when virtual channels are employed. The benefit is minimal for the first-touch policy. The work reported in [9] assumed a unrealistic hand-optimized memory allocation and contradicted earlier results in <ref> [8] </ref> without realizing that the latter paper used a buddy allocation policy. As we demonstrated in this paper, the degree of performance improvement varies widely with the memory management techniques and the applications.
Reference: [9] <author> A. Vaidya, A. Sivasubramaniam and C. Das, </author> <title> "Performance Benefits of Virtual Channels and Adaptive Routing: An Application-Driven Study," </title> <booktitle> Proc. 11th International Conference on Supercomputing, </booktitle> <address> Vienna, </address> <month> July </month> <year> 1997. </year>
Reference-contexts: The effect of all these advances in the INs has to be judged from the dynamic changes during the execution of applications. Execution-based evaluations for mesh interconnection networks have been reported recently <ref> [8, 9] </ref> to test the effectiveness of virtual channels and adaptive routing. These studies evaluate the multiprocessors at the network level and do not reflect the effect of design details of switch hardware. <p> Results showed that buddy and round-robin memory management techniques greatly improve their performance when virtual channels are employed. The benefit is minimal for the first-touch policy. The work reported in <ref> [9] </ref> assumed a unrealistic hand-optimized memory allocation and contradicted earlier results in [8] without realizing that the latter paper used a buddy allocation policy. As we demonstrated in this paper, the degree of performance improvement varies widely with the memory management techniques and the applications.
Reference: [10] <author> M. Galles, </author> <title> "Scalable Pipelined Interconnect for Distributed Endpoint Routing: The SGI SPIDER Chip," </title> <booktitle> Proc. Symp. High Performance Interconnects (Hot Interconnects 4), </booktitle> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: Our network is based on the same topology as Butterfly and Cedar, but has the following differences: (a) switching: we adopt wormhole routing, virtual cut-through switching and virtual channels, as employed in commercial multiprocessor switches <ref> [10, 11] </ref> instead of packet switching in Cedar and circuit switching in Butterfly, (b) cache coherence: we employ a directory-based cache coherence whereas Cedar and Butterfly have no hardware support for cache coherence and (c) organization: Cedar is a uniform memory access (UMA) system where memory modules are centrally located and <p> The input synchronization takes one to two cycles, and it takes one more cycle for transmission. As a result, the switch delay is about 4 cycles, which is similar to the time taken in SGI Spider and Intel Cavallino switches <ref> [10, 11] </ref>. Many times the requests arrive in bulk. For example, in case of a write miss on a block, the interface/coherence controller sends invalidation signals to processors having a copy of the block one after another. In such situations, providing some buffer in the switches will be helpful. <p> In this situation, the arbiter consists of two simple 4 to 1 crossbar arbiters. When the number of inputs is more (6 inputs with 4 VC's or buffers per input which is equivalent to 24 possible inputs per output in Cavallino and Spider <ref> [10, 11] </ref>), two-stage arbiters are needed to keep arbitration time within limit. However, we have no such problem with a simple 2x2 switch. Each of the virtual channels can again have space for multiple flits. <p> The switch parameters are set as explained in section 2. A flit size of 16 bits was considered which is same as the width of the link. The switch latency or delay is considered to be 1, 2 or 4 processor cycles. The SGI Spider and Cavallino <ref> [10, 11] </ref> both have 4 cycles of latency. Additionally, we consider 1 or 2 cycle delays for comparison since our aim is to design high-speed switches. The interface delay for protocol handling, etc. is 20 cycles, similar to DASH [16]. <p> The current switch delay in the commercial machines is about 5 switch core cycles (switch core operates slower than the CPU) for a 6x6 crossbar <ref> [10, 11] </ref>. With a 2x2 switch, we could obtain a faster switch because it takes much less time to arbitrate.
Reference: [11] <author> J. Carbonaro and F. Verhoorn, "Cavallino: </author> <title> The Teraflops Router and NIC," </title> <booktitle> Proc. Symp. High Performance Interconnects (Hot Interconnects 4), </booktitle> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: Our network is based on the same topology as Butterfly and Cedar, but has the following differences: (a) switching: we adopt wormhole routing, virtual cut-through switching and virtual channels, as employed in commercial multiprocessor switches <ref> [10, 11] </ref> instead of packet switching in Cedar and circuit switching in Butterfly, (b) cache coherence: we employ a directory-based cache coherence whereas Cedar and Butterfly have no hardware support for cache coherence and (c) organization: Cedar is a uniform memory access (UMA) system where memory modules are centrally located and <p> The input synchronization takes one to two cycles, and it takes one more cycle for transmission. As a result, the switch delay is about 4 cycles, which is similar to the time taken in SGI Spider and Intel Cavallino switches <ref> [10, 11] </ref>. Many times the requests arrive in bulk. For example, in case of a write miss on a block, the interface/coherence controller sends invalidation signals to processors having a copy of the block one after another. In such situations, providing some buffer in the switches will be helpful. <p> In this situation, the arbiter consists of two simple 4 to 1 crossbar arbiters. When the number of inputs is more (6 inputs with 4 VC's or buffers per input which is equivalent to 24 possible inputs per output in Cavallino and Spider <ref> [10, 11] </ref>), two-stage arbiters are needed to keep arbitration time within limit. However, we have no such problem with a simple 2x2 switch. Each of the virtual channels can again have space for multiple flits. <p> Multiple input channels can request the same output channel in this phase. This conflict is resolved in the second step. A similar arbitration policy is used in the Intel Cavallino switch <ref> [11] </ref>. 3 Simulator Design Our simulator is based on Proteus [17] which implemented MIN using an analytical model. We have modified the simulator extensively to exactly model the MIN with wormhole routing. We have also incorporated the switch architectures with virtual channels and multi-flit buffers, as explained in section 2. <p> The switch parameters are set as explained in section 2. A flit size of 16 bits was considered which is same as the width of the link. The switch latency or delay is considered to be 1, 2 or 4 processor cycles. The SGI Spider and Cavallino <ref> [10, 11] </ref> both have 4 cycles of latency. Additionally, we consider 1 or 2 cycle delays for comparison since our aim is to design high-speed switches. The interface delay for protocol handling, etc. is 20 cycles, similar to DASH [16]. <p> The current switch delay in the commercial machines is about 5 switch core cycles (switch core operates slower than the CPU) for a 6x6 crossbar <ref> [10, 11] </ref>. With a 2x2 switch, we could obtain a faster switch because it takes much less time to arbitrate.
Reference: [12] <author> R. P. Larowe and C. S. Ellis, </author> <title> "Experimental Comparisons of Memory Management Policies for NUMA Multiprocessors," </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> vol. 9, no. 4, </volume> <pages> pp. 319-363, </pages> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: To ensure high performance, it is essential for the system to employ memory management policies that allocate data in local memories intelligently, thus increasing data locality and eliminating or atleast significantly reducing the need for remote access. Previous work on memory management policies <ref> [12, 13, 14] </ref> was based on distributed shared memory multiprocessors without hardware cache coherence. The impact of memory management policies on cache coherent NUMA system performance has been recently studied only in [15]. <p> In this section we present a detailed analysis of the effect of other memory management policies on the CC-NUMA network performance. Several policies have been suggested for a distributed shared memory system <ref> [12, 13, 14] </ref>, but not much work has been done for CC-NUMA architectures. The only notable work reported in [15] examines the performance of static and dynamic allocation schemes at the application execution time level.
Reference: [13] <author> C. Scheurich and M. Dubois, </author> <title> "Dynamic Page Migration in Multiprocessors with Distributed Global Memory," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 38, no. 8, </volume> <month> Aug. </month> <year> 1989. </year>
Reference-contexts: To ensure high performance, it is essential for the system to employ memory management policies that allocate data in local memories intelligently, thus increasing data locality and eliminating or atleast significantly reducing the need for remote access. Previous work on memory management policies <ref> [12, 13, 14] </ref> was based on distributed shared memory multiprocessors without hardware cache coherence. The impact of memory management policies on cache coherent NUMA system performance has been recently studied only in [15]. <p> In this section we present a detailed analysis of the effect of other memory management policies on the CC-NUMA network performance. Several policies have been suggested for a distributed shared memory system <ref> [12, 13, 14] </ref>, but not much work has been done for CC-NUMA architectures. The only notable work reported in [15] examines the performance of static and dynamic allocation schemes at the application execution time level.
Reference: [14] <author> J. Ramanathan and L. M. Ni, </author> <title> "Critical Factors in NUMA memory management," </title> <booktitle> In Proc of the 11th Conference on Distributed Computing Systems, </booktitle> <pages> pp. 500-507, </pages> <address> Arlington , Texas, </address> <month> May </month> <year> 1991, </year> <journal> IEEE Computer Society. </journal>
Reference-contexts: To ensure high performance, it is essential for the system to employ memory management policies that allocate data in local memories intelligently, thus increasing data locality and eliminating or atleast significantly reducing the need for remote access. Previous work on memory management policies <ref> [12, 13, 14] </ref> was based on distributed shared memory multiprocessors without hardware cache coherence. The impact of memory management policies on cache coherent NUMA system performance has been recently studied only in [15]. <p> In this section we present a detailed analysis of the effect of other memory management policies on the CC-NUMA network performance. Several policies have been suggested for a distributed shared memory system <ref> [12, 13, 14] </ref>, but not much work has been done for CC-NUMA architectures. The only notable work reported in [15] examines the performance of static and dynamic allocation schemes at the application execution time level.
Reference: [15] <author> B. Vergese, et al., </author> <title> "Operating System Support for improving data locality on CC-NUMA Computer Servers," </title> <booktitle> Proc. ASPLOS-VII, </booktitle> <pages> pp. 279-289, </pages> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: Previous work on memory management policies [12, 13, 14] was based on distributed shared memory multiprocessors without hardware cache coherence. The impact of memory management policies on cache coherent NUMA system performance has been recently studied only in <ref> [15] </ref>. This study focuses at an application execution time level and assumes constant local and remote access latencies in their trace-based analysis. However, the interference at the IN due to the data placement in the memory has not been studied. <p> Several policies have been suggested for a distributed shared memory system [12, 13, 14], but not much work has been done for CC-NUMA architectures. The only notable work reported in <ref> [15] </ref> examines the performance of static and dynamic allocation schemes at the application execution time level.
Reference: [16] <author> D. Lenoski, et. al., </author> <title> "The DASH Prototype: Logic Overhead and Performance," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 4, no. 1, </volume> <pages> pp. 41-61, </pages> <month> Jan </month> <year> 1993. </year>
Reference-contexts: The interface provides services such as dividing the message into packets or flits, initializing header flit of every packet with necessary routing information, etc. It contains a coherence controller, memory controller, directory controller and a reply controller, as described in <ref> [16] </ref>. The schematic of the network is shown in Fig. 1. It is a multistage interconnection network (MIN) employing 2x2 switches. In general, an NxN MIN consists of log 2 N stages of 2x2 switches with N/2 such switches per stage. The interconnection between the stages is perfect shu*e. <p> The SGI Spider and Cavallino [10, 11] both have 4 cycles of latency. Additionally, we consider 1 or 2 cycle delays for comparison since our aim is to design high-speed switches. The interface delay for protocol handling, etc. is 20 cycles, similar to DASH <ref> [16] </ref>. We assumed an infinite buffer at the interface, thus avoiding any deadlock problems. As explained in section 2, the messages are of two different lengths in a cache coherent multiprocessor. <p> We also observe that the invalidation overhead is significant. We can reduce the invalidation and write times by one network latency by incorporating dirty forward where the processor having a dirty copy sends the block directly to the requester instead of through the directory update, like in DASH <ref> [16] </ref>. The invalidation overhead is also highly dependent on the network design. To reduce this overhead we need to employ techniques such as broadcasting and multicasting. We can thus perform the memory access and many invalidations simultaneously. <p> The distribution of the pages is extremely uniform thus smoothening out the traffic generated in the network and in most cases making sure that simultaneous access to different data from different processors are interleaved among the different nodes. This scheme is used in DASH <ref> [16] </ref>. However, the scheme does not place data intelligently enough to reduce the number of remote accesses. Since the distribution is regular in most scientific applications, many processors tend to access multiple pages on the same node in a given time interval.
Reference: [17] <author> E. A. Brewer, C. N. Dellarocas, A. Colbrook, and W. E. Weihl, "Proteus: </author> <title> A High-Performance Parallel-Architecture Simulator," </title> <type> Technical Report MIT/LCS/TR-516, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, MA, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: Multiple input channels can request the same output channel in this phase. This conflict is resolved in the second step. A similar arbitration policy is used in the Intel Cavallino switch [11]. 3 Simulator Design Our simulator is based on Proteus <ref> [17] </ref> which implemented MIN using an analytical model. We have modified the simulator extensively to exactly model the MIN with wormhole routing. We have also incorporated the switch architectures with virtual channels and multi-flit buffers, as explained in section 2.
Reference: [18] <author> L. M. Censier and P. Feautrier, </author> <title> "A New Solution to Coherence Problems in Multi-cache Systems," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-27, no. 12, </volume> <pages> pp. 1112-1118, </pages> <month> December </month> <year> 1978. </year> <month> 30 </month>
Reference-contexts: We assumed message lengths of 8 and 40 bytes for control and data messages, respectively, similar to the SPIDER switch. 3.1 Cache Coherence And Synchronization We implemented the full-map directory-based cache coherence protocol <ref> [18] </ref> with some modifications for evaluation in this paper. In this scheme, each shared memory block is assigned to a node, called home node, which maintains the directory entries for that block. Each entry in the directory is a bit-vector of same length as the number of nodes.
Reference: [19] <author> J. M. Mellor-Crummey and M. L. Scott, </author> <title> "Synchronization Without Contention," </title> <booktitle> In Proceedings of Fourth Int. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 269-278, </pages> <address> Santa Clara, CA, </address> <month> April </month> <year> 1991, </year> <note> ACM. </note>
Reference-contexts: Such a situation is possible due to the existence of virtual channels. The synchronization method used in our simulations is based on spin-locks using test-and-set operation with exponential backoff <ref> [19] </ref>. Barriers used in many of the applications were implemented using a shared counter. We also experimented with other types of barriers to reduce contention, however, our experience suggests that the main overhead in synchronization is the contention for the lock itself, not for the shared variable in the barrier.
Reference: [20] <author> J. P. Singh, W.-D. Weber, and A. Gupta, </author> <title> "SPLASH: Stanford Parallel Applications for Shared-Memory," </title> <journal> ACM SIGARCH Computer Architecture News, </journal> <volume> vol. 20, no. 1, </volume> <pages> pp. 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: The overhead due to contention on synchronization variables is significant for some applications, such as FFT and MP3D <ref> [20] </ref>. The network latency also plays an important role on the synchronization overhead. In this paper, we do not discuss the synchronization overhead associated with any of the simulations. <p> During these stages, instead of using two separate input and output arrays, we interleave these arrays to avoid conflict misses. MP3D is a three-dimensional particle simulator used in rarefied fluid flow simulation. 8 We used 25000 molecules with the default geometry provided with SPLASH <ref> [20] </ref> which uses a 14 fi 24 fi 4 (2646-cell) space containing a single flat sheet placed at an angle to the free stream. The simulation was done for 5 time steps.
Reference: [21] <author> J. Torrellas and Z. Zheng, </author> <title> "The Performance of the Cedar Multistage Switching Network," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 8, no. 4, </volume> <pages> pp. 321-336, </pages> <month> April </month> <year> 1997. </year>
Reference-contexts: Figs.7 and 8 present the average message latencies for the second group of applications, namely, FFT and FWA. In these two figures, one can notice a huge waiting delay at stage 0 (interface) for the backward messages. A similar situation was also observed in Cedar network <ref> [21] </ref> where there was a large delay at the input of the backward network. In our case, the same network is used both for forward and backward requests and we use wormhole or cut-through switching instead of packet switching in Cedar.
Reference: [22] <author> R. Iyer, et al, </author> <title> "Effect of CC-NUMA Memory Management on the Performance of Interconnection Networks," </title> <type> Technical Report 98-002, </type> <institution> Dept. of Computer Science, Texas A&M University, </institution> <year> 1998. </year> <note> (http://www.cs.tamu.edu/faculty/bhuyan/Bhuyan-publications.html) 31 </note>
Reference-contexts: While the results in Table 3 present the memory access probabilities for the entire execution, they say nothing about the behaviour of each application when considered on a time scale. A detailed trace-based analysis of the time distribution of the memory accesses can be found in <ref> [22] </ref>. While the above probabilities characterize the spatial distribution of memory accesses, the performance of the application is also dependent on temporal locality of a processor's request. The think time of a processor is the time a processor spends in execution before sending a memory request.
References-found: 22

