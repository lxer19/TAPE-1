URL: http://www.cs.washington.edu/research/jair/volume7/nevill97a.ps
Refering-URL: http://www.cs.washington.edu/research/jair/abstracts/nevill97a.html
Root-URL: 
Email: CGN@CS.WAIKATO.AC.NZ  IHW@CS.WAIKATO.AC.NZ  
Title: Identifying Hierarchical Structure in Sequences: A linear-time algorithm  
Author: Craig G. Nevill-Manning Ian H. Witten 
Address: Hamilton, New Zealand.  
Affiliation: Department of Computer Science University of Waikato,  
Note: Journal of Artificial Intelligence Research 7 (1997) 6782 Submitted 5/97, published 9/97 1997 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved  
Abstract: SEQUITUR is an algorithm that infers a hierarchical structure from a sequence of discrete symbols by replacing repeated phrases with a grammatical rule that generates the phrase, and continuing this process recursively. The result is a hierarchical representation of the original sequence, which offers insights into its lexical structure. The algorithm is driven by two constraints that reduce the size of the grammar, and produce structure as a byproduct. S EQUITUR breaks new ground by operating incrementally. Moreover, the methods simple structure permits a proof that it operates in space and time that is linear in the size of the input. Our implementation can process 50,000 symbols per second and has been applied to an extensive range of real world sequences.
Abstract-found: 1
Intro-found: 1
Reference: <author> Andreae, J.H. </author> <title> (1977) Thinking with the teachable machine. </title> <publisher> London: Academic Press. </publisher>
Reference-contexts: A brief review of pertinent systems appears in Section 8. Techniques of Markov modeling and hidden Markov modeling make no attempt to abstract information in hierarchical form (Rabiner and Juang, 1986, Laird and Saul, 1994). Sequence learning also occurs in areas such as automaton modeling (Gaines, 1976), adaptive systems <ref> (Andreae, 1977) </ref>, programming by demonstration (Cypher, 1993), and human performance studies (Cohen et al., 1990), but generally plays only a peripheral role. In this paper we describe S EQUITUR, an algorithm that infers a hierarchical structure from a sequence of discrete symbols.
Reference: <author> Angluin, D. </author> <title> (1982) Inference of reversible languages, </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 29, </volume> <pages> 741765. </pages>
Reference: <author> Bell, T.C., Cleary, J.G., and Witten, I.H. </author> <title> (1990) Text compression . Englewood Cliffs, </title> <address> NJ: </address> <publisher> Prentice-Hall. </publisher>
Reference-contexts: The search for structure in sequences occurs in many different fields. Adaptive text compression seeks models of sequences that can be used to predict upcoming symbols so that they can be encoded efficiently <ref> (Bell et al ., 1990) </ref>. However, text compression models are extremely opaque, and do not illuminate any hierarchical structure in the sequence. Grammatical inference techniques induce grammars from a set of example sentences, possibly along with a set of negative examples (Gold, 1967; Angluin, 1982; Berwick and Pilato, 1987). <p> The whole question of accurate probabilistic prediction of sequences is tantamount to the compression of sequences, a substantial field in its own right <ref> (Bell et al., 1990) </ref>. We have in fact evaluated S EQUITURs performance in compression and found that it vies with the best compression algorithms, particularly when a large amount of text is available (Nevill-Manning and Witten, 1997).
Reference: <author> Berwick, R.C., and Pilato, S. </author> <title> (1987) Learning syntax by automata induction, </title> <journal> Machine Learning, </journal> <volume> 2, </volume> <pages> 938. </pages>
Reference: <author> Cohen, A., Ivry, R.I., and Keele, </author> <title> S.W. (1990) Attention and structure in sequence learning, </title> <journal> Journal of Experimental Psychology, </journal> <volume> 16(1), </volume> <pages> 1730. </pages>
Reference-contexts: Sequence learning also occurs in areas such as automaton modeling (Gaines, 1976), adaptive systems (Andreae, 1977), programming by demonstration (Cypher, 1993), and human performance studies <ref> (Cohen et al., 1990) </ref>, but generally plays only a peripheral role. In this paper we describe S EQUITUR, an algorithm that infers a hierarchical structure from a sequence of discrete symbols.
Reference: <author> Cook, C.M., Rosenfeld, A., & Aronson, A. </author> <year> (1976). </year> <title> Grammatical inference by hill climbing, </title> <journal> Informational Sciences, </journal> <volume> 10, </volume> <pages> 59-80. </pages>
Reference: <author> Cypher, A., </author> <title> editor (1993) Watch what I do: programming by demonstration , Cambridge, </title> <address> Massachusetts: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Techniques of Markov modeling and hidden Markov modeling make no attempt to abstract information in hierarchical form (Rabiner and Juang, 1986, Laird and Saul, 1994). Sequence learning also occurs in areas such as automaton modeling (Gaines, 1976), adaptive systems (Andreae, 1977), programming by demonstration <ref> (Cypher, 1993) </ref>, and human performance studies (Cohen et al., 1990), but generally plays only a peripheral role. In this paper we describe S EQUITUR, an algorithm that infers a hierarchical structure from a sequence of discrete symbols.
Reference: <author> Gaines, B.R. </author> <title> (1976) Behaviour/structure transformations under uncertainty, </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 8, </volume> <pages> 337365. </pages>
Reference-contexts: A brief review of pertinent systems appears in Section 8. Techniques of Markov modeling and hidden Markov modeling make no attempt to abstract information in hierarchical form (Rabiner and Juang, 1986, Laird and Saul, 1994). Sequence learning also occurs in areas such as automaton modeling <ref> (Gaines, 1976) </ref>, adaptive systems (Andreae, 1977), programming by demonstration (Cypher, 1993), and human performance studies (Cohen et al., 1990), but generally plays only a peripheral role. In this paper we describe S EQUITUR, an algorithm that infers a hierarchical structure from a sequence of discrete symbols.
Reference: <author> Gold, M. </author> <title> (1967) Language identification in the limit, </title> <journal> Information and Control, </journal> <volume> 10, </volume> <pages> 447474. </pages>
Reference: <author> Johansson, S., Leech, G., and Goodluck, H. </author> <title> (1978) Manual of Information to Accompany the Lancaster-Oslo/Bergen Corpus of British English, for Use with Digital Computers, </title> <institution> Oslo: Department of English, University of Oslo. </institution>
Reference-contexts: The German version in Figure2c correctly identifies all words in the sentence, as well as the phrase die Himmel und die . In fact, the hierarchy for the heaven and the in Figure2a bears some similarity to the German equivalent. The London/Oslo-Bergen corpus <ref> (Johansson et al., 1978) </ref> contains 1.2 million words tagged with word classes. For example, the sentence Most Labour sentiment would still favour the abolition of the House of Lords is tagged with the classes determiner noun noun auxiliary adverb verb article noun preposition article noun preposition noun .
Reference: <author> NEVILL-MANNING & WITTEN 82 Knuth, D.E. </author> <booktitle> (1968) The art of computer programming 1: fundamental algorithms . Addison-Wesley. </booktitle>
Reference: <author> Laird, P. & Saul, R. </author> <title> (1994) Discrete sequence prediction and its applications, </title> <booktitle> Machine Learning 15, </booktitle> <pages> 4368. </pages>
Reference: <author> Langley, P. </author> <year> (1994). </year> <title> Simplicity and representation change in grammar induction. </title> <type> Unpublished manuscript, </type> <institution> Robotics Laboratory, Computer Science Department, Stanford University, Stanford, </institution> <address> CA. </address>
Reference: <author> Nevill-Manning, C.G. & Witten, I.H. </author> <title> Compression and explanation using hierarchical grammars, </title> <journal> Computer Journal, </journal> <note> in press. </note>
Reference: <author> Nevill-Manning, </author> <title> C.G. (1996) Inferring sequential structure , Ph.D. </title> <type> thesis, </type> <institution> Department of Computer Science, University of Waikato, </institution> <address> New Zealand. </address>
Reference: <author> Nevill-Manning, C.G., Witten, I.H. & Paynter, G.W. </author> <title> (1997) Browsing in digital libraries: a phrase-based approach, </title> <booktitle> Proc. Second ACM International Conference on Digital Libraries, 230236, </booktitle> <address> Philadelphia, PA. </address>
Reference: <author> Rabiner, L.R. and Juang, B.H. </author> <title> (1986) An introduction to hidden Markov models, </title> <journal> IEEE ASSP Magazine, </journal> <volume> 3(1), </volume> <pages> 416. </pages>
Reference-contexts: A brief review of pertinent systems appears in Section 8. Techniques of Markov modeling and hidden Markov modeling make no attempt to abstract information in hierarchical form <ref> (Rabiner and Juang, 1986, Laird and Saul, 1994) </ref>. Sequence learning also occurs in areas such as automaton modeling (Gaines, 1976), adaptive systems (Andreae, 1977), programming by demonstration (Cypher, 1993), and human performance studies (Cohen et al., 1990), but generally plays only a peripheral role.
Reference: <author> Stolcke, A., & Omohundro, S. </author> <year> (1994). </year> <title> Inducing probabilistic grammars by Bayesian model merging. </title> <booktitle> Proc. Second International Conference on Grammatical Inference and Applications, 106118, </booktitle> <address> Alicante, Spain: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> VanLehn, K., & Ball, W. </author> <year> (1987). </year> <title> A version space approach to learning context-free grammars. </title> <journal> Machine Learning, </journal> <volume> 2, </volume> <pages> 3974. </pages>
Reference: <author> Wharton, R. M. </author> <year> (1977). </year> <title> Grammar enumeration and inference. </title> <journal> Information and Control , 33, </journal> <volume> 253 272. </volume>
Reference: <author> Wolff, J.G. </author> <title> (1975) An algorithm for the segmentation of an artificial language analogue, </title> <journal> British Journal of Psychology, </journal> <volume> 66, </volume> <pages> 7990. </pages>
Reference-contexts: 68 is not strictly a grammar, for the rules are not generalized and generate only one string. (It does provide a good basis for inferring a grammar, but that is beyond the scope of this paper.) A scheme that resembles the one developed here arose from work in language acquisition <ref> (Wolff, 1975, 1977, 1980, 1982) </ref>, but it operated in time that is quadratic with respect to the length of the input sequence, whereas the algorithm we describe takes linear time.
Reference: <author> Wolff, J.G. </author> <title> (1977) The discovery of segments in natural language, </title> <journal> British Journal of Psychology, </journal> <volume> 68, </volume> <pages> 97106. </pages>
Reference: <author> Wolff, J.G. </author> <title> (1980) Language acquisition and the discovery of phrase structure, </title> <journal> Language and Speech, </journal> <volume> 23(3), </volume> <pages> 255269. </pages>
Reference: <author> Wolff, J.G. </author> <title> (1982) Language acquisition, data compression and generalization, </title> <journal> Language and Communication, </journal> <volume> 2(1), </volume> <pages> 5789. </pages>
References-found: 24

