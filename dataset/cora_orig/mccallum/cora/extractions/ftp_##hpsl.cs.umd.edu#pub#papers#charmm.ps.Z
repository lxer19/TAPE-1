URL: ftp://hpsl.cs.umd.edu/pub/papers/charmm.ps.Z
Refering-URL: http://www.cs.umd.edu/projects/hpsl/papers.brandnew/LocalResources/tech-10-23.htm
Root-URL: 
Email: saltzg@cs.umd.edu fmilan, brbrooksg@helix.nih.gov  
Title: Parallelizing Molecular Dynamics Programs for Distributed Memory Machines: An Application of the CHAOS Runtime Support Library  
Author: Yuan-Shin Hwang Raja Das Joel H. Saltz Milan Hodo s cek Bernard Brooks fshin, raja, 
Note: This work was sponsored in part by ARPA (NAG-1-1485) and NSF (ASC 9213821).  
Address: College Park, MD 20742 Bethesda, MD 20892  
Affiliation: UMIACS and Department of Computer Science Laboratory of Structural Biology, DCRT University of Maryland National Institutes of Health  
Abstract: CHARMM (Chemistry at Harvard Macromolecular Mechanics) is a program that is widely used to model and simulate macromolecular systems. CHARMM has been parallelized by using the CHAOS runtime support library on distributed memory architectures. This implementation distributes both data and computations over processors. This data parallel strategy should make it possible to simulate very large molecules on large numbers of processors. In order to minimize communication among processors and to balance computational load, a variety of partitioning approaches are employed to distribute the atoms and computations over processors. In this implementation, atoms are partitioned based on geometrical positions and computational load by using unweighted or weighted recursive coordinate bisection. The experimental results reveal that taking computational load into account is essential. The performance of two iteration partitioning algorithms, atom decomposition and force decomposition, is also compared. A new irregular force decomposition algorithm is introduced and implemented. The CHAOS library is designed to facilitate parallelization of irregular applications. This library (1) couples partitioners to the application programs, (2) remaps data and partitions work among processors, and (3) optimizes interprocessor communications. This paper presents an application of CHAOS that can be used to support efficient execution of irregular problems on distributed memory machines. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. R. Brooks, R. E. Bruccoleri, B. D. Olafson, D. J. States, S. Swaminathan, and M. Karplus. Charmm: </author> <title> A program for macromolecular energy, minimization, and dynamics calculations. </title> <journal> Journal of Computational Chemistry, </journal> <volume> 4:187, </volume> <year> 1983. </year>
Reference-contexts: 1 Introduction Molecular dynamics (MD) simulation programs, such as CHARMM <ref> [1] </ref>, GROMOS [2], and AMBER [3], are useful to study the structural, equilibrium, and dynamic properties of molecules. These programs are very complicated and computationally intensive. Implementing them on massively parallel processing systems not only reduces execution times but also allows users to solve larger problems. <p> Conclusions are presented in Section 7. 2 Molecular Dynamics (CHARMM) CHARMM is a program which calculates empirical energy functions to model macromolecular systems. The purpose of CHARMM is to derive structural and dynamic properties of molecules using the first and second order derivative techniques <ref> [1] </ref>. The computationally intensive part of CHARMM is the molecular dynamics simulation. This calculation is usually performed when the molecular structure reaches a local stable state (low energy) through energy minimization. It simulates the dynamic interactions among all atoms in the system for a period of time.
Reference: [2] <author> W. F. van Gunsteren and H. J. C. Berendsen. Gromos: </author> <title> Groningen molecular simulation software. </title> <type> Technical report, </type> <institution> Laboratory of Physical Chemistry, University of Groningen, </institution> <address> Nijenborgh, The Netherlands, </address> <year> 1988. </year> <month> 19 </month>
Reference-contexts: 1 Introduction Molecular dynamics (MD) simulation programs, such as CHARMM [1], GROMOS <ref> [2] </ref>, and AMBER [3], are useful to study the structural, equilibrium, and dynamic properties of molecules. These programs are very complicated and computationally intensive. Implementing them on massively parallel processing systems not only reduces execution times but also allows users to solve larger problems.
Reference: [3] <author> P. K. Weiner and P. A. Kollman. </author> <title> Amber:assisted model building with energy refinement. a general program for modeling molecules and their interactions. </title> <journal> Journal of Computational Chemistry, </journal> <volume> 2:287, </volume> <year> 1981. </year>
Reference-contexts: 1 Introduction Molecular dynamics (MD) simulation programs, such as CHARMM [1], GROMOS [2], and AMBER <ref> [3] </ref>, are useful to study the structural, equilibrium, and dynamic properties of molecules. These programs are very complicated and computationally intensive. Implementing them on massively parallel processing systems not only reduces execution times but also allows users to solve larger problems.
Reference: [4] <author> B. R. Brooks and M. Hodoscek. </author> <title> Parallelization of charmm for mimd machines. Chemical Design Automation News, </title> <address> 7:16, </address> <year> 1992. </year>
Reference-contexts: These programs are very complicated and computationally intensive. Implementing them on massively parallel processing systems not only reduces execution times but also allows users to solve larger problems. In the past, a number of algorithms have been designed and implemented for MIMD architectures <ref> [4] </ref>. However, certain difficulties arise when trying to achieve high performance with large numbers of processors, due to the irregular computational structure of MD programs. This paper presents an implementation of the CHARMM program that scales well on MIMD distributed memory machines. <p> This program is parallelized using a set of efficient runtime primitives called the CHAOS runtime support library [5]. Several parallel MD algorithms use the replicated approach, i.e. the entire system's coordinates and forces are stored at each processor <ref> [4] </ref>. The replicated approach eliminates the overhead in maintaining the data distributions, but it does require more memory space. This prohibits users from simulating larger systems. <p> As predicated in Section 4.2, force decomposition has lower communication overheads than atom decomposition. The results also show that CHARMM scales well 2 Estimate (based on model systems due to memory limitations with 1 node) <ref> [4] </ref> 12 Table 5: Performance of Parallel CHARMM (Force Decomposition) Number of Processors 1 16 32 64 128 Execution Time 74595.5 4433.6 2296.7 1239.0 750.4 Computation Time 74595.5 4240.0 2096.8 1039.8 518.9 Communication Time 0 134.8 122.3 137.5 188.1 Load Balance Index 1.00 1.01 1.04 1.06 1.08 and that good load <p> T3D is slightly less than half of that on SP-2 on 16 processors, and it reduces to only 6% as the number of processors increases to 128 processors. 5.5 Comparison with Optimized Full Communication Methods In comparing the distributed data versions with the full communication replicated data version of CHARMM <ref> [4] </ref>, it is clear that distributed data versions can achieve somewhat better overall performance and that much larger systems can be simulated due to the reduced memory requirements of the distributed data methods. <p> Several parallel algorithms have been proposed and implemented on MIMD systems <ref> [4] </ref>. It has also been proposed and observed that spatial domain decomposition algorithms can be applied to achieve good load balance [8]. More recently there have been scalable algorithms created for large MIMD multiprocessor systems [6, 9]. Several widely used MD programs have been parallelized on MIMD computers. <p> Several widely used MD programs have been parallelized on MIMD computers. Clark et al. [9] applied spatial decomposition algorithms to develop EULERGROMOS, a parallelization of the GROMOS MD program. Brooks and Hodo scek implemented an efficient parallel version of CHARMM using the replicated data approach <ref> [4] </ref>. The parallelization effort of CHARMM done by Das and Saltz [10] was based on the distributed data approach. All data arrays associated with atoms were partitioned over processors in BLOCK distribution. This implementation did not achieve good performance and scalability because BLOCK distribution did not exploit locality.
Reference: [5] <author> Joel H. Saltz and et al. </author> <title> A manual for the CHAOS runtime library. </title> <type> Technical report, </type> <institution> Department of Computer Science, University of Maryland, </institution> <year> 1993. </year> <note> Available via anonymous ftp directory hyena.cs.umd.edu:/pub/chaos distribution or Mosaic URL location http://www.cs.umd.edu/projects/hpsl.html. </note>
Reference-contexts: This paper presents an implementation of the CHARMM program that scales well on MIMD distributed memory machines. This program is parallelized using a set of efficient runtime primitives called the CHAOS runtime support library <ref> [5] </ref>. Several parallel MD algorithms use the replicated approach, i.e. the entire system's coordinates and forces are stored at each processor [4]. The replicated approach eliminates the overhead in maintaining the data distributions, but it does require more memory space. This prohibits users from simulating larger systems.
Reference: [6] <author> Steve Plimpton. </author> <title> Fast parallel algorithms for short-range molecular dynamics. </title> <type> Technical Report SAND91-1144, </type> <institution> Sandia National Laboratories, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: In this paper, two approaches, data decomposition and force decomposition <ref> [6] </ref>, have been employed to parallelize the non-bonded force calculations. Although the regular force decomposition algorithm partitions atoms evenly over processors, the (non-uniformly sparse) non-bonded list is distributed unevenly and hence severe load imbalance occurs. <p> Several parallel algorithms have been proposed and implemented on MIMD systems [4]. It has also been proposed and observed that spatial domain decomposition algorithms can be applied to achieve good load balance [8]. More recently there have been scalable algorithms created for large MIMD multiprocessor systems <ref> [6, 9] </ref>. Several widely used MD programs have been parallelized on MIMD computers. Clark et al. [9] applied spatial decomposition algorithms to develop EULERGROMOS, a parallelization of the GROMOS MD program. Brooks and Hodo scek implemented an efficient parallel version of CHARMM using the replicated data approach [4]. <p> All data arrays associated with atoms were partitioned over processors in BLOCK distribution. This implementation did not achieve good performance and scalability because BLOCK distribution did not exploit locality. Plimpton proposed the regular force decomposition algorithm for parallel short-range MD computation <ref> [6] </ref>. The algorithm was adapted in this paper to decompose force matrices irregularly in order to achieve good load balance. CHAOS has been used to parallelize irregular scientific computation application programs, such as computational chemistry and computational fluid dynamics (CFD).
Reference: [7] <author> R. Mirchandaney, J. H. Saltz, R. M. Smith, D. M. Nicol, and Kay Crowley. </author> <title> Principles of runtime support for parallel processors. </title> <booktitle> In Proceedings of the 1988 ACM International Conference on Supercomputing, </booktitle> <pages> pages 140-152, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: This paper presents an irregular force decomposition algorithm that maintains good load balance and achieves good performance. The CHAOS library is designed to facilitate parallelization of irregular applications on distributed memory multiprocessor systems. It is a superset of the PARTI library <ref> [7] </ref>. This library (1) couples 1 partitioners to the application programs, (2) remaps data and partitions work among processors, and (3) optimizes interprocessor communications. This implementation of parallel CHARMM presents an application of CHAOS that can be used to support efficient execution of irregular problems on distributed memory machines. <p> It is a superset of the PARTI library <ref> [7] </ref>. These primitives have been designed to ease the implementation of computational problems on parallel architecture machines by relieving users of low-level machine specific issues.
Reference: [8] <author> Scott B. Baden. </author> <title> Programming abstractions for run-time partitioning of scientific continuum calculations running on multiprocessors. </title> <booktitle> In Proceedings of the Third SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> Los Angeles, California, </address> <month> December </month> <year> 1987. </year> <note> Also Tech Report LBL-24643, </note> <institution> Mathematics Dept., Lawrence Berkeley Laboratory. </institution>
Reference-contexts: Several parallel algorithms have been proposed and implemented on MIMD systems [4]. It has also been proposed and observed that spatial domain decomposition algorithms can be applied to achieve good load balance <ref> [8] </ref>. More recently there have been scalable algorithms created for large MIMD multiprocessor systems [6, 9]. Several widely used MD programs have been parallelized on MIMD computers. Clark et al. [9] applied spatial decomposition algorithms to develop EULERGROMOS, a parallelization of the GROMOS MD program.
Reference: [9] <author> T. W. Clark, R. v. Hanxleden, J. A. McCammon, and L. R. Scott. </author> <title> Parallelizing molecular dynamics using spatial decomposition. </title> <booktitle> In Proceedings of the '94 Scalable High Performance Computing Conference, </booktitle> <pages> pages 95-102, </pages> <address> Knoxville, Tennessee, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: Several parallel algorithms have been proposed and implemented on MIMD systems [4]. It has also been proposed and observed that spatial domain decomposition algorithms can be applied to achieve good load balance [8]. More recently there have been scalable algorithms created for large MIMD multiprocessor systems <ref> [6, 9] </ref>. Several widely used MD programs have been parallelized on MIMD computers. Clark et al. [9] applied spatial decomposition algorithms to develop EULERGROMOS, a parallelization of the GROMOS MD program. Brooks and Hodo scek implemented an efficient parallel version of CHARMM using the replicated data approach [4]. <p> More recently there have been scalable algorithms created for large MIMD multiprocessor systems [6, 9]. Several widely used MD programs have been parallelized on MIMD computers. Clark et al. <ref> [9] </ref> applied spatial decomposition algorithms to develop EULERGROMOS, a parallelization of the GROMOS MD program. Brooks and Hodo scek implemented an efficient parallel version of CHARMM using the replicated data approach [4]. The parallelization effort of CHARMM done by Das and Saltz [10] was based on the distributed data approach.
Reference: [10] <author> R. Das and J. Saltz. </author> <title> Parallelizing molecular dynamics codes using the Parti software primitives. </title> <booktitle> In Proceedings of the Sixth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 187-192. </pages> <publisher> SIAM, </publisher> <month> March </month> <year> 1993. </year>
Reference-contexts: Clark et al. [9] applied spatial decomposition algorithms to develop EULERGROMOS, a parallelization of the GROMOS MD program. Brooks and Hodo scek implemented an efficient parallel version of CHARMM using the replicated data approach [4]. The parallelization effort of CHARMM done by Das and Saltz <ref> [10] </ref> was based on the distributed data approach. All data arrays associated with atoms were partitioned over processors in BLOCK distribution. This implementation did not achieve good performance and scalability because BLOCK distribution did not exploit locality. Plimpton proposed the regular force decomposition algorithm for parallel short-range MD computation [6].
Reference: [11] <author> Ravi Ponnusamy, Yuan-Shin Hwang, Joel Saltz, Alok Choudhary, and Geoffrey Fox. </author> <title> Supporting irregular distributions in FORTRAN 90D/HPF compilers. </title> <institution> Technical Report CS-TR-3268 and UMIACS-TR-94-57, University of Maryland, Department of Computer Science and UMIACS, </institution> <month> May </month> <year> 1994. </year> <note> To appear in IEEE Parallel and Distributed Technology, Spring 1995. </note>
Reference-contexts: CHAOS has been used to parallelize irregular scientific computation application programs, such as computational chemistry and computational fluid dynamics (CFD). CHAOS has also been used by several parallel compilers to handle irregular computations on distributed memory systems <ref> [11, 12] </ref>. 7 Conclusions This paper presented an implementation of the CHARMM program that scaled well on MIMD distributed memory machines. This implementation was done by using a set of efficient runtime primitives called the CHAOS runtime support library.
Reference: [12] <author> R. v. Hanxleden, K. Kennedy, and J. Saltz. </author> <title> Value-based distributions in Fortran D a preliminary report. </title> <type> Technical Report CRPC-TR93365-S, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> December </month> <year> 1993. </year> <title> Submitted to Journal of Programming Languages Special Issue on Compiling and Run-Time Issues for Distributed Address Space Machines. </title> <type> 20 </type>
Reference-contexts: CHAOS has been used to parallelize irregular scientific computation application programs, such as computational chemistry and computational fluid dynamics (CFD). CHAOS has also been used by several parallel compilers to handle irregular computations on distributed memory systems <ref> [11, 12] </ref>. 7 Conclusions This paper presented an implementation of the CHARMM program that scaled well on MIMD distributed memory machines. This implementation was done by using a set of efficient runtime primitives called the CHAOS runtime support library.
References-found: 12

