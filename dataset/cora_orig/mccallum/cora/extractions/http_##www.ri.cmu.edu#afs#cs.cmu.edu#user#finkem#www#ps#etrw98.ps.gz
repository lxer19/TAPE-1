URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/finkem/www/ps/etrw98.ps.gz
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/finkem/www/publications.html
Root-URL: 
Title: STOCHASTIC PRONUNCIATION MODELLING FROM HAND-LABELLED PHONETIC CORPORA  
Author: M. Riley W. Byrne M. Finke S. Khudanpur A. Ljolje J. McDonough H. Nock M. Saraclar C. Wooters G. Zavaliagkos 
Note: U.S. Department of Defense,  
Address: Florham Park, NJ, USA 1  Baltimore, MD, USA 2  Pittsburgh, PA, USA 3  Cambridge, UK 4  Fort Meade, MD, USA 5 BBN, Cambridge, MA, USA 6  
Affiliation: AT&T Labs Research,  Johns Hopkins University,  Carnegie-Mellon University,  Cambridge University Engineering Department,  
Abstract: In the early '90s, the availability of the TIMIT read-speech phonetically transcribed corpus led to work at AT&T on the automatic inference of pronunciation variation. This work, briefly summarized here, used stochastic decisions trees trained on phonetic and linguistic features, and was applied to the DARPA North American Business News read-speech ASR task. More recently, the ICSI spontaneous-speechphonetically transcribed corpus was collected at the behest of the 1996 and 1997 LVCSR Summer Workshops held at Johns Hopkins University. A 1997 workshop (WS97) group focused on pronunciation inference from this corpus for application to the DoD Switchboard spontaneous telephone speech ASR task. We describe several approaches taken there. These include (1) one analogousto the AT&T approach, (2) one, inspired by work at WS96 and CMU, that involved adding pronunciation variants of a sequence of one or more words (`mul-tiwords') in the corpus (with corpus-derived probabilities) into the ASR lexicon, and (1+2) a hybrid approach in which a decision-tree model was used to automatically phonetically transcribe a much larger speech corpus than ICSI and then the multiword approach was used to construct an ASR recognition pronunciation lexicon. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> W. Byrne, et al, </author> <title> Pronunciation Modelling for Conversational Speech Recognition: A Status Report from WS97, </title> <booktitle> presented at the 1997 IEEE Workshop on Speech Recognition and Understanding, </booktitle> <address> Santa Barbara, CA, </address> <month> Dec. </month> <year> 1997. </year>
Reference-contexts: In fact, preliminary attempts at WS97 to retrain acoustic models using tree-based pronunciation lexicons lead to significantly worse results <ref> [1] </ref>. There were various conjectures made why the ICSI+TIMIT dictionary gave a worse result and we launched an series of experiments to investigate them. These are described in the next few paragraphs.
Reference: [2] <author> W. Byrne, et al, </author> <title> Pronunciation Modelling Using a Hand-labelled Corpus for Conversational Speech Recognition, </title> <booktitle> Proc. ICASSP '98 Seattle, </booktitle> <address> WA. </address>
Reference-contexts: We report methods used to address this issue applied to read speech at AT&T [9] and to spontaneous speech at and after WS97, the Fifth LVCSR Summer Workshop, held at Johns Hopkins University, Baltimore, in July-August, 1997 <ref> [2] </ref>. As a first step towards alleviating this common limitation of pronouncing dictionaries, we identify a systematic way of generating alternate pronunciations of words by using phonetically labelled portions of the TIMIT [5] and Switchboard [6] corpora.
Reference: [3] <author> F. Chen, </author> <title> Identification of Contextual Factors for Pronunciation Networks, </title> <booktitle> Proc. ICASSP `90, </booktitle> <address> S14.9, </address> <year> 1990. </year>
Reference-contexts: TREE BASED DICTIONARY EXPANSION Our tree based pronunciation models were inspired by phonological rules in acoustic phonetic studies (cf., e.g., [7]) which characterize allophonic variations in certain phonemic contexts, and by the successful use of similar methods to model pronunciation variability and constraints by other researchers (e.g., <ref> [3, 8, 4, 10, 12, 11] </ref>). Figure 1 illustrates the deletion or alteration of a phoneme in context which we modelled via decision trees. 2.1.
Reference: [4] <author> M. Finke and A. Waibel, </author> <title> Speaker Mode Dependent Pronunciation Modelling in Large Vocabulary Conversational Speech Recognition, </title> <booktitle> in Proc. </booktitle> <address> EUROSPEECH'97, </address> <year> 1997. </year>
Reference-contexts: The tree based expansion implicitly adds many more new pronunciations than the explicit expansion. However, it does not attempt to model any cross-word coarticulation. The explicit expansion does so by allowing as dictionary entries a select set (cf. <ref> [4] </ref>) of multiwords word pairs and triples. <p> TREE BASED DICTIONARY EXPANSION Our tree based pronunciation models were inspired by phonological rules in acoustic phonetic studies (cf., e.g., [7]) which characterize allophonic variations in certain phonemic contexts, and by the successful use of similar methods to model pronunciation variability and constraints by other researchers (e.g., <ref> [3, 8, 4, 10, 12, 11] </ref>). Figure 1 illustrates the deletion or alteration of a phoneme in context which we modelled via decision trees. 2.1. <p> As such, all experiments from here on apply to Switchboard. 3.1. ICSI Multiword Dictionary The PronLex dictionary is first enhanced with all the pronunciations for words seen in the hand-labelled (ICSI) portion of the corpus. A candidate list of 172 multiwords (cf. <ref> [4] </ref>) is also appended to the dictionary to capture coarticulation, and pronunciations for these are similarly extended using the hand-labelled corpus. The word transcription of the training corpus is then expanded using these alternate pronunciations and aligned with the acoustics using our baseline models.
Reference: [5] <author> W. Fisher, V. Zue, J. Bernstein, and D. </author> <title> Pallet, An Acoustic-Phonetic Data Base, </title> <journal> J. Acoust. Soc. Am. </journal> <volume> 81, Suppl. </volume> <year> 1,1987. </year>
Reference-contexts: As a first step towards alleviating this common limitation of pronouncing dictionaries, we identify a systematic way of generating alternate pronunciations of words by using phonetically labelled portions of the TIMIT <ref> [5] </ref> and Switchboard [6] corpora. One viewpoint we explore is that pronunciation variability may be mod-elled by a statistical mapping from canonical pronunciations (base-forms) to symbolic surface forms, and we use decision trees to cap ture this mapping.
Reference: [6] <author> S. Greenberg, </author> <title> The Switchboard Transcription Project, </title> <booktitle> 1996 LVCSR Summer Workshop Technical Reports, </booktitle> <year> 1996, </year> <note> http://www.icsi.berkeley.edu/real/stp/ </note>
Reference-contexts: As a first step towards alleviating this common limitation of pronouncing dictionaries, we identify a systematic way of generating alternate pronunciations of words by using phonetically labelled portions of the TIMIT [5] and Switchboard <ref> [6] </ref> corpora. One viewpoint we explore is that pronunciation variability may be mod-elled by a statistical mapping from canonical pronunciations (base-forms) to symbolic surface forms, and we use decision trees to cap ture this mapping.
Reference: [7] <author> P. Ladefoged, </author> <title> A Course in Phonetics, </title> <publisher> Harcourt Brace Jovan-ovich, Inc., </publisher> <address> New York, </address> <year> 1975. </year>
Reference-contexts: Further, we show in Sections 4 and 5 that reductions persist when the baseline systems are improved by coarticulation sensitive acoustic modelling and improved language modelling. 2. TREE BASED DICTIONARY EXPANSION Our tree based pronunciation models were inspired by phonological rules in acoustic phonetic studies (cf., e.g., <ref> [7] </ref>) which characterize allophonic variations in certain phonemic contexts, and by the successful use of similar methods to model pronunciation variability and constraints by other researchers (e.g., [3, 8, 4, 10, 12, 11]).
Reference: [8] <author> M. </author> <title> Randolph A Data-Driven Method for Discovering and Predicting Allophonic Variation, </title> <booktitle> Proc. ICASSP `90, </booktitle> <address> S14.10, </address> <year> 1990. </year>
Reference-contexts: TREE BASED DICTIONARY EXPANSION Our tree based pronunciation models were inspired by phonological rules in acoustic phonetic studies (cf., e.g., [7]) which characterize allophonic variations in certain phonemic contexts, and by the successful use of similar methods to model pronunciation variability and constraints by other researchers (e.g., <ref> [3, 8, 4, 10, 12, 11] </ref>). Figure 1 illustrates the deletion or alteration of a phoneme in context which we modelled via decision trees. 2.1.
Reference: [9] <author> M. Riley and A. Ljolje, </author> <title> Automatic generation of detailed pronunciation lexicons. Automatic Speech and Speaker Recognition: Advanced Topics. </title> <publisher> Kluwer. </publisher> <year> 1995. </year>
Reference-contexts: The failure of ASR systems to capture this important source of variability is potentially a significant source for recognition errors, particularly in spontaneous, conversational speech. We report methods used to address this issue applied to read speech at AT&T <ref> [9] </ref> and to spontaneous speech at and after WS97, the Fifth LVCSR Summer Workshop, held at Johns Hopkins University, Baltimore, in July-August, 1997 [2]. <p> The phonemic transcriptions were then lined up with the phonetic labels, using as the alignment criterion the minimization of the phonetic feature distance between the two symbol streams <ref> [9] </ref>. Table 1 gives an example alignment from the ICSI corpus. This gave us a corpus of phoneme-to-phone transformations together with the phonemic environment or context for each instance. Decision tree models were then built to represent this phoneme to phone mapping. <p> Dictionary Expansion Using Pronunciation Trees We applied the ICSI+TIMIT trees of Table 3 to successive phonemes of each baseform in the WS97 baseline dictionary to obtain a Dictionary WER TTS 12.7% TIMIT 10.8% Retrained2 10.0% Table 7: NAB recognition results with Enhanced Dictionaries weighted pronunciation network as described in <ref> [9] </ref>. Figure 2 illustrates such a network for the word pretty. Applied statically, this resulted in an expanded dictionary which we call the ICSI+TIMIT dictionary.
Reference: [10] <author> G. Tajchman, E. Fosler, and D. Jurafsky, </author> <title> Building Multiple Pronunciation Models for Novel Words using Exploratory Computational Phonology, </title> <booktitle> Proc. Eurospeech '95, </booktitle> <year> 1995. </year>
Reference-contexts: TREE BASED DICTIONARY EXPANSION Our tree based pronunciation models were inspired by phonological rules in acoustic phonetic studies (cf., e.g., [7]) which characterize allophonic variations in certain phonemic contexts, and by the successful use of similar methods to model pronunciation variability and constraints by other researchers (e.g., <ref> [3, 8, 4, 10, 12, 11] </ref>). Figure 1 illustrates the deletion or alteration of a phoneme in context which we modelled via decision trees. 2.1.
Reference: [11] <author> M. Weintraub, E. Fosler, C. Galles, Y. Kao, S. Khudanpur, M. Saraclar, S. Wegmann, </author> <title> Automatic Learning of Word Pronunciation from Data, </title> <booktitle> 1996 LVCSR Summer Workshop Technical Reports, </booktitle> <year> 1996. </year>
Reference-contexts: TREE BASED DICTIONARY EXPANSION Our tree based pronunciation models were inspired by phonological rules in acoustic phonetic studies (cf., e.g., [7]) which characterize allophonic variations in certain phonemic contexts, and by the successful use of similar methods to model pronunciation variability and constraints by other researchers (e.g., <ref> [3, 8, 4, 10, 12, 11] </ref>). Figure 1 illustrates the deletion or alteration of a phoneme in context which we modelled via decision trees. 2.1. <p> Large quantities of automatic phonetic transcriptions were generated to augment the hand-labelled corpora using the 37,000 training sentences (SI-284 training data set) for the NAB task at AT&T and using the 60-hour acoustic training corpus for the Switchboard task at WS97. Unlike <ref> [11] </ref>, where unconstrained phone recognition was used to generate phone transcriptions, we constrained the words in our training utterances to assume only pronunciations generated by application of the decision trees to their phonemic baseforms: a forced alignment was performed on the resulting network of alternate pronunciations in an utterance and the
Reference: [12] <author> M. Weintraub, H. Murveit, M. Cohen, P. Price, J. Bern-stein, C. Baldwin, D. Bell, </author> <title> Linguistic Constraints in Hidden Markov Model Based Speech Recognition, </title> <booktitle> Proc. ICASSP '89, </booktitle> <address> S13.2, </address> <year> 1989. </year>
Reference-contexts: TREE BASED DICTIONARY EXPANSION Our tree based pronunciation models were inspired by phonological rules in acoustic phonetic studies (cf., e.g., [7]) which characterize allophonic variations in certain phonemic contexts, and by the successful use of similar methods to model pronunciation variability and constraints by other researchers (e.g., <ref> [3, 8, 4, 10, 12, 11] </ref>). Figure 1 illustrates the deletion or alteration of a phoneme in context which we modelled via decision trees. 2.1.
Reference: [13] <author> S. Young, J. Jansen, J. Odell, D. Ollasen, P. Woodland, </author> <note> The HTK Book (Version 2.0), </note> <institution> Entropic Cambridge Research Laboratory, </institution> <year> 1995. </year>
References-found: 13

