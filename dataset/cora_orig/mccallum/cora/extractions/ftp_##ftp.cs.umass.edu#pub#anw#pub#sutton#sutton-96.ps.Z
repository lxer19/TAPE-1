URL: ftp://ftp.cs.umass.edu/pub/anw/pub/sutton/sutton-96.ps.Z
Refering-URL: http://www-anw.cs.umass.edu/~rich/publications.html
Root-URL: 
Email: rich@cs.umass.edu  
Title: Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding  
Author: Richard S. Sutton 
Address: Amherst, MA 01003 USA  
Affiliation: University of Massachusetts  
Note: Advances in Neural Information Processing Systems 8, pp. 1038-1044, MIT Press, 1996.  
Abstract: On large problems, reinforcement learning systems must use parameterized function approximators such as neural networks in order to generalize between similar situations and actions. In these cases there are no strong theoretical results on the accuracy of convergence, and computational results have been mixed. In particular, Boyan and Moore reported at last year's meeting a series of negative results in attempting to apply dynamic programming together with function approximation to simple control problems with continuous state spaces. In this paper, we present positive results for all the control tasks they attempted, and for one that is significantly larger. The most important differences are that we used sparse-coarse-coded function approximators (CMACs) whereas they used mostly global function approximators, and that we learned online whereas they learned o*ine. Boyan and Moore and others have suggested that the problems they encountered could be solved by using actual outcomes ("rollouts"), as in classical Monte Carlo methods, and as in the TD() algorithm when = 1. However, in our experiments this always resulted in substantially poorer performance. We conclude that reinforcement learning can work robustly in conjunction with function approximators, and that there is little justification at present for avoiding the case of general .
Abstract-found: 1
Intro-found: 1
Reference: <author> Albus, J. S. </author> <title> (1981) Brain, Behavior, </title> <journal> and Robotics, </journal> <volume> chapter 6, </volume> <pages> pages 139-179. </pages> <publisher> Byte Books. </publisher>
Reference: <author> Baird, L. C. </author> <title> (1995) Residual Algorithms: Reinforcement Learning with Function Approximation. </title> <booktitle> Proc. </booktitle> <address> ML95. </address> <publisher> Morgan Kaufman, </publisher> <address> San Francisco, CA. </address>
Reference: <author> Barto, A. G., Bradtke, S. J., & Singh, S. P. </author> <title> (1995) Real-time learning and control using asynchronous dynamic programming. </title> <journal> Artificial Intelligence. </journal>
Reference: <author> Barto, A. G., Sutton, R. S., & Anderson, C. W. </author> <title> (1983) Neuronlike elements that can solve difficult learning control problems. </title> <journal> Trans. IEEE SMC, </journal> <volume> 13, </volume> <pages> 835-846. </pages>
Reference: <author> Bertsekas, D. P. </author> <title> (1995) A counterexample to temporal differences learning. </title> <journal> Neural Computation, </journal> <volume> 7, </volume> <pages> 270-279. </pages>
Reference: <author> Boyan, J. A. & Moore, A. W. </author> <title> (1995) Generalization in reinforcement learning: Safely approximating the value function. </title> <address> NIPS-7. San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Indeed some such methods have been shown to be unstable in theory (Baird, 1995; Gordon, 1995; Tsitsiklis & Van Roy, 1994) and in practice <ref> (Boyan & Moore, 1995) </ref>. On the other hand, other methods have been proven stable in theory (Sutton, 1988; Dayan, 1992) and very effective in practice (Lin, 1991; Tesauro, 1992; Zhang & Dietterich, 1995; Crites & Barto, 1996).
Reference: <author> Crites, R. H. & Barto, A. G. </author> <title> (1996) Improving elevator performance using reinforcement learning. </title> <address> NIPS-8. Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Dayan, P. </author> <title> (1992) The convergence of TD() for general . Machine Learning, </title> <booktitle> 8, </booktitle> <pages> 341-362. </pages>
Reference: <author> Dean, T., Basye, K. & Shewchuk, J. </author> <title> (1992) Reinforcement learning for planning and control. In S. Minton, Machine Learning Methods for Planning and Scheduling. </title> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> DeJong, G. & Spong, M. W. </author> <title> (1994) Swinging up the acrobot: An example of intelligent control. </title> <booktitle> In Proceedings of the American Control Conference, </booktitle> <pages> pages 2158-2162. </pages>
Reference: <author> Gordon, G. </author> <title> (1995) Stable function approximation in dynamic programming. </title> <booktitle> Proc. </booktitle> <address> ML95. </address>
Reference: <author> Lin, L. J. </author> <title> (1992) Self-improving reactive agents based on reinforcement learning, planning and teaching. </title> <journal> Machine Learning, </journal> <volume> 8 (3/4), </volume> <pages> 293-321. </pages>
Reference: <author> Lin, C-S. & Kim, H. </author> <title> (1991) CMAC-based adaptive critic self-learning control. </title> <journal> IEEE Trans. Neural Networks, </journal> <volume> 2, </volume> <pages> 530-533. </pages>
Reference: <author> Miller, W. T., Glanz, F. H., & Kraft, L. G. </author> <title> (1990) CMAC: An associative neural network alternative to backpropagation. </title> <journal> Proc. of the IEEE, </journal> <volume> 78, </volume> <pages> 1561-1567. </pages>
Reference: <author> Rummery, G. A. & Niranjan, M. </author> <title> (1994) On-line Q-learning using connectionist systems. </title> <type> Technical Report CUED/F-INFENG/TR 166, </type> <institution> Cambridge University Engineering Dept. </institution>
Reference: <author> Singh, S. P. & Sutton, R. S. </author> <title> (1996) Reinforcement learning with replacing eligibility traces. </title> <booktitle> Machine Learning. </booktitle>
Reference-contexts: However, a small fraction, *, of the time, the action was instead selected randomly uniformly from the action set (which was always discrete and finite). There are two variations of the sarsa algorithm, one using conventional accumulate traces and one using replace traces <ref> (Singh & Sutton, 1996) </ref>. This and other details of the algorithm we used are given in Figure 1. <p> The upper right panel concerns a 21-state Markov chain, the objective being to predict, for each state, the probability of terminating in one terminal state as opposed to the other <ref> (Singh & Sutton, 1996) </ref>. The lower left panel concerns the pole balancing task studied by Barto, Sutton and Anderson (1983). This is previously unpublished data from an earlier study (Sutton, 1984).
Reference: <author> Spong, M. W. & Vidyasagar, M. </author> <title> (1989) Robot Dynamics and Control. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference: <author> Sutton, R. S. </author> <title> (1984) Temporal Credit Assignment in Reinforcement Learning. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <address> Amherst, MA. </address>
Reference-contexts: The lower left panel concerns the pole balancing task studied by Barto, Sutton and Anderson (1983). This is previously unpublished data from an earlier study <ref> (Sutton, 1984) </ref>.
Reference: <author> Sutton, R. S. </author> <title> (1988) Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 9-44. </pages>
Reference-contexts: The reinforcement learning methods we use are variations of the sarsa algorithm (Rum-mery & Niranjan, 1994; Singh & Sutton, 1996). This method is the same as the TD () algorithm <ref> (Sutton, 1988) </ref>, except applied to state-action pairs instead of states, and where the predictions are used as the basis for selecting actions. The learning agent estimates action-values, Q (s; a), defined as the expected future reward starting in state s, taking action a, and thereafter following policy . <p> Theoretically, the former has asymptotic advantages when function approximators are used (Dayan, 1992; Bertsekas, 1995), but empirically the latter is thought to achieve better learning rates <ref> (Sutton, 1988) </ref>. However, hitherto this question has not been put to an empirical test using function approximators. Figures 6 shows the results of such a test. inverted-U shaped function of , and performance degrades rapidly as approaches 1, where the worst performance is obtained.
Reference: <author> Sutton, R. S. & Whitehead, S. D. </author> <title> (1993) Online learning with random representations. </title> <booktitle> Proc. ML93, </booktitle> <pages> pages 314-321. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: See Figure 2. The overall effect is much like a network with fixed radial basis functions, except that it is particularly efficient computationally (in other respects one would expect RBF networks and similar methods <ref> (see Sutton & Whitehead, 1993) </ref> to work just as well). It is important to note that the tilings need not be simple grids. For example, to avoid the "curse of dimensionality," a common trick is to ignore some dimensions in some tilings, i.e., to use hyperplanar slices instead of boxes.
Reference: <author> Tham, C. K. </author> <title> (1994) Modular On-Line Function Approximation for Scaling up Reinforcement Learning. </title> <type> PhD thesis, </type> <institution> Cambridge Univ., </institution> <address> Cambridge, England. </address>
Reference: <author> Tesauro, G. J. </author> <title> (1992) Practical issues in temporal difference learning. </title> <journal> Machine Learning, </journal> <volume> 8 (3/4), </volume> <pages> 257-277. </pages>
Reference: <author> Tsitsiklis, J. N. & Van Roy, B. </author> <title> (1994) Feature-based methods for large-scale dynamic programming. Techical Report LIDS-P2277, </title> <publisher> MIT, </publisher> <address> Cambridge, MA 02139. </address>
Reference: <author> Watkins, C. J. C. H. </author> <title> (1989) Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Cambridge Univ. </institution>
Reference: <author> Zhang, W. & Dietterich, T. G., </author> <title> (1995) A reinforcement learning approach to job-shop scheduling. </title> <booktitle> Proc. IJCAI95. </booktitle>
References-found: 25

