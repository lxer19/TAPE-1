URL: http://www.isi.edu/~shen/ijcai93.ps
Refering-URL: http://www.isi.edu/~shen/selected.html
Root-URL: http://www.isi.edu
Title: Learning Finite Automata Using Local Distinguishing Experiments  
Author: Wei-Min Shen 
Address: 3500 West Balcones Center Drive Austin, TX 78759, U.S.A.  
Affiliation: Microelectronics and Computer Technology Corporation  
Abstract: One of the open problems listed in [ Rivest and Schapire, 1989 ] is whether and how that the copies of L fl in their algorithm can be combined into one for better performance. This paper describes an algorithm called D fl that does that combination. The idea is to represent the states of the learned model using observable symbols as well as hidden symbols that are constructed during learning. These hidden symbols are created to reflect the distinct behaviors of the model states. The distinct behaviors are represented as local distinguishing experiments (LDEs) (not to be confused with global distinguishing sequences), and these LDEs are created when the learner's prediction mismatches the actual observation from the unknown machine. To synchronize the model with the environment, these LDEs can also be concatenated to form a homing sequence. It can be shown that D fl can learn, with probability 1 , a model that is an *-approximation of the unknown machine, in a number of actions polynomial in the size of the environment and 
Abstract-found: 1
Intro-found: 1
Reference: [ Angluin, 1987 ] <author> D. Angluin. </author> <title> Learning regular sets from queries and counter-examples. </title> <journal> Information and Computation, </journal> <volume> 75(2), </volume> <month> November </month> <year> 1987. </year>
Reference-contexts: An existing solution to the construction problem is the L fl algorithm developed by <ref> [ Angluin, 1987 ] </ref> . L fl can learn regular expressions using membership queries and equivalence queries but it requires a means to reset the unknown automaton to a fixed state for synchronization. <p> If the model is only required to be an *-approximation of E with probability 1- <ref> [ Angluin, 1987 ] </ref> , then we can estimate 1 ). It is still an open problem, however, that why wasted states are created and what the upper bound of the number of such states is.
Reference: [ Rivest and Schapire, 1989 ] <author> R. Rivest and R. Schapire. </author> <title> Inference of finite automata using homing sequences. </title> <booktitle> In Proceedings of 21th Annual ACM Symposium on Theory of Computing, </booktitle> <year> 1989. </year>
Reference-contexts: L fl can learn regular expressions using membership queries and equivalence queries but it requires a means to reset the unknown automaton to a fixed state for synchronization. The algorithm developed by <ref> [ Rivest and Schapire, 1989 ] </ref> extends L fl to deal with the synchronization problem by learning a homing sequence of the unknown machine. However, for each different output of the current homing sequence, they create an independent copy of L fl . <p> Because the appearance function ae is many-to-one, two or more environmental (and model) states may have the same output and the learner's current observation may not uniquely determine what the current environmental (and model) state is. For example, in the SHAPE environment (copied from <ref> [ Rivest and Schapire, 1989 ] </ref> ) shown in Figure 2, there are two actions, x and y, and four environmental states I, II, III, and IV. A learner, however, can only see the two shapes of the nodes: 2 or 0. <p> The main idea is to use the concatenation of the actions in the existing LDEs as a homing sequence of the current model, and construct a homing function for identifying the model states that are reached at end of executions of the homing sequence. A homing sequence <ref> [ Rivest and Schapire, 1989 ] </ref> for an environment E = (A; Z; Q; ffi; ae; r) is a sequence of actions h such that the state reached by executing h from any state q 2 Q is uniquely determined by the observation sequence OS (q; h).
Reference: [ Rivest and Schapire, 1993 ] <author> R. Rivest and R. Schapire. </author> <title> Inference of finite automata using homing sequences. </title> <note> Information and Computation (to appear), </note> <year> 1993. </year>
Reference-contexts: But there are graphs that cause D fl to generate over 50 states and uses more than 30,000 actions before a correct model is learned. We are currently investigating solutions for this problem using adpative homing sequences <ref> [ Rivest and Schapire, 1993 ] </ref> . Finally, the motivation for this algorithm is rather peculiar. I would like to combine the abilities of concept learning and learning from delayed feedback in the surprise-identify-revise framework.
Reference: [ Shen, 1989 ] <author> W. Shen. </author> <title> Learning from the Environment Based on Actions and Percepts. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <year> 1989. </year>
Reference-contexts: can be used to construct both hidden states and homing sequences, there is no need for keeping separate copies of models or discarding the models when new LDEs are discovered. 2 The Basic Definitions Learning deterministic finite automata can be viewed as a special type of learning from the environment <ref> [ Shen, 1989; Shen, 1993a ] </ref> . The task of the learner is to construct an exact model of the environment to predict the consequences of its actions. <p> Clearly, the simple D algorithm must be extended to deal with these problems. 4 Model Construction with LDEs To solve the construction problem, we use a technique called surprise-identify-split <ref> [ Shen, 1989; Shen, 1993a ] </ref> . The main idea is that the learner always predicts the consequence of its action based on the model that is learned.
Reference: [ Shen, 1993a ] <author> W. Shen. </author> <title> Autonomous Learning from the Environment. W.H. </title> <publisher> Freeman, Computer Science Press, </publisher> <month> Forthcoming </month> <year> 1993. </year>
Reference-contexts: can be used to construct both hidden states and homing sequences, there is no need for keeping separate copies of models or discarding the models when new LDEs are discovered. 2 The Basic Definitions Learning deterministic finite automata can be viewed as a special type of learning from the environment <ref> [ Shen, 1989; Shen, 1993a ] </ref> . The task of the learner is to construct an exact model of the environment to predict the consequences of its actions. <p> Clearly, the simple D algorithm must be extended to deal with these problems. 4 Model Construction with LDEs To solve the construction problem, we use a technique called surprise-identify-split <ref> [ Shen, 1989; Shen, 1993a ] </ref> . The main idea is that the learner always predicts the consequence of its action based on the model that is learned.
Reference: [ Shen, 1993b ] <author> W. Shen. </author> <title> Discovery as autonomous learning from the environment. </title> <journal> Machine Learning, </journal> <volume> 10, </volume> <year> 1993. </year>
Reference-contexts: When a surprise occurs, depending on whether there is any visible difference between the conditions of a failed prediction and a successful prediction, this algorithm can either learn a concept (i.e., a class of states that share some common properties) using complementary discrimination learning <ref> [ Shen, 1993b ] </ref> , or create new state variables using the split technique here. In the latter case, this work can be viewed as an constructive approach to overcome inductive biases [ Utgoff, 1986 ] and perceptual aliasing.
Reference: [ Utgoff, 1986 ] <author> P. Utgoff. </author> <title> Machine Learning of Inductive Bias. </title> <publisher> Kluwer Academic, </publisher> <year> 1986. </year>
Reference-contexts: In the latter case, this work can be viewed as an constructive approach to overcome inductive biases <ref> [ Utgoff, 1986 ] </ref> and perceptual aliasing.
References-found: 7

