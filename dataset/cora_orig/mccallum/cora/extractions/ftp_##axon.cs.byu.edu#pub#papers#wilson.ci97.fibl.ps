URL: ftp://axon.cs.byu.edu/pub/papers/wilson.ci97.fibl.ps
Refering-URL: http://synapse.cs.byu.edu/~randy/misc/pubs.html
Root-URL: 
Email: E-mail: randy@axon.cs.byu.edu, martinez@cs.byu.edu  
Phone: Tel. (801) 378-6464  
Title: DISTANCE-WEIGHTING AND CONFIDENCE IN INSTANCE-BASED LEARNING  
Author: D. Randall Wilson and Tony R. Martinez 
Keyword: Neural Network Machine Learning Laboratory World-Wide Web: http://axon.cs.byu.edu  Key words: inductive learning, instance-based, nearest neighbor.  
Address: Provo, Utah 84602, USA  
Affiliation: Computer Science Department Brigham Young University  
Abstract: Many extensions have been proposed to help instance-based learning algorithms perform better on a wide variety of real-world applications. However, it is not trivial to decide what parameters or options to use when applying an instance-based learning algorithm to a particular problem. Traditionally, cross-validation has been used to choose some parameters such as k in a k nearest neighbor classifier. This paper points out why cross validation often does not provide enough information to allow for fine-tuning the classifier, and how confidence levels can be used to break ties that are all too common when cross-validation is used. It proposes the Fuzzy Instance Based Learning (FIBL) algorithm that uses distance-weighted voting with parameters set via a combination of cross-validation and confidence levels. In experiments on 31 datasets, FIBL had higher average generalization accuracy than using majority voting or using cross-validation alone to determine parameters. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, David W. </author> <year> 1992. </year> <title> Tolerating noisy, irrelevant and novel attributes in instance-based learning algorithms. </title> <journal> International Journal of Man-Machine Studies. </journal> <volume> 36. </volume> <pages> pp. 267-287. </pages>
Reference-contexts: However, if applied to the selection of k and other parameters, the Fuzzy k-NN system would have the same problem of favoring any parameters that increase the weight of nearer neighbors, regardless of their effect on classification accuracy, as discussed in Section 3.2. The IB4 algorithm <ref> (Aha 1992) </ref> is an instance-based learning algorithm that was designed to handle irrelevant attributes. It also finds class memberships similar to FIBL.
Reference: <author> Aha, David W., Dennis Kibler, Marc K. Albert. </author> <year> 1991. </year> <title> Instance-Based Learning Algorithms. </title> <journal> Machine Learning. </journal> <volume> 6, </volume> <pages> pp. 37-66. </pages>
Reference: <author> Batchelor, Bruce G. </author> <year> 1978. </year> <title> Pattern Recognition: </title> <booktitle> Ideas in Practice. </booktitle> <address> New York: </address> <publisher> Plenum Press. </publisher> <pages> pp. 71-72. </pages>
Reference-contexts: Heterogeneous Distance Function The distance function is critical to the success of an instance-based algorithm. A variety of distance functions are available, including the Minkowsky <ref> (Batchelor 1978) </ref>, Mahalanobis (Nadler & Smith 1993), Camberra, Chebychev, Quadratic, Correlation and Chisquare distance metrics (Michalski, Stepp & Diday 1981; Diday 1974); the ContextSimilarity measure (Biberman 1994); the Contrast Model (Tversky 1977); hyperrectangle distance functions (Salzberg 1991; Domingos 1995) and others.
Reference: <author> Biberman, Yoram. </author> <year> 1994. </year> <title> A Context Similarity Measure. </title> <booktitle> In Proceedings of the European Conference on Machine Learning (ECML-94). </booktitle> <address> Catalina, Italy: </address> <publisher> Springer Verlag. </publisher> <pages> pp. 49 - 63. </pages>
Reference-contexts: A variety of distance functions are available, including the Minkowsky (Batchelor 1978), Mahalanobis (Nadler & Smith 1993), Camberra, Chebychev, Quadratic, Correlation and Chisquare distance metrics (Michalski, Stepp & Diday 1981; Diday 1974); the ContextSimilarity measure <ref> (Biberman 1994) </ref>; the Contrast Model (Tversky 1977); hyperrectangle distance functions (Salzberg 1991; Domingos 1995) and others. Although there have been many distance functions proposed, by far the most commonly used is the Euclidean Distance function (which is a special case of the Minkowsky metric).
Reference: <author> Cost, Scott, and Steven Salzberg. </author> <year> 1993. </year> <title> A Weighted Nearest Neighbor Algorithm for Learning with Symbolic Features, </title> <journal> Machine Learning. </journal> <volume> 10. </volume> <pages> pp. 57-78. </pages>
Reference: <author> Cover, T. M., and P. E. Hart. </author> <year> 1967. </year> <title> Nearest Neighbor Pattern Classification, </title> <journal> Institute of Electrical and Electronics Engineers Transactions on Information Theory. </journal> <pages> 13-1, </pages> <month> January </month> <year> 1967. </year> <pages> pp. 21-27. </pages>
Reference-contexts: Such algorithms have had much success on a wide variety of applications (real-world classification tasks). The original nearest neighbor rule <ref> (Cover & Hart 1967) </ref> uses the nearest instance in T to a new input vector y to choose an output class.
Reference: <author> Dasarathy, Belur V. </author> <year> 1991. </year> <title> Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques, </title> <publisher> Los Alamitos, </publisher> <address> CA: </address> <publisher> IEEE Computer Society Press. </publisher>
Reference: <author> Diday, Edwin. </author> <year> 1974. </year> <title> Recent Progress in Distance and Similarity Measures in Pattern Recognition. </title> <booktitle> Second International Joint Conference on Pattern Recognition. </booktitle> <pages> pp. 534-539. </pages>
Reference: <author> Domingos, Pedro. </author> <year> 1995. </year> <title> Rule Induction and Instance-Based Learning: A Unified Approach, </title> <booktitle> in The 1995 International Joint Conference on Artificial Intelligence (IJCAI-95). </booktitle>
Reference: <author> Dudani, Sahibsingh A. </author> <year> 1976. </year> <title> The Distance-Weighted k-Nearest-Neighbor Rule, </title> <journal> IEEE Transactions on Systems, Man and Cybernetics. </journal> <pages> 6-4, </pages> <month> April </month> <year> 1976. </year> <pages> pp. 325-327. </pages>
Reference: <author> Giraud-Carrier, Christophe, and Tony Martinez. </author> <year> 1995. </year> <title> An Efficient Metric for Heterogeneous Inductive Learning Applications in the Attribute-Value Language, </title> <booktitle> Intelligent Systems. </booktitle> <pages> pp. 341-350. </pages>
Reference: <author> Keller, James M., Michael R. Gray, and James A. Givens, Jr. </author> <year> 1985. </year> <title> A Fuzzy K-Nearest Neighbor Algorithm, </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics. </journal> <pages> 15-4, </pages> <month> July/August </month> <year> 1985. </year> <pages> pp. 580-585. </pages> <note> 15 Kohavi, </note> <author> Ron. </author> <year> 1995. </year> <title> A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection, </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI95). </booktitle>
Reference: <author> Lebowitz, Michael. </author> <year> 1985. </year> <title> Categorizing Numeric Information for Generalization. </title> <journal> Cognitive Science. </journal> <volume> 9. </volume> <pages> pp. 285-308. </pages>
Reference-contexts: On the other hand, the VDM does not directly handle linear attributes but instead requires discretization <ref> (Lebowitz 1985) </ref>, which can lead to inferior generalization accuracy (Ventura & Martinez 1995). We previously introduced several new heterogeneous distance functions that substantially improved average generalization accuracy on a collection of 48 different datasets (Wilson & Martinez 1997).
Reference: <author> Merz, C. J., and P. M. Murphy. </author> <year> 1996. </year> <title> UCI Repository of Machine Learning Databases. </title> <address> Irvine, CA: </address> <institution> University of California Irvine, Department of Information and Computer Science. Internet: </institution> <address> http://www.ics.uci.edu/~mlearn/ MLRepository.html. </address>
Reference-contexts: During execution, classification takes O ( mn) time, which is the same as the basic nearest neighbor rule. 5. EXPERIMENTAL RESULTS The Fuzzy Instance-Based Learning (FIBL) algorithm was implemented and tested on 31 applications from the Machine Learning Database Repository at the University of California, Irvine <ref> (Merz & Murphy 1996) </ref>. FIBL was compared to a static instance-based learning algorithm that is identical to FIBL except that it uses k=3 and majority voting and thus does not fine-tune parameters.
Reference: <author> Michalski, Ryszard S., Robert E. Stepp, and Edwin Diday. </author> <year> 1981. </year> <title> A Recent Advance in Data Analysis: Clustering Objects into Classes Characterized by Conjunctive Concepts. Progress in Pattern Recognition. 1, </title> <editor> Laveen N. Kanal and Azriel Rosenfeld (Eds.). </editor> <address> New York: </address> <publisher> North-Holland. </publisher> <pages> pp. 33-56. </pages>
Reference: <author> Moore, Andrew W., and Mary S. Lee. </author> <year> 1993. </year> <title> Efficient Algorithms for Minimizing Cross Validation Error. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: LCV is often described as being desirable but computationally expensive <ref> (Moore & Lee 1993) </ref>. However, in our implementation (as described in Section 4), LCV is performed efficiently during the learning stage in a way that actually makes it faster than using larger partitions.
Reference: <author> Nadler, Morton, and Eric P. Smith. </author> <year> 1993. </year> <title> Pattern Recognition Engineering. </title> <address> New York: </address> <publisher> Wiley. </publisher> <pages> pp. 293-294. </pages>
Reference-contexts: Heterogeneous Distance Function The distance function is critical to the success of an instance-based algorithm. A variety of distance functions are available, including the Minkowsky (Batchelor 1978), Mahalanobis <ref> (Nadler & Smith 1993) </ref>, Camberra, Chebychev, Quadratic, Correlation and Chisquare distance metrics (Michalski, Stepp & Diday 1981; Diday 1974); the ContextSimilarity measure (Biberman 1994); the Contrast Model (Tversky 1977); hyperrectangle distance functions (Salzberg 1991; Domingos 1995) and others.
Reference: <author> Rachlin, John, Simon Kasif, Steven Salzberg, David W. Aha. </author> <year> 1994. </year> <title> Towards a Better Understanding of Memory-Based and Bayesian Classifiers. </title> <booktitle> in Proceedings of the Eleventh International Machine Learning Conference, </booktitle> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher> <pages> pp. 242-250. </pages>
Reference: <author> Salzberg, Steven. </author> <year> 1991. </year> <title> A Nearest Hyperrectangle Learning Method. </title> <journal> Machine Learning. </journal> <volume> 6. </volume> <pages> pp. 277-309. </pages>
Reference: <author> Schaffer, Cullen. </author> <year> 1993. </year> <title> Selecting a Classification Method by Cross-Validation. </title> <booktitle> Machine Learning. </booktitle>
Reference: <author> Stanfill, C., and D. Waltz. </author> <year> 1986. </year> <title> Toward memory-based reasoning. </title> <journal> Communications of the ACM. </journal> <volume> 29, </volume> <month> December </month> <year> 1986. </year> <pages> pp. 1213-1228. </pages>
Reference: <author> Tversky, Amos. </author> <year> 1977. </year> <title> Features of Similarity. </title> <journal> Psychological Review. </journal> <pages> 84-4. pp. 327-352. </pages>
Reference-contexts: A variety of distance functions are available, including the Minkowsky (Batchelor 1978), Mahalanobis (Nadler & Smith 1993), Camberra, Chebychev, Quadratic, Correlation and Chisquare distance metrics (Michalski, Stepp & Diday 1981; Diday 1974); the ContextSimilarity measure (Biberman 1994); the Contrast Model <ref> (Tversky 1977) </ref>; hyperrectangle distance functions (Salzberg 1991; Domingos 1995) and others. Although there have been many distance functions proposed, by far the most commonly used is the Euclidean Distance function (which is a special case of the Minkowsky metric).
Reference: <author> Ventura, Dan, and Tony R. </author> <title> Martinez (1995. An Empirical Comparison of Discretization Methods. </title> <booktitle> In Proceedings of the Tenth International Symposium on Computer and Information Sciences. </booktitle> <pages> pp. 443-450. </pages>
Reference-contexts: On the other hand, the VDM does not directly handle linear attributes but instead requires discretization (Lebowitz 1985), which can lead to inferior generalization accuracy <ref> (Ventura & Martinez 1995) </ref>. We previously introduced several new heterogeneous distance functions that substantially improved average generalization accuracy on a collection of 48 different datasets (Wilson & Martinez 1997). FIBL uses one of the most successful functions, the Heterogeneous Value Difference Metric (HVDM).
Reference: <author> Watson, I., and F. Marir. </author> <year> 1994. </year> <title> Case-Based Reasoning: A Review. </title> <journal> The Knowledge Engineering Review. </journal> <pages> 9-4. </pages>
Reference-contexts: Related paradigms include memory-based reasoning methods (Stanfill & Waltz 1986; Cost & Salzberg 1993; Rachlin et al. 1994), exemplar-based generalization (Salzberg 1991; Wettschereck & Dietterich 1995), and case-based reasoning (CBR) <ref> (Watson & Marir 1994) </ref>. Such algorithms have had much success on a wide variety of applications (real-world classification tasks). The original nearest neighbor rule (Cover & Hart 1967) uses the nearest instance in T to a new input vector y to choose an output class.
Reference: <author> Wettschereck, Dietrich, and Thomas G. Dietterich. </author> <year> 1995. </year> <title> An Experimental Comparison of Nearest-Neighbor and Nearest-Hyperrectangle Algorithms. </title> <booktitle> Machine Learning. </booktitle> <pages> 19-1. pp. 5-28. </pages>
Reference: <author> Wilson, D. Randall, and Tony R. Martinez. </author> <year> 1997. </year> <title> Improved Heterogeneous Distance Functions. </title> <journal> Journal of Artificial Intelligence Research. </journal> <pages> 6-1. pp. 1-34. </pages>
Reference-contexts: We previously introduced several new heterogeneous distance functions that substantially improved average generalization accuracy on a collection of 48 different datasets <ref> (Wilson & Martinez 1997) </ref>. FIBL uses one of the most successful functions, the Heterogeneous Value Difference Metric (HVDM). This distance function is briefly defined below, and more details on this distance function are available in (Wilson & Martinez 1997). <p> distance functions that substantially improved average generalization accuracy on a collection of 48 different datasets <ref> (Wilson & Martinez 1997) </ref>. FIBL uses one of the most successful functions, the Heterogeneous Value Difference Metric (HVDM). This distance function is briefly defined below, and more details on this distance function are available in (Wilson & Martinez 1997). The HVDM distance function defines the distance between two input vectors x and y as 6 [6] HVDM (x, y) = d a (x a , y a ) 2 a=1 where m is the number of attributes. <p> All of these algorithms have substantially higher generalization accuracy than the basic nearest neighbor rule using a Euclidean distance function <ref> (Wilson & Martinez 1997) </ref>. 6. RELATED WORK The standard k-nearest neighbor algorithm uses majority voting.
Reference: <author> Zadeh, Lotfi A. </author> <year> 1965. </year> <title> Fuzzy Sets. </title> <journal> Information and Control. </journal> <volume> 8. </volume> <pages> pp. 338-353. </pages>
Reference-contexts: The Fuzzy Instance-Based Learning (FIBL) algorithm, on the other hand, uses principles from fuzzy logic <ref> (Zadeh 1965) </ref> to find a class membership of the input vector for each class. The class with the highest class membership is used as the output class, which is similar to using the majority class in the k-NN scheme. <p> The confidence for each instance is [9] conf = votes correct votes c c=1 where votes correct is the sum of weighted votes received for the correct class and votes c is the sum of weighted votes received for class c. In terms of fuzzy logic <ref> (Zadeh 1965) </ref>, the confidence of each instance can be thought of as its class membership in the correct class. When majority voting is used, votes c is simply a count of how many of the k nearest neighbors were of class c, since the weights are all equal to 1.
References-found: 27

