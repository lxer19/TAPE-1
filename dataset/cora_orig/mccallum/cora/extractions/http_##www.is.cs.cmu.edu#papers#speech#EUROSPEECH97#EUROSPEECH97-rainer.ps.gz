URL: http://www.is.cs.cmu.edu/papers/speech/EUROSPEECH97/EUROSPEECH97-rainer.ps.gz
Refering-URL: http://www.is.cs.cmu.edu/ISL.multimodal.publications.html
Root-URL: 
Email: fstiefeljuweg@ira.uka.de, yang+@cs.cmu.edu  
Title: REAL-TIME LIP-TRACKING FOR LIPREADING  
Author: Rainer Stiefelhagen, Uwe Meier, Jie Yang 
Address: USA  
Affiliation: Interactive Systems Laboratories University of Karlsruhe Germany, Carnegie Mellon University  
Abstract: This paper presents a new approach to lip tracking for lipreading. Instead of only tracking features on lips, we propose to track lips along with other facial features such as pupils and nostril. In the new approach, the face is first located in an image using a stochastic skin-color model, the eyes, lip-corners and nostrils are then located and tracked inside the facial region. The new approach can effectively improve the robustness of lip-tracking and simplify automatic detection and recovery of tracking failure. The feasibility of the proposed approach has been demonstrated by implementation of a lip tracking system. The system has been tested by a database that contains 900 image sequences of different speakers spelling words. The system has successfully extract lip regions from the image sequences to obtain training data for the audio-visual speech recognition system. The system has been also applied to extract the lip region in real-time from live video images to obtain the visual input for an audio-visual speech recognition system. On test sequences we have achieved a reduction of the number of frames with tracking failures by a factor of two using detection and prediction of outliers in the set of found features. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Mase and Alex Pentland. </author> <title> Automatic lipreading by optical flow analysis. </title> <journal> Systems and Computers in Japan, </journal> <volume> 22(6):67 - 76, </volume> <year> 1991. </year>
Reference-contexts: 1. INTRODUCTION It has been demonstrated that not only humans benefit from visual input in speech perception and language understanding, but also the performance of automatic speech recognition systems can be significantly improved by adding lip movement information to the acoustic data <ref> [1, 2, 3] </ref>. A major problem of the previous systems was the intrusive way in which the user had to interact with the speech-reading systems.
Reference: [2] <author> E.D. Petajan. </author> <title> Automatic lipreading to enhance speech recognition. </title> <booktitle> In Proceedings of IEEE Communications Society Global Telecom. Conference, </booktitle> <month> November </month> <year> 1984. </year>
Reference-contexts: 1. INTRODUCTION It has been demonstrated that not only humans benefit from visual input in speech perception and language understanding, but also the performance of automatic speech recognition systems can be significantly improved by adding lip movement information to the acoustic data <ref> [1, 2, 3] </ref>. A major problem of the previous systems was the intrusive way in which the user had to interact with the speech-reading systems.
Reference: [3] <author> D. G. Stork, G. Wolff, and E. Levine. </author> <title> Neural network lipreading system for improved speech recognition. </title> <booktitle> In Proceedings of IJCNN, </booktitle> <year> 1992. </year>
Reference-contexts: 1. INTRODUCTION It has been demonstrated that not only humans benefit from visual input in speech perception and language understanding, but also the performance of automatic speech recognition systems can be significantly improved by adding lip movement information to the acoustic data <ref> [1, 2, 3] </ref>. A major problem of the previous systems was the intrusive way in which the user had to interact with the speech-reading systems.
Reference: [4] <author> Paul Duchnowski, Uwe Meier, and Alex Waibel. </author> <title> See me, hear me: Integrating automatic speech recognition and lipreading. </title> <booktitle> In Proceedings of ICSLP, </booktitle> <year> 1994. </year>
Reference-contexts: In order to acquire the visual data the user had to wear head-mounted cameras or reflective markers on his/her lips, or the relevant lip-region had to be extracted manually. Over the last 4 years, the Interactive Systems Laboratory has been developing lip-reading techniques to enhance speech recognition <ref> [4, 5, 6] </ref>. We have attempted to achieve non-intrusive human-computer interaction and free the users from these interferences from very beginning. We have been developing lipreading systems based on a modular MS-TDNN structure. <p> We have been developing lipreading systems based on a modular MS-TDNN structure. The visual and acoustic TDNNs are trained separately, and visual and acoustic information are combined at the phonetic level. In the system described in <ref> [4] </ref>, the data acquisition was automatic and without any marker on the user's face, but the process required the speaker to position his lips in a window shown on a workstation screen.
Reference: [5] <author> Paul Duchnowski, Martin Hunke, Dietrich Busching, and Uwe Meier. </author> <title> Toward movement-invariant automatic lip-reading and speech recognition. </title> <booktitle> In Proceedings of International Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <year> 1995. </year>
Reference-contexts: In order to acquire the visual data the user had to wear head-mounted cameras or reflective markers on his/her lips, or the relevant lip-region had to be extracted manually. Over the last 4 years, the Interactive Systems Laboratory has been developing lip-reading techniques to enhance speech recognition <ref> [4, 5, 6] </ref>. We have attempted to achieve non-intrusive human-computer interaction and free the users from these interferences from very beginning. We have been developing lipreading systems based on a modular MS-TDNN structure. <p> In the system described in [4], the data acquisition was automatic and without any marker on the user's face, but the process required the speaker to position his lips in a window shown on a workstation screen. In the system described in <ref> [5] </ref>, we integrated a face tracking and a lip-localization module into our system which allowed the user to freely position himself/herself in the view of the camera. The system is for speaker dependent continuous spelling of German letters.
Reference: [6] <author> Uwe Meier, Wolfgang Hurst, and Paul Duch-nowski. </author> <title> Adaptive bimodal sensor fusion for automatic speechreading. </title> <booktitle> In Proceedings of International Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <year> 1996. </year>
Reference-contexts: In order to acquire the visual data the user had to wear head-mounted cameras or reflective markers on his/her lips, or the relevant lip-region had to be extracted manually. Over the last 4 years, the Interactive Systems Laboratory has been developing lip-reading techniques to enhance speech recognition <ref> [4, 5, 6] </ref>. We have attempted to achieve non-intrusive human-computer interaction and free the users from these interferences from very beginning. We have been developing lipreading systems based on a modular MS-TDNN structure.
Reference: [7] <author> Jie Yang and Alex Waibel. </author> <title> A real-time face tracker. </title> <booktitle> In Proceedings of WACV, </booktitle> <pages> pages 142-147, </pages> <year> 1996. </year>
Reference-contexts: We will discuss these tracking procedures below. 2.2. Searching the Face To find and track the face, we use a statistical color-model consisting of a two-dimensional Gaussian distribution of normalized skin colors <ref> [7] </ref>. The input image is searched for pixels with skin colors and the largest connected region of skin-colored pixels in the camera-image is considered as the region of the face.
Reference: [8] <author> Takeo Kanade. </author> <title> Picture processing by computer complex and recognition of human faces. </title> <type> Technical report, </type> <institution> Kyoto Univ., Dept. Inform. Sci., </institution> <year> 1973. </year>
Reference-contexts: This approach to search the lip corners using integral projections is based on ideas already described for example in <ref> [8] </ref>. We have developped a method to track lip corners in real-time in an illumination-independent way. Our approach consists of the following steps: 1. Search the darkest pixel in a search-region right of the predicted position of the left corner and left of the predicted position of the right corner.
Reference: [9] <author> Daniel F. DeMenthon and Larry S. Davis. </author> <title> Model based object pose in 25 lines of code. </title> <booktitle> In Proceedings of Second European Conference on Computer Vision, </booktitle> <pages> pages 335 - 343. </pages> <publisher> Springer Verlag, </publisher> <month> May </month> <year> 1992. </year>
Reference-contexts: Given the 3D locations of the facial feature points in a simple head-model and their locations in the camera image, the rotation and translation (pose) of the head with respect to the camera can be computed from these 3D- to 2D-point correspondences <ref> [9, 10] </ref>. The computed pose can be described through a 3x3 rotation matrix R and translation vector ~ t which maps the head coordinate system onto the camera coordinate system.
Reference: [10] <author> Rainer Stiefelhagen, Jie Yang, and Alex Waibel. </author> <title> A model-based gaze tracking system. </title> <booktitle> In Proceedings of IEEE International Joint Symposia on Intelligence and Systems, </booktitle> <pages> pages 304 - 310, </pages> <year> 1996. </year>
Reference-contexts: Given the 3D locations of the facial feature points in a simple head-model and their locations in the camera image, the rotation and translation (pose) of the head with respect to the camera can be computed from these 3D- to 2D-point correspondences <ref> [9, 10] </ref>. The computed pose can be described through a 3x3 rotation matrix R and translation vector ~ t which maps the head coordinate system onto the camera coordinate system.
Reference: [11] <author> Andrew H. Gee and Roberto Cipolla. </author> <title> Fast visual tracking by temporal consensus. </title> <type> Technical Report CUED/F-INFENG/TR-207, </type> <institution> University of Cambridge, </institution> <month> February </month> <year> 1995. </year>
Reference-contexts: The features that are not included in this best set are considered as outliers. To find a best subset we used a method proposed by Gee & Cipolla <ref> [11] </ref>. Using this method, the subset that leads to the pose implying the smoothest motion of the head is chosen as the best subset.
Reference: [12] <author> Uwe Meier, Rainer Stiefelhagen, and Jie Yang. </author> <title> Preprocessing of visual speech under real world conditions. </title> <booktitle> In Proceedings of European Tutorial & Research Workshop on Audio-Visual Speech Processing, </booktitle> <year> 1997. </year>
Reference-contexts: Figure 6 shows some sample images of this set. Average location error for the lip-corners on these sequences was 3 pixel in x-direction and 2 pixel in y-direction as shown in table 1. In addition, to obtain new training data for our current audio-visual speech recognition system <ref> [12] </ref>, we ran the lip-tracking system on all sequences of a database of ten different male and female speakers, which were spelling words. The database contains ninehundred greyscale image sequences with a total of around 80.000 frames. In these images faces of the speakers covered the whole images. <p> Even when the speaker is moving or rotates his head, the lip-corners can successfully be tracked. On camera images of size 160x120 pixel as used for our current speech-reading system <ref> [12] </ref> the lip-tracker runs at frame rates of around 25 frames per second. In addition, we have examined the number of frames where tracking failure occurred in the test sequences in set 1 (sequences with head movement). The results are shown in table 2.
References-found: 12

