URL: http://www.doc.ntu.ac.uk/Papers/tgr/vegas_cr.ps
Refering-URL: http://www.doc.ntu.ac.uk/HAND/Papers/index.html
Root-URL: 
Title: A Context-Based Approach to Text Recognition  
Author: T.G. Rose, L.J. Evett and A.C. Jobbins 
Address: Nottingham NG1 4BU England  
Affiliation: Dept. of Computing, Nottingham Trent University  
Abstract: e.g. Cattell [3]) have shown that characters are more easily recognised when they form part of a word than when they do not. Studies of eye movement during the reading process provide further evidence of the role of higher level knowledge. Javal [4] showed that the eyes do not scan smoothly across the lines of print, but instead make a series of discrete fixations with rapid movements in between. Analysis of these eye fixations during reading provides insight into the visual information being processed. For example, Just and Carpenter [5] showed that typically only 68% of the words may be fixated during normal reading, suggesting that higher level knowledge must contribute to the processing of the remaining 32%. Furthermore, they showed that over 80% of the content words were fixated, compared to only 40% of the function words. For this distinction to take place higher-level knowledge must be affecting the reading process. The performance of text recognition systems may be improved by applying higher-level knowledge in the form of contextual information. However, the acquisition of such information for a realistically sized vocabulary presents a major problem, since hand-coding is feasible for only the smallest of vocabularies. This paper describes a number of methods for extracting contextual knowledge from text corpora, and compares the effect of each on the performance of text recognition systems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Rayner, M. Carlson & L. </author> <title> Frazier (1983) "The interaction of syntax and semantics during sentence processing: Eye movements in the analysis of semantically biased sentences", </title> <journal> Journal of Verbal Learning and Verbal Behavior, </journal> <volume> 22 , pp.358-374. </volume>
Reference-contexts: This is largely because human readers use an understanding of the text that can guide the reading process. Word images occur within a meaningful context, and human readers are able to exploit the syntactic and semantic constraints of the textual material <ref> [1] </ref>. Indeed, it is argued that the conspicuous gap between the reading performance of people and that of machines may reflect the fact that few text recognition systems utilise the many knowledge sources or recognition strategies of the human reader [2].
Reference: [2] <author> J.J. </author> <title> Hull (1987) 'A computational theory and algorithm for fluent reading', </title> <booktitle> Proc. 3rd. IEEE Conf. on AI, </booktitle> <address> pp.176-181. </address>
Reference-contexts: Indeed, it is argued that the conspicuous gap between the reading performance of people and that of machines may reflect the fact that few text recognition systems utilise the many knowledge sources or recognition strategies of the human reader <ref> [2] </ref>. By analogy, therefore, computational text recognition systems could be improved by exploiting such information.
Reference: [3] <author> J.M. </author> <title> Cattell (1885) 'The inertia of eye and brain', Brain, </title> <type> 8 </type> . 
Reference: [4] <author> E. </author> <title> Javal (1879) 'Essai sur la physiologie de la lecture', </title> <journal> Annales D'Occulistique, </journal> <volume> 82 , pp.242-253. </volume>
Reference: [5] <author> M.A. </author> <title> Just & P.A. Carpenter (1987) "The Psychology of Reading and Language Comprehension", </title> <publisher> Allyn & Bacon Inc., </publisher> <address> Boston. </address>
Reference: [6] <author> T.G. Rose & L.J. </author> <title> Evett (1993a) "Semantic analysis for large vocabulary cursive script recognition", </title> <booktitle> Proc. of Second IAPR Conference on Document Analysis and Recognition, </booktitle> <address> Tsukuba Science City, Japan. </address>
Reference-contexts: A more practical approach is to exploit existing sources such as machine-readable dictionaries <ref> [6] </ref> and text corpora [7]. There is much evidence to suggest that there is more to the process of reading than just the recognition of individual characters.
Reference: [7] <author> T.G. Rose & L.J. </author> <title> Evett (1993b) "Text recognition using collocations and domain codes", </title> <booktitle> Proc. of Workshop on Very Large Corpora, </booktitle> <institution> Ohio State University, pp.65-73. </institution>
Reference-contexts: A more practical approach is to exploit existing sources such as machine-readable dictionaries [6] and text corpora <ref> [7] </ref>. There is much evidence to suggest that there is more to the process of reading than just the recognition of individual characters. <p> In the case of the former, it has been possible to test the z-score collocations using genuine data from an existing system. It was found that collocation analysis identified the correct word in 81.58% of cases, with 7.89% incorrect and the remaining 10.53% as ties <ref> [7] </ref>. Rose, Evett and Jobbins Evidently, there are a number of limitations to the bigram methods. Firstly, although it is typical in research of this type to use lemmatised types and ignore function words [16], it is inevitable that some collocations will only exist in particular inflected forms [17]. <p> This constitutes a further constraint that could be effectively exploited. One final technique by which error-rates may be reduced is to adopt a domainspecific approach. Indeed, such an approach has already been shown to be effective using domainspecific z-score collocations <ref> [7] </ref>, but it remains to be seen to what extent association ratios would share this characteristic.
Reference: [8] <author> F. </author> <title> Smadja (1989) "Macrocoding the lexicon with cooccurrence knowledge", </title> <booktitle> Proc. 1st International Lexical Acquisition Workshop, </booktitle> <address> Detroit, Michigan, pp.197-204. </address>
Reference-contexts: For example, in Figure 1 the word "account" may be favoured over "gallant" and "accept" since it is more likely than the others to cooccur with "savings". Such predisposed combinations are called cooccurrence relations or collocations, and account for a large proportion of English word combinations <ref> [8] </ref>. 1.1 Text Recognition Systems Due to its greater inherent degree of ambiguity, handwritten text is seen as the main application of the following techniques.
Reference: [9] <author> I. </author> <title> Lancashire (1987) "Using a Textbase for English-language research", </title> <booktitle> Proc. 3rd Ann. Conf. of the UWC for the New Oxford English Dictionary, </booktitle> <address> Waterloo. </address>
Reference-contexts: The remaining strings are then combined to produce possible phrases. There are a number of methods by which collocations may be acquired from text corpora. Lancashire <ref> [9] </ref> describes a simple statistical procedure known as the z-score, which puts an arbitrary value on the strength For example, consider the sentence "this is a new savings account which you can open with one pound" written as input to the A Context-Based Approach to Text Recognition of association between a
Reference: [10] <author> K. Church & P. </author> <title> Hanks (1989) "Word association norms, mutual information and lexicography", </title> <booktitle> Proc. 27th Meeting of the ACL, </booktitle> <address> pp.76-83. </address>
Reference-contexts: Moreover, if this process is performed on all the words in a lexicon, the output constitutes a "collocation dictionary" for that lexicon. Another measure of cooccurrence is the association ratio <ref> [10] </ref>, which compares the probability of observing two words together (the joint probability) with the probability of observing them independently (by chance). An important difference between this method and the z-score method is that the association ratio is nonsymmetric, i.e. it encodes linear precedence.
Reference: [11] <author> F.G. </author> <title> Keenan (1990) 'The Use of Linguistic Knowledge in a Handwriting Recognition System', Unpublished Transfer Report for CNAA, </title> <institution> Nottingham Polytechnic, </institution> <address> England. </address>
Reference-contexts: Unigram Statistics Due to constraints on the availability of hardware it was not always possible to obtain genuine handwritten (or OCR) data. As large data samples were required, programs were written to facilitate this process by simulating recogniser output for any given sentence. This simulator program <ref> [11] </ref> was designed to give output similar to the genuine recogniser, by working on the same database of characters, Rose, Evett and Jobbins Correct (%) Tied (%) Incorrect (%) Applied Science 77.80 0.00 22.20 Commerce 80.26 0.20 19.54 Pure Science 82.59 0.00 17.41 Social Science 80.62 0.19 19.19 World Affairs 86.32
Reference: [12] <author> D. </author> <title> Summers (1991) 'Longman /Lancaster English language corpus: criteria and design', </title> <type> Technical Report, Longman. </type>
Reference-contexts: The complete test data sample therefore consisted of over 10,000 words of recognition data (i.e. 10,000 word positions with alternative candidates as in the above example). A word-frequency distribution was derived from a balanced 5 million-word subset of the Longman Corpus <ref> [12] </ref>. For each of the 20 documents, the word candidates were assigned scores equal to their corpus frequency. Once all the data had been processed, the candidates with the highest scores in each position were examined to see what proportion were actually correct.
References-found: 12

