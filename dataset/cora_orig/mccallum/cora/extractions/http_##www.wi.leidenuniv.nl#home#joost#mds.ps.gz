URL: http://www.wi.leidenuniv.nl/home/joost/mds.ps.gz
Refering-URL: http://www.wi.leidenuniv.nl/home/joost/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: Email: fmichiel, joost, kostersg@wi.leidenuniv.nl  
Title: Two Neural Network Methods for Multidimensional Scaling  
Author: Michiel C. van Wezel, Joost N. Kok and Walter A. Kosters 
Date: December 3, 1996  
Address: P.O. Box 9512, 2300 RA Leiden, The Netherlands  
Affiliation: Leiden University Dept. of Mathematics and Computer Science  
Abstract: Multidimensional scaling (MDS) embeds points in a Euclidean space given only dissimilarity data. Only very recently MDS has gotten some attention from neural network researchers. We propose two neural network methods for MDS and evaluate them using both artificially generated and real data. Training uses two inputs at a time.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Davidson, Mark L., </author> <title> Multidimensional Scaling, Wiley Series in Probability and Mathematical Statistics, </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: Moreover, excellent results are already obtained with our simpler methods. In this paper, two new neural network methods for MDS are proposed that only assume dissimilarities between objects, analogous to the original MDS formulation (see e.g. [6] or a textbook on MDS, e.g. <ref> [1, 5] </ref>). The first neural network is a feedforward neural network trained with a gradient based learning 1 rule. The second neural network is an unsupervised competitive neural network. Both networks are tested on both artificial and real data.
Reference: [2] <author> Hofmann, Thomas and Joachim M. Buhmann, </author> <title> Multidimensional Scaling and Data Clustering, </title> <booktitle> in Advances in Neural Information Processing Systems 7, </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1995. </year>
Reference-contexts: Dissimilarities between objects are monotonically related to Euclidean distances between the objects. Various types of MDS procedures and objective functions have been presented in the past. Recently MDS also got some attention from neural network researchers <ref> [2, 3] </ref>. However, in [3] the focus is on MDS as a dimensionality reduction technique, and the proposed method still assumes coordinate representations for all the objects. <p> Recently MDS also got some attention from neural network researchers [2, 3]. However, in [3] the focus is on MDS as a dimensionality reduction technique, and the proposed method still assumes coordinate representations for all the objects. In <ref> [2] </ref> a mean field approach to the MDS problem is presented, which does not work as intuitively as the methods we will present. Moreover, excellent results are already obtained with our simpler methods.
Reference: [3] <author> Lowe, David and Michael E. Tipping, </author> <title> Feed-forward Neural Networks and Topographic Mappings for Exploratory Data Analysis, </title> <booktitle> Neural Computing and Applications 4, </booktitle> <pages> 83-95, </pages> <year> 1996. </year>
Reference-contexts: Dissimilarities between objects are monotonically related to Euclidean distances between the objects. Various types of MDS procedures and objective functions have been presented in the past. Recently MDS also got some attention from neural network researchers <ref> [2, 3] </ref>. However, in [3] the focus is on MDS as a dimensionality reduction technique, and the proposed method still assumes coordinate representations for all the objects. <p> Dissimilarities between objects are monotonically related to Euclidean distances between the objects. Various types of MDS procedures and objective functions have been presented in the past. Recently MDS also got some attention from neural network researchers [2, 3]. However, in <ref> [3] </ref> the focus is on MDS as a dimensionality reduction technique, and the proposed method still assumes coordinate representations for all the objects. In [2] a mean field approach to the MDS problem is presented, which does not work as intuitively as the methods we will present.
Reference: [4] <author> Mao, Jianchang and Anil K. Jain, </author> <title> Artificial Neural Networks for Feature Extraction and Multiviariate Data Projection, </title> <journal> IEEE Transactions on Neural Networks 6, </journal> <pages> 296-317, </pages> <year> 1995. </year>
Reference-contexts: Training is stopped if a maximum number of iterations is exceeded or STRESS drops below a certain threshold. This training procedure, where two patterns are selected each time the weights are updated, is similar to the training procedure proposed for the SAMANN network in <ref> [4] </ref>. 3 2.2 Description of the Unsupervised Neural Network The unsupervised neural network works simpler and more intuitive than the feedforward neural network. In the unsupervised neural network, every object is represented by a neuron.
Reference: [5] <author> Schiffman, Susan S., M. Lance Reynolds and Forrest W. Young, </author> <title> Introduction to Multidimensional Scaling | Theory, Methods and Applications, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: Moreover, excellent results are already obtained with our simpler methods. In this paper, two new neural network methods for MDS are proposed that only assume dissimilarities between objects, analogous to the original MDS formulation (see e.g. [6] or a textbook on MDS, e.g. <ref> [1, 5] </ref>). The first neural network is a feedforward neural network trained with a gradient based learning 1 rule. The second neural network is an unsupervised competitive neural network. Both networks are tested on both artificial and real data.
Reference: [6] <author> Wish, Myron and J. Douglas Carroll, </author> <title> Multidimensional Scaling and its Applications, </title> <editor> in Krishnaiah, P. R. and L.N. Kanal (eds.): </editor> <booktitle> Handbook of Statistics, </booktitle> <volume> Vol. </volume> <month> 2: </month> <title> Classification, Pattern Recognition and Reduction of Dimensionality, </title> <address> 317-345, </address> <publisher> North Holland, </publisher> <address> Amsterdam, </address> <year> 1982. </year> <month> 7 </month>
Reference-contexts: Moreover, excellent results are already obtained with our simpler methods. In this paper, two new neural network methods for MDS are proposed that only assume dissimilarities between objects, analogous to the original MDS formulation (see e.g. <ref> [6] </ref> or a textbook on MDS, e.g. [1, 5]). The first neural network is a feedforward neural network trained with a gradient based learning 1 rule. The second neural network is an unsupervised competitive neural network. Both networks are tested on both artificial and real data.
References-found: 6

