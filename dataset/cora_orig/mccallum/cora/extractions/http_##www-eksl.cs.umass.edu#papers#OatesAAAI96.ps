URL: http://www-eksl.cs.umass.edu/papers/OatesAAAI96.ps
Refering-URL: http://eksl-www.cs.umass.edu/publications.html
Root-URL: 
Email: oates@cs.umass.edu, cohen@cs.umass.edu  
Title: Searching for Planning Operators with Context-Dependent and Probabilistic Effects  
Author: Tim Oates and Paul R. Cohen 
Address: Box 34610 Amherst, MA 01003-4610  
Affiliation: Computer Science Department, LGRC University of Massachusetts  
Abstract: Providing a complete and accurate domain model for an agent situated in a complex environment can be an extremely difficult task. Actions may have different effects depending on the context in which they are taken, and actions may or may not induce their intended effects, with the probability of success again depending on context. We present an algorithm for automatically learning planning operators with context-dependent and probabilistic effects in environments where exogenous events change the state of the world. Empirical results show that the algorithm successfully finds operators that capture the true structure of an agent's interactions with its environment, and avoids spurious associations between actions and exogenous events. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Benson, S. </author> <year> 1995. </year> <title> Inductive learning of reactive action models. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> 47-54. </pages>
Reference-contexts: One notable exception is <ref> (Benson 1995) </ref>, in which the primary effect of a durative action is assumed to be deterministic, but side effects may occur with some probability. In contrast, the work described in this paper applies to domains that contain uncertainties associated with the outcomes of actions, and noise from exogenous events. <p> Much of the work on learning planning operators assumes the availability of fairly sophisticated forms of domain knowledge, such as advice or problem solving traces generated by domain experts <ref> (Benson 1995) </ref> (Wang 1995), or initial approximate planning operators (Gil 1994). Our approach assumes that the learning agent initially knows nothing of the dynamics of its environment.
Reference: <author> Fikes, R. E., and Nilsson, N. J. </author> <year> 1971. </year> <title> STRIPS: A new approach to the application of theorem proving to problem solving. </title> <booktitle> Artificial Intelligence 2(2) </booktitle> <pages> 189-208. </pages>
Reference-contexts: GD, HBg A = fDRY, NEW, PAINT, PICKUPg T ACTION = fDRY, NEW, PAINT, PICKUP, NONEg T BP = fBP, NOT-BPg; T GC = fGC, NOT-GCg Planning Operators Operator representations used by classical planners, such as STRIPS, often include a set of preconditions, an add list, and a delete list <ref> (Fikes & Nilsson 1971) </ref>. The STRIPS planner assumed that actions taken in a world state matching an operator's preconditions would result in the state changes indicated by the oper-ator's add and delete lists without fail.
Reference: <author> Gil, Y. </author> <year> 1994. </year> <title> Learning by experimentation: Incremental refinement of incomplete planning domains. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> 87-95. </pages>
Reference-contexts: Consequently, msdd finds planning operators efficiently in an exponentially sized space. Our approach differs from other work on learning planning operators in that it requires minimal domain knowledge; there is no need for access to advice or examples from domain experts (Wang 1995), nor for initial approximate planning operators <ref> (Gil 1994) </ref>. We assume that the learning agent's initial domain model is weak, consisting only of a list of the different types of actions that it can take. <p> Related Work Existing symbolic approaches to learning planning operators via interaction with the environment have typically assumed a deterministic world in which actions always have their intended effects, and the state of the world never changes in the absence of an action <ref> (Gil 1994) </ref> (Shen 1993) (Wang 1995). One notable exception is (Benson 1995), in which the primary effect of a durative action is assumed to be deterministic, but side effects may occur with some probability. <p> Much of the work on learning planning operators assumes the availability of fairly sophisticated forms of domain knowledge, such as advice or problem solving traces generated by domain experts (Benson 1995) (Wang 1995), or initial approximate planning operators <ref> (Gil 1994) </ref>. Our approach assumes that the learning agent initially knows nothing of the dynamics of its environment.
Reference: <author> Kushmerick, N.; Hanks, S.; and Weld, D. </author> <year> 1994. </year> <title> An algorithm for probabilistic least-commitment planning. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> 1074-1078. </pages>
Reference-contexts: These assumptions, which are unrealistic for many real-world domains, are being relaxed by current research in AI planning systems <ref> (Kushmerick, Hanks, & Weld 1994) </ref> (Mansell 1993). However, as planning domains become more complex, so does the task of generating domain models. In this paper, we present an algorithm for automatically learning planning operators with context-dependent and probabilistic effects in environments where exogenous events change the state of the world.
Reference: <author> Mahadevan, S., and Connell, J. </author> <year> 1992. </year> <title> Automatic programming of behavior-based robots using reinforcement learning. </title> <journal> Artificial Intelligence 55(2-3):189-208. </journal>
Reference-contexts: In contrast, the work described in this paper applies to domains that contain uncertainties associated with the outcomes of actions, and noise from exogenous events. Subsymbolic approaches to learning environmental dynamics, such as reinforcement learning <ref> (Mahadevan & Connell 1992) </ref>, are capable of handling a variety of forms of noise. Reinforcement learning requires a reward function that allows the agent to learn a mapping from states to actions that maximizes reward.
Reference: <author> Mansell, T. M. </author> <year> 1993. </year> <title> A method for planning given uncertain and incomplete information. </title> <booktitle> In Proceedings of the Ninth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> 350-358. </pages>
Reference-contexts: These assumptions, which are unrealistic for many real-world domains, are being relaxed by current research in AI planning systems (Kushmerick, Hanks, & Weld 1994) <ref> (Mansell 1993) </ref>. However, as planning domains become more complex, so does the task of generating domain models. In this paper, we present an algorithm for automatically learning planning operators with context-dependent and probabilistic effects in environments where exogenous events change the state of the world.
Reference: <author> Oates, T., and Cohen, P. R. </author> <year> 1996. </year> <title> Searching for structure in multiple streams of data. </title> <booktitle> To appear in Proceedings of the Thirteenth International Conference on Machine Learning. </booktitle>
Reference-contexts: of the world changed in the manner described by the operator significantly often in the past? Exploration of operator space is performed by an algorithm called Multi-Stream Dependency Detection (msdd) that was designed to find dependencies among categorical values in multiple streams of data over time (Oates et al. 1995) <ref> (Oates & Cohen 1996) </ref>. msdd provides a general search framework, and relies on domain knowledge both to guide the search and reason about when to prune. Consequently, msdd finds planning operators efficiently in an exponentially sized space. <p> The MSDD Algorithm The msdd algorithm finds dependencies| unexpectedly frequent or infrequent co-occurrences of values|in multiple streams of categorical data (Oates et al. 1995) <ref> (Oates & Cohen 1996) </ref>. msdd is general in that it performs a simple best-first search over the space of possible dependencies, terminating when a user-specified number of search nodes have been explored. <p> If some aspect of the domain makes one or more of these features undesirable, then the tree can be safely pruned at this node. Refer to <ref> (Oates & Cohen 1996) </ref> for a more complete and formal statement of the msdd algorithm.
Reference: <author> Oates, T.; Schmill, M. D.; Gregory, D. E.; and Co-hen, P. R. </author> <year> 1995. </year> <title> Detecting complex dependencies in categorical data. </title> <editor> In Fisher, D., and Lenz, H., eds., </editor> <title> Finding Structure in Data: </title> <booktitle> Artificial Intelligence and Statistics V. </booktitle> <publisher> Springer Verlag. </publisher>
Reference-contexts: Has the state of the world changed in the manner described by the operator significantly often in the past? Exploration of operator space is performed by an algorithm called Multi-Stream Dependency Detection (msdd) that was designed to find dependencies among categorical values in multiple streams of data over time <ref> (Oates et al. 1995) </ref> (Oates & Cohen 1996). msdd provides a general search framework, and relies on domain knowledge both to guide the search and reason about when to prune. Consequently, msdd finds planning operators efficiently in an exponentially sized space. <p> The MSDD Algorithm The msdd algorithm finds dependencies| unexpectedly frequent or infrequent co-occurrences of values|in multiple streams of categorical data <ref> (Oates et al. 1995) </ref> (Oates & Cohen 1996). msdd is general in that it performs a simple best-first search over the space of possible dependencies, terminating when a user-specified number of search nodes have been explored.
Reference: <author> Riddle, P.; Segal, R.; and Etzioni, O. </author> <year> 1994. </year> <title> Representation design and brute-force induction in a boeing manufacturing domain. </title> <booktitle> Applied Artificial Intelligence 8 </booktitle> <pages> 125-147. </pages>
Reference-contexts: A model of those dynamics is constructed based only on the agent's own past interactions with its environment. msdd's approach to expanding the search tree to avoid redundant generation of search nodes is similar to that of other algorithms (Rymon 1992) (Schlimmer 1993) <ref> (Riddle, Segal, & Etzioni 1994) </ref>. msdd's search differs from those mentioned above in that it explores the space of rules containing both conjunctive left-hand-sides and conjunctive right-hand-sides.
Reference: <author> Rymon, R. </author> <year> 1992. </year> <title> Search through systematic set enumeration. </title> <booktitle> In Proceedings of the Third International Conference on Principles of Knowledge Representation and Reasoning. </booktitle>
Reference-contexts: A model of those dynamics is constructed based only on the agent's own past interactions with its environment. msdd's approach to expanding the search tree to avoid redundant generation of search nodes is similar to that of other algorithms <ref> (Rymon 1992) </ref> (Schlimmer 1993) (Riddle, Segal, & Etzioni 1994). msdd's search differs from those mentioned above in that it explores the space of rules containing both conjunctive left-hand-sides and conjunctive right-hand-sides.
Reference: <author> Schlimmer, J. C. </author> <year> 1993. </year> <title> Efficiently inducing determinations: A complete and systematic search algorithm that uses optimal pruning. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> 284-290. </pages>
Reference-contexts: A model of those dynamics is constructed based only on the agent's own past interactions with its environment. msdd's approach to expanding the search tree to avoid redundant generation of search nodes is similar to that of other algorithms (Rymon 1992) <ref> (Schlimmer 1993) </ref> (Riddle, Segal, & Etzioni 1994). msdd's search differs from those mentioned above in that it explores the space of rules containing both conjunctive left-hand-sides and conjunctive right-hand-sides.
Reference: <author> Shen, W.-M. </author> <year> 1993. </year> <title> Discovery as autonomous learning from the environment. </title> <booktitle> Machine Learning 12(1-3):143-165. </booktitle>
Reference-contexts: Related Work Existing symbolic approaches to learning planning operators via interaction with the environment have typically assumed a deterministic world in which actions always have their intended effects, and the state of the world never changes in the absence of an action (Gil 1994) <ref> (Shen 1993) </ref> (Wang 1995). One notable exception is (Benson 1995), in which the primary effect of a durative action is assumed to be deterministic, but side effects may occur with some probability.
Reference: <author> Wang, X. </author> <year> 1995. </year> <title> Learning by observation and practice: An incremental approach for planning operator acquisition. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning. </booktitle>
Reference-contexts: Consequently, msdd finds planning operators efficiently in an exponentially sized space. Our approach differs from other work on learning planning operators in that it requires minimal domain knowledge; there is no need for access to advice or examples from domain experts <ref> (Wang 1995) </ref>, nor for initial approximate planning operators (Gil 1994). We assume that the learning agent's initial domain model is weak, consisting only of a list of the different types of actions that it can take. <p> We also require that each non-wildcard in the effects be different from the value given by the context for the corresponding sensor. That is, operators must describe what changes in response to an action, not what stays the same. This restriction is similar to Wang's use of delta-state <ref> (Wang 1995) </ref>, the difference between the states of the world before and after the execution of an action, to drive learning of operator effects. Likewise, Benson (Ben-son 1995) uses differences between state descriptions to identify the effects of actions when learning from execution traces generated by domain experts. <p> Related Work Existing symbolic approaches to learning planning operators via interaction with the environment have typically assumed a deterministic world in which actions always have their intended effects, and the state of the world never changes in the absence of an action (Gil 1994) (Shen 1993) <ref> (Wang 1995) </ref>. One notable exception is (Benson 1995), in which the primary effect of a durative action is assumed to be deterministic, but side effects may occur with some probability. <p> Much of the work on learning planning operators assumes the availability of fairly sophisticated forms of domain knowledge, such as advice or problem solving traces generated by domain experts (Benson 1995) <ref> (Wang 1995) </ref>, or initial approximate planning operators (Gil 1994). Our approach assumes that the learning agent initially knows nothing of the dynamics of its environment.
Reference: <author> Wilkins, D. E. </author> <year> 1988. </year> <title> Practical Planning: Extending the Classical AI Planning Paradigm. </title> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: Introduction Research in classical planning has assumed that the effects of actions are deterministic and the state of the world is never altered by exogenous events, simplifying the task of encoding domain knowledge in the form of planning operators <ref> (Wilkins 1988) </ref>. These assumptions, which are unrealistic for many real-world domains, are being relaxed by current research in AI planning systems (Kushmerick, Hanks, & Weld 1994) (Mansell 1993). However, as planning domains become more complex, so does the task of generating domain models.
References-found: 14

