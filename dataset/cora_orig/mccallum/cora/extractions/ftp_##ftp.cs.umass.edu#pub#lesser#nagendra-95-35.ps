URL: ftp://ftp.cs.umass.edu/pub/lesser/nagendra-95-35.ps
Refering-URL: http://dis.cs.umass.edu/research/team.html
Root-URL: 
Title: Learning Organizational Roles in a Heterogeneous Multi-agent System  
Author: M V Nagendra Prasad, Victor R Lesser and Susan E Lander 
Date: 95-35  
Affiliation: Department of Computer Science University of Massachusetts UMass Computer Science  
Pubnum: Technical Report  
Abstract: Previous work in self-organization for efficient distributed search control has, for the most part, involved simple agents with simple interaction patterns. The work presented in this paper represents one of the few attempts at demonstrating the viability and utility of self-organization in an agent-based system involving complex interactions within the agent set. We present a multi-agent parametric design system called L-TEAM where a set of heterogeneous agents learn their organizational roles in negotiated search for mutually acceptable designs. We tested the system on a steam condenser design domain and empirically demonstrated its usefulness. L-TEAM produced better results than its non-learning predecessor, TEAM, which required elaborate knowledge engineering to hand-code organizational roles for its agent set. In addition, we discuss experiments with L-TEAM that highlight the importance of certain learning issues in multi-agent systems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Barto, R. Sutton and C. Watkins, </author> <title> Learning and Sequential Decision Making, </title> <editor> in M. Gariel and J. W. Moore (eds), </editor> <title> Learning and Computational Neuroscience, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA 1990. </address>
Reference-contexts: Tan [16], Sandholm and Nagen-dra Prasad [8], Sandholm and Crites [7], and Sen and Sekaran [9] discuss multi-agent reinforcement learning systems. All these systems rely on reinforcement learning methods <ref> [1, 14] </ref>. While these works highlight interesting aspects of multi-agent learning systems, they are primarily centered around toy problems on a grid world.
Reference: [2] <author> D. D. Corkill, </author> <title> A Framework for Organizational Self-design in Distributed Problem-solving Networks, </title> <type> Ph.D. Dissertation, </type> <institution> Dept. of Computer Science, University of Massachusetts, Amherst, </institution> <year> 1983. </year>
Reference: [3] <author> E. H. Durfee and V. R. Lesser, </author> <title> Predictability versus Responsiveness: </title> <booktitle> Coordinating Problem Solvers in Dynamic Domains in Proceedings of the Seventh National Conference on Artificial Intelligence, </booktitle> <pages> pp 66-71, </pages> <address> St. Paul, MN, </address> <year> 1988. </year>
Reference-contexts: This involves a trade-off where an agent has to distribute its limited resources between communication and assimilation of information received from other agents on one hand and its own local computational requirements on the other hand <ref> [3] </ref>. Given such a scenario, an agent may be forced to function in uncertain environments, where it may have an outdated or partial view of the global situation.
Reference: [4] <author> S. E. Lander, </author> <title> Distributed Search in Heterogeneous and Reusable Multi-Agent Systems, </title> <type> Ph.D. Thesis, </type> <institution> Dept.of Computer Science, University of Massachusetts, Amherst, </institution> <year> 1993. </year>
Reference-contexts: 1 Introduction In this paper, we present a multi-agent heterogeneous system called L-TEAM that learns to organize itself to let the agents play the roles they are best suited for in a distributed search process. L-TEAM is an extension of the TEAM framework <ref> [4] </ref> for cooperative search among a set of heterogeneous reusable agents. A reusable agent system is an open system assembled by minimal customized integration of a dynamically selected subset from a catalogue of existing agents. <p> As a part of this search process agents augment their local view of the composite search space with meta-level information about search spaces of other agents through negotiation to minimize the likelihood of generating conflicting solutions <ref> [4] </ref>. TEAM was introduced in the context of parametric design in multi-agent systems. Each of the agents has its own local state information, a local database with static and dynamic constraints on its design components and a local agenda of potential actions. <p> The evolution of a composite solution in TEAM can be viewed as a series of state transitions as shown in Figure 1 (from <ref> [4] </ref>). For a composite solution in a given state, an agent can apply a set of negotiated search operators represented by the set of arcs leaving that state. An agent can be working on several composite solutions concurrently. <p> This decision is complicated by the fact that an agent has to achieve this choice within its local view of the problem-solving situations (see <ref> [4] </ref>). The objective of this paper is to investigate the utility of machine learning techniques as an aid to such a decision process in situations where the set of agents involved in problem solving are not necessarily known to the designer of any single agent. <p> the 5 communicated violated local constraints, which can be used to determine the potential of a sequence of operators ending in a conflict. 4 Experimental Results To demonstrate the effectiveness of the mechanisms in L-TEAM and compare them to those in TEAM, we used the same domain as in Lander <ref> [4] </ref> parametric design of steam condensers. The prototype multi-agent system for this domain, built on top of the TEAM framework, consists of seven agents: pump-agent, heat-exchanger-agent, motor-agent, vbelt-agent, shaft-agent, platform-agent, and frequency-critic. The problem solving process starts by placing a problem specification on a central blackboard (BB). <p> Situation-specific L-TEAM still produced an improvement of 41.39 units over TEAM in the cost of designs [6]. Performance of the L-TEAM and TEAM systems in the steam condenser domain relative to true optimality is not known but, as discussed in Lander <ref> [4] </ref>, we suspect that there may be a floor effect in the data and the designs produced by the systems are close to optimal. This suggests that the gains from situation-specific learning that can possibly be achieved may be limited.
Reference: [5] <author> V. R. Lesser, and L. D. Erman, </author> <title> Distributed Interpretation: A Model and Experiment, </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-29(12), </volume> <pages> pp 1144-1163, </pages> <month> Dec. </month> <year> 1980. </year>
Reference-contexts: On rare occasions, (twice in our 100 experimental runs), situation-specific-L-TEAM performed worse than non-situation-specific-L-TEAM . We attribute this observation to the phenomenon of distraction frequently observed in multi-agent systems <ref> [5] </ref>. In the context of role assignments, this phenomenon maps to the ability of the agents to judge whether it is effective to work on its own designs or respond to the designs generated by the other members of the agent set in the present situation.
Reference: [6] <author> M V Nagendra Prasad, Victor Lesser, and Susan Lander, </author> <title> Learning Organizational Roles in a Heterogeneous Multi-agent System Forthcoming Technical Report, </title> <institution> Department of Computer Science, University of Massachusetts, Amherst, </institution> <year> 1994. </year>
Reference-contexts: We changed the problem specification to take one more component the required power. Availability of this parameter permits pump and motor agents to perform more informed initiations of design. Situation-specific L-TEAM still produced an improvement of 41.39 units over TEAM in the cost of designs <ref> [6] </ref>. Performance of the L-TEAM and TEAM systems in the steam condenser domain relative to true optimality is not known but, as discussed in Lander [4], we suspect that there may be a floor effect in the data and the designs produced by the systems are close to optimal.
Reference: [7] <author> T. Sandholm and R. Crites, </author> <title> Multi-agent Reinforcement Learning in the Repeated Prisoner's Dilemma to appear in Biosystems, </title> <year> 1995. </year>
Reference-contexts: If it is the case that such a floor effect exists, larger improvements can be expected in some other domains. 5 Related Work Previous work related to learning in multi-agent systems is limited. Tan [16], Sandholm and Nagen-dra Prasad [8], Sandholm and Crites <ref> [7] </ref>, and Sen and Sekaran [9] discuss multi-agent reinforcement learning systems. All these systems rely on reinforcement learning methods [1, 14]. While these works highlight interesting aspects of multi-agent learning systems, they are primarily centered around toy problems on a grid world.
Reference: [8] <author> T. Sandholm and M V Nagendra Prasad, </author> <title> MUSCLE: Multi-agent system for Coordinated Learning Experiments, </title> <type> Unpublished memo, </type> <institution> Department of Computer Science, University of Massachusetts, Amherst, </institution> <year> 1993. </year>
Reference-contexts: If it is the case that such a floor effect exists, larger improvements can be expected in some other domains. 5 Related Work Previous work related to learning in multi-agent systems is limited. Tan [16], Sandholm and Nagen-dra Prasad <ref> [8] </ref>, Sandholm and Crites [7], and Sen and Sekaran [9] discuss multi-agent reinforcement learning systems. All these systems rely on reinforcement learning methods [1, 14]. While these works highlight interesting aspects of multi-agent learning systems, they are primarily centered around toy problems on a grid world.
Reference: [9] <author> S. Sen and M. Sekaran, </author> <title> Learning to Coordinate without Sharing Information, </title> <booktitle> in Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pp 426-431, </pages> <address> Washington, </address> <year> 1994. </year>
Reference-contexts: If it is the case that such a floor effect exists, larger improvements can be expected in some other domains. 5 Related Work Previous work related to learning in multi-agent systems is limited. Tan [16], Sandholm and Nagen-dra Prasad [8], Sandholm and Crites [7], and Sen and Sekaran <ref> [9] </ref> discuss multi-agent reinforcement learning systems. All these systems rely on reinforcement learning methods [1, 14]. While these works highlight interesting aspects of multi-agent learning systems, they are primarily centered around toy problems on a grid world.
Reference: [10] <author> M. J. Shaw, and A. B. Whinston, </author> <title> Learning and Adaptation in DAI systems, </title> <journal> in Distributed Artificial Intelligence, </journal> <volume> vol. 2, </volume> <publisher> Morgan Kaufmann Pub., </publisher> <address> San Mateo, California, </address> <pages> pp. 413-429, </pages> <year> 1989. </year>
Reference-contexts: However, the nature of the interactions is simple and the agents are fine-grained 11 in their work. In this paper, we are concerned with large-grained agents and complex interactions between activities of the agents. Shaw and Whinston <ref> [10] </ref> discuss a classifier system based multi-agent learning system that represents an interesting alternative to the learning mechanism we proposed in this paper but as discussed previously, in this paper our interest is more than just learning the organizational roles.
Reference: [11] <author> Y. Shoham and M. Tennenholtz, </author> <title> Emergent Conventions in Multi-Agent Systems: initial experimental results and observations, </title> <booktitle> in the Proceedings of KR-92. </booktitle>
Reference-contexts: Such an explicit notion of organizational control knowledge led us to studies on the importance of concepts like potential and divergence of subjective and objective views of the world. Shoham and Tennenholtz <ref> [11] </ref> discuss co-learning and the emergence of conventions in multi-agent systems. However, the nature of the interactions is simple and the agents are fine-grained 11 in their work. In this paper, we are concerned with large-grained agents and complex interactions between activities of the agents.
Reference: [12] <author> S. S. Sian, </author> <title> Extending learning to multiple agents: issues and a model for multi-agent machine learning, in Machine Learning - EWSL 91, </title> <editor> Y. Kodratoff (Ed.), </editor> <publisher> Springer-Verlag, </publisher> <pages> pp. 440-456, </pages> <year> 1991. </year>
Reference-contexts: Certain multi-agent learning systems in the literature deal with a different task from that presented in this paper. Systems like ILS [13] and MALE <ref> [12] </ref> use multi-agent techniques to build hybrid learners from multiple learning agents. On the other hand, L-TEAM learns problem-solving control for multi-agent systems. 6 Implications and Conclusion Previous work in self-organization for efficient distributed search control has, for the most part, involved simple agents with simple interaction patterns.
Reference: [13] <author> B. Silver, W. Frawely, G. Iba, J. Vittal, and K. Bradford, </author> <title> A framework for multi-paradigmatic learning, </title> <booktitle> in Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <address> Austin, Texas, </address> <pages> pp 348-358, </pages> <year> 1990. </year>
Reference-contexts: TEAM-like systems may not be amenable to knowledge-intensive task of extracting coordination rules or situation-specific organizational rules from histories due to the heterogeneity of its agents. Certain multi-agent learning systems in the literature deal with a different task from that presented in this paper. Systems like ILS <ref> [13] </ref> and MALE [12] use multi-agent techniques to build hybrid learners from multiple learning agents.
Reference: [14] <author> R. Sutton, </author> <title> Learning to Predict by the Methods of temporal differences, </title> <booktitle> Machine Learning 3, </booktitle> <pages> pp 9-44, </pages> <year> 1988. </year>
Reference-contexts: Once the learning is done, an agent chooses the role with maximum rating in a given situation. This implies that after the learning phase, each agent organizes itself to play a fixed role in a given situation. We use the supervised-learning approach to prediction learning (see <ref> [14] </ref>) to learn estimates for the UPC vectors for each of the states. <p> following sequence of operator applications leads to a terminal state say a success. 1 A node represents a decision point for an agent. 2 Note that the supervised learning approach to prediction learning is different from reinforcement learning which assigns credit by means of the differences between temporally successive predictions <ref> [14] </ref>. In this paper we are primarily concerned with showing the benefits and characteristics of learning in multi-agent systems rather than with the merits of a particular learning method over others. <p> Tan [16], Sandholm and Nagen-dra Prasad [8], Sandholm and Crites [7], and Sen and Sekaran [9] discuss multi-agent reinforcement learning systems. All these systems rely on reinforcement learning methods <ref> [1, 14] </ref>. While these works highlight interesting aspects of multi-agent learning systems, they are primarily centered around toy problems on a grid world.
Reference: [15] <author> T. Sugawara, and V. R. Lesser, </author> <title> "On-Line Learning of Coordination Plans," </title> <booktitle> Twelfth Annual Workshop on Distributed Artificial Intelligence, </booktitle> <year> 1993. </year>
Reference-contexts: Shaw and Whinston [10] discuss a classifier system based multi-agent learning system that represents an interesting alternative to the learning mechanism we proposed in this paper but as discussed previously, in this paper our interest is more than just learning the organizational roles. Sugawra and Lesser <ref> [15] </ref> discuss a distributed networking system where each local segment of the network has an intelligent diagnosis agent called LODES that monitors traffic on the network and uses an explanation-based learning technique to develop coordination rules for the LODES agents.
Reference: [16] <author> M. Tan, </author> <title> Multi-agent Reinforcement Learning: Independent vs. </title> <booktitle> Cooperative Agents, Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <address> Amherst, </address> <publisher> Mas-sachusetts, </publisher> <pages> pp 330-337, </pages> <year> 1993. </year>
Reference-contexts: If it is the case that such a floor effect exists, larger improvements can be expected in some other domains. 5 Related Work Previous work related to learning in multi-agent systems is limited. Tan <ref> [16] </ref>, Sandholm and Nagen-dra Prasad [8], Sandholm and Crites [7], and Sen and Sekaran [9] discuss multi-agent reinforcement learning systems. All these systems rely on reinforcement learning methods [1, 14].
Reference: [17] <author> R. Whitehair and V. R. Lesser, </author> <title> A Framework for the Analysis of Sophisticated Control in Interpretation Systems, </title> <type> Technical Report Number 93-53, </type> <institution> Dept.of Computer Science, University of Massachusetts, Amherst, </institution> <year> 1993. </year> <month> 14 </month>
Reference-contexts: In this paper we propose learning methods that let the agents construct these rating functions based on past problem solving experience. 3 Learning Role Assignments The formal basis for learning search strategies adopted in this paper is derived from the UPC formalism for search control (see Whitehair and Lesser <ref> [17] </ref>) that relies on the calculation and use of the Utility, Probability and Cost (UPC) values associated with each hstate; op; f inal statei tuple. Utility component represents the present state's estimate of the final state's expected value or utility if we apply operator op in the present state.
References-found: 17

