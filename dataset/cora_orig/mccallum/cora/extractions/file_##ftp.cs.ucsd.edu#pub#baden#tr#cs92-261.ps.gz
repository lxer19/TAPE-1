URL: file://ftp.cs.ucsd.edu/pub/baden/tr/cs92-261.ps.gz
Refering-URL: http://www.cs.ucsd.edu/groups/hpcl/scg/tr.html
Root-URL: http://www.cs.ucsd.edu
Title: Lattice Parallelism: A Parallel Programming Model for Non-Uniform, Structured Scientific Computations  
Author: Scott B. Baden Scott R. Kohn 
Address: La Jolla, California 92093-0114 USA  
Affiliation: Department of Computer Science and Engineering University of California, San Diego  
Date: September, 1992  
Pubnum: CSE Technical Report Number CS92-261  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> C. R. Anderson, </author> <title> A method of local corrections for computing the velocity field due to a distribution of vortex blobs, </title> <journal> Journal of Computational Physics, </journal> <volume> 62 (1986), </volume> <pages> pp. 111-123. </pages>
Reference-contexts: Accrete () will often be used to extend a domain by a uniform amount in each dimension. LPAR provides a shorthand notation to support this common case: we may substitute an integer for a replicated list of that single value. Thus, accrete (D, 1) is equivalent to accrete (D, <ref> [ 1, 1 ] </ref>). Negative accretion values are also permitted. In this case the object is shrunk rather than grown. However, since accrete () is not reversible in general, successive applications with accretion values that take on alternating signs must be handled with care. <p> Preliminary results show that the code required to parallelize applications such as multigrid and a rapid summation algorithm for solving the N-body problem <ref> [1] </ref> is at least an order of magnitude smaller under LPAR as compared to the hand coded approach [4]. LPAR preserves the spatial locality inherent in many structured nonuniform problems, and can avoid the potentially high overheads associated with communication.
Reference: [2] <author> S. B. Baden, </author> <title> Programming abstractions for dynamically partitioning and coordinating localized scientific calculations running on multiprocessors, </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 12 (1991), </volume> <pages> pp. 145-157. 19 </pages>
Reference-contexts: LPAR preserves the spatial locality inherent in many structured nonuniform problems, and can avoid the potentially high overheads associated with communication. LPAR is based in part on the GenMP programming model <ref> [2] </ref> which may be used to develop portable implementations of particle methods on MIMD multiprocessors [3] and in part on the 18 FIDIL programming language, mentioned above.
Reference: [3] <author> S. B. Baden and S. Kohn, </author> <title> The reference guide to genmp the generic multiprocessor, </title> <type> Tech. Report CS92-243, </type> <institution> University of California - San Diego, CSE 0114, </institution> <address> 9500 Gilman Drive, La Jolla, CA 92092-0114, </address> <month> June </month> <year> 1992. </year> <note> Also available via anonymous ftp from cs.ucsd.edu in directory pub/baden/genmp. </note>
Reference-contexts: LPAR preserves the spatial locality inherent in many structured nonuniform problems, and can avoid the potentially high overheads associated with communication. LPAR is based in part on the GenMP programming model [2] which may be used to develop portable implementations of particle methods on MIMD multiprocessors <ref> [3] </ref> and in part on the 18 FIDIL programming language, mentioned above. Other programming tools for non-uniform structured problems include: PARTI [7], DINO [16], P++ [14], and HPF dialects such as Fortran D [12] and CM-Fortran [18].
Reference: [4] <author> S. B. Baden and S. R. Kohn, </author> <title> Lattice parallelism: A parallel programming model for manipulating non-uniform structured scientific data structures, </title> <booktitle> SIGPLAN, 28 (1993), </booktitle> <pages> pp. 24-27. </pages> <booktitle> Conference proceedings for the Second Workshop on Languages, Compilers, and Run-Time Environments for Distributed Memory Multiprocessors, </booktitle> <address> Boulder, Colorado, Septem-ber, </address> <year> 1992. </year>
Reference-contexts: Preliminary results show that the code required to parallelize applications such as multigrid and a rapid summation algorithm for solving the N-body problem [1] is at least an order of magnitude smaller under LPAR as compared to the hand coded approach <ref> [4] </ref>. LPAR preserves the spatial locality inherent in many structured nonuniform problems, and can avoid the potentially high overheads associated with communication.
Reference: [5] <author> M. J. Berger and P. Colella, </author> <title> Local adaptive mesh refinement for shock hydrodynamics, </title> <journal> Journal of Computational Physics, </journal> <volume> 82 (1989), </volume> <pages> pp. 64-84. </pages>
Reference-contexts: It is intended for treating non-uniform computational domains represented as piecewise collections of uniform sets of data arising, for example, in adaptive <ref> [6, 5] </ref> and multilevel [15] methods, and in particle methods [13]. <p> K loop we would violate the restriction that a distributed map may not be updated using an index variable from a non-distributed loop. 5 Detailed Application We illustrate the use of LPAR's facilities in a detailed application, adaptive mesh refinement (AMR) for solving hyperbolic conservation laws in two space dimensions <ref> [5] </ref>. This example builds on that employed in the previous sections. AMR is a finite difference method which computes a solution to a uniform level of accuracy by locally varying the mesh spacing and the timestep.
Reference: [6] <author> M. J. Berger and J. Oliger, </author> <title> Adaptive mesh refinement for hyperbolic partial differential equations, </title> <journal> Journal of Computational Physics, </journal> <volume> 53 (1984), </volume> <pages> pp. 484-512. </pages>
Reference-contexts: It is intended for treating non-uniform computational domains represented as piecewise collections of uniform sets of data arising, for example, in adaptive <ref> [6, 5] </ref> and multilevel [15] methods, and in particle methods [13]. <p> DINO hides many of the details of message passing, but requires that the programmer be aware of message passing activity. P++ is a C++ run time library which supports a virtual shared grid abstraction for structured grid methods such as adaptive mesh refinement <ref> [6] </ref>. LPAR complements the fine-grained data parallel model supported by HPF, supplying the coarse grained analogs of the forall loop and structural manipulation. The two could be employed together: LPAR to manage the top level parallel structure and HPF to handle low-level parallelization involving vector units or processing clusters.
Reference: [7] <author> H. Berryman, J. Saltz, and J. Scroggs, </author> <title> Execution time support for adaptive scientific algorithms on distributed memory machines, </title> <type> Tech. Report 90-41, </type> <institution> ICASE, NASA Langley Research Center, </institution> <month> February </month> <year> 1991. </year>
Reference-contexts: LPAR does not apply to unstructured problems such as sparse matrix linear algebra and finite element problems, which give rise to a different behavior on a parallel computer and require a different set of primitives. (PARTI <ref> [7] </ref> is one example of a programming model for unstructured problems.) LPAR is based on the scientific programming language FIDIL [11]. <p> LPAR is based in part on the GenMP programming model [2] which may be used to develop portable implementations of particle methods on MIMD multiprocessors [3] and in part on the 18 FIDIL programming language, mentioned above. Other programming tools for non-uniform structured problems include: PARTI <ref> [7] </ref>, DINO [16], P++ [14], and HPF dialects such as Fortran D [12] and CM-Fortran [18]. PARTI was originally intended for unstructured problems but has recently been extended [9] to handle certain kinds of block structured problems.
Reference: [8] <author> T. A. Budd, </author> <title> An apl compiler for a vector processor, </title> <type> Tech. Report 82-6, </type> <institution> University of Arizona Department of Computer Science, </institution> <month> July </month> <year> 1982. </year>
Reference-contexts: Only a fraction of fineGrids [L] is actually live. We refer to the above optimizations as live transmission analysis; they are related to Hilfin-ger and Semenzato's live domain analysis in compiling the FIDIL programming language [17] and to Guibas and Wyatt's [10] and Budd's <ref> [8] </ref> compilation techniques for APL. To support this optimization it is necessary for each distributed map definition to replicate on all processors a 17 the full structural description of the map.
Reference: [9] <author> C. Chase, K. Crowley, J. Saltz, and A. Reeves, </author> <title> Parallelization of irregularly coupled regular meshes, </title> <type> Tech. Report 92-1, </type> <institution> ICASE, NASA Langley Research Center, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: Other programming tools for non-uniform structured problems include: PARTI [7], DINO [16], P++ [14], and HPF dialects such as Fortran D [12] and CM-Fortran [18]. PARTI was originally intended for unstructured problems but has recently been extended <ref> [9] </ref> to handle certain kinds of block structured problems. It will be interesting to compare LPAR with this new model to gain a better understanding of the underlying design tradeoffs of the two models.
Reference: [10] <author> L. J. Guibas and D. K. Wyatt, </author> <title> Compilation and delayed evaluation in apl, </title> <booktitle> in Conference Record of the Fifth Annual ACM Symposium on Principles of Programming Languages, ACM, </booktitle> <year> 1978. </year>
Reference-contexts: Only a fraction of fineGrids [L] is actually live. We refer to the above optimizations as live transmission analysis; they are related to Hilfin-ger and Semenzato's live domain analysis in compiling the FIDIL programming language [17] and to Guibas and Wyatt's <ref> [10] </ref> and Budd's [8] compilation techniques for APL. To support this optimization it is necessary for each distributed map definition to replicate on all processors a 17 the full structural description of the map.
Reference: [11] <author> P. N. Hilfinger and P. Colella, Fidil: </author> <title> A language for scientific programming, </title> <type> Tech. Report UCRL-98057, </type> <institution> Lawrence Livermore National Laboratory, </institution> <month> January </month> <year> 1988. </year>
Reference-contexts: problems such as sparse matrix linear algebra and finite element problems, which give rise to a different behavior on a parallel computer and require a different set of primitives. (PARTI [7] is one example of a programming model for unstructured problems.) LPAR is based on the scientific programming language FIDIL <ref> [11] </ref>. It extends FIDIL's domain mechanisms to distributed data structures and permits the user to manipulate structural information as a first class object, independently of the underlying physical processor data layout. LPAR supports coarse grained data parallelism and provides generalized coarse grained analogs of HPF's array section and where constructs. <p> LPAR's lattice data structure and associated operators are based on the FIDIL programming language <ref> [11] </ref>. However, unlike FIDIL, LPAR provides no facilities for arithmetic computation; LPAR is intended only for managing the parallel structure of numerical computations.
Reference: [12] <author> S. Hiranandani, K. Kennedy, and C. W. Tseng, </author> <title> Compiler optimizations for fortran d on mimd distributed memory machines, </title> <booktitle> in Proceedings of the 1991 ACM International Conference on Supercompuing, </booktitle> <address> Houston, Texas, </address> <month> November </month> <year> 1991, </year> <pages> pp. 86-100. </pages>
Reference-contexts: Other programming tools for non-uniform structured problems include: PARTI [7], DINO [16], P++ [14], and HPF dialects such as Fortran D <ref> [12] </ref> and CM-Fortran [18]. PARTI was originally intended for unstructured problems but has recently been extended [9] to handle certain kinds of block structured problems. It will be interesting to compare LPAR with this new model to gain a better understanding of the underlying design tradeoffs of the two models.
Reference: [13] <author> R. W. Hockney and J. W. Eastwood, </author> <title> Computer Simulation Using Particles, </title> <publisher> McGraw-Hill, </publisher> <year> 1981. </year>
Reference-contexts: It is intended for treating non-uniform computational domains represented as piecewise collections of uniform sets of data arising, for example, in adaptive [6, 5] and multilevel [15] methods, and in particle methods <ref> [13] </ref>.
Reference: [14] <author> M. Lemke and D. Quinlan, </author> <title> P++: a c++ virtual shared grids based programming environment for architecture-independent development of structured grid applications, </title> <type> tech. report, </type> <institution> Computational Mathematics Group, University of Colorado at Boulder, </institution> <address> Denver, CO, </address> <month> January </month> <year> 1992. </year>
Reference-contexts: Other programming tools for non-uniform structured problems include: PARTI [7], DINO [16], P++ <ref> [14] </ref>, and HPF dialects such as Fortran D [12] and CM-Fortran [18]. PARTI was originally intended for unstructured problems but has recently been extended [9] to handle certain kinds of block structured problems.
Reference: [15] <author> S. McCormick, </author> <title> Multilevel Adaptive Methods for Partial Differential Equations, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1989. </year> <month> 20 </month>
Reference-contexts: It is intended for treating non-uniform computational domains represented as piecewise collections of uniform sets of data arising, for example, in adaptive [6, 5] and multilevel <ref> [15] </ref> methods, and in particle methods [13].
Reference: [16] <author> M. Rosing, R. B. Schnabel, and R. P. Weaver, </author> <title> Expressing complex parallel algo-rithms in dino, </title> <type> Tech. Report CU-CS-430-88, </type> <institution> Department of Computer Science, University of Colorado at Boulder, </institution> <address> Denver, CO, </address> <month> March </month> <year> 1989. </year>
Reference-contexts: LPAR is based in part on the GenMP programming model [2] which may be used to develop portable implementations of particle methods on MIMD multiprocessors [3] and in part on the 18 FIDIL programming language, mentioned above. Other programming tools for non-uniform structured problems include: PARTI [7], DINO <ref> [16] </ref>, P++ [14], and HPF dialects such as Fortran D [12] and CM-Fortran [18]. PARTI was originally intended for unstructured problems but has recently been extended [9] to handle certain kinds of block structured problems.
Reference: [17] <author> L. Semenzato and P. Hilfinger, </author> <booktitle> Arrays in fidil, in Proceedings of the First International Workshop on Arrays, Functional Programming, and Parallel Systems, </booktitle> <month> July </month> <year> 1990. </year>
Reference-contexts: Only a fraction of fineGrids [L] is actually live. We refer to the above optimizations as live transmission analysis; they are related to Hilfin-ger and Semenzato's live domain analysis in compiling the FIDIL programming language <ref> [17] </ref> and to Guibas and Wyatt's [10] and Budd's [8] compilation techniques for APL. To support this optimization it is necessary for each distributed map definition to replicate on all processors a 17 the full structural description of the map.
Reference: [18] <author> Thinking Machines, Inc., </author> <title> CM Fortran User's Guide, </title> <address> Cambridge, MA, </address> <month> July </month> <year> 1990. </year> <month> 21 </month>
Reference-contexts: Other programming tools for non-uniform structured problems include: PARTI [7], DINO [16], P++ [14], and HPF dialects such as Fortran D [12] and CM-Fortran <ref> [18] </ref>. PARTI was originally intended for unstructured problems but has recently been extended [9] to handle certain kinds of block structured problems. It will be interesting to compare LPAR with this new model to gain a better understanding of the underlying design tradeoffs of the two models.
References-found: 18

