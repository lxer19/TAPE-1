URL: http://robotics.stanford.edu/~urszula/papers/expl-uai97.ps
Refering-URL: http://robotics.stanford.edu/~urszula/
Root-URL: http://www.cs.stanford.edu
Email: urszula@cs.stanford.edu  halpern@cs.cornell.edu  
Title: Defining Explanation in Probabilistic Systems  
Author: Urszula Chajewska Joseph Y. Halpern 
Address: Stanford, CA 94305-9010  Ithaca, NY 14853  
Affiliation: Stanford University Department of Computer Science  Cornell University Computer Science Department  
Abstract: As probabilistic systems gain popularity and are coming into wider use, the need for a mechanism that explains the system's findings and recommendations becomes more critical. The system will also need a mechanism for ordering competing explanations. We examine two representative approaches to explanation in the literature one due to G ardenfors and one due to Pearland show that both suffer from significant problems. We propose an approach to defining a notion of better explanation that combines some of the features of both together with more recent work by Pearl and others on causality.
Abstract-found: 1
Intro-found: 1
Reference: <author> Alchourr on, C. E., P. G ardenfors, and D. </author> <title> Makinson (1985). On the logic of theory change: partial meet functions for contraction and revision. </title> <journal> Journal of Symbolic Logic 50, </journal> <pages> 510-530. </pages>
Reference-contexts: The amount of dissonance is measured by the surprise value of the explanan-dum in the belief state in which we reject our belief in the explanandum while holding as many as possible of our other beliefs intact (this operation is called contraction and comes from the belief revision framework <ref> (Alchourr on, G ardenfors, and Makinson 1985) </ref>). An explanation provides cognitive relief; the degree of cognitive relief is measured by the degree to which the explanation decreases the surprise value. <p> G ardenfors describes a number of postulates that K E should satisfy, such as K E = K if E =2 K. It is beyond the scope of this paper to discuss these postulates (see <ref> (Alchourr on, G ardenfors, and Makinson 1985) </ref>). However, these postulates do not serve to specify K E uniquely; that is, given K and E, there may be several epistemic states K 0 that satisfy the postulates. <p> This is the notion called belief expansion <ref> (Alchourr on, G ardenfors, and Makinson 1985) </ref>. Thus, we add clause (3) to the definition of explanation. Note, however, if F is independent of E, then E ^ F would be a causal explanation of E.
Reference: <author> Bacchus, F., A. J. Grove, J. Y. Halpern, and D. </author> <title> Koller (1996). From statistical knowledge bases to degrees of belief. </title> <booktitle> Artificial Intelligence 87(1-2), </booktitle> <pages> 75-143. </pages>
Reference-contexts: If the domain is finite, we could simplify things and assume that the distribution is the uniform distribution, as is done in <ref> (Bacchus, Grove, Halpern, and Koller 1996) </ref>.) While it is not necessary to consider such a rich language to make sense of G ardenfors' definition, one of his key insights is that statistical assertions are an important component of explanations.
Reference: <author> Balke, A. and J. </author> <title> Pearl (1994). Counterfactual probabilities: Computational methods, bounds and applications. </title> <booktitle> In Proc. Tenth Conference on Uncertainty in Artificial Intelligence (UAI '94), </booktitle> <pages> pp. 46-54. </pages>
Reference: <author> Boutilier, C. and V. </author> <title> Becher (1995, August). Abduction as belief revision. </title> <booktitle> Artificial Intelligence 77, </booktitle> <pages> 43-94. </pages>
Reference: <author> Dean, T. and K. </author> <title> Kanazawa (1989). A model for reasoning about persistence and causation. </title> <booktitle> Computational Intelligence 5, </booktitle> <pages> 142-150. </pages>
Reference: <author> Druzdzel, M. J. </author> <year> (1996). </year> <title> Explanation in probabilistic systems: </title> <booktitle> Is it feasible? will it work? In Proceedings of the Fifth International Workshop on Inteligent Information Systems (WIS-96), </booktitle> <address> Deblin, Poland. </address>
Reference: <author> Druzdzel, M. J. and H. A. </author> <title> Simon (1993). Causality in bayesian belief networks. </title> <booktitle> In Uncertainty in Artificial Intelligence 9, </booktitle> <pages> pp. 3-11. </pages>
Reference: <author> G ardenfors, P. </author> <year> (1988). </year> <title> Knowledge in Flux: Modeling the Dynamics of Epistemic States. </title> <publisher> MIT Press. </publisher>
Reference-contexts: An explanation for one agent may not be an explanation for another, as the following example, essentially taken from <ref> (G ardenfors 1988) </ref>, shows. Example 2.1 If we ask why Mr. <p> For example, if Pr is determined by a Bayesian network together with some observations, including E, then Pr E is just the distribution that results from the Bayesian network and all the observations but E. We can now present G ardenfors' definition of explanation. Definition 2.2 (from <ref> (G ardenfors 1988) </ref>) X is an explana tion of E relative to a state of belief K = hW; Pri (where E 2 K) if 2. Pr (X ) &lt; 1 (that is, X =2 K). We have already seen the first clause of this definition. <p> a Boolean combination of atomic sentences in a first-order language with only unary predicates. (Either conjunct may be omitted.) As we shall argue, we need to generalize this somewhat to allow causal assertions as well as statistical assertions. 2.2 A CRITIQUE While G ardenfors' definition has some compelling features (see <ref> (G ardenfors 1988) </ref> for further discussion), it also has some serious problems, both practical and philosophical. We describe some of them in this section. 1. While the second clause prevents E from being an explanation of itself, there are many other explanations that it does not block.
Reference: <author> Glesner, S. and D. </author> <title> Koller (1995). Constructing flexible dynamic belief networks from first-order probabilistic knowledge bases. </title> <booktitle> In Proceedings of the Euro-pean Conference on Symbolic and Quantitative Approaches to Reasoning and Uncertainty (ECSQARU '95), </booktitle> <pages> pp. 217-226. </pages>
Reference: <author> Haddawy, P. </author> <year> (1994). </year> <title> Generating Bayesian networks from probability logic knowledge bases. </title> <booktitle> In Proc. Tenth Conference on Uncertainty in Artificial Intelligence (UAI '94), </booktitle> <pages> pp. 262-269. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Halpern, J. Y. </author> <year> (1990). </year> <title> An analysis of first-order logics of probability. </title> <booktitle> Artificial Intelligence 46, </booktitle> <pages> 311-350. </pages>
Reference-contexts: To make sense of this, G ardenfors associates with a world not only a first-order interpretation, but a distribution over individuals in the domain. (This type of model is also considered in <ref> (Halpern 1990) </ref>, where a structure consists of possible worlds, with a distribution over the worlds, and, in each world there is a distribution on the individuals in that world; a formal language is provided for reasoning about such models.
Reference: <author> Heckerman, D. and R. </author> <title> Shachter (1995). Decision-theoretic foundations for causal reasoning. </title> <journal> Journal of Artificial Intelligence Research 3, </journal> <pages> 405-430. </pages>
Reference: <author> Hempel, C. G. </author> <year> (1965). </year> <title> Aspects of Scientific Explanation. </title> <publisher> Free Press. </publisher>
Reference: <author> Hempel, C. G. and P. </author> <title> Oppenheim (1948). Studies in the logic of explanation. </title> <booktitle> Philosophy of Science 15. </booktitle>
Reference-contexts: Of course, we are not the first to examine explanation. It has been has analyzed by philosophers for many years. Traditionally, it has been modeled by introducing a deductive relation between the explanation and the fact to be explained (explanandum) <ref> (Hempel and Oppenheim 1948) </ref>. While perhaps applicable to scientific enquiry, this approach is not easily applicable in domains with uncertainty. <p> pardon the pun) not to accept 70% of the time that the barometer reading goes down there is a storm as an explanation of a storm (unless we happen to believe that barometer readings have a causal influence on storms). 7 Originally, the idea came from Hempel's work on explanation <ref> (Hempel and Oppenheim 1948) </ref>. However, the situation is different if we try to explain our beliefs to someone else. In this case, the causal structure is symmetric.
Reference: <author> Henrion, M. and M. J. </author> <month> Druzdzel </month> <year> (1990). </year> <title> Qualitative propagation and scenario-based approaches to explanation of probabilistic reasoning. </title> <booktitle> In Uncertainty in Artificial Intelligence 6, </booktitle> <pages> pp. 17-32. </pages>
Reference-contexts: This approach, which we call Maximum A Posteriori model (MAP) after (Shimony 1991), has been also known under other names: Most Probable Explanation (MPE) (Pearl 1988) and Scenario-Based Explanation <ref> (Henrion and Druzdzel 1990) </ref>. Formally, according to Pearl, given an epistemic state K = hW; Pri, an explanation for E is simply a world w in which E is true. This notion of explanation induces an obvious ordering on explanations.
Reference: <author> Pearl, J. </author> <year> (1988). </year> <title> Probabilistic Reasoning in Intelligent Systems. </title> <address> San Francisco, Calif.: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: There is no notion of cognitive dissonance or surprise. The explanation is an (informed) guess about the possible world we are currently in, based on the evidence (which includes the explanandum). In some cases (e.g., <ref> (Pearl 1988) </ref>), the guess must specify the world completelyformulas describing sets of worlds are not allowed as explanations. This approach, which we call Maximum A Posteriori model (MAP) after (Shimony 1991), has been also known under other names: Most Probable Explanation (MPE) (Pearl 1988) and Scenario-Based Explanation (Henrion and Druzdzel 1990). <p> In some cases (e.g., <ref> (Pearl 1988) </ref>), the guess must specify the world completelyformulas describing sets of worlds are not allowed as explanations. This approach, which we call Maximum A Posteriori model (MAP) after (Shimony 1991), has been also known under other names: Most Probable Explanation (MPE) (Pearl 1988) and Scenario-Based Explanation (Henrion and Druzdzel 1990). Formally, according to Pearl, given an epistemic state K = hW; Pri, an explanation for E is simply a world w in which E is true. This notion of explanation induces an obvious ordering on explanations. <p> probability, and pointing out how the most probable one differs from the other likely scenarios. 5 Recall that a Bayesian network is an acyclic directed graph whose nodes represent primitive propositions (or random variables), together with conditional probability tables describing the probability of a node given instantiations of its parents <ref> (Pearl 1988) </ref>. 6 Shimony (1991) calls this the overspecification problem. for symptom s might be d 1 , or, more precisely, the world characterized by s ^ d 1 ^ :d 2 . <p> We just reinterpret all the nodes so that a node labeled X talks about the agent's belief in X, moralize the graph and change all the directed edges to undirected edges. The resulting Markov network <ref> (Pearl 1988) </ref> captures the causal as well as probabilistic dependencies between the agent's beliefs. Note that the resulting network is no longer asymmetric.
Reference: <author> Pearl, J. </author> <year> (1995). </year> <title> Causal diagrams for empirical research. </title> <journal> Biometrika, </journal> <volume> 82(4), </volume> <pages> 669-709, </pages> <month> December </month> <year> 1995. </year>
Reference: <author> Salmon, W. C. </author> <year> (1984). </year> <title> Scientific Explanation and the Causal Structure of the World. </title> <publisher> Princeton University Press. </publisher>
Reference: <author> Shimony, S. E. </author> <year> (1991). </year> <title> Explanation, irrelevance and statistical independence. </title> <booktitle> In Proc. National Conference on Artificial Intelligence (AAAI '91), </booktitle> <pages> pp. 482-487. </pages>
Reference-contexts: In some cases (e.g., (Pearl 1988)), the guess must specify the world completelyformulas describing sets of worlds are not allowed as explanations. This approach, which we call Maximum A Posteriori model (MAP) after <ref> (Shimony 1991) </ref>, has been also known under other names: Most Probable Explanation (MPE) (Pearl 1988) and Scenario-Based Explanation (Henrion and Druzdzel 1990). Formally, according to Pearl, given an epistemic state K = hW; Pri, an explanation for E is simply a world w in which E is true. <p> The relevant nodes include the evidence nodes and only ancestors of evidence nodes can be relevant. Roughly speaking, an ancestor of a given node is irrelevant if it has the property that it is independent of that node given the values of the other ancestors. In <ref> (Shimony 1991) </ref>, the best explanation is taken to be the one with the highest posterior probability. <p> Shimony's work can be viewed as an attempt to provide principles as to when to consider disjunctive explanations. The partial explanations of <ref> (Shimony 1991) </ref> are sets of worlds where the truth values of some primitive propositions are fixed, while the rest can be arbitrary. The sets of partial explanations of (Shimony 1993) correspond to more general sets of worlds, but there are still significant restrictions.
Reference: <author> Shimony, S. E. </author> <year> (1993). </year> <title> Relevant explanations: Allowing disjunctive assignments. </title> <booktitle> In Proc. Ninth Conference on Uncertainty in Artificial Intelligence (UAI '93), </booktitle> <pages> pp. 200-207. </pages>
Reference-contexts: Roughly speaking, an ancestor of a given node is irrelevant if it has the property that it is independent of that node given the values of the other ancestors. In (Shimony 1991), the best explanation is taken to be the one with the highest posterior probability. In <ref> (Shimony 1993) </ref>, this is extended to allow explanations to be sets of partial truth assignments, subject to certain constraints (discussed in more detail in Section 4.3.) 3.2 A CRITIQUE The MAP approach has an advantage over G ardenfors': it doesn't require contraction. However, it has its own problems. <p> The partial explanations of (Shimony 1991) are sets of worlds where the truth values of some primitive propositions are fixed, while the rest can be arbitrary. The sets of partial explanations of <ref> (Shimony 1993) </ref> correspond to more general sets of worlds, but there are still significant restrictions. For example, the disjunctive explanation must correspond to a node already in the network and the probability of the explanandum must be the same for every disjunct in the disjunctive explanation.
Reference: <author> Suermondt, H. J. </author> <year> (1992). </year> <title> Explanation in Bayesian Belief Networks. </title> <type> Ph. D. thesis, </type> <institution> Stanford University. </institution>
Reference-contexts: Indeed, in experiments with medical diagnosis systems, medical students not only trusted the system more when presented with an explanation of the diagnosis, but also were more confident about disagreeing with it when the explanations did not account adequately for all of the aspects of the case <ref> (Suermondt and Cooper 1992) </ref>. Explanation can also play an important role in refining and debugging probabilistic systems. An incorrect or partially correct explanation should be the best indication to an expert of a potential problem.
Reference: <author> Suermondt, H. J. and G. F. </author> <title> Cooper (1992). An evaluation of explanations of probabilistic inference. </title> <booktitle> In Proc. of the Sixteenth Annual Symposium on Computer Applications in Medical Care, </booktitle> <pages> pp. 579-585. </pages>
Reference-contexts: Indeed, in experiments with medical diagnosis systems, medical students not only trusted the system more when presented with an explanation of the diagnosis, but also were more confident about disagreeing with it when the explanations did not account adequately for all of the aspects of the case <ref> (Suermondt and Cooper 1992) </ref>. Explanation can also play an important role in refining and debugging probabilistic systems. An incorrect or partially correct explanation should be the best indication to an expert of a potential problem.
References-found: 22

