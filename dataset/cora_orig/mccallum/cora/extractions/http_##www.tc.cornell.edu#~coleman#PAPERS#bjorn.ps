URL: http://www.tc.cornell.edu/~coleman/PAPERS/bjorn.ps
Refering-URL: http://www.tc.cornell.edu/~coleman/papers.html
Root-URL: http://www.tc.cornell.edu
Title: Efficient Computation of Structured Gradients using Automatic Differentiation  
Author: Thomas F. Coleman Gudbjorn F. Jonsson 
Date: April 28, 1997  
Note: The  
Abstract: Cornell Theory Center Technical Report CTC97TR272 Abstract The advent of robust automatic differentiation tools is an exciting and important development in scientific computing. It is particularily noteworthy that the gradient of a scalar-valued function of many variables can be computed with essentially the same time complexity as required to evaluate the function itself. This is true, in theory, when the "reverse mode" of automatic differentiation is used (whereas the "forward mode" introduces an additional factor corresponding to the problem dimension). However, in practise performance on large problems can be significantly (and unacceptably) worse than predicted. In this paper we illustrate that when natural structure is exploited fast gradient computation can be recovered, even for large dimensional problems.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. H. Bischof, A. Carle, P. M. Khademi, and A. Mauer, ADIFOR 2.0: </author> <title> Automatic differentiation of Fortran 77 programs, </title> <journal> IEEE Computational Science & Engineering, </journal> <volume> 3(3) </volume> <year> 18-32,1996. </year>
Reference-contexts: The ADIFOR tool <ref> [1] </ref> is used for the numerical experiments in [2] and AD is compared to hand-coding and finite differences. However, not all functions are partially separable and we will examine two such cases. Furthermore, ADIFOR has only forward mode and we explore the use of reverse mode as well.
Reference: [2] <author> C. H. Bischof, A. Bouaricha, P. M. Khademi, and J. J. </author> <title> More, Computing gradients in large-scale optimization using automatic differentiation, </title> <type> Preprint MCS-P488-0195, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1995 </year>
Reference-contexts: In particular, our approach is to break the problem into manageable pieces, defined by the natural structure of the problem, and whenever possible take advantage of sparsity. Numerical experiments are made to compare the various approaches, including a straightforward application of reverse mode. Bischof, Bouaricha, Khademi and More <ref> [2] </ref> explore the use of AD to compute the gradient of a partially separable function, i.e., a function f : R n ! R that can be written in the form f (x) = i=1 where each f i depends on p i t n variables. <p> The ADIFOR tool [1] is used for the numerical experiments in <ref> [2] </ref> and AD is compared to hand-coding and finite differences. However, not all functions are partially separable and we will examine two such cases. Furthermore, ADIFOR has only forward mode and we explore the use of reverse mode as well. <p> Furthermore, ADIFOR has only forward mode and we explore the use of reverse mode as well. When computing a sparse Jacobian matrix using AD, graph-coloring algorithms can be used to significantly reduce the amount of work. The algorithm described in <ref> [2] </ref> illustrates the use of one-sided coloring. Two-sided coloring [3] combines the powers of both forward and reverse mode, by constructing thin matrices V and W so that the Jacobian J can be determined from the pair (JV , W T J).
Reference: [3] <author> T. F. Coleman and A. Verma, </author> <title> The efficient computation of sparse Jacobian matrices using automatic differentiation, </title> <type> Tech. Rep. </type> <institution> TR95-1557, Department of Computer Science, Cornell University, </institution> <month> November </month> <year> 1995. </year> <note> To appear in SIAM Journal on Scientific Computing. </note>
Reference-contexts: Furthermore, ADIFOR has only forward mode and we explore the use of reverse mode as well. When computing a sparse Jacobian matrix using AD, graph-coloring algorithms can be used to significantly reduce the amount of work. The algorithm described in [2] illustrates the use of one-sided coloring. Two-sided coloring <ref> [3] </ref> combines the powers of both forward and reverse mode, by constructing thin matrices V and W so that the Jacobian J can be determined from the pair (JV , W T J).
Reference: [4] <author> T. F. Coleman and A. Verma, </author> <title> Structure and efficient Jacobian Calculation, in Computational Differentiation: Techniques, Applications and Tools, </title> <editor> M. Berz, C. Bischof, G. Corliss and A. Griewank, editors, </editor> <publisher> SIAM, </publisher> <address> Philadelphia, Penn, </address> <year> 1996, </year> <pages> pp. 149-159. </pages>
Reference-contexts: The algorithm described in [2] illustrates the use of one-sided coloring. Two-sided coloring [3] combines the powers of both forward and reverse mode, by constructing thin matrices V and W so that the Jacobian J can be determined from the pair (JV , W T J). Coleman and Verma <ref> [4, 6] </ref> show how sparsity and structure can be exploited to compute Jaco-bian and Hessian matrices efficiently using AD. <p> The code that is differentiated computes f (x) using a sparse solver for symmetric and positive definite systems to solve the system Ay = ~ F . 2. Extended Jacobian: Following the structural ideas of Coleman and Verma <ref> [4] </ref> we form the extended function F E : x ! f (y) : (6) We use AD with graph-coloring techniques to compute the extended Jacobian J E = A x y ~ J A ! (both the blocks A x y ~ J and A are sparse).
Reference: [5] <author> T. F. Coleman and A. Verma, ADMIT-1: </author> <title> Automatic differentiation and MATLAB interface toolbox, </title> <type> Tech. Rep. </type> <institution> CTC97TR271, Theory Center, Cornell University, </institution> <month> March </month> <year> 1997. </year>
Reference-contexts: Note that in Methods 2 and 3 two sparse systems are solved: Ay = ~ F and Aw = r y f : (8) The software used is MATLAB, ADOLC, ADMIT and SSPD. ADOLC [7] is an automatic differentiation tool for C/C++ and ADMIT <ref> [5] </ref> is a MATLAB interface built on top of ADOLC, that also includes the graph-coloring algorithms. It offers both one-sided and two-sided coloring. SSPD 1 is a package written in C for solving large, sparse, symmetric and positive definite systems.
Reference: [6] <author> T. F. Coleman and A. Verma, </author> <title> Structure and efficient Hessian calculation, </title> <type> Tech. Rep. </type> <institution> CTC96TR258, Theory Center, Cornell University, </institution> <year> 1996. </year>
Reference-contexts: The algorithm described in [2] illustrates the use of one-sided coloring. Two-sided coloring [3] combines the powers of both forward and reverse mode, by constructing thin matrices V and W so that the Jacobian J can be determined from the pair (JV , W T J). Coleman and Verma <ref> [4, 6] </ref> show how sparsity and structure can be exploited to compute Jaco-bian and Hessian matrices efficiently using AD.
Reference: [7] <author> A. Griewank, D. Juedes and J. Utke, ADOL-C, </author> <title> a package for the automatic differentiation of algorithms written in C/C++, </title> <journal> ACM Trans. Math. Software, </journal> <volume> 22 (1996), </volume> <pages> pp. 131-167. 8 </pages>
Reference-contexts: Subsequently formula (5) is used to form r x f . Note that in Methods 2 and 3 two sparse systems are solved: Ay = ~ F and Aw = r y f : (8) The software used is MATLAB, ADOLC, ADMIT and SSPD. ADOLC <ref> [7] </ref> is an automatic differentiation tool for C/C++ and ADMIT [5] is a MATLAB interface built on top of ADOLC, that also includes the graph-coloring algorithms. It offers both one-sided and two-sided coloring. SSPD 1 is a package written in C for solving large, sparse, symmetric and positive definite systems. <p> f (x) is given by [rf (x)] T = [rf 0 (y k )] T J k1 J k2 J 1 J 0 (11) 2 This occurs when the number of variables stored exceeds the size of the parameter bufsize in ADOLC, which determines how much memory is allocated (see <ref> [7] </ref>). 5 where J i is the Jacobian of S at y i . We explore several different methods to compute the gradient: 1. Straightforward use of reverse mode: Reverse mode of AD is used on the program calculating the whole function. 2.
References-found: 7

