URL: http://www.cs.yale.edu/users/rasmussen/lib/papers/AAAI96.ps.gz
Refering-URL: http://www.cs.yale.edu/users/rasmussen/research.html
Root-URL: http://www.cs.yale.edu
Email: rasmuss@powered.cs.yale.edu, hager@cs.yale.edu  
Title: Robot Navigation Using Image Sequences  
Author: Christopher Rasmussen and Gregory D. Hager 
Address: 51 Prospect Street New Haven, CT 06520-8285  
Affiliation: Department of Computer Science, Yale University  
Abstract: We describe a framework for robot navigation that exploits the continuity of image sequences. Tracked visual features both guide the robot and provide predictive information about subsequent features to track. Our hypothesis is that image-based techniques will allow accurate motion without a precise geometric model of the world, while using predictive information will add speed and robustness. A basic component of our framework is called a scene, which is the set of image features stable over some segment of motion. When the scene changes, it is appended to a stored sequence. As the robot moves, correspondences and dissimilarities between current, remembered, and expected scenes provide cues to join and split scene sequences, forming a map-like directed graph. Visual servoing on features in successive scenes is used to traverse a path between robot and goal map locations. In our framework, a human guide serves as a scene recognition oracle during a map-learning phase; thereafter, assuming a known starting position, the robot can independently determine its location without general scene recognition ability. A prototype implementation of this framework uses as features color patches, sum-of-squared differences (SSD) subimages, or image projections of rectangles. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Barrett, E.; Brill, M.; Haag, N.; and Payton, P. </author> <year> 1992. </year> <title> Invariant Linear Methods in Photogrammetry and Model-matching. In Geometric Invariance in Computer Vision, Mundy, </title> <editor> J., and Zisserman, A. eds., </editor> <address> Cambridge, Mass.: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Since we do not have an underlying geometric map, all feature prediction must operate directly from image information. Thus, we have adapted a result from the theory of geometric invariance <ref> (Barrett et al. 1992) </ref> to vehicles moving in a plane.
Reference: <author> Chaumette, F.; Rives, P.; and Espian, B. </author> <year> 1991. </year> <title> Positioning of a Robot with respect to an Object, Tracking it, and Estimating its Velocity by Visual Servoing. </title> <booktitle> In Proc. IEEE Inter. Conf. Robotics and Automation, </booktitle> <pages> 2248-53. </pages> <address> Sacramento, CA, </address> <month> April. </month>
Reference-contexts: For our limited experiments, we have found matching rectangles by aspect ratio alone satisfactory, although we have also experimented with using statistics on the gray value distribution of the interior of the rectangle as well. Based on the techniques described in <ref> (Chaumette, Rives, & Espian 1991) </ref>, the four corners of one rectangle are sufficient to servo on. Thus, the scene evaluation function C (s) is simply that s 6= ;.
Reference: <author> Engelson, S. P. </author> <year> 1994. </year> <title> Passive Map Learning and Visual Place Recognition. </title> <type> Ph.D. </type> <institution> diss., Dept. of Comp. Sci., Yale Univ. </institution>
Reference-contexts: In (Taylor & Kriegman 1994) the model of the environment is a graph constructed while exploring, based on visibility of barcode landmarks. Similarly, in (Kortenkamp et al. 1992), (Kuipers & Byun 1981), and <ref> (Engelson 1994) </ref>, the robot learns a model of the environment by building a graph based on topological connectivity of interesting locations, but it attempts to decide what constitute landmarks on its own.
Reference: <author> Fennema, C.; Hanson, A.; Riseman, E.; Beveridge, J.; and Kumar, R. </author> <year> 1990. </year> <title> Model-Directed Mobile Robot Navigation. </title> <journal> IEEE Trans. Systems, Man, and Cybernetics 20(6): </journal> <pages> 1352-69. </pages>
Reference-contexts: Navigation then becomes a problem of moving from view to view. The desired motion is computed using visual feedback between what the robot currently sees, and what it saw when it was at the goal location before. The work described in <ref> (Fennema et al. 1990) </ref> is similar to ours. They use a servoing approach to navigation, but with reference to an accurate geometric model of the environment which we do not assume.
Reference: <author> Hager, G. </author> <year> 1995. </year> <title> The `X-Vision' System: A General-Purpose Substrate for Vision-Based Robotics. </title> <booktitle> Workshop on Vision for Robotics. </booktitle>
Reference-contexts: When a marker appears, the current scene is added. When a marker disappears, the last scene containing the disappearing marker is added. In essence, scenes serve as key frames for reconstructing movement later through visual servoing <ref> (Hutchinson, Hager, & Corke 1995) </ref>. A location is a set of two or more scenes from different sequences that are the same under the equivalency function; this is where sequences intersect. There are two kinds of locations: divergences and convergences. <p> We are using the XVision system <ref> (Hager 1995) </ref> for real-time visual tracking as the basis for our marker identification and tracking algorithms, which are described in detail in (Rasmussen & Hager 1996). Rectangles were chosen primarily because they are simple to detect and track, and in an indoor environment they are relatively common.
Reference: <author> Hong, J.; Tan, X.; Pinette, B.; Weiss, R.; and Riseman, E. </author> <year> 1992. </year> <title> Image-Based Homing. </title> <journal> IEEE Control Systems: </journal> <pages> 38-44. </pages>
Reference-contexts: Navigation proceeds by identifying landmarks along a path through the graph that are visible from their predecessors, and servoing on them in succession via visual feedback. Recognizing landmarks is a matching problem between 2-D image data and the 3-D world model. Other route-based approaches to navigation are described in <ref> (Hong et al. 1992) </ref> and (Zheng & Tsuji 1992). The robots in these papers are guided in a learning stage through the environment, periodically storing whole panoramic images.
Reference: <author> Hutchinson, S.; Hager, G.; and Corke, P. </author> <year> 1995. </year> <title> A Tutorial Introduction to Visual Servo Control. </title> <journal> IEEE Trans. Robotics and Automation. </journal>
Reference-contexts: When a marker appears, the current scene is added. When a marker disappears, the last scene containing the disappearing marker is added. In essence, scenes serve as key frames for reconstructing movement later through visual servoing <ref> (Hutchinson, Hager, & Corke 1995) </ref>. A location is a set of two or more scenes from different sequences that are the same under the equivalency function; this is where sequences intersect. There are two kinds of locations: divergences and convergences.
Reference: <author> Kortenkamp, D.; Weymouth, T.; Chown, E.; and Kaplan, S. </author> <year> 1992. </year> <title> A scene-based, multi-level representation for mobile robot spatial mapping and navigation. </title> <type> Technical Report, </type> <institution> CSE-TR-119-92, Univ. of Michigan. </institution>
Reference-contexts: In (Taylor & Kriegman 1994) the model of the environment is a graph constructed while exploring, based on visibility of barcode landmarks. Similarly, in <ref> (Kortenkamp et al. 1992) </ref>, (Kuipers & Byun 1981), and (Engelson 1994), the robot learns a model of the environment by building a graph based on topological connectivity of interesting locations, but it attempts to decide what constitute landmarks on its own. <p> A graph is built up by matching new images with the known corpus; paths can be planned in the completed graph with a standard algorithm. When executing a path, image-based servoing is used to traverse individual edge segments. <ref> (Kortenkamp et al. 1992) </ref> argue that taking snapshots at regular intervals leads to the storage of much unnecessary information. Moreover, new scenes must be matched against all remembered ones, an inefficient process.
Reference: <author> Kosaka, A., and Kak, A. C. </author> <year> 1992. </year> <title> Fast Vision-Guided Mobile Robot Navigation Using Model-Based Reasoning and Prediction of Uncertainties. CVGIP: </title> <booktitle> Image Understanding 56(3): </booktitle> <pages> 271-329. </pages>
Reference-contexts: Navigation depends on the notion of location, which can be characterized in a number of ways. Strictly metrical approaches describe it as a position in a fixed world coordinate system. The system in <ref> (Kosaka & Kak 1992) </ref> knows the environment's three-dimensional geometry and its initial position and orientation; the navigation task is to find a path to a goal position and orientation. Kalman filtering is used to reconstruct and predict the coordinates of model features, which are matched at regular intervals during motion.
Reference: <author> Kuipers, B., and Byun, Y. T. </author> <year> 1981. </year> <title> A robot exploration and mapping strategy based on a semantic hierarchy of spatial representations. </title> <booktitle> Robotics and Autonomous Systems 8: </booktitle> <pages> 47-63. </pages>
Reference-contexts: In (Taylor & Kriegman 1994) the model of the environment is a graph constructed while exploring, based on visibility of barcode landmarks. Similarly, in (Kortenkamp et al. 1992), <ref> (Kuipers & Byun 1981) </ref>, and (Engelson 1994), the robot learns a model of the environment by building a graph based on topological connectivity of interesting locations, but it attempts to decide what constitute landmarks on its own. <p> The robot in (Ko-rtenkamp et al. 1992) learns the environment's topol ogy from doorways detected by sonar while storing im-ages for place recognition. The robot in <ref> (Kuipers & Byun 1981) </ref> uses sonar to detect places that maximize a distinctiveness function and to match or add them to its map. A disadvantage of graph-based approaches is their tendency to rely on a solution to the difficult problem of general place recognition.
Reference: <author> Leonard, J.; Durrant-Whyte, H.; and Cox, I. </author> <year> 1990. </year> <title> Dynamic Map Building for an Autonomous Mobile Robot. </title> <booktitle> IEEE Inter. Workshop on Intelligent Robots and Systems: </booktitle> <pages> 89-95. </pages>
Reference-contexts: Kalman filtering is used to reconstruct and predict the coordinates of model features, which are matched at regular intervals during motion. The sonar-based algorithm of <ref> (Leonard, Durrant-Whyte, & Cox 1990) </ref> builds its own map of beacons' absolute positions using Kalman filtering to account for uncertainty. Their mapping process is dynamic in that the robot constantly compares sensory evidence with expectations, and updates its map and/or position estimate when the two disagree.
Reference: <author> Rasmussen, C., and Hager, G. </author> <year> 1996. </year> <title> Robot Navigation Using Image Sequences. </title> <type> Technical Report, </type> <institution> DCS-TR-1103, Yale Univ. </institution>
Reference-contexts: In Figure 2 the map-making process is illustrated for a sample environment. By continuing this process of exploration, the robot would eventually arrive at a complete map of the floor, but even in its intermediate forms it has partial usefulness. A more complete exposition is given in <ref> (Rasmussen & Hager 1996) </ref>. 1. If unfamiliar, does 9 s i 2 S j = (s 0 ; : : : ; s n ) s.t. s viewed s i ? (a) Yes (convergence): i. Add location fs viewed ; s i g to fLg ii. <p> Correcting such errors is critical, so we have investigated a predictive mode for marker location in which the robot dynamically estimates the image locations of M jk as it moves <ref> (Rasmussen & Hager 1996) </ref>. Since we do not have an underlying geometric map, all feature prediction must operate directly from image information. Thus, we have adapted a result from the theory of geometric invariance (Barrett et al. 1992) to vehicles moving in a plane. <p> We are using the XVision system (Hager 1995) for real-time visual tracking as the basis for our marker identification and tracking algorithms, which are described in detail in <ref> (Rasmussen & Hager 1996) </ref>. Rectangles were chosen primarily because they are simple to detect and track, and in an indoor environment they are relatively common. The default marker-finding method is random search over the entire image.
Reference: <author> Taylor, C., and Kriegman, D. </author> <year> 1994. </year> <title> Vision-Based Motion Planning and Exploration Algorithms for Mobile Robots. </title> <booktitle> Workshop on the Algorithmic Foundations of Robotics. </booktitle>
Reference-contexts: Graph-based techniques smear out exactness somewhat by relating position regions to nodes or "places" in a connectivity graph, and rely on closed-loop feedback to guide the robot from place to place. In <ref> (Taylor & Kriegman 1994) </ref> the model of the environment is a graph constructed while exploring, based on visibility of barcode landmarks.
Reference: <author> Turk, M.; Morgenthaler, D.; Gremban, K.; and Marra, M. </author> <year> 1988. </year> <title> VITS|A Vision System for Autonomous Land Vehicle Navigation. </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence 10(3): </journal> <pages> 342-61. </pages>
Reference-contexts: The servoing we have focused on so far is transformational, insofar as the robot tries, by moving, to make a tracked object change appearance to look like a target object. A different, homeostatic paradigm is suggested by road-following and wall-following systems <ref> (Turk et al. 1988) </ref>. In them, the goal is not to change one sensory entity into another through movement, but rather to move while maintaining a certain constancy of sensory input. Road-followers seek to move forward while keeping the sides of the road centered.
Reference: <author> Zheng, Z., and Tsuji, S. </author> <year> 1992. </year> <title> Panoramic Representation for Route Recognition by a Mobile Robot. </title> <journal> Inter. Journal of Computer Vision 9(1): </journal> <pages> 55-76. </pages>
Reference-contexts: Recognizing landmarks is a matching problem between 2-D image data and the 3-D world model. Other route-based approaches to navigation are described in (Hong et al. 1992) and <ref> (Zheng & Tsuji 1992) </ref>. The robots in these papers are guided in a learning stage through the environment, periodically storing whole panoramic images. A graph is built up by matching new images with the known corpus; paths can be planned in the completed graph with a standard algorithm.
References-found: 15

