URL: http://www.umiacs.umd.edu/users/vishkin/PUBLICATIONS/rimon.ps
Refering-URL: http://www.umiacs.umd.edu/users/vishkin/PUBLICATIONS/papers.html
Root-URL: 
Title: Two Computer Systems Paradoxes: Serialize-to-Parallelize, and Queuing Concurrent-Writes providing useful, easy-to-program programming paradigms to improve
Author: Rimon Orni and Uzi Vishkin 
Note: Assuming that  computer systems is of interest, this work is a modest  This work may be the first to relate  
Date: September 17, 1995  
Abstract: We present and examine the following Serialize-to-Parallelize Paradox: suppose a programmer has a parallel algorithm in mind; the programmer must serialize the algorithm, and is actually trained to suppress its parallelism, while writing code; later, however, compilation and runtime techniques are used to reverse the results of this serialization effort and extract as much parallelism as possible. This work actually provides examples where parallel or parallel-style code enables extracting more parallelism than standard serial code. The "arbitrary concurrent-write" convention is useful in parallel algorithms and programs and appears to be not too difficult to implement in hardware for serial machines. Still, typically concurrent-writes to the same memory location in a program are implemented by queuing the write operations, thus requiring time linear in the number of writes. We call this the Queuing Concurrent-Writes Paradox. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> ANSI. </author> <title> ANSI Fortran draft S8, </title> <type> version 111. </type>
Reference-contexts: We only state that we would like to see parallel programs which implement in code the PRAM programmer's model, much in the way of parallel programming languages such as Fortran90 <ref> [1] </ref>, HPF [11], and C* [21], or NESL for functional programming [3]. Programming should be relatively easy, as suggested by the wealth of literature on this general approach to parallel algorithms and parallel programming. <p> sequential C are the following: DO PARALLEL (Lines 10 and 16) Directs the compiler to treat the applicable section (in case of Line 10 this section would be from Line 11 to Line 27) as parallelizable, much in the way that is done in parallel programming languages such as Fortran90 <ref> [1] </ref>, HPF [11], and C* [21]. A possible execution is one where the system spawns a number of threads, each with a unique index, based on the argument in the DO PARALLEL command.
Reference: [2] <author> T. M. Austin and G. S. Sohi. </author> <title> Dynamic dependency analysis of ordinary programs. </title> <booktitle> In 19th Annual International Symposium on Computer Architecture, </booktitle> <year> 1992. </year>
Reference-contexts: Much effort has been devoted to measuring the amount of parallelism which can be automatically extracted from programs by these techniques <ref> [17, 5, 19, 26, 2] </ref>. These studies suggest that the amount of parallelism which can be thus extracted is limited, even when we invest much hardware and software in the effort, and is likely to be insufficient as the reliance on ILP increases. <p> Both studies show that any of several restrictions of these assumptions cuts the mean possible amount of parallelism by at least half. Austin and Sohi <ref> [2] </ref> use similar models, as well. Wall measured the parallelism extracted from a suite of 18 programs under different abstract machine model assumptions. When assuming his aggressive "perfect" machine model, he found that the mean parallelism is 23.6 over the chosen suite of benchmarks.
Reference: [3] <author> G. E. Blelloch, S. Chatterjee, J. C. Hardwick, J. Sipelstein, and M. Zagha. </author> <title> Implementation of a portable nested data-parallel language. </title> <booktitle> In 4th Symposium on Principles and Practice of Parallel Programming. ACM SIGPLAN, </booktitle> <year> 1993. </year>
Reference-contexts: We only state that we would like to see parallel programs which implement in code the PRAM programmer's model, much in the way of parallel programming languages such as Fortran90 [1], HPF [11], and C* [21], or NESL for functional programming <ref> [3] </ref>. Programming should be relatively easy, as suggested by the wealth of literature on this general approach to parallel algorithms and parallel programming. We demonstrate the usefulness of our approach by measuring the added amount of parallelism which could be extracted by the system we envision.
Reference: [4] <author> G. E. Blelloch and G. W. Sabot. </author> <title> Compiling collection-oriented languages onto massively parallel computers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(2) </volume> <pages> 119-134, </pages> <year> 1990. </year>
Reference-contexts: Since these dependences can be resolved by renaming techniques, and our simulator assumes perfect renaming, we did not even need to implement the arrays in the BFS program used for the tests. The second possible implementation would use techniques for flattening nested parallelism <ref> [4] </ref>. In the BFS algorithm, this would mean replacing the outer loop (Line 10) and the inner loop (Line 16) by a single "full" loop, where each iteration of an inner loop forms an iteration of the full loop.
Reference: [5] <author> D.-K. Chen, H.-M. Su, and P.-C. Yew. </author> <title> The impact of synchronization and granularity on parallel systems. </title> <booktitle> In 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 239-248, </pages> <year> 1990. </year>
Reference-contexts: Much effort has been devoted to measuring the amount of parallelism which can be automatically extracted from programs by these techniques <ref> [17, 5, 19, 26, 2] </ref>. These studies suggest that the amount of parallelism which can be thus extracted is limited, even when we invest much hardware and software in the effort, and is likely to be insufficient as the reliance on ILP increases. <p> Specifically, we focus on the works of [19, 26] that measured this parallelism using a simulator. Other works annotated the high level program, adding code so that data regarding parallelism is collected at runtime <ref> [17, 5] </ref>. When working with an instruction trace, The simulator analyzes the dependences for each instruction and determines accordingly the (earliest) parallel cycle in which the instruction can be executed. Each of these recent works makes several alternative assumptions regarding the ability of the simulated machine to extract parallelism.
Reference: [6] <author> D. M. Eckstein. </author> <title> Parallel Processing using Depth-First Search and Breadth-First Search. </title> <type> PhD thesis, </type> <institution> Computer Science Department, University of Iowa, </institution> <address> Iowa City, IA, </address> <year> 1977. </year>
Reference-contexts: Figure 3 shows the sequential algorithm (pseudo) code (SAC) of this program. As has been observed by Eckstein in <ref> [6] </ref>, all the nodes in the same layer could in fact be visited concurrently. <p> The order of the neighbors is arbitrary but is set throughout the execution. 5.2 The Parallel Algorithm Although BFS is known primarily as a sequential algorithm, it is also known that certain parts of the code can be executed in parallel <ref> [6] </ref>. First of all when visiting a certain node, we sequentially check all its neighbors and put them into the queue if they have not yet been visited. In fact we could treat all the neighbors in parallel since they are independent of each other.
Reference: [7] <author> J. Ferrante, K. J. Ottenstein, and J. D. Warren. </author> <title> The program dependence graph and its uses in optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <year> 1987. </year>
Reference-contexts: We also enhanced the LW Simulator so that it could simulate the capability of the envisioned system in addition to the automatic extraction capabilities it originally assumes (see Section 4.2). The new simulator is henceforth referred to as the Enhanced simulator. Ferrante et al. <ref> [7] </ref> introduced the Program Dependence Graph (PDG) which is a Directed 5 Acyclic Graph (DAG) representation of the program, incorporating the true data and con-trol dependences. The nodes of the PDG represent the instructions and the edges represent dependences between the instructions.
Reference: [8] <author> P. B. Gibbons, Y. Matias, and V. Ramachandran. </author> <title> The QRQW PRAM: Accounting for contention in parallel algorithms. </title> <booktitle> In Proceedings of the Fifth Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 638-648, </pages> <year> 1994. </year>
Reference-contexts: This is an overkill: correctness is indeed preserved but parallelism is lost. We see no inherent difficulty in preserving both parallelism and correctness for a parallel program being executed on a uniprocessor. It is interesting to relate this last paradox to the work of Gibbons, Matias and Ra-machandrans in <ref> [8] </ref>. They contributed a modeling of the queuing of concurrent writes (QRQW PRAM model).
Reference: [9] <author> J. L. Hennessy and D. A. Patterson. </author> <title> Computer Architecture A Quantitative Approach. </title> <publisher> Morgan Kaufman Publishers Inc., </publisher> <address> San Mateo, California, </address> <year> 1990. </year>
Reference-contexts: The full results (see appendix) show improvements ranging from 5% to 247%. 3 Studies on the Automatic Extraction of ILP Compiler techniques for extracting ILP include software pipelining [18, 14, 16], loop unrolling and trace scheduling <ref> [9] </ref>. Runtime techniques include speculative execution, conditional execution, and branch prediction [9]. One kind of machine which uses the parallelism 9 found by compiler or runtime techniques is multiscalar machines [27]. <p> The full results (see appendix) show improvements ranging from 5% to 247%. 3 Studies on the Automatic Extraction of ILP Compiler techniques for extracting ILP include software pipelining [18, 14, 16], loop unrolling and trace scheduling <ref> [9] </ref>. Runtime techniques include speculative execution, conditional execution, and branch prediction [9]. One kind of machine which uses the parallelism 9 found by compiler or runtime techniques is multiscalar machines [27]. <p> (ii) Indirect jumps: The lack of this foreknowledge would mean that all instructions after the jump cannot be executed until the target address is resolved. perfect register renaming Register storage dependences which are not required for preserving correctness of the original code can be eliminated by using register renaming techniques <ref> [9] </ref>.
Reference: [10] <author> J. L. Hennessy and D. A. Patterson. </author> <title> Computer Architecture A Quantitative Approach, page 346. </title> <publisher> Morgan Kaufman Publishers Inc., </publisher> <address> San Mateo, California, </address> <note> second edition, 1994. Uncorrected preliminary manuscript. </note>
Reference-contexts: To run such programs efficiently, the hardware may need to have the ability to handle a number of concurrent threads in more efficient ways than in many existing machines. However, this is hardly a disadvantage in current times. In <ref> [10] </ref>, Hennessy and Patterson imply that an increase in ability to efficiently handle multi-threading is already needed in the context of exploiting large amounts of parallelism which are extracted from serial code.
Reference: [11] <author> High Performance Fortran Forum. </author> <title> High performance Fortran language specification, </title> <year> 1993. </year>
Reference-contexts: We only state that we would like to see parallel programs which implement in code the PRAM programmer's model, much in the way of parallel programming languages such as Fortran90 [1], HPF <ref> [11] </ref>, and C* [21], or NESL for functional programming [3]. Programming should be relatively easy, as suggested by the wealth of literature on this general approach to parallel algorithms and parallel programming. <p> are the following: DO PARALLEL (Lines 10 and 16) Directs the compiler to treat the applicable section (in case of Line 10 this section would be from Line 11 to Line 27) as parallelizable, much in the way that is done in parallel programming languages such as Fortran90 [1], HPF <ref> [11] </ref>, and C* [21]. A possible execution is one where the system spawns a number of threads, each with a unique index, based on the argument in the DO PARALLEL command.
Reference: [12] <author> W. W. Hwu, T. M. Conte, and P. P. Chang. </author> <title> Comparing software and hardware schemes for reducing the cost of branches. </title> <booktitle> In 13th International Symposium on Computer Architecture, </booktitle> <pages> pages 224-233, </pages> <year> 1989. </year>
Reference-contexts: Accuracy of current branch prediction techniques has been shown to exceed 85% for many applications <ref> [20, 12] </ref>. perfect memory disambiguation At the beginning of the execution the contents of all address registers are already known.
Reference: [13] <author> J. JaJa. </author> <title> An Introduction to Parallel Algorithms. </title> <publisher> Addison Wesley Publishing Company, </publisher> <year> 1992. </year>
Reference-contexts: It is used by the so-called "work-time methodology" for describing parallel algorithms. This methodology was first presented in [23] (relying on a 1974 theorem by Brent for an abstract model of parallel computation). Later the book <ref> [13] </ref> used the methodology as a framework for describing most of the parallel algorithms presented there. For brevity, we avoid elaborating on this issue here. 4 where one may need to compare two programs, the one having a larger total number of operations but fewer parallel cycles than the other.
Reference: [14] <author> R. B. Jones and V. H. Allan. </author> <title> Software pipelining: An evaluation of enhanced pipelining. </title> <booktitle> In Proceedings of the 24th International Symposium on Microarchitecture, </booktitle> <pages> pages 82-92, </pages> <year> 1991. </year>
Reference-contexts: The full results (see appendix) show improvements ranging from 5% to 247%. 3 Studies on the Automatic Extraction of ILP Compiler techniques for extracting ILP include software pipelining <ref> [18, 14, 16] </ref>, loop unrolling and trace scheduling [9]. Runtime techniques include speculative execution, conditional execution, and branch prediction [9]. One kind of machine which uses the parallelism 9 found by compiler or runtime techniques is multiscalar machines [27].
Reference: [15] <author> D. E. Knuth. </author> <title> The Stanford GraphBase. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference-contexts: Machine Nodes Edges Queuing Improvement Model Yes No CD-MF 100 750 159.70 216.74 1.36 SP-CD-MF 100 750 218.69 351.81 1.90 Table 2: Comparison of Amounts of parallelism extracted from PASC version of code with the Enhanced simulator, with or without queuing of concurrent writes, for random graphs The Stanford GraphBase <ref> [15] </ref>. Results for all abstract machine models and additional input graphs appear in Section 6 and in the appendix. For brevity, we have excluded from the current paper results for other families of graphs. <p> Following [19], we assume perfect procedure inlining for both executions. This means that all call and return instructions are removed from the trace, as are instructions which manipulate the position of the stack pointer. For the results reported in this section, random graphs constructed using Knuth's Stan-ford GraphBase <ref> [15] </ref> were used as inputs to bfs. We bring here a sample of the tests we conducted. <p> The columns marked (3), or PASC+queuing, hold results for running the Enhanced simulator, with queuing of concurrent writes, over the PASC version. The random graphs used as input to BFS were all constructed using The Stanford GraphBase <ref> [15] </ref>, which generated them by choosing the endpoints for each edge uniformly at random from the set of nodes. 31 Num. Machine SAC PASC PASC PASC Improvements of Model + LW + LW + + Enh. Edges Sim.
Reference: [16] <author> S. M. Krishnamurthy. Ultra-pipelining: </author> <title> An efficient software pipelining algorithm. </title> <booktitle> In COMPCON Spring '93, </booktitle> <pages> pages 586-591, </pages> <year> 1993. </year>
Reference-contexts: The full results (see appendix) show improvements ranging from 5% to 247%. 3 Studies on the Automatic Extraction of ILP Compiler techniques for extracting ILP include software pipelining <ref> [18, 14, 16] </ref>, loop unrolling and trace scheduling [9]. Runtime techniques include speculative execution, conditional execution, and branch prediction [9]. One kind of machine which uses the parallelism 9 found by compiler or runtime techniques is multiscalar machines [27].
Reference: [17] <author> M. Kumar. </author> <title> Measuring parallelism in computation-intensive scientific/engineering applications. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(9) </volume> <pages> 1088-1098, </pages> <year> 1988. </year>
Reference-contexts: Much effort has been devoted to measuring the amount of parallelism which can be automatically extracted from programs by these techniques <ref> [17, 5, 19, 26, 2] </ref>. These studies suggest that the amount of parallelism which can be thus extracted is limited, even when we invest much hardware and software in the effort, and is likely to be insufficient as the reliance on ILP increases. <p> Specifically, we focus on the works of [19, 26] that measured this parallelism using a simulator. Other works annotated the high level program, adding code so that data regarding parallelism is collected at runtime <ref> [17, 5] </ref>. When working with an instruction trace, The simulator analyzes the dependences for each instruction and determines accordingly the (earliest) parallel cycle in which the instruction can be executed. Each of these recent works makes several alternative assumptions regarding the ability of the simulated machine to extract parallelism.
Reference: [18] <author> M. Lam. </author> <title> Software pipelining: An effective scheduling technique for vliw machines. </title> <booktitle> In SIGPLAN '88 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 318-328. </pages> <publisher> ACM, </publisher> <year> 1988. </year>
Reference-contexts: The full results (see appendix) show improvements ranging from 5% to 247%. 3 Studies on the Automatic Extraction of ILP Compiler techniques for extracting ILP include software pipelining <ref> [18, 14, 16] </ref>, loop unrolling and trace scheduling [9]. Runtime techniques include speculative execution, conditional execution, and branch prediction [9]. One kind of machine which uses the parallelism 9 found by compiler or runtime techniques is multiscalar machines [27].
Reference: [19] <author> M. S. Lam and R. P. Wilson. </author> <title> Limits of control flow on parallelism. </title> <booktitle> In 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 46-57, </pages> <year> 1992. </year>
Reference-contexts: Much effort has been devoted to measuring the amount of parallelism which can be automatically extracted from programs by these techniques <ref> [17, 5, 19, 26, 2] </ref>. These studies suggest that the amount of parallelism which can be thus extracted is limited, even when we invest much hardware and software in the effort, and is likely to be insufficient as the reliance on ILP increases. <p> The simulator we used to produce the instruction trace and to analyze it is the one used by Lam and Wilson in <ref> [19] </ref> for MIPS instruction code. We also used an enhanced form of the simulator which analyzes the instructions as if they were coming from parallel code (as explained in Section 4.2). <p> The results obtained in our tests are presented in Section 6, and finally Section 7 concludes our paper. 2 Overview of Tests and Results In studies done by Lam and Wilson <ref> [19] </ref> and by Wall [26] the amount of parallelism in programs was measured by running the instruction trace through a simulator, as explained below. <p> The abovementioned studies measured in this manner the parallelism extracted from a chosen set of standard benchmarks. For the strongest and most unrealistic abstract machine models, a mean parallelism of 23.6 was found in [26] and of 158.26 in <ref> [19] </ref>. When some of the unrealistic capabilities were modified to more feasible ones, the mean parallelism for their respective benchmarks dropped to 11.9 in [26] and to 39.62 in [19]. <p> the strongest and most unrealistic abstract machine models, a mean parallelism of 23.6 was found in [26] and of 158.26 in <ref> [19] </ref>. When some of the unrealistic capabilities were modified to more feasible ones, the mean parallelism for their respective benchmarks dropped to 11.9 in [26] and to 39.62 in [19]. We propose an envisioned system in which the programmer can identify parts of the code as parallelizable, and the compiler and hardware have the capability to utilize this parallelism, as well as extract parallelism automatically. <p> We examined the extent to which the envisioned system could increase the amount of parallelism which can be extracted overall. For this, we used the simulator written by Robert P. Wilson for the study in <ref> [19] </ref> (henceforth the LW simulator). We also enhanced the LW Simulator so that it could simulate the capability of the envisioned system in addition to the automatic extraction capabilities it originally assumes (see Section 4.2). The new simulator is henceforth referred to as the Enhanced simulator. <p> For (1) and (2) we simply ran the Lam and Wilson simulator over the MIPS trace of the respective code. In each of the tests we obtained results for each of the seven abstract machine models defined by Lam and Wilson in <ref> [19] </ref> (see section 4.1). Table 1 gives a representative sample of our results. <p> The literature reports numerous efforts to study and observe the amount of parallelism which can be automatically extracted from a program. For brevity, we elaborate only on works that measured the parallelism in the instruction trace of C or Fortran programs. Specifically, we focus on the works of <ref> [19, 26] </ref> that measured this parallelism using a simulator. Other works annotated the high level program, adding code so that data regarding parallelism is collected at runtime [17, 5]. <p> Such an abstract model is valuable as it provides a yardstick for the amount of parallelism which one could realistically expect. These two works consider also weaker abstract machine models, where less parallelism can be extracted. Each of the most aggressive abstract machine models in <ref> [19, 26] </ref> assumes the following capabilities: perfect branch prediction At the beginning of the execution it is already known which path will be taken by every branch. <p> its enhancements for simulating two different systems that do allow explicit parallel programming: Section 4.2 describes one in which concurrent-writes take one parallel cycle and Section 4.3 slows this by queuing concurrent-write. 4.1 The LW Simulator We worked with what we call the LW simulator, which was originally used in <ref> [19] </ref> to measure parallelism. Some of the basic results in the present paper were obtained using this simulator as is. Using pixie [24], the simulator produces the instruction trace of the program. It then analyzes the dependences and determines the parallel cycle in which to execute each instruction. <p> Using different combinations of these three techniques, we can measure parallelism for seven abstract models of machines defined by Lam and Wilson in <ref> [19] </ref> as follows: BASE No technique used for relaxing control flow constraints. CD Using only control dependence analysis. CD-MF Using control dependence analysis and executing multiple flows of control. SP Using only speculation with branch prediction. SP-CD Using speculation with branch prediction and control dependence analysis. <p> SP-CD Using speculation with branch prediction and control dependence analysis. SP-CD-MF Using all three techniques for relaxing control flow constraints. ORACLE Assuming there are no control flow constraints at all, only data dependences are considered. The machine has oracle knowledge of the control flow. (See <ref> [19] </ref> for more details and examples). 4.2 Enhancing the Simulator To measure the parallelism which could be achieved by using a parallel programmer's model, we enhanced the LW simulator so that it would disregard dependences between those blocks of code that the programmer specified as executable in parallel. <p> We have defined the amount of parallelism in a given piece of code as the ratio between the number of instructions in a sequential execution (i.e., total number of instructions executed) and the number of cycles required for the parallel execution of the code. Following <ref> [19] </ref>, we assume perfect procedure inlining for both executions. This means that all call and return instructions are removed from the trace, as are instructions which manipulate the position of the stack pointer.
Reference: [20] <author> S. McFarling and J. Hennessy. </author> <title> Reducing the cost of branches. </title> <booktitle> In 13th Annual International Symposium on Computer Architecture, Tokyo, </booktitle> <pages> pages 396-403, </pages> <year> 1986. </year>
Reference-contexts: Accuracy of current branch prediction techniques has been shown to exceed 85% for many applications <ref> [20, 12] </ref>. perfect memory disambiguation At the beginning of the execution the contents of all address registers are already known.
Reference: [21] <author> J. R. Rose and G. L. Steele, Jr. </author> <title> C*: An extended c language for data parallel programming. </title> <booktitle> In Proceedings Second International Conference on Supercomputing, </booktitle> <volume> volume 2, </volume> <pages> pages 2-16, </pages> <year> 1987. </year>
Reference-contexts: We only state that we would like to see parallel programs which implement in code the PRAM programmer's model, much in the way of parallel programming languages such as Fortran90 [1], HPF [11], and C* <ref> [21] </ref>, or NESL for functional programming [3]. Programming should be relatively easy, as suggested by the wealth of literature on this general approach to parallel algorithms and parallel programming. <p> DO PARALLEL (Lines 10 and 16) Directs the compiler to treat the applicable section (in case of Line 10 this section would be from Line 11 to Line 27) as parallelizable, much in the way that is done in parallel programming languages such as Fortran90 [1], HPF [11], and C* <ref> [21] </ref>. A possible execution is one where the system spawns a number of threads, each with a unique index, based on the argument in the DO PARALLEL command.
Reference: [22] <author> Y. Shiloach and U. Vishkin. </author> <title> An O(logn) parallel connectivity algorithm. </title> <journal> Journal of Algorithms, </journal> <volume> 3(1) </volume> <pages> 57-56, </pages> <year> 1982. </year>
Reference-contexts: In a parallel program several instructions that can be executed in parallel may include an update of the same variable. Suppose the programming language allows the programmer to specify that only one of these updates, chosen arbitrarily, should occur (the so-called Arbitrary Concurrent-Write primitive in <ref> [22] </ref>). Most current parallel (and serial) systems implement concurrent writes by queuing all the updates and handling them in sequence, relying on run-time protocols resolving "Write After Write" data dependences. This is an overkill: correctness is indeed preserved but parallelism is lost.
Reference: [23] <author> Y. Shiloach and U. Vishkin. </author> <title> An O(n 2 logn) parallel max-flow algorithm. </title> <journal> Journal of Algorithms, </journal> <volume> 3(2) </volume> <pages> 128-146, </pages> <year> 1982. </year>
Reference-contexts: It is used by the so-called "work-time methodology" for describing parallel algorithms. This methodology was first presented in <ref> [23] </ref> (relying on a 1974 theorem by Brent for an abstract model of parallel computation). Later the book [13] used the methodology as a framework for describing most of the parallel algorithms presented there.
Reference: [24] <author> M. D. Smith. </author> <title> Tracing with pixie. </title> <type> Version 1.1, </type> <institution> Stanford University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: Some of the basic results in the present paper were obtained using this simulator as is. Using pixie <ref> [24] </ref>, the simulator produces the instruction trace of the program. It then analyzes the dependences and determines the parallel cycle in which to execute each instruction. Two major forms of dependence are taken into account data dependences and control dependences.
Reference: [25] <author> U. Vishkin. </author> <title> Can parallel algorithms enhance serial implementation? (extended abstract). </title> <booktitle> In 8th International Parallel Processing Symposium. IEEE, </booktitle> <year> 1994. </year>
Reference-contexts: The programmer will be able to specify this loop as executable in parallel and the enhanced simulator will "understand" this specification. The general thesis that led to this work <ref> [25] </ref> advocated that using a parallel programmer's model is worthwhile. In order to test this thesis in a meaningful way we need to present results which are as widely applicable as possible. <p> We chose Breadth First Search as a test algorithm. We showed that even for existing systems, running strictly sequential code, we can significantly improve ILP by identifying parallelism in the algorithm and designing the code so that it reveals this parallelism. The current paper enforces the call in <ref> [25] </ref> to start immediately including the topic of parallel algorithms and parallel algorithmic thinking in the standard curriculum for computer science and engineering undergraduates, since this curriculum should expose them to principles which they need to command as they graduate as well as over a life time career.
Reference: [26] <author> D. W. Wall. </author> <title> Limits of instruction-level parallelism. </title> <type> RR 93/6, </type> <institution> Digital Western Research Laboratory, Palo Alto, California, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: Much effort has been devoted to measuring the amount of parallelism which can be automatically extracted from programs by these techniques <ref> [17, 5, 19, 26, 2] </ref>. These studies suggest that the amount of parallelism which can be thus extracted is limited, even when we invest much hardware and software in the effort, and is likely to be insufficient as the reliance on ILP increases. <p> The results obtained in our tests are presented in Section 6, and finally Section 7 concludes our paper. 2 Overview of Tests and Results In studies done by Lam and Wilson [19] and by Wall <ref> [26] </ref> the amount of parallelism in programs was measured by running the instruction trace through a simulator, as explained below. <p> The abovementioned studies measured in this manner the parallelism extracted from a chosen set of standard benchmarks. For the strongest and most unrealistic abstract machine models, a mean parallelism of 23.6 was found in <ref> [26] </ref> and of 158.26 in [19]. When some of the unrealistic capabilities were modified to more feasible ones, the mean parallelism for their respective benchmarks dropped to 11.9 in [26] and to 39.62 in [19]. <p> For the strongest and most unrealistic abstract machine models, a mean parallelism of 23.6 was found in <ref> [26] </ref> and of 158.26 in [19]. When some of the unrealistic capabilities were modified to more feasible ones, the mean parallelism for their respective benchmarks dropped to 11.9 in [26] and to 39.62 in [19]. We propose an envisioned system in which the programmer can identify parts of the code as parallelizable, and the compiler and hardware have the capability to utilize this parallelism, as well as extract parallelism automatically. <p> The literature reports numerous efforts to study and observe the amount of parallelism which can be automatically extracted from a program. For brevity, we elaborate only on works that measured the parallelism in the instruction trace of C or Fortran programs. Specifically, we focus on the works of <ref> [19, 26] </ref> that measured this parallelism using a simulator. Other works annotated the high level program, adding code so that data regarding parallelism is collected at runtime [17, 5]. <p> Such an abstract model is valuable as it provides a yardstick for the amount of parallelism which one could realistically expect. These two works consider also weaker abstract machine models, where less parallelism can be extracted. Each of the most aggressive abstract machine models in <ref> [19, 26] </ref> assumes the following capabilities: perfect branch prediction At the beginning of the execution it is already known which path will be taken by every branch. <p> This becomes an issue in two different settings: (i) Indirect memory accesses: If these contents are not known in advance we must assume that all indirect memory accesses are to the same location, possibly enforcing false data dependences. Wall <ref> [26] </ref> refers to this problem as the aliasing problem. (ii) Indirect jumps: The lack of this foreknowledge would mean that all instructions after the jump cannot be executed until the target address is resolved. perfect register renaming Register storage dependences which are not required for preserving correctness of the original code
Reference: [27] <author> S. Weiss and J. E. Smith. </author> <title> Power and PowerPC. </title> <publisher> Morgan Kaufman Publishers Inc., </publisher> <address> San Francisco, CA, </address> <year> 1994. </year> <month> 35 </month>
Reference-contexts: Runtime techniques include speculative execution, conditional execution, and branch prediction [9]. One kind of machine which uses the parallelism 9 found by compiler or runtime techniques is multiscalar machines <ref> [27] </ref>. Since relatively little attention has been given to using these techniques for programs which were not initially designed for a strictly serial processor, we mention one outstanding issue with respect to doing so.
References-found: 27

