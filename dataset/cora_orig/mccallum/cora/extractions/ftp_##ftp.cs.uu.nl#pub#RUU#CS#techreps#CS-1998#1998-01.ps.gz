URL: ftp://ftp.cs.uu.nl/pub/RUU/CS/techreps/CS-1998/1998-01.ps.gz
Refering-URL: http://www.cs.ruu.nl/docs/research/publication/TechList1.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: E-mail: fniels,lucasg@cs.ruu.nl  
Title: Trade-offs in Decision-theoretic Planning  
Author: Niels Peek and Peter Lucas 
Keyword: planning, decision theory, probabilistic networks.  
Address: P.O. Box 80.089, 3508 TB Utrecht, The Netherlands  
Affiliation: Department of Computer Science, Utrecht University  
Abstract: Decision making under uncertainty can be viewed as a planning task, because it basically amounts to determining a sequence of actions that is optimal for a problem under study. Many different formalisms are used in decision making under uncertainty, with many different restrictions enforced. In this paper, a novel, general decision-theoretic planning framework is proposed, and used to analyse various representation formalisms for decision making. Several features of these formalisms are highlighted in terms of the framework, using examples from the domain of cancer-treatment planning in medicine. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Boutilier, T. Dean, and S. Hanks. </author> <title> Planning under uncertainty: structural assumptions and computational leverage. </title> <booktitle> In Ghallab and Milani [4], </booktitle> <pages> pages 157-171. </pages>
Reference-contexts: In many real-world situations, however, the effects of actions are essentially uncertain. The field of planning under uncertainty is concerned with planning tasks taking the inherent uncertainty in the effects of actions into account. The term decision-theoretic planning <ref> [2, 1, 3] </ref> was recently introduced in AI to refer to approaches to planning under uncertainty incorporating decision-theoretic techniques. <p> The probability distributions governing transitions depend on action choice. That is, with each action a 2 A is associated a transition probability function t a : C X fi C X ! <ref> [0; 1] </ref>, where t a (c 0 X j c X ) expresses the probability of arriving at configuration c 0 X 2 C X after performing action a in configuration c X 2 C X . <p> Being the most general representation method, Markov decision processes are becoming increasingly popular in AI as a basis for decision-theoretic planning. For instance, Boutilier and colleagues <ref> [1] </ref> have developed an alternative framework for decision-theoretic planning, taking Markov decision processes as a starting-point. The generality of their framework ensures that many more restricted decision-theoretic formalisms can be represented, which they accomplish by switching to different representations.
Reference: [2] <author> T. Dean and M. Wellman. </author> <title> Planning and Control. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: In many real-world situations, however, the effects of actions are essentially uncertain. The field of planning under uncertainty is concerned with planning tasks taking the inherent uncertainty in the effects of actions into account. The term decision-theoretic planning <ref> [2, 1, 3] </ref> was recently introduced in AI to refer to approaches to planning under uncertainty incorporating decision-theoretic techniques.
Reference: [3] <author> A. Doan, P. Haddawy, and C.E. Kahn. </author> <title> Decision-theoretic refinement planning: a new method for clinical decision analysis. </title> <booktitle> In Proceedings of the 19th Annual Symposium on Computer Applications in Medical Care (SCAMC'95), </booktitle> <year> 1995. </year>
Reference-contexts: In many real-world situations, however, the effects of actions are essentially uncertain. The field of planning under uncertainty is concerned with planning tasks taking the inherent uncertainty in the effects of actions into account. The term decision-theoretic planning <ref> [2, 1, 3] </ref> was recently introduced in AI to refer to approaches to planning under uncertainty incorporating decision-theoretic techniques. <p> In recent years, some research has been devoted to developing extensions to the representations discussed above, either to enhance their expressiveness as a knowledge-representation formalism (e.g. [13]), or to improve the computational efficiency of their evaluation (e.g. <ref> [3, 6] </ref>). Being the most general representation method, Markov decision processes are becoming increasingly popular in AI as a basis for decision-theoretic planning. For instance, Boutilier and colleagues [1] have developed an alternative framework for decision-theoretic planning, taking Markov decision processes as a starting-point.
Reference: [4] <author> M. Ghallab and A. Milani, </author> <title> editors. New Directions in AI Planning. </title> <publisher> IOS Press, </publisher> <address> Amsterdam, </address> <year> 1996. </year>
Reference: [5] <author> R.A. Howard and J.E. Matheson. </author> <title> Influence diagrams. </title> <editor> In R.A. Howard and J.E. Matheson, editors, </editor> <booktitle> Readings on the Principles and Applications of Decision Analysis. Strategic Decisions Group, </booktitle> <address> Menlo Park, CA, </address> <year> 1981. </year>
Reference-contexts: Since a decision tree is an explicit enumeration of plans for a decision-theoretic problem, the size of the tree will typically grow exponentially in the number of actions and uncertain events modelled. 4.2 Influence Diagrams An influence diagram <ref> [5] </ref> is a concise represen ext age stage hlth biop d surg d chem d radio surv U Fig. 4. Example influence diagram. tation of a decision problem under uncertainty in a directed, acyclic graph. Fig. 4 shows an influence diagram graph for the cancer treatment example.
Reference: [6] <author> S.-H. Lin and T. Dean. </author> <title> Generating optimal policies for high-level plans with conditional branches and loops. </title> <booktitle> In Ghallab and Milani [4], </booktitle> <pages> pages 187-200. </pages>
Reference-contexts: In recent years, some research has been devoted to developing extensions to the representations discussed above, either to enhance their expressiveness as a knowledge-representation formalism (e.g. [13]), or to improve the computational efficiency of their evaluation (e.g. <ref> [3, 6] </ref>). Being the most general representation method, Markov decision processes are becoming increasingly popular in AI as a basis for decision-theoretic planning. For instance, Boutilier and colleagues [1] have developed an alternative framework for decision-theoretic planning, taking Markov decision processes as a starting-point.
Reference: [7] <author> W.S. Lovejoy. </author> <title> A survey of algorithmic methods for partially observed Markov decision processes. </title> <journal> Annals of Operations Research, </journal> <volume> 28 </volume> <pages> 47-66, </pages> <year> 1991. </year>
Reference-contexts: Markov decision process for cancer treatment. problems, efficient solution methods exist, based on the principle of dynamic programming [10]. Straightforward application of dynamic programming techniques to partially observable problems is not possible, and algorithms tend to be complicated and limited <ref> [7] </ref>. 5 Comparison In this section, we compare the three formalisms discussed in the previous section with respect to their flexibility and expressiveness as a knowledge-representation method.
Reference: [8] <author> G.E. Monahan. </author> <title> A survey of partially observed Markov decision processes: theory, models, and algorithms. </title> <journal> Management Science, </journal> <volume> 28(1) </volume> <pages> 1-16, </pages> <year> 1982. </year>
Reference-contexts: Fig. 5 offers a schematic depiction of an MDP for the cancer-treatment example. In MDPs, it is assumed that the decision maker can observe the actual configuration of all random variables at each point in time. Partially observable Markov decision processes (POMDPs) <ref> [8] </ref> are a generalisation of MDPs which permit uncertainty regarding the actual configurations and allow for observations depending on action choice. Formally, a POMDP model is an MDP model extended with an observation function ! : A ! 2 X .
Reference: [9] <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> Palo Alto, </address> <year> 1988. </year>
Reference-contexts: These variables are summarised in Fig. 2a. Fig. 2b shows the graphical part of a Bayesian belief network <ref> [9] </ref> comprising the random variables distinguished; the network describes a joint probability distribution on these variables. There are two types of interaction between the random variables and the actions. First, performing any of the three therapeutic actions may affect the clinical stage and the extent of the primary tumour.
Reference: [10] <author> M.L. Puterman. </author> <title> Markov Decision Processes: Discrete Stochastic Dynamic Programming. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: Evaluating an influence diagram, i.e. finding an optimal plan, is possible by performing a sequence of transformations on the graph [12]. The general problem is combinatoric, but evaluation is feasible in polynomial time for sparse, bounded, graphs. 4.3 Markov Decision Processes In a Markov decision process <ref> [10] </ref>, or MDP for short, the dynamics of a planning problem under uncertainty are modelled as a discrete-time stochastic process over the set C X of configurations of the random variables involved. The probability distributions governing transitions depend on action choice. <p> Markov decision process for cancer treatment. problems, efficient solution methods exist, based on the principle of dynamic programming <ref> [10] </ref>. Straightforward application of dynamic programming techniques to partially observable problems is not possible, and algorithms tend to be complicated and limited [7]. 5 Comparison In this section, we compare the three formalisms discussed in the previous section with respect to their flexibility and expressiveness as a knowledge-representation method.
Reference: [11] <author> H. Raiffa. </author> <title> Decision Analysis: Introductory Lectures on Choice under Uncertainty. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1968. </year>
Reference-contexts: These formalisms will now be analysed in terms of the framework presented in the previous section, using examples from the cancer-treatment domain. 4.1 Decision Trees A decision tree <ref> [11] </ref> is a rooted tree, where each internal node is either a decision node, denoting a point in time where the decision maker faces a choice among several actions, or a chance node, denoting a point in time where the decision maker can observe the actual value of a random variable.
Reference: [12] <author> R.D. Shachter. </author> <title> Evaluating influence diagrams. </title> <journal> Operations Research, </journal> <volume> 34(6) </volume> <pages> 79-90, </pages> <year> 1986. </year>
Reference-contexts: Evaluating an influence diagram, i.e. finding an optimal plan, is possible by performing a sequence of transformations on the graph <ref> [12] </ref>.
Reference: [13] <author> N. L. Zhang, R. Qi, and D. Poole. </author> <title> A computational theory of decision networks. </title> <journal> International Journal of Approximate Reasoning, </journal> <volume> 11(2) </volume> <pages> 83-158, </pages> <year> 1994. </year> <month> 12 </month>
Reference-contexts: In this sense, these formalisms are either too restrictive, or offer too much freedom, lacking the features of an appropriate knowledge-representation formalism. In recent years, some research has been devoted to developing extensions to the representations discussed above, either to enhance their expressiveness as a knowledge-representation formalism (e.g. <ref> [13] </ref>), or to improve the computational efficiency of their evaluation (e.g. [3, 6]). Being the most general representation method, Markov decision processes are becoming increasingly popular in AI as a basis for decision-theoretic planning.
References-found: 13

