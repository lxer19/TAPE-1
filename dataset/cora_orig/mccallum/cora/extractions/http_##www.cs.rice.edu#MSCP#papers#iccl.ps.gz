URL: http://www.cs.rice.edu/MSCP/papers/iccl.ps.gz
Refering-URL: http://www.cs.rice.edu/MSCP/publications.html
Root-URL: 
Title: Procedure Cloning  
Author: Keith D. Cooper Mary W. Hall Ken Kennedy 
Address: Houston, TX 77251-1892  
Affiliation: Department of Computer Science, Rice University,  
Abstract: Procedure cloning is an interprocedural optimization where the compiler creates specialized copies of procedure bodies. To clone a procedure, the compiler replicates it and then divides the incoming calls between the original procedure and the copy. By carefully partitioning the call sites, the compiler can ensure that each clone inherits an environment that allows for better code optimization. Subsequent optimization tailors the various procedure bodies. This paper examines the problem of procedure cloning. It describes an experiment where cloning was required to enable other transformations. It presents a three-phase algorithm for deciding how to clone a program, and analyzes the algorithm's complexity. Finally, it presents a set of assumptions that bound both the running time of the algorithm and the expansion in code size. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. E. Ball. </author> <title> Predicting the effects of optimization on a procedure body. </title> <booktitle> In Proceedings of the SIGPLAN 79 Symposium on Compiler Construction, SIGPLAN Notices 14(8), </booktitle> <pages> pages 214-220. </pages> <publisher> ACM, </publisher> <month> August </month> <year> 1979. </year>
Reference-contexts: For example, the merging cost can be the number of positions that differ between a pair of cloning vectors. This can be improved by taking into account execution frequency estimates and weighting the effects of each piece of information <ref> [1] </ref>. Given a method to compute the cost of merging two cloning vectors, the compiler can adopt a relatively simple rationing scheme. Assume that we set a quota for the total number of cloning vectors allowed during compilation and a quota for each procedure.
Reference: [2] <author> J. P. Banning. </author> <title> An efficient way to find the side effects of procedure calls and the aliases of variables. </title> <booktitle> In Proceedings of the Sixth Annual Symposium on Principles of Programming Languages. ACM, </booktitle> <month> January </month> <year> 1979. </year>
Reference-contexts: If two cloning vectors have unique constant values that produce the same effect on optimization, they are merged. Other interprocedural problems exist for which this merging phase is unnecessary. As an example, we briefly describe cloning based on alias analysis <ref> [2] </ref>. Two variable names are aliases in a procedure if they can refer to the same memory location. A compiler uses alias information to verify the safety of certain optimizations.
Reference: [3] <author> P. Briggs, K. D. Cooper, M. W. Hall, and L. Torczon. </author> <title> Goal-directed interprocedural optimization. </title> <type> Technical Report TR90-148, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> November </month> <year> 1990. </year>
Reference-contexts: (: : : ; jb) dgemv subroutine dgemv (: : : ; job) real A (100,1) if (f (job)) then ii=1 else ii=100 do j call daxpy (A (k; 1); ii) daxpy subroutine daxpy (A,ia) real A (ia,1) do i tions that block the computation for cache and register reuse <ref> [3] </ref>. Matrix300 computes eight variants on matrix multiplication, selectively transposing the input and output matrices. The goal of the experiment was to obtain the best possible execution time using cache blocking and scalar replacement. <p> Because the relationship between changes to the call graph and changes in the information is much more indirect than for forward problems, it is not clear to us that cloning for improvement in backward problems makes any practical sense. directed <ref> [3] </ref>. In the matrix300 example, we clone only to expose constants needed to improve dependence information. These constants fall into three categories: (1) they specify the dimension size of an array parameter; (2) they determine control flow; or, (3) they appear in a subscript expression. <p> A bottom-up pass over the program propagates these variables, translating from formal to actual parameters at calls. Upon completion, we know at each procedure the variables that, if constant, might improve dependence information in this procedure or one of its descendants <ref> [3, 12] </ref>. In general, a goal-directed approach depends both on the interprocedural problem and the desired optimization effects. Designing a strategy for a specific compiler necessarily involves experimentation to understand how well the compiler takes advantage of the kind of facts that cloning can expose. <p> These applications come from diverse areas: compiling for scalar architectures, compiling for both shared-memory and distributed-memory parallel architectures, and instrumenting code for run-time detection of race conditions in shared-memory parallel programs. To date, we have effectively employed cloning in experiments with interprocedural constant propagation <ref> [3, 12] </ref> and interprocedural transformations for parallel code generation [13]. ParaScope is devoted to high-performance Fortran programming, but the need for cloning arises in many other contexts.
Reference: [4] <author> M. A. Bulyonkov. </author> <title> Polyvariant mixed computation for analyzer programs. </title> <journal> Acta Informatica, </journal> <volume> 21 </volume> <pages> 473-484, </pages> <year> 1984. </year>
Reference-contexts: This paper presents an algorithm for deciding which procedures to clone and how many instances to create. The algorithm finds potential improvements in forward interprocedural data-flow solutions and clones those procedures that lead to sharper information. We discuss similar techniques used in partial evaluation <ref> [4, 15] </ref> and intraprocedural optimization [16]. <p> The algorithm reduces code growth by avoiding replication using a number of heuristics, but these heuristics are not sufficient to always prevent exponential code growth. In the partial evaluation literature, the technique called specialization involves replicating code in order to tailor the code to particular variable values or types <ref> [4] </ref>. Bulyonkov describes an approach based on abstract interpretation to locate program points where specialization improves information. He observes that the problem is appropriate in both interprocedural and intraprocedural settings. The execution time of the specialization algorithm is bounded by the execution time of the program.
Reference: [5] <author> C. D. Callahan, K. D. Cooper, R. T. Hood, K. Kennedy, and L. Torczon. </author> <title> ParaScope: a parallel programming environment. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 2(4) </volume> <pages> 84-89, </pages> <month> December </month> <year> 1988. </year>
Reference-contexts: This general approach bases cloning on any forward interprocedural data-flow analysis problem. The algorithm was designed in the context of the program compiler for the ParaScope programming environment the tool that manages interprocedural issues in compilation <ref> [5] </ref>. This general algorithm supports a number of emerging applications for cloning. These applications come from diverse areas: compiling for scalar architectures, compiling for both shared-memory and distributed-memory parallel architectures, and instrumenting code for run-time detection of race conditions in shared-memory parallel programs.
Reference: [6] <author> D. Callahan, S. Carr, and K. Kennedy. </author> <title> Improving register allocation for subscripted variables. </title> <booktitle> In Proceedings of the SIGPLAN 90 Conference on Programming Language Design and Implementation, SIGPLAN Notices 25(6), </booktitle> <pages> pages 53-65. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1990. </year>
Reference-contexts: These transformations reorder the iteration space of a loop to increase locality, thus exposing reuse of values in registers and decreasing cache misses. The most important of these optimizations, unroll and jam, has demonstrated dramatic improvements on linear algebra kernels <ref> [6] </ref>. Unroll and jam cannot be applied directly to the key computational kernel of matrix300 because of the program's structure. Unroll and jam transforms a nest of two or more loops; in matrix300, each loop is in a different procedure. The leaf procedure, daxpy, only contains a single loop.
Reference: [7] <author> D. Callahan, K. D. Cooper, K. Kennedy, and L. Tor-czon. </author> <title> Interprocedural constant propagation. </title> <booktitle> In Proceedings of the SIGPLAN 86 Symposium on Compiler Construction, SIGPLAN Notices 21(7), </booktitle> <pages> pages 152-161. </pages> <publisher> ACM, </publisher> <month> July </month> <year> 1986. </year>
Reference-contexts: For interprocedural constants, the StateVector can be the values of important expressions appearing in the procedure: control flow tests, subscripts and array dimensions. For each one of these, we construct a jump function that describes its value as a function of potential interprocedural constants <ref> [7] </ref>. Only jump functions for program points that can be determined by interprocedural constants need be kept. With this information and a cloning vector describing interprocedural values, the value of a state vector can be determined. Page 6 The example in Figure 5 illustrates these points.
Reference: [8] <author> K. D. Cooper, K. Kennedy, and L. Torczon. </author> <title> The impact of interprocedural analysis and optimization in the R n environment. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 8(4) </volume> <pages> 491-523, </pages> <month> October </month> <year> 1986. </year>
Reference-contexts: Simply using inter-procedural analysis lets the structure of the program constrain the compiler; it assumes that each procedure should be implemented once. To improve the latter approach, an aggressive compiler can consider procedure cloning | creating multiple implementations of a single procedure and partitioning the calls among them <ref> [8] </ref>. * Cloning differs from straightforward application of interprocedural data-flow analysis.
Reference: [9] <author> K. D. Cooper, K. Kennedy, and L. Torczon. </author> <title> Inter-procedural optimization: Eliminating unnecessary recompilation. </title> <booktitle> In Proceedings of the SIGPLAN 86 Symposium on Compiler Construction. ACM, </booktitle> <month> June </month> <year> 1986. </year>
Reference-contexts: These annotations direct the optimizer to apply the desired optimizations. They also enable recompilation analysis to ensure on subsequent compilations that the cloning is still valid <ref> [9] </ref>. Recompilation analysis is used to avoid unnecessary recompilation of procedures by an inter-procedural optimizing compiler. The problem of recompilation analysis in the presence of this cloning algorithm is discussed elsewhere [12]. 5 Time Complexity Phase 1.
Reference: [10] <author> K. D. Cooper, K. Kennedy, L. Torczon, A. Weingarten, and M. Wolcott. </author> <title> Editing and compiling whole programs. </title> <booktitle> In Proceedings of the SIGSOFT/SIGPLAN 87 Software Engineering Symposium on Practical Software Development Environments, SIGPLAN Notices 22(1), </booktitle> <pages> pages 92-101. </pages> <publisher> ACM, </publisher> <month> January </month> <year> 1987. </year>
Reference-contexts: It is related to the algorithm for minimizing the number of states in a Deterministic Finite Automaton (DFA) [14]. It is also similar to an algorithm used to minimize the number of implementations of a procedure required when multiple definitions of the same procedure occur in a program <ref> [10] </ref>. 1. Initially, all CloningVectors for a particular proce dure are placed in the same partition. 2.
Reference: [11] <author> K.D. Cooper, M.W. Hall, and L. Torczon. </author> <title> An experiment with inline substitution. </title> <journal> Software | Practice and Experience, </journal> <volume> 21(6) </volume> <pages> 581-601, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: It has been widely assumed that call overhead is the more significant effect; a recent study suggested that call overhead may play less of a role in run-time performance than expected <ref> [11] </ref>. Traditionally, two approaches have emerged for breaking down the call site barrier. The first, inline substitution, replaces call sites with distinct copies of the body of the called procedure. The code is then optimized in the context of the calling procedure. <p> Each technique has limitations. Inlining can lead to code growth, increased compile time, and degradation in code quality <ref> [11] </ref>. Simply using inter-procedural analysis lets the structure of the program constrain the compiler; it assumes that each procedure should be implemented once.
Reference: [12] <author> M. W. Hall. </author> <title> Managing Interprocedural Optimization. </title> <type> PhD thesis, </type> <institution> Rice University, Houston, TX, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: A bottom-up pass over the program propagates these variables, translating from formal to actual parameters at calls. Upon completion, we know at each procedure the variables that, if constant, might improve dependence information in this procedure or one of its descendants <ref> [3, 12] </ref>. In general, a goal-directed approach depends both on the interprocedural problem and the desired optimization effects. Designing a strategy for a specific compiler necessarily involves experimentation to understand how well the compiler takes advantage of the kind of facts that cloning can expose. <p> The original call graph has only n nodes and 2 (n 1) edges. Because cloning can exhibit exponential behavior, our algorithm must anticipate this possibility and impose restrictions when necessary. However, based on experience, the amount of useful cloning on a program is likely to be small <ref> [12] </ref>. For this reason, we anticipate that the restrictions on cloning will rarely be necessary. Nevertheless, the algorithm will perform well even in the event of pathological behavior. 4 Cloning Algorithm This section presents a polynomial-time algorithm for procedure cloning. The algorithm has three phases. <p> An ideal ordering of cloning decisions would also take into account how a decision would affect performance. A Page 7 simple approach is to estimate the execution frequency of procedures and perform cloning along paths leading to the most frequently executed procedures <ref> [12] </ref>. We could also use a strategy similar to the merging of vectors for an individual procedure (see Section 6). For the interprocedural problem being used as the basis for cloning, we need to associate with a clone its set of CloningVectors and its StateVector. <p> They also enable recompilation analysis to ensure on subsequent compilations that the cloning is still valid [9]. Recompilation analysis is used to avoid unnecessary recompilation of procedures by an inter-procedural optimizing compiler. The problem of recompilation analysis in the presence of this cloning algorithm is discussed elsewhere <ref> [12] </ref>. 5 Time Complexity Phase 1. In the algorithm from Figure 4, the outer loop iterates over procedures, and the inner loop iterates over cloning vectors at a call site. <p> These applications come from diverse areas: compiling for scalar architectures, compiling for both shared-memory and distributed-memory parallel architectures, and instrumenting code for run-time detection of race conditions in shared-memory parallel programs. To date, we have effectively employed cloning in experiments with interprocedural constant propagation <ref> [3, 12] </ref> and interprocedural transformations for parallel code generation [13]. ParaScope is devoted to high-performance Fortran programming, but the need for cloning arises in many other contexts.
Reference: [13] <author> M. W. Hall, K. Kennedy, and K. S. McKinley. </author> <title> Inter-procedural transformations for parallel code generation. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <month> November </month> <year> 1991. </year>
Reference-contexts: To date, we have effectively employed cloning in experiments with interprocedural constant propagation [3, 12] and interprocedural transformations for parallel code generation <ref> [13] </ref>. ParaScope is devoted to high-performance Fortran programming, but the need for cloning arises in many other contexts. For example, in languages with implicit typing, cloning enables separate calls to a procedure to be customized according to the types of the input parameters.
Reference: [14] <author> J. Hopcroft. </author> <title> An nlogn algorithm for minimizing states in a finite automaton. </title> <editor> In Z. Kohavi and A. Paz, editors, </editor> <booktitle> Theory of Machines and Computations, </booktitle> <pages> pages 189-196. </pages> <publisher> Academic Press, </publisher> <address> New York, NY, </address> <year> 1971. </year>
Reference-contexts: It is related to the algorithm for minimizing the number of states in a Deterministic Finite Automaton (DFA) <ref> [14] </ref>. It is also similar to an algorithm used to minimize the number of implementations of a procedure required when multiple definitions of the same procedure occur in a program [10]. 1. Initially, all CloningVectors for a particular proce dure are placed in the same partition. 2. <p> If we test for equality by hashing the strings, the partitioning step for each procedure has an expected time linear in the number of its cloning vectors. (A different approach would yield O (n log n) time, even for worst-case performance <ref> [14] </ref>.) Phase 3. The final phase of the cloning algorithm is accomplished by a single top-down pass over the call graph. The number of clones created is less than the total number of cloning vectors. Thus, Phase 3 is also bounded by the number of cloning vectors.
Reference: [15] <author> E. Ruf and D. Weise. </author> <title> Using types to avoid redundant specialization. </title> <booktitle> In Proceedings of the PEPM 91 Symposium on Partial Evaluation and Semantics-Based Program Manipulation, SIGPLAN Notices 26(9), </booktitle> <pages> pages 321-333. </pages> <publisher> ACM, </publisher> <month> September </month> <year> 1991. </year>
Reference-contexts: This paper presents an algorithm for deciding which procedures to clone and how many instances to create. The algorithm finds potential improvements in forward interprocedural data-flow solutions and clones those procedures that lead to sharper information. We discuss similar techniques used in partial evaluation <ref> [4, 15] </ref> and intraprocedural optimization [16]. <p> The execution time of the specialization algorithm is bounded by the execution time of the program. This time may still be exponential in the size of the program. Ruf and Weise present an algorithm to reduce the amount of specialization in a partial evaluator <ref> [15] </ref>. Their algorithm computes the value of each statement in a specialization. Two specializations are equivalent if they result in the same value for every statement, even if the information they provide is different. This approach is very similar to our Phase 2 algorithm.
Reference: [16] <author> M. Wegman. </author> <title> General and Efficient Methods for Global Code Improvement. </title> <type> PhD thesis, </type> <institution> University of Califor-nia, Berkeley, </institution> <address> CA, </address> <month> December </month> <year> 1981. </year>
Reference-contexts: This paper presents an algorithm for deciding which procedures to clone and how many instances to create. The algorithm finds potential improvements in forward interprocedural data-flow solutions and clones those procedures that lead to sharper information. We discuss similar techniques used in partial evaluation [4, 15] and intraprocedural optimization <ref> [16] </ref>. <p> Wegman describes an algorithm to replicate basic blocks in the control flow graph based on intra-procedural data-flow solutions and incrementally propagate the more precise solutions <ref> [16] </ref>. The algorithm reduces code growth by avoiding replication using a number of heuristics, but these heuristics are not sufficient to always prevent exponential code growth.
Reference: [17] <author> F. K. Zadeck. </author> <title> Incremental data flow analysis in a structured program editor. </title> <booktitle> In Proceedings of the SIGPLAN 84 Symposium on Compiler Construction, SIGPLAN Notices 19(6), </booktitle> <pages> pages 132-143. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1984. </year> <pages> Page 10 </pages>
Reference-contexts: For recursive cycles, we locate strongly-connected regions in the call graph and replace the cycle with a representative node <ref> [17] </ref>. This step ensures correctness of the cloning and allows propagation to occur in a single pass over the call graph. When the algorithm reaches a node representing a cycle, it must take each incoming cloning vector and propagate it within nodes in the recursive cycle until its information stabilizes.
References-found: 17

