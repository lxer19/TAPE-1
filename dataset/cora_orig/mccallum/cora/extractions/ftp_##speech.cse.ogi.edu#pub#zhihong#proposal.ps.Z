URL: ftp://speech.cse.ogi.edu/pub/zhihong/proposal.ps.Z
Refering-URL: http://cslu.cse.ogi.edu/people/hu/index.html
Root-URL: http://www.cse.ogi.edu
Title: Understanding speaker variability using correlation-based principal component analysis segment classification tasks where other variant conditions
Author: Zhihong Hu 
Degree: Thesis Proposal  
Note: This hypothesis is tested in  are minimized. Various possible applications of this method are also  
Date: July 2, 1998  
Affiliation: Department of Computer Science and Engineering Oregon Graduate Institute of Science Technology  
Abstract: In this research, we study the relationship amongst speakers and different sounds in speech. We propose a new speaker normalization/adaptation model which incorporates correlations amongst phoneme classes, and explore the applications of the model. Using principal component analysis we construct a speaker space based on a speaker covariance matrix obtained from the training data. The speaker covariance matrix is constructed in such a manner as to explicitly describe the correlations between classes. The hypothesis of this thesis is that by explicitly modeling these correlations it is possible to adapt the model or normalize the speaker's features with limited adaptation data. This proposed method also allows researchers to understand some of the properties of speaker variability and gives insight into some physical aspects of speaker differences. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Nearey, "Static,dynamic, </author> <title> and relational properties in vowel perception," </title> <journal> J. Acoust. Soc. Am., </journal> <volume> vol. 85, </volume> <pages> pp. 2088-2113, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Generally, these kinds of noises are additive. Channel variation is caused by different channels, such as the telephone hand sets, microphones, transmission lines, etc. These effects can be described as convolutional noise. Designing a speech recognition system to cope with all these variations is a difficult problem. Nearey's <ref> [1] </ref> experiments suggest that the speaker-dependent variation is the second biggest source of variation in vowel quality (the largest one is the vowel identity itself).
Reference: [2] <author> E. Eide and H. Gish, </author> <title> "A parametric approach to vocal tract length normalization," </title> <booktitle> Proceedings of ICASSP, </booktitle> <pages> pp. 346-348, </pages> <year> 1996. </year>
Reference-contexts: Shifting of the warped frequency scale in effect moves the formant values of the speaker towards a "neutral" position. The formant values correspond to resonance frequencies of the vocal tract. Eide and Gish <ref> [2] </ref> use a parametric normalization method to compute k s . This value k s is estimated with formant values for a test speaker as the median of the speaker's third formant over a subset of frames satisfying certain criteria.
Reference: [3] <author> T. Kamm, A. Andreou, and J. Cohen, </author> <title> "Vocal tract normalization in speech recognition: Compensating for systematic speaker variability," </title> <year> 1995. </year>
Reference-contexts: This method, however, involves in large amount of computation in estimating the perfect warp parameters, and does not compensate for the various phoneme-dependent effects of vocal tract length variation. In similar work done by others <ref> [3, 4] </ref>, the vocal tract length variation is crudely compensated for by finding an appropriate linear scaling of the warped frequency axis which also involves a large amount of computation in searching for the best value of k s . 2.2 Transformation-based techniques Another approach is to use transformation-based techniques to
Reference: [4] <author> D. Burnett, </author> <title> Rapid Speaker Adaptation for Neural Network Speech Recognizers. </title> <type> PhD thesis, </type> <institution> Oregon Graduate Institute, </institution> <month> December </month> <year> 1996. </year>
Reference-contexts: This method, however, involves in large amount of computation in estimating the perfect warp parameters, and does not compensate for the various phoneme-dependent effects of vocal tract length variation. In similar work done by others <ref> [3, 4] </ref>, the vocal tract length variation is crudely compensated for by finding an appropriate linear scaling of the warped frequency axis which also involves a large amount of computation in searching for the best value of k s . 2.2 Transformation-based techniques Another approach is to use transformation-based techniques to
Reference: [5] <author> V. Leggetter, </author> <title> Statistical Trajectory Models for Phonetic Recognition. </title> <type> PhD thesis, </type> <institution> Cambridge University, </institution> <month> August </month> <year> 1995. </year>
Reference-contexts: In the following descriptions of different methods, the baseline systems are standard continuous HMM speech recognizers without any adaptation method being incorporated. Maximum Likelihood Linear Regression (MLLR) MLLR <ref> [5, 6, 7] </ref> adapts the acoustic models rather than the acoustic features. Linear transformations are associated with each component distribution within the Hidden Markov Model (HMM) system and estimated using a maximum likelihood approach similar to the standard HMM parameter estimation. <p> The adaptation parameters (A; b) are estimated to maximize the likelihood of the model on the adaptation data. Leggetter reports an average 10% reduction in word error rate on the Wall Street Journal task with 65000 word dictionary <ref> [5] </ref>. Speaker Adaptive Training (SAT) The aim of speaker adaptive training (SAT)[9] is to estimate acoustic models that are invariant to long-term variations, specifically speaker-induced variations.
Reference: [6] <author> M. Gales, </author> <title> "The generation and use of regression class trees for MLLR adaptation," </title> <type> Tech. Rep. </type> <institution> CUED/F-INFENG/TR263, Cambridge University, </institution> <month> August </month> <year> 1996. </year>
Reference-contexts: In the following descriptions of different methods, the baseline systems are standard continuous HMM speech recognizers without any adaptation method being incorporated. Maximum Likelihood Linear Regression (MLLR) MLLR <ref> [5, 6, 7] </ref> adapts the acoustic models rather than the acoustic features. Linear transformations are associated with each component distribution within the Hidden Markov Model (HMM) system and estimated using a maximum likelihood approach similar to the standard HMM parameter estimation.
Reference: [7] <author> M. Gales and P. Woodland, </author> <title> "Variance compensation within the MLLR framework," </title> <type> Tech. Rep. </type> <institution> CUED/F-INFENG/TR242, Cambridge University, </institution> <month> February </month> <year> 1996. </year>
Reference-contexts: In the following descriptions of different methods, the baseline systems are standard continuous HMM speech recognizers without any adaptation method being incorporated. Maximum Likelihood Linear Regression (MLLR) MLLR <ref> [5, 6, 7] </ref> adapts the acoustic models rather than the acoustic features. Linear transformations are associated with each component distribution within the Hidden Markov Model (HMM) system and estimated using a maximum likelihood approach similar to the standard HMM parameter estimation.
Reference: [8] <author> P. Ladefoged, </author> <title> A Course in Phonetics. </title> <publisher> Harcourt Brace Jovanovich College Publishers, </publisher> <year> 1975. </year> <note> ISBN 0-15-500173-6. </note>
Reference-contexts: An example of this relationship between different sounds can be found by studying the formant frequencies for the vowel sounds of speech. A plot of the mean first (F 1) and second (F 2) formant values for vowels exhibits a pattern called the vowel-triangle <ref> [8] </ref>, where the point vowels (/iy/ as in "beet", /aa/ as in "father" and /uw/ as in "boot") have extreme F 1-F 2 values and most other vowles have formant values close to one of the sides of the triangle.
Reference: [9] <author> A. Anastasakos, </author> <title> Speaker normalization Methods for Speaker Independent Speech Recognition. </title> <type> PhD thesis, </type> <institution> Northeastern University, </institution> <month> October </month> <year> 1996. </year>
Reference-contexts: The smaller variance results in a more precise model. Anastasakos reported that in a model space approach, speaker normalization is incorporated within a maximum likelihood speaker-independent training paradigm <ref> [9] </ref>. During training, the speaker dependent transformations (A; b) are estimated for each speaker in the training data. The "normalized" model is updated using the inverse transform. In most experimental tests, the SAT adaptive training technique results in reducing the average word-error by 10% [9] compared to baseline systems, which incoporate <p> a maximum likelihood speaker-independent training paradigm <ref> [9] </ref>. During training, the speaker dependent transformations (A; b) are estimated for each speaker in the training data. The "normalized" model is updated using the inverse transform. In most experimental tests, the SAT adaptive training technique results in reducing the average word-error by 10% [9] compared to baseline systems, which incoporate MLLR adaptation. Prediction Adaptation Although MLLR is a very powerful adaptation technique it does not model correlations among phonemes explicitly. With limited data available for adaptation (i.e. not all classes are necessarily represented), not all model means are adapted optimally.
Reference: [10] <author> S. Cox, </author> <title> "Predictive speaker adaptation in speech recognition," </title> <booktitle> Computer Speech and Language, </booktitle> <volume> vol. 9, </volume> <pages> pp. 1-17, </pages> <year> 1995. </year>
Reference-contexts: For any given speaker, if the positions of vowel /uw/ and /iy/ are known, the positions of the vowels /aa/ may be estimated relatively well. The work in this research explores these relations to more effectively update/normalize the speaker parameters. Cox <ref> [10] </ref> presented an approach that used the training data to build linear models between sounds. When only a subset of the phonemes are present in the adaptation data, all unseen phonemes (models) can be adapted using this precomputed linear relation. <p> Methods such as MLLR adaptation have proven to be successful. However when very little adaptation data are available not all classes can be adapted optimally. Cox <ref> [10] </ref> built linear relations between classes. This allowed the adaptation of both the presented and unseen classes. This model, however, assumes an invariant (for all speakers) relationship among classes. The extended MAP algorithm provides a framework in which the correlation among classes can be used to adapt unseen models.
Reference: [11] <author> K. Fukunaga, </author> <title> Statistical Pattern Recognition. </title> <publisher> Academic Press, Inc, </publisher> <year> 1990. </year> <note> General Intro : ISBN 0-12-269851-7. </note>
Reference-contexts: Given the initial model mean 0 with covariance 0 , the updated model parameters can be computed as follows <ref> [11] </ref>: 0 = N = N ] 1 0 + 0 [ 0 + N 1 N X X i ] (4) Here represents the covariance of the adaptation data, and N the total number of adaptation examples presented.
Reference: [12] <author> M. Lasry and R. Stern, </author> <title> "A posterior estimation of correlated jointly gaussian mean vectors," </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> vol. PAMI-6, no. 4, </volume> <pages> pp. 530-535, </pages> <year> 1984. </year>
Reference-contexts: In this method data from one class can not be used to update any other class. In order to solve this problem, an extension of MAP (called EMAP) was introduced by Lasry and Stern <ref> [12] </ref> and further investigated by Rozzi [13]. The aim of the EMAP algorithm is to use the correlations between classes to estimate the transformation for unseen classes from the available adaptation data. EMAP differs from MAP in that EMAP updates a concatenated mean of all classes.
Reference: [13] <author> W. Rozzi, </author> <title> Speaker Adaptation in Continuous Speech Recognition via Estimation of Correlated Mean Vector. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: In this method data from one class can not be used to update any other class. In order to solve this problem, an extension of MAP (called EMAP) was introduced by Lasry and Stern [12] and further investigated by Rozzi <ref> [13] </ref>. The aim of the EMAP algorithm is to use the correlations between classes to estimate the transformation for unseen classes from the available adaptation data. EMAP differs from MAP in that EMAP updates a concatenated mean of all classes. <p> Because of the dimensionality it is very difficult to estimate 0 accurately. To estimate this matrix accurately would require full coverage of each class (phoneme) spoken by every speaker in the training data. Furthermore the matrix inversion ( + L 0 ) 1 is also prohibitive. Rozzi <ref> [13] </ref> extended the basic EMAP algorithm so that the computation is more efficient.
Reference: [14] <author> B. Kumar and H. Murakami, </author> <title> "Principal-component imagery for statistical pattern recognition correla-tors," </title> <journal> Optical Engineering, </journal> <volume> vol. 21, no. 1, </volume> <pages> pp. 43-47, </pages> <year> 1982. </year>
Reference-contexts: The advantage of this model over EMAP is that it avoids the estimation problem for 0 . The top n eigen vectors of 0 (n &lt;< CD) can be estimated accurately using much less training data using singular value decomposition <ref> [14] </ref> when the number of speakers used in the estimation is less than the dimension of 0 and bigger than n . This method is similar to the computation of eigen-faces in image recognition [15].
Reference: [15] <author> M. Kirby and L. Sirovich, </author> <title> "Application of the karhunen-loeve procedure for the characterization of human faces," </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 12, </volume> <pages> pp. 103-108, </pages> <month> Jan. </month> <year> 1990. </year>
Reference-contexts: This method is similar to the computation of eigen-faces in image recognition <ref> [15] </ref>. At the same time, the proposed model retains most of the important information represented in 0 .
Reference: [16] <author> G. Peterson and H. Barney, </author> <title> "Control methods used in a study of the vowels," </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> vol. 24, </volume> <pages> pp. 175-184, </pages> <year> 1952. </year>
Reference-contexts: The Hillenbrand data set is an extension of the Peterson & Barney data <ref> [16] </ref> which were collected at the University of Michigan [17].
Reference: [17] <author> J. Hillenbrand, L. Getty, M. Clark, and K. Wheeler, </author> <title> "Acoustic characteristics of american english vowels," </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> vol. 97, no. 5, </volume> <pages> pp. 3099-3111, </pages> <year> 1995. </year> <month> 21 </month>
Reference-contexts: The Hillenbrand data set is an extension of the Peterson & Barney data [16] which were collected at the University of Michigan <ref> [17] </ref>. In this data set, F 1-F 4 contours are measured for 12 vowels in the /h-V-d/ context ( each vowel has phoneme "h" as the left context and phoneme "d" as the right context") for 45 men, 48 women, and 46 children.
Reference: [18] <author> L. Lamel, R. Kassel, and S. Seneff, </author> <title> "Speech database development: design and analysis of the acoustic--phonetic corpus," </title> <booktitle> in Proc. DARPA Speech Recognition Workshop, </booktitle> <volume> no. SAIC-86/1546, </volume> <pages> pp. 100-1090, </pages> <month> February </month> <year> 1986. </year>
Reference-contexts: This data set has extra dynamic information compared to the Peterson and Barney data. The 12 vowels are: ae ah aw eh ei er ih iy oa oo uh uw 2. The TIMIT data set <ref> [18] </ref> is a phonetically hand-labeled set which contains phonetically-balanced sentences. The entire corpus consists of 10 sentences recorded from each of 630 speaker of American English.
Reference: [19] <author> L. Welling and H. Ney, </author> <title> "A model for efficient formant estimation," </title> <booktitle> in icassp, </booktitle> <pages> pp. 797-800, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: In our study, the features we use are mostly formants. The three formant frequency values for each vowel in the TIMIT data set are estimated using a formant estimation method proposed by Welling and Ney <ref> [19] </ref>. 4.2 Physical meaning of the speaker space In this section, we investigate the relationship between the model we have proposed and that of the speech production process. The studies are conducted on Hillenbrand data. Projections on eigenvectors the speaker space.
Reference: [20] <author> P. Vermeulen and D. Casasent, </author> <title> "Karhunen-loeve techniques for optimal processing of time-sequential imagery," </title> <journal> Optical Engineering, </journal> <volume> vol. 30, no. 4, </volume> <pages> pp. 415-423, </pages> <year> 1991. </year>
Reference: [21] <author> G. Zavaliagkos, </author> <title> Maximum A Posteriori Adaptation Techniques for Speech Recognition. </title> <type> PhD thesis, </type> <institution> Northeastern University, </institution> <month> August </month> <year> 1996. </year> <month> 22 </month>
References-found: 21

