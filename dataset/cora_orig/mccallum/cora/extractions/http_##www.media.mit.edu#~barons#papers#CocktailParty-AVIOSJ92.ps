URL: http://www.media.mit.edu/~barons/papers/CocktailParty-AVIOSJ92.ps
Refering-URL: http://www.media.mit.edu/~barons/AronsAnnotatedBibliography.html
Root-URL: http://www.media.mit.edu
Title: A Review of The Cocktail Party Effect  
Author: Barry Arons 
Web: barons@media-lab.mit.edu  
Address: 20 Ames Street, E15-353 Cambridge MA 02139  
Affiliation: MIT Media Lab  
Abstract: The cocktail party effectthe ability to focus one's listening attention on a single talker among a cacophony of conversations and background noisehas been recognized for some time. This specialized listening ability may be because of characteristics of the human speech production system, the auditory system, or high-level perceptual and language processing. This paper investigates the literature on what is known about the effect, from the original technical descriptions through current research in the areas of auditory streams and spatial display systems. The underlying goal of the paper is to analyze the components of this effect to uncover relevant attributes of the speech production and perception chain that could be exploited in future speech communication systems. The motivation is to build a system that can simultaneously present multiple streams of speech information such that a user can focus on one stream, yet easily shift attention to the others. A set of speech applications and user interfaces that take advantage of the ability to computationally simulate the cocktail party effect are also considered. 
Abstract-found: 1
Intro-found: 1
Reference: [Aro91] <author> B. Arons. Hyperspeech: </author> <title> Navigating in speech-only hypermedia. </title> <booktitle> In Hypertext '91, </booktitle> <pages> pages 133-146. </pages> <publisher> ACM, </publisher> <year> 1991. </year>
Reference-contexts: In a user interface it may be desirable to present multiple digitized speech recordings simultaneously, providing browsing capabilities while circumventing the time bottleneck inherent in speech communication because of the serial nature of audio <ref> [Aro91, SA89] </ref>. Synthesis of perceptual cues by a machine for human listeners might allow an application to perceptually nudge the user, making it easier to attend to a particular voice, or suggest that a new voice come into focus. <p> Such a system could also use other stream segregation effects to enhance the voice of the speaker who has the floor at any given instant. Another emerging application area is speech-only hypermedia <ref> [Aro91] </ref>. In this context, speech provides navigational input in a hypermedia database among a linked network of voice recordings. It is desirable to present multiple streams of audio information simultaneously, as can easily be done in a graphics-based system, to circumvent the linear, single channel, nature of speech signals.
Reference: [Bla83] <author> J. Blauert. </author> <title> Spatial Hearing: The Psychophysics of Human Sound Localization. </title> <publisher> MIT Press, </publisher> <year> 1983. </year>
Reference-contexts: The cocktail party effect can thus be partly explained by BMLD's. When listening binaurally, the desired signal coming from one direction is less effectively masked by noise that originates in a different direction <ref> [Bla83] </ref>. Such a technique is often exploited in earphones for fighter pilots to help separate speech signals from the high noise level of the cockpit. <p> A contributing factor has also been advances in understanding of human spatial hearing and computational ability to synthesize head-related transfer functions (HRTFs; directionally sensitive models of the head, body, and pinna transfer functions) <ref> [Bla83] </ref>. These systems usually rely on the use of stereo headphones, and synthesize sounds that are localized outside of the head.
Reference: [BN82] <author> J. P. L. Brokx and S. G. Noteboom. </author> <title> Intonation and the perceptual separation of simultaneous voices. </title> <journal> Journal of Phonetics, </journal> <volume> 10 </volume> <pages> 23-36, </pages> <year> 1982. </year>
Reference-contexts: These coarticulatory features provide spectral continuity within and between utterances. Continuities of the fundamental and the formant frequencies are important at keeping the speech signals integrated into a single stream. Pitch-based Segregation. It is harder to separate two spoken stories if they both have the same pitch <ref> [BN82] </ref>. By digitally re-synthesizing speech using LPC analysis, it is possible to hold the pitch of an utterance perfectly constant. It was found that as the fundamentals of two passages were separated in frequency, the number of errors decreased 3 .
Reference: [Bre90] <author> A. S. Bregman. </author> <title> Auditory Scene Analysis: The Perceptual Organization of Sound. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Acoustically, the problem is akin to separating out a single talker's speech from a spectrogram containing signals from several speakers under noisy conditions. Even an expert spectrogram reader would find this task impossible <ref> [Bre90] </ref>. Most of the evidence presented has been obtained from perceptual experiments that have been performed over the last 40-odd years. Unfortunately, such perceptual evidence is often not as quantifiable as, for example, physical resonances of the vocal tract. <p> Auditory Scene Analysis A great variety of research relating to perceptual grouping of auditory stimuli into streams has recently been performed, and summarized, by Bregman <ref> [Bre90] </ref>. In the introduction to his book, Bregman talks about perceptual constancies in audition, and how they relate to vision: A friend's voice has the same perceived timbre in a quiet room as at a cocktail party. <p> Streams are a way of putting sensory information together. If the properties far and lion roar are assigned to one auditory stream, and near and crackling fire assigned to a different stream, we will probably behave differently than if the distance percepts were reversed <ref> [Bre90, Han89] </ref>. Many of the ideas of auditory scene analysis can be traced back to visual work done by the Gestaltists of the early 1900's [Han89]. Visual and auditory events are combined to make the most coherent perceptual objects. <p> The following figures visually illustrate these types of groupings (the circles represent sounds at a particular frequency). In the figure below (after <ref> [Bre90] </ref>), the segregation is stronger in figure 1b than figure 1a, as the frequency separation between the high and low tones is greater. Similarly, the segregation is still greater in figure 1c where there is an increase in speed.
Reference: [Bro58] <author> D. E. Broadbent. </author> <title> Perception and Communication. </title> <publisher> Pergammon Press, </publisher> <year> 1958. </year>
Reference-contexts: Selective Listening to Speech In 1958, Broadbent summarized much of this early work, including his own experiments, and that of a variety of other researchers <ref> [Bro58] </ref>. It had been experimentally established by that time that the probability of a listener correctly hearing a word varies with the probability of the word occurring in a particular context.
Reference: [Che53] <author> E. C. Cherry. </author> <title> Some experiments on the recognition of speech, with one and two ears. </title> <journal> Journal of the Acoustic Society of America, </journal> <volume> 25 </volume> <pages> 975-979, </pages> <year> 1953. </year>
Reference-contexts: Hearing the intermixed voices of many pilots over a single loudspeaker made the controller's task very difficult [KS83]. Recognition of Speech With One and Two Ears In 1953, Cherry reported on objective experiments performed at MIT on the recognition of messages received by one and two ears <ref> [Che53] </ref>. This appears to be the first technical work that directly addresses what the author termed the cocktail party problem. Cherry proposed a few factors that may ease the task of designing a filter that could separate voices: 1. The voices come from different directions 2.
Reference: [CL91] <author> M. Cohen and L. F. Ludwig. </author> <title> Multidimensional window management. </title> <journal> International Journal of Man/Machine Systems, </journal> <volume> 34 </volume> <pages> 319-336, </pages> <year> 1991. </year>
Reference-contexts: While the use of these rock-n-roll effects may seem extreme, a recent description of the work discusses the use of just noticeable effects that are barely over edge of perceptibility <ref> [CL91] </ref>. Similar effects are used for highlighting pieces of audio to draw one's attention to it. Unfortunately, the combination of auditory effects needed to generate these relations appears to have been chosen in a somewhat ad hoc manner, and no formal perceptual studies were performed.
Reference: [Coh90] <author> E. A. Cohen. </author> <title> Technologies for three dimensional sound presentation and issues in subjective evaluation of the spatial image. </title> <booktitle> In Audio Engineering Society 89th Convention, </booktitle> <year> 1990. </year> <type> preprint number 2943. </type>
Reference-contexts: The use of this technique is feasible for limited domain tasks, but it is unlikely to be computationally tractable for any large domains in the near future. Stream Segregation Synthesis There has been a recent surge of work in the area of real-time three-dimensional auditory display systems <ref> [Coh90] </ref>. This activity has been partially motivated by the availability of inexpensive digital signal processing hardware and the great interest in virtual environments and teleoperator systems.
Reference: [CT54] <author> E. C. Cherry and W. K. Taylor. </author> <title> Some further experiments on the recognition of speech, with one and two ears. </title> <journal> Journal of the Acoustic Society of America, </journal> <volume> 26 </volume> <pages> 554-559, </pages> <year> 1954. </year>
Reference-contexts: Introduction One of the most striking facts about our ears is that we have two of them and yet we hear one acoustic world; only one voice per speaker <ref> [CT54] </ref> This paper investigates aspects of selective attention in the auditory systemunder what conditions can a listener attend to one of several competing messages? Humans are adept at listening to one voice in the midst of other conversations and noise, but not all the mechanisms for this process are completely understood. <p> By switching one message periodically between the ears, the time interval needed to transfer attention between the ears was determined. For most subjects this interval was about 170 ms. A further study investigates this in more detail, defining t as the average word recognition delay <ref> [CT54] </ref>. Note that t represents the entire complex hearing process, and is not just because of the sensory system.
Reference: [DC78] <author> N. I. Durlach and H. S. Colburn. </author> <title> Binaural phenomena. </title> <editor> In E. C. Carterette and M. P. Friedman, editors, </editor> <booktitle> Hearing, volume IV of Handbook of Perception, chapter 10. </booktitle> <publisher> Academic Press, </publisher> <year> 1978. </year>
Reference-contexts: Binaural Unmasking Our ability to detect a signal in a background masking signal is greatly improved with two ears. Under ideal conditions, the detection threshold for binaural listening will exceed monaural listening by 25 dB <ref> [DC78] </ref>. Consider, for example, a control condition where a signal and noise are played to a single ear.
Reference: [FBE90] <author> J. L. Flanagan, D. A. Berkley, and G. W. Elko. </author> <note> Autodirective microphone systems. Preprint of invited paper for a special issue of Acustica honoring Professor G. </note> <author> M. Sessler, </author> <year> 1990. </year>
Reference-contexts: It was proposed that an ultrasonic transmitter could be carried, so that the system could track the speaker. Recent work in beam-forming signal-seeking microphone arrays appears promising, though much of the effort is geared toward teleconferencing and auditorium environments <ref> [FBE90] </ref>. With three microphones it is possible to reject interfering speech arriving from non-preferred directions [LM87] Bregman discusses several systems based primarily on tracking fundamentals for computationally separating speakers (see also [Zis90]).
Reference: [Han89] <author> S. Handel. </author> <title> Listening: An Introduction to the Perception of Auditory Events. </title> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: This attentional ability has been colloquially termed the cocktail party effect <ref> [Han89] </ref>. The phenomenon can be viewed in many ways. From a listener's point of view, the task is intuitive and simple. <p> the human skill, there is some hope that a computer could be designed to mimic it. ([Bre90] page 2) Scene analysis in audition is concerned with the perceptual questions of deciding how many sound sources there are, what are the characteristics of each source, and where each source is located <ref> [Han89] </ref>. A baby, for example, imitates its mother's voice, but does not insert the cradle squeaks that have occurred simultaneously with the mother's speech. <p> Streams are a way of putting sensory information together. If the properties far and lion roar are assigned to one auditory stream, and near and crackling fire assigned to a different stream, we will probably behave differently than if the distance percepts were reversed <ref> [Bre90, Han89] </ref>. Many of the ideas of auditory scene analysis can be traced back to visual work done by the Gestaltists of the early 1900's [Han89]. Visual and auditory events are combined to make the most coherent perceptual objects. <p> Many of the ideas of auditory scene analysis can be traced back to visual work done by the Gestaltists of the early 1900's <ref> [Han89] </ref>. Visual and auditory events are combined to make the most coherent perceptual objects. Elements belonging to one stream are maximally similar and predictable, while elements belonging to different streams are maximally dissimilar.
Reference: [KS83] <author> B. H. Kantowitz and R. D. Sorkin. </author> <title> Human Factors: Understanding People-System Relationships. </title> <publisher> John Wiley and Sons, </publisher> <year> 1983. </year>
Reference-contexts: At that time, controllers received messages from pilots over loudspeakers in the control tower. Hearing the intermixed voices of many pilots over a single loudspeaker made the controller's task very difficult <ref> [KS83] </ref>. Recognition of Speech With One and Two Ears In 1953, Cherry reported on objective experiments performed at MIT on the recognition of messages received by one and two ears [Che53]. This appears to be the first technical work that directly addresses what the author termed the cocktail party problem.
Reference: [LM87] <author> H. Liang and N. Malik. </author> <title> Reducing cocktail party noise by adaptive array filtering. </title> <booktitle> In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 185-188. </pages> <publisher> IEEE, </publisher> <year> 1987. </year>
Reference-contexts: Recent work in beam-forming signal-seeking microphone arrays appears promising, though much of the effort is geared toward teleconferencing and auditorium environments [FBE90]. With three microphones it is possible to reject interfering speech arriving from non-preferred directions <ref> [LM87] </ref> Bregman discusses several systems based primarily on tracking fundamentals for computationally separating speakers (see also [Zis90]). This scheme is somewhat impractical because not all speech sounds are voiced, and the fundamental frequency becomes difficult to track as the number of speakers increases.
Reference: [LPC90] <author> L. Ludwig, N. Pincever, and M. Cohen. </author> <title> Extending the notion of a window system to audio. </title> <journal> IEEE Computer, </journal> <volume> 23(8) </volume> <pages> 66-72, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: A different approach to the synthesis of auditory streams has been developed by the Integrated Media Architecture Laboratory at Bellcore in the context of a multiperson multimedia teleconferencing system <ref> [LPC90] </ref>. This audio windowing system primarily uses off-the-shelf music processing equipment to synthesize, or enhance, many of the primitive segregation features mentioned in previous sections. Filters, pitch shifters, harmonic enhancers, distortions, reverberations, echos, etc. were used to create peer and hierarchical relationships among several spoken channels.
Reference: [Moo89] <author> B. C. J. Moore. </author> <title> An Introduction to the Psychology of Hearing. </title> <publisher> Academic Press, </publisher> <address> 3d edition, </address> <year> 1989. </year>
Reference: [MRY71] <author> O. M. Mitchell, C. A. Ross, and G. H. Yates. </author> <title> Signal processing for a cocktail party effect. </title> <journal> Journal of the Acoustic Society of America, </journal> <volume> 50(2) </volume> <pages> 656-660, </pages> <year> 1971. </year>
Reference-contexts: In 1971, researchers at Bell Labs reported on a signal processing system for separating a speech signal originating at a known location from a background of other sounds <ref> [MRY71] </ref>. The system used an array of four microphones and simple computational elements to achieve a 3-6 dB noise suppression. This scheme was somewhat impractical, as the source had to remain exactly centered in the microphone array.
Reference: [Nor76] <author> D. A. Norman. </author> <title> Memory and Attention. </title> <publisher> John Wiley and Sons, </publisher> <year> 1976. </year>
Reference-contexts: The recognition process can easily be switched to either ear at will. The subject could readily shadow one message while listening, though with a slight delay. Norman states that the longer the lag, the greater advantage that can be taken of the structure of the language <ref> [Nor76] </ref>. Note that the subject's voice is usually monotonic and they typically have little idea of the content of the message in the attended to ear. Virtually nothing can be recalled about the message content presented to the other (rejected) ear, except that sounds were occurring.
Reference: [SA89] <author> C. Schmandt and B. Arons. </author> <title> Desktop audio. Unix Review, </title> <month> October </month> <year> 1989. </year>
Reference-contexts: In a user interface it may be desirable to present multiple digitized speech recordings simultaneously, providing browsing capabilities while circumventing the time bottleneck inherent in speech communication because of the serial nature of audio <ref> [Aro91, SA89] </ref>. Synthesis of perceptual cues by a machine for human listeners might allow an application to perceptually nudge the user, making it easier to attend to a particular voice, or suggest that a new voice come into focus.
Reference: [SCW54] <author> W. Spieth, J. F. Curtis, and J. C. Webster. </author> <title> Responding to one of two simultaneous messages. </title> <journal> Journal of the Acoustic Society of America, </journal> <volume> 26(1) </volume> <pages> 391-396, </pages> <month> May </month> <year> 1954. </year>
Reference-contexts: Note that t represents the entire complex hearing process, and is not just because of the sensory system. Responding to One of Two Simultaneous Messages Spieth et al. at the Navy Electronics Laboratory in San Diego performed a series of experiments investigating responses to the presentation of simultaneous messages <ref> [SCW54] </ref>. The goal of the first set of experiments was to find conditions under which a communication's operator could best recognize and attend to one speech message when it was presented simultaneously with another irrelevant message.
Reference: [SW63] <author> C. E. Shannon and W. Weaver. </author> <title> The Mathematical Theory of Communication. </title> <publisher> University of Illinois Press, </publisher> <year> 1963. </year>
Reference-contexts: The author stated the result is a babel, but nevertheless the messages may be separated. In a Shannonesque analysis, Cherry suggested that humans have a vast memory of transition probabilities that make it easy for us to predict word sequences <ref> [SW63] </ref>. A series of experiments were performed that involved the shadowing of recordings; the subject repeated words after hearing them from a tape recording. The contents of the recordings were often related, and in the same style, such as by selecting adjacent paragraphs from the same book.
Reference: [Wei86] <author> M. Weintraub. </author> <title> A computational model for separating two simultaneous talkers. </title> <booktitle> In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 81-84. </pages> <publisher> IEEE, </publisher> <year> 1986. </year>
Reference-contexts: This scheme is somewhat impractical because not all speech sounds are voiced, and the fundamental frequency becomes difficult to track as the number of speakers increases. Weintraub found improvements in speech recognition accuracy in separating a stronger voice from a weaker one <ref> [Wei86] </ref>. Keep in mind that much of the speech segregation task performed by humans is based in part on knowledge of the transition probabilities between words in a particular context.
Reference: [WT54] <author> J. C. Webster and P. O. Thompson. </author> <title> Responding both of two overlapping messages. </title> <journal> Journal of the Acoustic Society of America, </journal> <volume> 26(1) </volume> <pages> 396-402, </pages> <month> May </month> <year> 1954. </year>
Reference-contexts: Responding to Both of Two Simultaneous Messages A related study by Webster and Thomas investigated responding to both of two overlapping messages <ref> [WT54] </ref>. As in the previous experiment, more correct identifications for sequential messages were found using six loudspeakers than one. Having a pulldown facility (the ability to manually switch the audio from one particular loudspeaker to a headphone or near-field loudspeaker) gave considerably better results.
Reference: [WWF88] <author> E. M. Wenzel, F. L. Wightman, and S. H. Foster. </author> <title> A virtual display system for conveying three-dimensional acoustic information. </title> <booktitle> In Proceedings of the Human Factors Society 32nd Annual Meeting, </booktitle> <pages> pages 86-90, </pages> <year> 1988. </year>
Reference-contexts: The fundamental idea behind these binaural simulators is that in addition to creating realistic cues such as reflections and amplitude differences, a computational model of the person-specific HRTF simulates an audio world <ref> [WWF88] </ref>. Multiple sound sources, for example, can be placed at virtual locations allowing a user to move within a simulated acoustical environment. The user can translate, rotate, or tilt their head and receive the same auditory cues as if a physical sound source were present.

References-found: 24

