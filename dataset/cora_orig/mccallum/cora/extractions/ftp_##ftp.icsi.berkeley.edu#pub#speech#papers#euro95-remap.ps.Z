URL: ftp://ftp.icsi.berkeley.edu/pub/speech/papers/euro95-remap.ps.Z
Refering-URL: http://www.icsi.berkeley.edu/real/papers.html
Root-URL: http://www.icsi.berkeley.edu
Email: morgan@icsi.berkeley.edu  
Title: REMAP: RECURSIVE ESTIMATION AND MAXIMIZATION OF A POSTERIORI PROBABILITIES IN CONNECTIONIST SPEECH RECOGNITION  
Author: Herve Bourlard Yochai Konig and Nelson Morgan Emails: bourlard, konig, 
Address: Berkeley, Berkeley, CA  
Affiliation: International Computer Science Institute, Berkeley, CA Faculte Polytechnique de Mons, Mons, Belgium University of California at  
Abstract: In this paper, we briefly describe REMAP, an approach for the training and estimation of posterior probabilities, and report its application to speech recognition. REMAP is a recursive algorithm that is reminiscent of the Expectation Maximization (EM) [5] algorithm for the estimation of data likelihoods. Although very general, the method is developed in the context of a statistical model for transition-based speech recognition using Artificial Neural Networks (ANN) to generate probabilities for Hidden Markov Models (HMMs). In the new approach, we use local conditional posterior probabilities of transitions to estimate global posterior probabilities of word sequences. As with earlier hybrid HMM/ANN systems we have developed, ANNs are used to estimate posterior probabilities. In the new approach, however, the network is trained with targets that are themselves estimates of local posterior probabilities. Initial experimental results support the theory by showing an increase in the estimates of posterior probabilities of the correct sentences after REMAP iterations, and a decrease in error rate for an independent test set. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Bengio, Y. R., De Mori, R., Flammia, G., & Kompe, R. </author> <year> (1992). </year> <title> "Global optimization of a neural-hidden Markov model hybrid," </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> vol. 3, </volume> <pages> pp. 252-258. </pages>
Reference-contexts: Although, in principle, we could use a generalized back-propagation-like gradient procedure in fi to maximize (5) (see, e.g., <ref> [1] </ref>), an EM-like algorithm should have better convergence properties, and would preserve the statistical interpretation of the ANN outputs. <p> at iteration t providing a parameter set fi t and, consequently, estimates of P (q ` n jx n ; q k can we determine new ANN targets that: 1. will be smooth estimates of conditional transition probabilities, 8 possible (k; `) state transition pairs in M and 8n 2 <ref> [1; n] </ref>. 2. when training the ANN for iteration t + 1, will lead to new estimates of fi t+1 and P (q ` n jx n ; q k that are guaranteed to incrementally increase (5)? In [2], we prove that a re-estimate of ANN targets that guarantee convergence to <p> Compute ANN targets P (q ` n jX j ; q k cording to (7), 8 possible (k; `) state transition pairs in M and 8n 2 <ref> [1; n] </ref>. 2. For all x n 's in X, train the ANN to minimize the relative entropy between the outputs and targets. This provides us with a new set of parameters fi t , for t = t + 1. 3. Iterate from 1 until convergence.
Reference: [2] <author> Bourlard, H., Konig, Y., & Morgan, N. </author> <year> (1994). </year> <title> "REMAP: Recursive Estimation and Maximization of A Posteriori Probabilities Application to Transition-Based Connectionist Speech Recognition," </title> <type> ICSI Technical Report TR-94-064, Intl. </type> <institution> Computer Science Institute, Berkeley, </institution> <address> CA. </address>
Reference-contexts: This algorithm, which we call REMAP (Recursive Estimation and maximization of A Posteriori Probabilities), generates successive estimates of new (local) posterior probabilities as targets for an ANN training step to guarantee an iterative increase of the global posteriors. We show in <ref> [2] </ref> that estimation of the new ANN targets can be done using "forward" and "backward" recurrences that are reminiscent of the Expectation Maximization (EM) algorithm. In the experiments reported here, we use a modified approach that only uses a "forward" recurrence for both training and recognition. <p> As discussed in <ref> [2] </ref>, depending on what we encode into the acoustic models, this latter factor will represent phonological, lexical and/or syntactical information. <p> It is unfortunately possible that the interpolative capabilities of the network may not be sufficient to give these "impossible" pairs a sufficiently low probability during recognition <ref> [2] </ref>. This problem is can be viewed as a lack of negative examples (i.e., impossible transitions for some given acoustic data). One possible solution to these problems is to use a "full" MAP algorithm taking all possible paths into account to estimate conditional transition probabilities. <p> 8 possible (k; `) state transition pairs in M and 8n 2 [1; n]. 2. when training the ANN for iteration t + 1, will lead to new estimates of fi t+1 and P (q ` n jx n ; q k that are guaranteed to incrementally increase (5)? In <ref> [2] </ref>, we prove that a re-estimate of ANN targets that guarantee convergence to a local maximum of (5) is given by 2 : fl ` k ` k t which means that the new ANN target associated with x n and a specific transition q k ! q ` has to <p> In <ref> [2] </ref>, we further prove that alternating ANN target estimation (the "estimation" step) and ANN training (the "maximization" step) is guaranteed to incrementally increase (5) over t. 3 The remaining problem is to find an efficient algorithm to express P (q ` n jX; q k n1 ; M ) in terms <p> Similarly, REMAP increases the global posterior function during the M step (in the direction of targets that actually maximize that global function), rather than actually maximizing it. Convergence of this training scheme can however be proved <ref> [2] </ref>. As for the EM, the convergence proof relies on the definition of an auxiliary function with the following properties: 1. When increased, the global MAP is also guaranteed to increase. 2.
Reference: [3] <author> Bourlard, H. and Morgan, N. </author> <year> (1994). </year> <title> Connectionist Speech Recognition A Hybrid Approach, </title> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: In particular, we have shown that fairly simple layered structures, which we lately have termed Big Dumb Neural Networks (BDNNs), can be used to estimate local probabilities for HMMs <ref> [3] </ref>. This approach is now usually referred to as a hybrid HMM/ANN system. <p> Also, in addition to using all possible state sequences, the proposed training algorithm uses posterior probabilities at both local and global levels and is discriminant in nature. 2. DISCRIMINANT HMM In <ref> [3] </ref>, summarizing earlier work (such as [4]), we showed that it was possible to compute the global a posteriori probability P (M jX; fi) of a Discriminant HMM M given an acoustic vector sequence X = fx 1 ; : : : ; x n ; : : : ; x <p> Each term in (2) can further be decomposed into: P (q j;1 ; q j;2 ; : : : ; q j;N jX; fi)P (M i jq j;1 ; q j;2 ; : : : ; q j;N ; X; fi) and, under the assumptions stated in <ref> [3] </ref>, we have P (q j;1 ; q j;2 ; : : : ; q j;N jX; fi) = n=1 (4) The second factor in (3) can be considered independent of the acoustic sequence X (since the state sequence is given). <p> Results for the pilot test set are summarized in Table 1. Note that the row entitled "Classic Hybrid" refers to an ANN trained on targets that are 1's and 0's that have been obtained from a forced Viterbi procedure by our standard HMM/ANN system as described in <ref> [3] </ref>; the row entitled "Disc. HMM, pre-REMAP" means a Discriminant HMM using the same training approach, with hard targets determined by the first system, and additional inputs to represent the previous state.
Reference: [4] <author> Bourlard, H. and Wellekens, C.J. </author> <year> (1990). </year> <title> "Links between Markov models and multilayer perceptrons," </title> <journal> IEEE Trans. on PAMI, </journal> <volume> vol. 12, no. 12, </volume> <pages> pp. 1167-1178. </pages>
Reference-contexts: Also, in addition to using all possible state sequences, the proposed training algorithm uses posterior probabilities at both local and global levels and is discriminant in nature. 2. DISCRIMINANT HMM In [3], summarizing earlier work (such as <ref> [4] </ref>), we showed that it was possible to compute the global a posteriori probability P (M jX; fi) of a Discriminant HMM M given an acoustic vector sequence X = fx 1 ; : : : ; x n ; : : : ; x N g as: P (M jX;
Reference: [5] <author> Dempster, A.P., Laird, N.M., & Rubin, D.B. </author> <year> (1977). </year> <title> "Maximum likelihood from incomplete data via the EM algorithm," </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> vol. 39, </volume> <pages> pp. 1-38. </pages>
Reference: [6] <author> Glass, J.R. </author> <year> (1988). </year> <title> Finding Acoustic Regularities in Speech Applications to Phonetic Recognition, M.I.T. </title> <type> PhD Dissertation. </type>
Reference-contexts: REMAP FOR DISCRIMINANT HMMS 3.1. MOTIVATIONS Discriminant HMMs as described above use conditional transition probabilities as the key building block for acoustic recognition. It is, however, well known that estimating transitions accurately is a difficult problem <ref> [6] </ref>. In our previous hybrid systems, the targets used for ANN training are typically given by the best segmentation resulting from a Viterbi alignment. This procedure thus yields rigid transition targets, which may not be optimal in the case of training (and testing!) of conditional transition probabilities.
Reference: [7] <author> Steeneken, J.M. and Van Leeuwen, D.A. </author> <year> (1995). </year> <title> "Multi-Lingual Assessment of Speaker Independent large vocabulary speech-recognition systems: the SQALE project (speech recognition quality assessment for language engineering)," </title> <booktitle> Proceedings of EUROSPEECH'95 (Madrid), </booktitle> <month> September </month> <year> 1995. </year>
Reference-contexts: Recently, such a system has been evaluated under both the North American ARPA program and the European LRE SQALE project (20,000 word vocabulary, speaker independent continuous speech recognition). In the preliminary results of the SQALE evaluation (reported in <ref> [7] </ref>) the system was found to perform slightly better than any other leading European system and required an order of magnitude less CPU resources to complete the test. The initial Discriminant HMM theory has recently been extended to accommodate full MAP training of hybrid HMM/ANN systems.
References-found: 7

