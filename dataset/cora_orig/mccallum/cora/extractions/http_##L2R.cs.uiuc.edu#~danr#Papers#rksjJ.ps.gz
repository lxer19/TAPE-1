URL: http://L2R.cs.uiuc.edu/~danr/Papers/rksjJ.ps.gz
Refering-URL: http://L2R.cs.uiuc.edu/~danr/publications.html
Root-URL: http://www.cs.uiuc.edu
Email: E-mail: aizen+@pitt.edu  E-mail: avrim@cs.cmu.edu.  E-mail: roni@das.harvard.edu.  E-mail: eyalk@cs.technion.ac.il.  E-mail: pitt@cs.uiuc.edu.  E-mail: danr@wisdom.weizmann.ac.il.  
Title: On Learning Read-k-Satisfy-j DNF  
Author: Howard Aizenstein Avrim Blum Roni Khardon Eyal Kushilevitz Leonard Pitt Dan Roth k 
Keyword: DNF, learning, computational learning theory, decision trees.  
Web: http://www.cs.technion.ac.il/~eyalk.  
Address: 3811 O'Hara St. Pittsburgh, PA 15213  Pittsburgh, PA 15213-3891.  Cambridge, MA 02138.  Israel 32000.  Urbana, IL 61801.  Rehovot 76100, ISRAEL.  
Affiliation: School of Medicine University of Pittsburgh Western Psychiatric Institute and Clinic  School of Computer Science, Carnegie Mellon University,  Aiken Computation Lab., Harvard University,  (Center for Intelligent Control Systems). Department of Computer Science, Technion, Haifa,  at Aiken Computation Laboratory, Harvard University,  Department of Computer Science, University of Illinois,  Dept. of Appl. Math. CS, Weizmann Institute of Science,  
Note: To appear in SIAM Journal of Computing  Research supported in part by NSF Grant IRI-9014840.  Research supported in part by NSF National Young Investigator grant CCR-93-57793 and a Sloan Foundation Research Fellowship.  Research supported by grant DAAL03-92-G-0115  Part of this research was done while the author was  supported by research contracts ONR-N0001491-J-1981 and NSF-CCR-90-07677.  Research supported in part by NSF grant IRI-9014840. k  This work was done while at Harvard University, supported by NSF grant CCR-92-00884 and by DARPA AFOSR-F4962-92-J-0466.  
Abstract: We study the learnability of Read-k-Satisfy-j (RkSj) DNF formulas. These are boolean formulas in disjunctive normal form (DNF), in which the maximum number of occurrences of a variable is bounded by k, and the number of terms satisfied by any assignment is at most j. After motivating the investigation of this class of DNF formulas, we present an algorithm that for any unknown RkSj DNF formula to be learned, with high probability finds a logically equivalent DNF formula using the well-studied protocol of equivalence and membership queries. The algorithm runs in polynomial time for k j = O( log n log log n ), where n is the number of input variables.
Abstract-found: 1
Intro-found: 1
Reference: [AFP92] <author> D. Angluin, M. Frazier, and L. Pitt. </author> <title> Learning conjunctions of Horn clauses. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 147-164, </pages> <year> 1992. </year>
Reference-contexts: Among these learnable subclasses are: Monotone DNF [Val84, Ang88], Read twice DNF [Han91, AP91, PR95], "Horn" DNF (at most one literal negated in every term) <ref> [AFP92] </ref>, log n term DNF [BR92], and DNF " CNF (this class includes decision trees) [Bsh95]. Read-k-DNF Particularly relevant to our work is the considerable attention that has been given to the learnability of boolean formulas where each variable occurs some bounded (often constant) number k of times ("read k").
Reference: [AHK93] <author> D. Angluin, L. Hellerstein, and M. Karpinski. </author> <title> Learning read-once formulas with queries. </title> <journal> J. ACM, </journal> <volume> 40 </volume> <pages> 185-210, </pages> <year> 1993. </year>
Reference-contexts: Read-k-DNF Particularly relevant to our work is the considerable attention that has been given to the learnability of boolean formulas where each variable occurs some bounded (often constant) number k of times ("read k"). Recently, polynomial-time algorithms have been given for learning read-once and read-twice DNF <ref> [AHK93, Han91, AP91, PR95] </ref>. (The result of [AHK93] is actually much stronger, giving an algorithm for learning any read-once boolean formula, not just those efficiently representable as read-once DNF.) The learnability of read-k DNF for k 3 seems to be much more difficult. <p> Recently, polynomial-time algorithms have been given for learning read-once and read-twice DNF [AHK93, Han91, AP91, PR95]. (The result of <ref> [AHK93] </ref> is actually much stronger, giving an algorithm for learning any read-once boolean formula, not just those efficiently representable as read-once DNF.) The learnability of read-k DNF for k 3 seems to be much more difficult.
Reference: [AHP92] <author> H. Aizenstein, L. Hellerstein, and L. Pitt. </author> <title> Read-thrice DNF is hard to learn with membership and equivalence queries. </title> <booktitle> In Proc. of the 33rd Symposium on the Foundations of Comp. Sci., </booktitle> <pages> pages 523-532. </pages> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1992. </year> <note> A revised manuscript with additional results will appear in Computational Complexity, as Aizenstein, </note> <author> Hegedus, Hellerstein, and Pitt, </author> <title> "Complexity Theoretic Hardness Results for Query Learning". </title>
Reference-contexts: In the model of learning from equivalence and membership queries, it has been shown ([PR94], see also <ref> [AHP92] </ref>) that there is no polynomial-time algorithm for exactly learning read-thrice DNF using read-thrice DNF equivalence queries, unless P = NP.
Reference: [Aiz93] <author> H. Aizenstein. </author> <title> On the Learnability of Disjunctive Normal Form Formulas and Decision Trees. </title> <type> PhD thesis, </type> <institution> Dept of Computer Science, University of Illinois, </institution> <year> 1993. </year> <type> Technical report UIUCDCS-R-93-1813, </type> <month> June, </month> <year> 1993, </year> <institution> Urbana, Illinois. </institution>
Reference-contexts: For example, the learnability of those DNF formulas with a bounded number of terms, or with bounded size terms, or with each variable appearing only a bounded number of times, has received considerable attention (see e.g. <ref> [Val84, Ang87, PV88, BS90, Han91, Aiz93, PR95] </ref>). <p> The family of functions given in Section 6 was defined in [AP92], where it was shown that the class R2D is not included in Read-k decision trees. This result was later strengthened in <ref> [Aiz93] </ref> to show that the family requires decision trees of size 2 n2 . Its current form is from [BKK + 94].
Reference: [AK95] <author> D. Angluin and M. Kharitonov. </author> <title> When won't membership queries help? Journal of Computer and System Sciences, </title> <booktitle> 50, 1995. Special issue for the 23rd Annual ACM Symposium on Theory of Computing. </booktitle> <pages> 20 </pages>
Reference-contexts: Recent results show that in the PAC model with membership queries learning read-thrice DNF is no easier than learning general DNF formulas <ref> [AK95, Han91] </ref>. <p> Recent results show that in the PAC model with membership queries learning read-thrice DNF is no easier than learning general DNF formulas [AK95, Han91]. In the PAC model, assuming the existence of one-way functions, Angluin and Kharitonov <ref> [AK95] </ref> have shown that membership queries cannot help in learning DNF (assuming that the learning algorithm is not required to express its hypotheses in any particular form, and that the distribution on examples is "polynomially bounded").
Reference: [Ang87] <author> D. Angluin. </author> <title> Learning k-term DNF formulas using queries and counterexamples. </title> <type> Technical Report YALEU/DCS/RR-559, </type> <institution> Department of Computer Science, Yale University, </institution> <month> August </month> <year> 1987. </year>
Reference-contexts: For example, the learnability of those DNF formulas with a bounded number of terms, or with bounded size terms, or with each variable appearing only a bounded number of times, has received considerable attention (see e.g. <ref> [Val84, Ang87, PV88, BS90, Han91, Aiz93, PR95] </ref>). <p> Finally, note that RkSj DNF may be thought of as a generalization of k-term DNF (those DNFs with at most k terms): Every k-term DNF formula is trivially an RkSk-DNF formula. Thus, our results may also be viewed as an extension of previous results for learning k-term DNF formulas <ref> [Ang87, BR92, Bsh95] </ref>, although a true generalization of the strongest of these results would learn RkSj DNF for k + j = O (log n).
Reference: [Ang88] <author> D. Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 319-342, </pages> <month> April </month> <year> 1988. </year>
Reference-contexts: The "Satisfy-j" constraint relaxes the notion of disjointness, thus incorporating a wider class of functions. In the next section we will discuss the relationships between RkSj DNF, RkD DNF, and previously investigated subclasses of DNF formulas. The learning model we use is the standard equivalence and membership queries model <ref> [Ang88] </ref>: We assume some unknown target RkSj formula F to be learned is chosen by nature. <p> This protocol is a probabilistic generalization of the slightly more demanding exact learning model, which requires that deterministically, in time polynomial in n and jF j, the learning algorithm necessarily finds some logically equivalent H. Learnability in this model implies learnability in the well-studied "PAC" model with membership queries <ref> [Val84, Ang88] </ref>. <p> In the model of learning with equivalence queries only, or in the PAC model without membership queries [Val84], learning algorithms are known only for k-DNF formulas (DNF formulas with a 3 constant number of literals in each term) <ref> [Val84, Ang88] </ref>, DNF formulas with a constant number of terms [KLPV87, PV88] (provided that the hypotheses need not be in the same form), or for "polynomially explainable" sub-classes of DNF in which the number of possible terms causing an example to be positive is bounded by a polynomial [Val85, KR93]. <p> Among these learnable subclasses are: Monotone DNF <ref> [Val84, Ang88] </ref>, Read twice DNF [Han91, AP91, PR95], "Horn" DNF (at most one literal negated in every term) [AFP92], log n term DNF [BR92], and DNF " CNF (this class includes decision trees) [Bsh95].
Reference: [Ang90] <author> D. Angluin. </author> <title> Negative results for equivalence queries. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 121-150, </pages> <year> 1990. </year>
Reference-contexts: Using information theoretic arguments, Angluin <ref> [Ang90] </ref> has shown that DNF in general is not learnable in polynomial-time using just DNF equivalence queries, although the negative result does not extend to the PAC model.
Reference: [AP91] <author> H. Aizenstein and L. Pitt. </author> <title> Exact learning of read-twice DNF formulas. </title> <booktitle> In Proceedings of the IEEE Symp. on Foundation of Computer Science, </booktitle> <volume> number 32, </volume> <pages> pages 170-179, </pages> <address> San Juan, </address> <year> 1991. </year>
Reference-contexts: Among these learnable subclasses are: Monotone DNF [Val84, Ang88], Read twice DNF <ref> [Han91, AP91, PR95] </ref>, "Horn" DNF (at most one literal negated in every term) [AFP92], log n term DNF [BR92], and DNF " CNF (this class includes decision trees) [Bsh95]. <p> Read-k-DNF Particularly relevant to our work is the considerable attention that has been given to the learnability of boolean formulas where each variable occurs some bounded (often constant) number k of times ("read k"). Recently, polynomial-time algorithms have been given for learning read-once and read-twice DNF <ref> [AHK93, Han91, AP91, PR95] </ref>. (The result of [AHK93] is actually much stronger, giving an algorithm for learning any read-once boolean formula, not just those efficiently representable as read-once DNF.) The learnability of read-k DNF for k 3 seems to be much more difficult.
Reference: [AP92] <author> H. Aizenstein and L. Pitt. </author> <title> Exact learning of read-k disjoint DNF and not-so-disjoint DNF. </title> <booktitle> In Proceedings of the Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 71-76, </pages> <address> Pittsburgh, Pennsylvania, 1992. </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: We thus have that C (a) = 0 but t C (a) = 1 (and hence F n (a) = 1), a contradiction. 7 Bibliographic Remarks The work presented here appeared in preliminary forms as <ref> [AP92] </ref> and [BKK + 94]. The algorithm for constant k and j is from [AP92] and the extension to k j = O (log n= log log n) from [BKK + 94]. The family of functions given in Section 6 was defined in [AP92], where it was shown that the class <p> We thus have that C (a) = 0 but t C (a) = 1 (and hence F n (a) = 1), a contradiction. 7 Bibliographic Remarks The work presented here appeared in preliminary forms as <ref> [AP92] </ref> and [BKK + 94]. The algorithm for constant k and j is from [AP92] and the extension to k j = O (log n= log log n) from [BKK + 94]. The family of functions given in Section 6 was defined in [AP92], where it was shown that the class R2D is not included in Read-k decision trees. <p> presented here appeared in preliminary forms as <ref> [AP92] </ref> and [BKK + 94]. The algorithm for constant k and j is from [AP92] and the extension to k j = O (log n= log log n) from [BKK + 94]. The family of functions given in Section 6 was defined in [AP92], where it was shown that the class R2D is not included in Read-k decision trees. This result was later strengthened in [Aiz93] to show that the family requires decision trees of size 2 n2 . Its current form is from [BKK + 94].
Reference: [AP95] <author> H. Aizenstein and L. Pitt. </author> <title> On the learnability of disjunctive normal form formulas. </title> <journal> Machine Learning, </journal> <volume> 19(3) </volume> <pages> 183-208, </pages> <year> 1995. </year>
Reference-contexts: Aizenstein and Pitt <ref> [AP95] </ref> show that even if subset queries are allowed, disjoint DNF (and also read-twice DNF) is not learnable by algorithms that collect prime implicants in a greedy manner. Yet, the learnability for disjoint DNF remains unresolved in any reasonable learning model, other than the result above assuming a uniform distribution.
Reference: [BFJ + 94] <author> A. Blum, M. Furst, J. Jackson, M. Kearns, Y. Mansour, and S. Rudich. </author> <title> Weakly learning DNF and characterizing statistical query learning using fourier analysis. </title> <booktitle> In Proceedings of Twenty-sixth ACM Symposium on Theory of Computing, </booktitle> <year> 1994. </year>
Reference-contexts: This extends results on learning decision trees, disjoint DNF and Satisfy-j DNF formulas in the same model <ref> [KM93, Kha94, BFJ + 94] </ref>. <p> This extends results on learning decision trees, disjoint DNF and Satisfy-j DNF formulas in the same model [KM93, Kha94, BFJ + 94]. On the negative side, some recent hardness results for learning DNF in restricted models apply for disjoint DNF as well: Blum et al. <ref> [BFJ + 94] </ref> prove a hardness result for learning log n disjoint DNF in the "statistical queries" model (note that in this model the learner does not have access to a membership oracle).
Reference: [BFOS84] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and Regression Trees. </title> <booktitle> Wadsworth International Group, </booktitle> <year> 1984. </year>
Reference-contexts: The motivation for considering disjoint DNF expressions arises from the fact that they generalize the decision-tree representation of boolean functions, which have been used extensively in machine-learning theory and practice <ref> [BFOS84, Qui86, Qui93] </ref>. The "Satisfy-j" constraint relaxes the notion of disjointness, thus incorporating a wider class of functions. In the next section we will discuss the relationships between RkSj DNF, RkD DNF, and previously investigated subclasses of DNF formulas.
Reference: [BKK + 94] <author> A. Blum, R. Khardon, A. Kushilevitz, L. Pitt, and D. Roth. </author> <booktitle> On learning read-k satisfy-j DNF. In Proceedings of the Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 110-117, </pages> <year> 1994. </year>
Reference-contexts: We thus have that C (a) = 0 but t C (a) = 1 (and hence F n (a) = 1), a contradiction. 7 Bibliographic Remarks The work presented here appeared in preliminary forms as [AP92] and <ref> [BKK + 94] </ref>. The algorithm for constant k and j is from [AP92] and the extension to k j = O (log n= log log n) from [BKK + 94]. <p> (and hence F n (a) = 1), a contradiction. 7 Bibliographic Remarks The work presented here appeared in preliminary forms as [AP92] and <ref> [BKK + 94] </ref>. The algorithm for constant k and j is from [AP92] and the extension to k j = O (log n= log log n) from [BKK + 94]. The family of functions given in Section 6 was defined in [AP92], where it was shown that the class R2D is not included in Read-k decision trees. This result was later strengthened in [Aiz93] to show that the family requires decision trees of size 2 n2 . <p> This result was later strengthened in [Aiz93] to show that the family requires decision trees of size 2 n2 . Its current form is from <ref> [BKK + 94] </ref>.
Reference: [BR92] <author> A. Blum and S. Rudich. </author> <title> Fast learning of k-term DNF formulas with queries. </title> <booktitle> In Proceedings of Twenty-Fourth ACM Symposium on Theory of Computing, </booktitle> <pages> pages 382-389. </pages> <publisher> ACM, </publisher> <year> 1992. </year>
Reference-contexts: Among these learnable subclasses are: Monotone DNF [Val84, Ang88], Read twice DNF [Han91, AP91, PR95], "Horn" DNF (at most one literal negated in every term) [AFP92], log n term DNF <ref> [BR92] </ref>, and DNF " CNF (this class includes decision trees) [Bsh95]. Read-k-DNF Particularly relevant to our work is the considerable attention that has been given to the learnability of boolean formulas where each variable occurs some bounded (often constant) number k of times ("read k"). <p> Finally, note that RkSj DNF may be thought of as a generalization of k-term DNF (those DNFs with at most k terms): Every k-term DNF formula is trivially an RkSk-DNF formula. Thus, our results may also be viewed as an extension of previous results for learning k-term DNF formulas <ref> [Ang87, BR92, Bsh95] </ref>, although a true generalization of the strongest of these results would learn RkSj DNF for k + j = O (log n). <p> We avoid the enumerative approach, and obtain an algorithm tolerating larger (non-constant) values of k and j, by implementing a probabilistic search in the spirit of <ref> [BR92] </ref>. We show the learnability of RkSj DNF for k j = O ( log n log log n ). Our algorithm has the following high level structure (see Figure 1), which we describe here. The low level of the algorithm is described in the next two sub-sections.
Reference: [BS90] <author> A. Blum and M. Singh. </author> <title> Learning functions of k terms. </title> <booktitle> In Proceedings of COLT '90, </booktitle> <pages> pages 144-153. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year> <month> 21 </month>
Reference-contexts: For example, the learnability of those DNF formulas with a bounded number of terms, or with bounded size terms, or with each variable appearing only a bounded number of times, has received considerable attention (see e.g. <ref> [Val84, Ang87, PV88, BS90, Han91, Aiz93, PR95] </ref>).
Reference: [Bsh95] <author> N. H. Bshouty. </author> <title> Exact learning boolean functions via the monotone theory. </title> <journal> Information and Computation, </journal> <volume> 123(1) </volume> <pages> 146-153, </pages> <year> 1995. </year> <note> Earlier version appeared in Proc. </note> <editor> 34rd Ann. </editor> <booktitle> IEEE Symp. on Foundations of Computer Science, </booktitle> <year> 1993. </year>
Reference-contexts: Among these learnable subclasses are: Monotone DNF [Val84, Ang88], Read twice DNF [Han91, AP91, PR95], "Horn" DNF (at most one literal negated in every term) [AFP92], log n term DNF [BR92], and DNF " CNF (this class includes decision trees) <ref> [Bsh95] </ref>. Read-k-DNF Particularly relevant to our work is the considerable attention that has been given to the learnability of boolean formulas where each variable occurs some bounded (often constant) number k of times ("read k"). <p> However, every decision tree can be efficiently expressed as a disjoint DNF formula, by creating a term corresponding to the assignment of variables along each branch that leads to a leaf labeled "1". Recently, Bshouty <ref> [Bsh95] </ref> has given an algorithm (using equivalence and membership queries) for learning boolean formulas in time polynomial in the size of their DNF and CNF representations. <p> Finally, note that RkSj DNF may be thought of as a generalization of k-term DNF (those DNFs with at most k terms): Every k-term DNF formula is trivially an RkSk-DNF formula. Thus, our results may also be viewed as an extension of previous results for learning k-term DNF formulas <ref> [Ang87, BR92, Bsh95] </ref>, although a true generalization of the strongest of these results would learn RkSj DNF for k + j = O (log n). <p> Note that this also implies that the size of any decision tree for these functions must also be super-polynomial, even without restricting the number of variable occurrences. Bshouty <ref> [Bsh95] </ref> gives an algorithm for learning functions (such as 18 decision trees) which have both small DNFs and small CNFs. Our family of functions fF n g show that this result does not apply here as the size of the CNFs for even read-2 disjoint DNFs may be super-polynomial. <p> Our family of functions fF n g show that this result does not apply here as the size of the CNFs for even read-2 disjoint DNFs may be super-polynomial. It remains open, however, whether techniques similar to those used in <ref> [Bsh95] </ref> can be applied to yield a comparable, or stronger result. Such a result would rely on showing that every RkSj DNF has a small "monotone basis"; we suspect this is not the case as even read-twice DNFs (though not disjoint) require a exponentially large monotone basis (Bshouty, private communication).
Reference: [Han91] <author> T. Hancock. </author> <title> Learning 2 DNF formulas and k decision trees. </title> <booktitle> In Proceedings of the Fourth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 199-209, </pages> <address> Santa Cruz, California, August 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: For example, the learnability of those DNF formulas with a bounded number of terms, or with bounded size terms, or with each variable appearing only a bounded number of times, has received considerable attention (see e.g. <ref> [Val84, Ang87, PV88, BS90, Han91, Aiz93, PR95] </ref>). <p> Among these learnable subclasses are: Monotone DNF [Val84, Ang88], Read twice DNF <ref> [Han91, AP91, PR95] </ref>, "Horn" DNF (at most one literal negated in every term) [AFP92], log n term DNF [BR92], and DNF " CNF (this class includes decision trees) [Bsh95]. <p> Read-k-DNF Particularly relevant to our work is the considerable attention that has been given to the learnability of boolean formulas where each variable occurs some bounded (often constant) number k of times ("read k"). Recently, polynomial-time algorithms have been given for learning read-once and read-twice DNF <ref> [AHK93, Han91, AP91, PR95] </ref>. (The result of [AHK93] is actually much stronger, giving an algorithm for learning any read-once boolean formula, not just those efficiently representable as read-once DNF.) The learnability of read-k DNF for k 3 seems to be much more difficult. <p> Recent results show that in the PAC model with membership queries learning read-thrice DNF is no easier than learning general DNF formulas <ref> [AK95, Han91] </ref>.
Reference: [Jac94] <author> J. Jackson. </author> <title> An efficient membership-query algorithm for learning DNF with respect to the uniform distribution. </title> <booktitle> In Proceedings of the IEEE Symp. on Foundation of Computer Science, </booktitle> <volume> number 35, </volume> <pages> pages 42-53, </pages> <address> Santa Fe, NM., </address> <year> 1994. </year>
Reference-contexts: In particular, we present a family of functions fF n g that have short (poly (n)) read-2 disjoint DNF formulas but require CNF formulas of size 2 ( p Other results concerning the learnability of disjoint DNF have also been obtained. On the positive side, Jackson <ref> [Jac94] </ref> gives a polynomial-time algorithm for learning arbitrary DNF formulas in the PAC model with membership queries, provided that the error is measured with respect to the uniform distribution.
Reference: [Kha94] <author> R. Khardon. </author> <title> On using the Fourier transform to learn disjoint DNF. </title> <journal> Information Processing Letters, </journal> <volume> 49(5) </volume> <pages> 219-222, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: This extends results on learning decision trees, disjoint DNF and Satisfy-j DNF formulas in the same model <ref> [KM93, Kha94, BFJ + 94] </ref>.
Reference: [KLPV87] <author> M. Kearns, M. Li, L. Pitt, and L. Valiant. </author> <title> On the learnability of Boolean formulae. </title> <booktitle> In Proc. 19th Annu. ACM Sympos. Theory Comput., </booktitle> <pages> pages 285-294. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1987. </year>
Reference-contexts: In the model of learning with equivalence queries only, or in the PAC model without membership queries [Val84], learning algorithms are known only for k-DNF formulas (DNF formulas with a 3 constant number of literals in each term) [Val84, Ang88], DNF formulas with a constant number of terms <ref> [KLPV87, PV88] </ref> (provided that the hypotheses need not be in the same form), or for "polynomially explainable" sub-classes of DNF in which the number of possible terms causing an example to be positive is bounded by a polynomial [Val85, KR93].
Reference: [KM93] <author> E. Kushilevitz and Y. Mansour. </author> <title> Learning decision trees using the fourier spectrum. </title> <journal> SIAM Journal of Computing, </journal> <volume> 22(6) </volume> <pages> 1331-1348, </pages> <year> 1993. </year> <note> Earlier version appeared in the 23rd Annual ACM Symposium on Theory of Computing, </note> <year> 1991. </year>
Reference-contexts: This extends results on learning decision trees, disjoint DNF and Satisfy-j DNF formulas in the same model <ref> [KM93, Kha94, BFJ + 94] </ref>.
Reference: [KR93] <author> E. Kushilevitz and D. Roth. </author> <title> On learning visual concepts and DNF formulae. </title> <booktitle> In Proceedings of the ACM Workshop on Computational Learning Theory '93, </booktitle> <pages> pages 317-326. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year> <note> To apppear in Machine Learning, </note> <year> 1996. </year>
Reference-contexts: each term) [Val84, Ang88], DNF formulas with a constant number of terms [KLPV87, PV88] (provided that the hypotheses need not be in the same form), or for "polynomially explainable" sub-classes of DNF in which the number of possible terms causing an example to be positive is bounded by a polynomial <ref> [Val85, KR93] </ref>. Using information theoretic arguments, Angluin [Ang90] has shown that DNF in general is not learnable in polynomial-time using just DNF equivalence queries, although the negative result does not extend to the PAC model.
Reference: [Mat93] <author> S. Matar. </author> <title> Learning with minimal number of queries. </title> <type> Master's thesis, </type> <institution> University of Alberta, Canada, </institution> <year> 1993. </year>
Reference-contexts: Hence, the number of equivalence queries is O (sm 2 m 3 (kjq 2 ) 4kj ). Note that just by the Read-k property of F we have s kn. A tighter bound of s = O ( p is given by <ref> [Mat93] </ref>. Claim 18 Let ffi &gt; 0 be a constant and kj = O ( log n log log n ). The algorithm Learn-RkSj runs in time poly (n) and finds a function logically equivalent to F with probability at least 1 ffi.
Reference: [PR94] <author> K. Pillapakkamnatt and V. Raghavan. </author> <title> On the limits of proper learnability of subsets of DNF formulas. </title> <booktitle> In Proceedings of the Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 118-129, </pages> <year> 1994. </year> <note> To apppear in Machine Learning, </note> <year> 1996. </year>
Reference: [PR95] <author> K. Pillapakkamnatt and V. Raghavan. </author> <title> Read twice DNF formulas are properly learnable. </title> <journal> Information and Computation, </journal> <volume> 122(2) </volume> <pages> 236-267, </pages> <year> 1995. </year>
Reference-contexts: For example, the learnability of those DNF formulas with a bounded number of terms, or with bounded size terms, or with each variable appearing only a bounded number of times, has received considerable attention (see e.g. <ref> [Val84, Ang87, PV88, BS90, Han91, Aiz93, PR95] </ref>). <p> Among these learnable subclasses are: Monotone DNF [Val84, Ang88], Read twice DNF <ref> [Han91, AP91, PR95] </ref>, "Horn" DNF (at most one literal negated in every term) [AFP92], log n term DNF [BR92], and DNF " CNF (this class includes decision trees) [Bsh95]. <p> Read-k-DNF Particularly relevant to our work is the considerable attention that has been given to the learnability of boolean formulas where each variable occurs some bounded (often constant) number k of times ("read k"). Recently, polynomial-time algorithms have been given for learning read-once and read-twice DNF <ref> [AHK93, Han91, AP91, PR95] </ref>. (The result of [AHK93] is actually much stronger, giving an algorithm for learning any read-once boolean formula, not just those efficiently representable as read-once DNF.) The learnability of read-k DNF for k 3 seems to be much more difficult.
Reference: [PV88] <author> L. Pitt and L. Valiant. </author> <title> Computational limitations on learning from examples. </title> <journal> J. ACM, </journal> <volume> 35 </volume> <pages> 965-984, </pages> <year> 1988. </year> <month> 22 </month>
Reference-contexts: For example, the learnability of those DNF formulas with a bounded number of terms, or with bounded size terms, or with each variable appearing only a bounded number of times, has received considerable attention (see e.g. <ref> [Val84, Ang87, PV88, BS90, Han91, Aiz93, PR95] </ref>). <p> In the model of learning with equivalence queries only, or in the PAC model without membership queries [Val84], learning algorithms are known only for k-DNF formulas (DNF formulas with a 3 constant number of literals in each term) [Val84, Ang88], DNF formulas with a constant number of terms <ref> [KLPV87, PV88] </ref> (provided that the hypotheses need not be in the same form), or for "polynomially explainable" sub-classes of DNF in which the number of possible terms causing an example to be positive is bounded by a polynomial [Val85, KR93].
Reference: [Qui86] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: The motivation for considering disjoint DNF expressions arises from the fact that they generalize the decision-tree representation of boolean functions, which have been used extensively in machine-learning theory and practice <ref> [BFOS84, Qui86, Qui93] </ref>. The "Satisfy-j" constraint relaxes the notion of disjointness, thus incorporating a wider class of functions. In the next section we will discuss the relationships between RkSj DNF, RkD DNF, and previously investigated subclasses of DNF formulas.
Reference: [Qui93] <author> J. R. Quinlan. C4.5: </author> <title> Programs for machine learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: The motivation for considering disjoint DNF expressions arises from the fact that they generalize the decision-tree representation of boolean functions, which have been used extensively in machine-learning theory and practice <ref> [BFOS84, Qui86, Qui93] </ref>. The "Satisfy-j" constraint relaxes the notion of disjointness, thus incorporating a wider class of functions. In the next section we will discuss the relationships between RkSj DNF, RkD DNF, and previously investigated subclasses of DNF formulas.
Reference: [Sim83] <author> H. U. Simon. </author> <title> A tight (log log n)-bound on the time for parallel RAM's to compute nondegenerate boolean functions. </title> <booktitle> In ICALP, number 158 in Lecture Notes in Computer Science, </booktitle> <pages> pages 439-444. </pages> <publisher> Springer-Verlag, </publisher> <year> 1983. </year>
Reference-contexts: Also, let "true" be the "all 1's" concept. Lemma 10 If F is an RkSj DNF, and F 6 true, then Prob x2U [F (x) = 0] 2 2kj Proof: The proof uses an argument similar to one in <ref> [Sim83] </ref>. Think of the cube f0; 1g n as a graph in which each vertex is represented by a n-bit string and there is an edge between any two vertices of Hamming distance 1.
Reference: [Val84] <author> L. G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: Among natural classes of formulas, those efficiently representable in disjunctive normal form (DNF) have been extensively studied since the seminal paper of Valiant <ref> [Val84] </ref>. Nonetheless, whether for such formulas there are learning algorithms that can be guaranteed to succeed in polynomial time using any of the standard learning protocols remains a challenging open question. Consequently, recent work has focused on the learnability of various restricted subclasses of DNF formulas. <p> For example, the learnability of those DNF formulas with a bounded number of terms, or with bounded size terms, or with each variable appearing only a bounded number of times, has received considerable attention (see e.g. <ref> [Val84, Ang87, PV88, BS90, Han91, Aiz93, PR95] </ref>). <p> This protocol is a probabilistic generalization of the slightly more demanding exact learning model, which requires that deterministically, in time polynomial in n and jF j, the learning algorithm necessarily finds some logically equivalent H. Learnability in this model implies learnability in the well-studied "PAC" model with membership queries <ref> [Val84, Ang88] </ref>. <p> In the model of learning with equivalence queries only, or in the PAC model without membership queries <ref> [Val84] </ref>, learning algorithms are known only for k-DNF formulas (DNF formulas with a 3 constant number of literals in each term) [Val84, Ang88], DNF formulas with a constant number of terms [KLPV87, PV88] (provided that the hypotheses need not be in the same form), or for "polynomially explainable" sub-classes of DNF <p> In the model of learning with equivalence queries only, or in the PAC model without membership queries [Val84], learning algorithms are known only for k-DNF formulas (DNF formulas with a 3 constant number of literals in each term) <ref> [Val84, Ang88] </ref>, DNF formulas with a constant number of terms [KLPV87, PV88] (provided that the hypotheses need not be in the same form), or for "polynomially explainable" sub-classes of DNF in which the number of possible terms causing an example to be positive is bounded by a polynomial [Val85, KR93]. <p> Among these learnable subclasses are: Monotone DNF <ref> [Val84, Ang88] </ref>, Read twice DNF [Han91, AP91, PR95], "Horn" DNF (at most one literal negated in every term) [AFP92], log n term DNF [BR92], and DNF " CNF (this class includes decision trees) [Bsh95].
Reference: [Val85] <author> L. G. Valiant. </author> <title> Learning disjunctions of conjunctions. </title> <booktitle> In Proceedings of the 9th International Joint Conference on Artificial Intelligence, </booktitle> <volume> vol. 1, </volume> <pages> pages 560-566, </pages> <address> Los Angeles, California, </address> <year> 1985. </year> <booktitle> International Joint Committee for Artificial Intelligence. </booktitle>
Reference-contexts: each term) [Val84, Ang88], DNF formulas with a constant number of terms [KLPV87, PV88] (provided that the hypotheses need not be in the same form), or for "polynomially explainable" sub-classes of DNF in which the number of possible terms causing an example to be positive is bounded by a polynomial <ref> [Val85, KR93] </ref>. Using information theoretic arguments, Angluin [Ang90] has shown that DNF in general is not learnable in polynomial-time using just DNF equivalence queries, although the negative result does not extend to the PAC model.
References-found: 32

