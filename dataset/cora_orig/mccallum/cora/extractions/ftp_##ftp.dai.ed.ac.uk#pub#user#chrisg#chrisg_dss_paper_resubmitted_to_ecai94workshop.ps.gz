URL: ftp://ftp.dai.ed.ac.uk/pub/user/chrisg/chrisg_dss_paper_resubmitted_to_ecai94workshop.ps.gz
Refering-URL: http://www.dai.ed.ac.uk/students/chrisg/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: fchrisg,peterg@aisb.ed.ac.uk  
Title: Some Training Subset Selection Methods for Supervised Learning in Genetic Programming  
Author: Chris Gathercole and Peter Ross 
Address: 80 South Bridge Edinburgh EH1 1HN U.K.  
Affiliation: Department of Artificial Intelligence University of Edinburgh  
Abstract: When using the Genetic Programming (GP) Algorithm on a difficult problem with a large set of training cases, a large population size is needed and a very large number of function-tree evaluations must be carried out. This paper describes how to reduce the number of such evaluations by selecting a small subset of the training data set on which to actually carry out the GP algorithm. Three subset selection methods described in the paper are: Dynamic Subset Selection (DSS), using the current GP run to select `difficult' and/or disused cases, Historical Subset Selection (HSS), using previous GP runs, Random Subset Selection (RSS). GP, GP+DSS, GP+HSS, GP+RSS are compared on a large classification problem. GP+DSS can produce better results in less than 20% of the time taken by GP. GP+HSS can nearly match the results of GP, and, perhaps surprisingly, GP+RSS can occasionally approach the results of GP. GP and GP+DSS are then compared on a smaller problem, and a hybrid Dynamic Fitness Function (DFF), based on DSS, is proposed.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Goldberg, D.E.: </author> <title> GENETIC ALGORITHMS in Search, Optimisation & Machine Learning. </title> <publisher> Addison-Wesley (1989) </publisher>
Reference-contexts: The next generation of trees is then evaluated against the problem. The best trees are selected to produce the next generation after that, and so on. As with Genetic Algorithms <ref> [2, 1] </ref>, the GP population is evolved to (hopefully) produce function-trees which can perform well on the problem in question. The code used for these experiments was adapted from the SGPC code [8], written in C.
Reference: 2. <author> Holland, J.H.: </author> <title> Adaption in Natural Selection and Artificial Systems. New edition of the original GA work. </title> <publisher> The MIT Press (1992) </publisher>
Reference-contexts: The next generation of trees is then evaluated against the problem. The best trees are selected to produce the next generation after that, and so on. As with Genetic Algorithms <ref> [2, 1] </ref>, the GP population is evolved to (hopefully) produce function-trees which can perform well on the problem in question. The code used for these experiments was adapted from the SGPC code [8], written in C.
Reference: 3. <author> Koza, J.: </author> <title> Genetic Programming: on the programming of computers by natural selection. Contains clear description of a basic Genetic Algorithm as well as a detailed de scription of Genetic Programming.MIT Press, </title> <address> Cambridge, MA, </address> <year> (1992) </year>
Reference-contexts: A smaller problem involving Tic-Tac-Toe endgame positions was also tried. Following on from the results obtained by DSS, a Dynamic Fitness Function (DFF), based on DSS, is proposed for further study. 2 GP, Code and Parameters GP <ref> [3] </ref> involves a population of Lisp-like function-tree expressions, each of which is made up from an allowed set of functions and terminals, all of the same data type.
Reference: 4. <author> Schiffmann, W., Joost, M., Werner, R.: </author> <title> Optimization of the Backpropogation Algorithm for Training Multilayer Perceptron s. </title> <institution> University of Koblenz, Institute of Physics, </institution> <month> 15 </month> <year> (1992) </year>
Reference-contexts: The results reported for Neural Networks <ref> [4, 5] </ref> provide an interesting comparison with the performance of GP, however, the main aim for this paper was to improve the performance of GP on a hard problem. The data is based upon measurements of the thyroid glands of patients at a hospital. <p> Results are given in Table 1 for a typical DSS run with a population size of 10000, and for typical GP,DSS,HSS,RSS runs with a population size of 5000, and for the best Neural Network results reported in <ref> [4] </ref>. It was not possible to complete a run of GP with a population size of 10000 in a reasonable time! to a surge around generation 48. These two methods often produce similar scores, but HSS achieves them with many fewer function-tree evaluations. function-tree evaluations. <p> Table 1: Thyroid Training and Test Results Pop. Subset Gener- Tree Training set Test set Algorithm Size Size ations Evals % correct % correct GP 10000 3772 n/a n/a n/a n/a GP 5000 3772 60 11.3e+08 99.70 99.00 GP+RSS 5000 400 124 2.5e+08 99.10 98.40 NN - <ref> [4] </ref> Cascade Correlation 100.00 98.48 The best tree produced by the DSS run (with population size = 10000) to distinguish between class 3 and not class 3, was found on Generation 69, giving only 25 errors on the test set. <p> This perhaps indicates one of the benefits of DSS, namely that, in effect, the fitness function is continually being changed, never allowing the GP to settle into a rut. When compared with the Neural Network results in <ref> [4] </ref>, the best of which is shown in Table 1 above, the GP+DSS produced a function-tree which generalised better from the training set. <p> Working with a larger population size of 10000, DSS produces better answers than any other method shown here, including those reported for various Neural Networks in <ref> [4, 5] </ref>. On the smaller, less messy, TicTacToe problem, Dynamic Subset Selection added to Genetic Programming also performs very well.
Reference: 5. <author> Schiffmann, W., Joost, M., Werner, R.: </author> <title> Synthesis and Performance Analysis of Multilayer Neural Network Architectures. </title> <institution> University of Koblenz, Institute of Physics, </institution> <month> 16 </month> <year> (1992) </year>
Reference-contexts: The results reported for Neural Networks <ref> [4, 5] </ref> provide an interesting comparison with the performance of GP, however, the main aim for this paper was to improve the performance of GP on a hard problem. The data is based upon measurements of the thyroid glands of patients at a hospital. <p> Working with a larger population size of 10000, DSS produces better answers than any other method shown here, including those reported for various Neural Networks in <ref> [4, 5] </ref>. On the smaller, less messy, TicTacToe problem, Dynamic Subset Selection added to Genetic Programming also performs very well.
Reference: 6. <author> Schiffmann, W., Joost, M., Werner, R.: </author> <title> THYROID training and test data sets. Obtained via electronic mail (1992) </title>
Reference-contexts: to ensure that the subset selected, on average, is of the target size. 8i : 1 i T; P i (g) = T As with the DSS method, the subset size fluctuates around the target size with each generation. 4 The `Large and Messy' Thyroid Problem The Thyroid data set <ref> [6] </ref> represents a hard classification problem. The results reported for Neural Networks [4, 5] provide an interesting comparison with the performance of GP, however, the main aim for this paper was to improve the performance of GP on a hard problem.
Reference: 7. <author> Swayne, D., Cook, D., Buja, A.: </author> <title> User's Manual for XGobi, a Dynamic Graphics Program for Data Analysis Implemen ted in the X Window System (Version 2). </title> <note> Bellcore Technical Memorandum TM ARH-020368 (1992) </note>
Reference-contexts: There are 3772 cases in the training set and 3428 cases in the test set. Examination of the data in graphical form, e.g. using XGobi <ref> [7] </ref>, reveals that the boundaries between the classes of points are very murky indeed. Points from different classes seem to mingle freely with each other.
Reference: 8. <author> Tackett, W.A., Carmi, A.: </author> <title> S G P C: Simple Genetic Programming in C. Original source code for GP program used in this paper. </title> <note> Available via FTP at sfi.santafe.edu:pub/Users/tackett (1993) </note>
Reference-contexts: As with Genetic Algorithms [2, 1], the GP population is evolved to (hopefully) produce function-trees which can perform well on the problem in question. The code used for these experiments was adapted from the SGPC code <ref> [8] </ref>, written in C. <p> Acknowledgments: Many thanks to SERC for PhD grant Number 93314680, and <ref> [8] </ref> for making their GP software publicly available.
Reference: 9. <author> Aha, D.W.: </author> <title> Tic-Tac-Toe Endgame Database. Available via The AI CD-ROM, Revision 2, 1993/1994, Network Cybernetics Corporation This article was processed using the L a T E X macro package with LLNCS style </title>
References-found: 9

