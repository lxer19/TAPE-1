URL: http://www.cs.ucsb.edu/~dsl/maya/podc/podc.ps
Refering-URL: http://www.cs.ucsb.edu/~dsl/maya/podc/podc.html
Root-URL: http://www.cs.ucsb.edu
Title: Mixed Consistency: A Model for Parallel Programming (Extended Abstract)  
Author: Divyakant Agrawal Manhoi Choy Hong Va Leong Ambuj K. Singh 
Keyword: distributed shared memory, memory consistency, concurrency, synchronization.  
Address: Santa Barbara, CA 93106  
Affiliation: Department of Computer Science University of California at Santa Barbara  
Abstract: A general purpose parallel programming model called mixed consistency is developed for distributed shared memory systems. This model combines two kinds of weak memory consistency conditions: causal memory and pipelined random access memory, and provides four kinds of explicit synchronization operations: read locks, write locks, barriers, and await operations. The resulting suite of memory and synchronization operations can be tailored to solve most programming problems in an efficient manner. Conditions are also developed under which the net effect of programming in this model is the same as programming with sequentially consistent memory. Several examples are included to illustrate the model and the correctness conditions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S.V. Adve and M.D. Hill. </author> <title> Weak ordering Anew definition. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-14. </pages> <publisher> IEEE, </publisher> <month> May </month> <year> 1990. </year>
Reference-contexts: This allows us to express concurrency within a process. Building on the general idea that synchronization in user programs should be explicitly identified, we introduce explicit primitives for modeling different kinds of synchronization patterns. The definitions of weak ordering <ref> [1, 10] </ref> and release consistency [14] iden 1 This is not to be confused with the Parallel RAM computational model [11]. tify a limited kind of synchronization by labeling mem-ory operations. Software implementations of these consistency models replace labels by lock operations [7, 18]. <p> Dubois, Scheurich, and Briggs presented the concept of weak ordering [10] in which coherence of caches is enforced only at user-defined synchronization points. This memory consistency condition was later refined by Adve and Hill <ref> [1] </ref>. The idea of identifying special synchronization points in the user program is extended further by Gharachor-loo et al. [14]. Here a synchronization access is further classified as acquire or release.
Reference: [2] <author> D. Agrawal, M. Choy, H.V. Leong, and A.K. Singh. </author> <title> Investigating weak memories using Maya. </title> <booktitle> In Proceedings of the Third International Symposium on High-Performance Distributed Computing, </booktitle> <month> August </month> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: The overhead of broadcasting messages for each update and of duplicating memory at each node may be avoided by making optimizations based on the patterns of accesses to shared variables <ref> [2] </ref>. The implementation of synchronization operations is more interesting as these operations impact the control flow as well as the correctness of a read operation. Ensuring correct control flow is not too difficult. <p> For example, some asynchronous relaxation algorithms such as Gauss-Seidel iteration converge even with PRAM. The mixed consistency model allows the programmer to choose the appropriate level of consistency needed for a given application. We have developed a platform called Maya <ref> [2] </ref> to implement and evaluate different memory consistencies. Currently, it supports a limited form of mixed consistency including the implementations of PRAM reads, causal reads, writes, and barriers. The best way to implement await and lock/unlock operations is still under consideration.
Reference: [3] <author> Mustaque Ahamad, James E. Burns, Phillip W. Hutto, and Gil Neiger. </author> <title> Causal memory. </title> <booktitle> In Proceedings of the 5th International Workshop on Distributed Algorithms, </booktitle> <pages> pages 9-30. </pages> <publisher> LNCS, </publisher> <month> October </month> <year> 1991. </year>
Reference-contexts: However, even sequential consistency imposes very strict requirements and inhibits many optimizations such as pipelined writes and out-of-order reads. Consequently, a number of weaker consistency conditions have been proposed recently. These include weak ordering [10], release consistency [14], hybrid consistency [6], causal memory <ref> [3] </ref>, and pipelined random access memory (PRAM 1 ) [23]. This paper proposes a new programming model called mixed consistency for DSM. The architecture that we target consists of a set of processors, each equipped with its own memory, interconnected by a message passing network. <p> Software implementations of these consistency models replace labels by lock operations [7, 18]. Some of the other memory models do not provide any synchronization operations <ref> [3, 6, 23] </ref>. The mixed consistency model combines two different abstractions of DSMs: PRAM and causal memory. PRAM admits efficient implementation as suggested by Lipton and Sandberg [23]. <p> The mixed consistency model combines two different abstractions of DSMs: PRAM and causal memory. PRAM admits efficient implementation as suggested by Lipton and Sandberg [23]. Causal memory on the other hand expresses the causality constraints of a user program very naturally and may simplify the design of parallel programs <ref> [3] </ref>. For programming convenience, our model also provides special synchronization primitives such as read locks, write locks, barriers, and await statements. <p> This kind of memory has a low latency; however, very little can be ensured about the consistency of the multiple copies of a shared object. For certain applications, the FIFO ordering of PRAM needs to be generalized to causal message delivery. Causal memory as defined by Ahamad et al. <ref> [3] </ref> is based on this idea. A causal order obtained from the program order and the reads-from order is defined for any history, and a read operation is constrained to return a value consistent with this causal order. <p> For simplicity, we assume that all write operations are associated with distinct values. This assumption is also made by Misra [25] and Ahamad et al. <ref> [3] </ref>. The definition of the synchronization order 7! appears in the next subsection. 3.1 Synchronization Operations The effect of a synchronization operation type s on a history is captured by its synchronization order 7! s . <p> A history in which all reads are causal reads is called a causal history and a memory system that admits only causal histories is called causal memory. In the absence of synchronization operations, this definition of causal memory reduces to that proposed by Ahamad et al. <ref> [3] </ref>. As opposed to causal memory, the operational definition of PRAM considers only pairwise interactions. This means that we only need to consider direct dependencies between processes in the formal definition. The relation obtained by removing the transitive edges from the causality relation is called PRAM order. <p> We consider applications of these results in the next section. 5 Applications In this section we illustrate the applicability of the mixed consistency model by considering some examples, mainly from scientific applications. The first two examples that we consider are iterative solution of linear equations <ref> [3] </ref> and computation of electromagnetic fields [24]. The computations in these examples can be decomposed into a set of phases so that updates made within a phase are made available in the subsequent phases. PRAM and causal reads along with barriers are used to solve these problems.
Reference: [4] <author> Mustaque Ahamad, Gil Neiger, Prince Kohli, James E. Burns, and Phillip W. Hutto. </author> <title> Causal memory: Definitions, implementation, and programming. </title> <type> Technical Report 93/55, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <month> September </month> <year> 1993. </year> <note> Submitted for Publication. </note>
Reference-contexts: Such conditions can be useful for a programmer and also for a compiler, which can exploit these conditions to speed up a computation transparently from the programmer. Similar conditions have also been investigated for release consistency [13] and causal memory <ref> [4, 28] </ref>. The discussion of these conditions appears in Section 4. In Section 5, we consider a number of examples that illustrate the applicability of our model. These include the solution of linear equations, computation of electromagnetic fields, and Cholesky factorization of sparse matrices. <p> This establishes the desired condition that h = h 3 ; o; h 4 is a sequential history. 2 The above theorem extends Singh's results for causal memory in [28] by considering synchronization operations. Ahamad et al. <ref> [4] </ref> have also developed similar conditions for causal memory with await statements and semaphore operations. Next, we discuss some consequences of the theorem. <p> The memory is maintained as a set of pages and each process keeps a local copy of the memory. Read operations are non-blocking and return local values. The implementation of write operations is similar to the implementation of causal memory <ref> [4] </ref>. Each process maintains a vector timestamp in order to define the causality between operations. The timestamp is updated after each write operation. Update messages for each variable (object) are broadcast along with the process vector timestamp to remote processes.
Reference: [5] <author> Hagit Attiya, Soma Chaudhuri, Roy Friedman, and Jennifer Welch. </author> <title> Shared memory consistency conditions for non-sequential execution: Definitions and programming strategies. </title> <booktitle> In Proceedings of the 5th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1993. </year>
Reference-contexts: Adjacent weak operations however can be observed to occur in different orders by different processes. Recently, Attiya et al. have extended hybrid consistency and a few other consistency conditions by considering the control flow in user programs and allowing non-sequential executions <ref> [5] </ref>. The mixed consistency model is similar to hybrid consistency in that it combines PRAM and causal operations instead of weak and strong operations.
Reference: [6] <author> Hagit Attiya and Roy Friedman. </author> <title> A correctness condition for high-performance multiprocessors. </title> <booktitle> In Proceedings of the 24th Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 679-690, </pages> <year> 1992. </year>
Reference-contexts: However, even sequential consistency imposes very strict requirements and inhibits many optimizations such as pipelined writes and out-of-order reads. Consequently, a number of weaker consistency conditions have been proposed recently. These include weak ordering [10], release consistency [14], hybrid consistency <ref> [6] </ref>, causal memory [3], and pipelined random access memory (PRAM 1 ) [23]. This paper proposes a new programming model called mixed consistency for DSM. The architecture that we target consists of a set of processors, each equipped with its own memory, interconnected by a message passing network. <p> Software implementations of these consistency models replace labels by lock operations [7, 18]. Some of the other memory models do not provide any synchronization operations <ref> [3, 6, 23] </ref>. The mixed consistency model combines two different abstractions of DSMs: PRAM and causal memory. PRAM admits efficient implementation as suggested by Lipton and Sandberg [23]. <p> We extend the prior work on causal memory and PRAM by combining them in the same model and by including explicit synchronization operations. Hybrid consistency, proposed by Attiya and Fried-man <ref> [6] </ref>, classifies read/write operations into strong and weak kinds. All processes observe the same ordering between a strong and a weak operation on the same process, as well as the same ordering between any pair of strong operations across different processes. <p> We could have defined PRAM reads so that they also preserve the operation orderings on account of previous causal operations. This would have been similar to the formalization of weak operations in hybrid consistency <ref> [6] </ref> and ordinary operations in release consistency [14]. However, such a definition complicates the implementation of PRAM. The memory operations in our model consist of writes, and reads that are labeled either as "PRAM" or "Causal".
Reference: [7] <author> B.N. Bershad, M.J. Zekauskas, </author> <title> and W.A. </title> <booktitle> Sawdon. The Midway distributed shared memory system. In The 38th IEEE Computer Society International Conference. IEEE, </booktitle> <year> 1993. </year>
Reference-contexts: The definitions of weak ordering [1, 10] and release consistency [14] iden 1 This is not to be confused with the Parallel RAM computational model [11]. tify a limited kind of synchronization by labeling mem-ory operations. Software implementations of these consistency models replace labels by lock operations <ref> [7, 18] </ref>. Some of the other memory models do not provide any synchronization operations [3, 6, 23]. The mixed consistency model combines two different abstractions of DSMs: PRAM and causal memory. PRAM admits efficient implementation as suggested by Lipton and Sandberg [23]. <p> The Memo system [18] develops a more efficient implementation of release consistency by delaying updates until they are needed. This is referred to as lazy release consistency . As a part of the Midway system, Bershad et al. <ref> [7] </ref> restrict release consistency by explicitly associating synchronization variables with critical sections. The resulting consistency condition is called entry consistency and can be implemented more efficiently. <p> The above definition is motivated by the definition of entry consistency by Bershad et al. <ref> [7] </ref>. Corollary 1 Any history of an entry-consistent program in which all reads of shared variables are causal is sequentially consistent. Let us call the computation between consecutive barriers a computation phase.
Reference: [8] <author> John B. Carter, John K. Bennett, and Willy Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 152-164. </pages> <publisher> ACM, </publisher> <year> 1991. </year>
Reference-contexts: Release consistency, though originally proposed for the DASH architecture [21], has also been adopted in software implementations of DSM. In these systems, explicit lock and unlock operations play the role of acquire and release instructions. The Munin system <ref> [8] </ref> classifies shared variables based on their patterns of accesses and uses this information to implement release consistency. The Memo system [18] develops a more efficient implementation of release consistency by delaying updates until they are needed. This is referred to as lazy release consistency .
Reference: [9] <author> D.E. Culler, A. Dusseau, S.C. Goldstein, A. Krishna-murthy, S. Lumetta, T. Von Eicken, and K. Yelick. </author> <title> Parallel programming in Split-C. </title> <type> Technical report, </type> <institution> Computer Science Division, University of California, Berke-ley, </institution> <year> 1993. </year>
Reference-contexts: Updates performed in a phase should be available in subsequent phases. We solve this problem by using barriers as shown in Figure 4. This program is also PRAM-consistent and therefore PRAM reads can be used while preserving correctness. This numeric solution is also discussed by Culler et al. <ref> [9] </ref> in the context of providing programming language while not done do forall E-nodes e do for each adjoining H-node h do update the value of e using h; barrier; forall H-nodes h do for each adjoining E-node e do update the value of h using e; barrier; endwhile; primitives for
Reference: [10] <author> Michael Dubois, Christoph Scheurich, and Faye A. Briggs. </author> <title> Memory access buffering in multiprocessors. </title> <booktitle> In Proceedings of the 13th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 434-442, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: However, even sequential consistency imposes very strict requirements and inhibits many optimizations such as pipelined writes and out-of-order reads. Consequently, a number of weaker consistency conditions have been proposed recently. These include weak ordering <ref> [10] </ref>, release consistency [14], hybrid consistency [6], causal memory [3], and pipelined random access memory (PRAM 1 ) [23]. This paper proposes a new programming model called mixed consistency for DSM. <p> This allows us to express concurrency within a process. Building on the general idea that synchronization in user programs should be explicitly identified, we introduce explicit primitives for modeling different kinds of synchronization patterns. The definitions of weak ordering <ref> [1, 10] </ref> and release consistency [14] iden 1 This is not to be confused with the Parallel RAM computational model [11]. tify a limited kind of synchronization by labeling mem-ory operations. Software implementations of these consistency models replace labels by lock operations [7, 18]. <p> Sequential consistency [19] weakens the requirement of atomicity by not enforcing that the ordering of non-overlapping operations be maintained in the equivalent global history. Dubois, Scheurich, and Briggs presented the concept of weak ordering <ref> [10] </ref> in which coherence of caches is enforced only at user-defined synchronization points. This memory consistency condition was later refined by Adve and Hill [1]. The idea of identifying special synchronization points in the user program is extended further by Gharachor-loo et al. [14].
Reference: [11] <author> S. Fortune and J. Wyllie. </author> <title> Parallelism in random access machines. </title> <booktitle> In Proceedings of the 10th Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 114-118, </pages> <year> 1978. </year>
Reference-contexts: The definitions of weak ordering [1, 10] and release consistency [14] iden 1 This is not to be confused with the Parallel RAM computational model <ref> [11] </ref>. tify a limited kind of synchronization by labeling mem-ory operations. Software implementations of these consistency models replace labels by lock operations [7, 18]. Some of the other memory models do not provide any synchronization operations [3, 6, 23].
Reference: [12] <author> A. George and J. Liu. </author> <title> Computer Solution of Large Sparse Positive Definite Systems. </title> <publisher> Prentice Hall, </publisher> <year> 1981. </year>
Reference-contexts: PRAM and causal reads along with barriers are used to solve these problems. The third example that we consider is Cholesky factorization of sparse matrices <ref> [12] </ref>. The computations in this case are non-uniform and cannot be decomposed easily into phases. Causal reads along with locks are used to solve this problem. 5.1 Linear Equation Solver The algorithm discussed in this section consists of a coordinator process and a set of worker processes.
Reference: [13] <author> K. Gharachorloo, S.V. Adve, A. Gupta, J.L. Hennessy, and M.D. Hill. </author> <title> Programming for different memory consistency models. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 15(4) </volume> <pages> 399-407, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Such conditions can be useful for a programmer and also for a compiler, which can exploit these conditions to speed up a computation transparently from the programmer. Similar conditions have also been investigated for release consistency <ref> [13] </ref> and causal memory [4, 28]. The discussion of these conditions appears in Section 4. In Section 5, we consider a number of examples that illustrate the applicability of our model. These include the solution of linear equations, computation of electromagnetic fields, and Cholesky factorization of sparse matrices.
Reference: [14] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J.L. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26. </pages> <publisher> IEEE, </publisher> <month> May </month> <year> 1990. </year>
Reference-contexts: However, even sequential consistency imposes very strict requirements and inhibits many optimizations such as pipelined writes and out-of-order reads. Consequently, a number of weaker consistency conditions have been proposed recently. These include weak ordering [10], release consistency <ref> [14] </ref>, hybrid consistency [6], causal memory [3], and pipelined random access memory (PRAM 1 ) [23]. This paper proposes a new programming model called mixed consistency for DSM. <p> This allows us to express concurrency within a process. Building on the general idea that synchronization in user programs should be explicitly identified, we introduce explicit primitives for modeling different kinds of synchronization patterns. The definitions of weak ordering [1, 10] and release consistency <ref> [14] </ref> iden 1 This is not to be confused with the Parallel RAM computational model [11]. tify a limited kind of synchronization by labeling mem-ory operations. Software implementations of these consistency models replace labels by lock operations [7, 18]. <p> This memory consistency condition was later refined by Adve and Hill [1]. The idea of identifying special synchronization points in the user program is extended further by Gharachor-loo et al. <ref> [14] </ref>. Here a synchronization access is further classified as acquire or release. The execution of a release operation cannot be completed before all preceding accesses have completed and the execution of any access cannot complete before all preceding acquire operations have completed. <p> We could have defined PRAM reads so that they also preserve the operation orderings on account of previous causal operations. This would have been similar to the formalization of weak operations in hybrid consistency [6] and ordinary operations in release consistency <ref> [14] </ref>. However, such a definition complicates the implementation of PRAM. The memory operations in our model consist of writes, and reads that are labeled either as "PRAM" or "Causal". We have defined the semantics of two different kinds of read operations but not discussed the write operations. <p> Therefore, it is important to characterize programs that behave similarly on weakly consistent memory and sequentially consistent memory (and, by transitivity, atomic memory). In this section, we isolate such a class of programs for our programming model. This is similar to the idea of properly-labeled programs for release consistency <ref> [14] </ref>. We begin with some definitions. Define two sequential histories h 1 and h 2 to be equiv-alent if they consist of the same set of operations and result in the same final state.
Reference: [15] <author> Phillip B. Gibbons and Michael Merritt. </author> <title> Specifying nonblocking shared memories. </title> <booktitle> In Proceedings of the 4th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 306-315, </pages> <year> 1992. </year>
Reference-contexts: To circumvent this problem, we define the mixed consistency model formally by identifying a clear interface between the user programs and the memory system. This model is presented in Section 3 and is general enough to permit non-blocking operations <ref> [15] </ref> and multi-threaded user processes. We also develop conditions under which programming in our model has the same final effect as that of using sequentially consistent memory. <p> Based on the above classification scheme, they propose a new correctness condition called release consistency that permits greater concurrency than before. More recently, Gibbons and Merritt <ref> [15] </ref> have presented a generalization of release consistency in which shared accesses are non-blocking. Release consistency, though originally proposed for the DASH architecture [21], has also been adopted in software implementations of DSM. In these systems, explicit lock and unlock operations play the role of acquire and release instructions. <p> The above formulation of the interface and the partial ordering is similar to the specification of non-blocking shared memories by Gibbons and Merritt <ref> [15] </ref>. A local history in which every invocation event has a matching response event is said to be complete. We consider only well-formed and complete local histories in the rest of this paper.
Reference: [16] <author> Maurice P. Herlihy and Jeannette M. Wing. Lineariz-ability: </author> <title> a correctness condition for concurrent objects. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 12(3) </volume> <pages> 463-492, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: It requires that even in the presence of concurrent access, the shared variables should behave as if each concurrent access occurred atomically [20, 25]. This idea was later generalized to arbitrary shared objects and termed linearizability by Her-lihy and Wing <ref> [16] </ref>. Sequential consistency [19] weakens the requirement of atomicity by not enforcing that the ordering of non-overlapping operations be maintained in the equivalent global history. Dubois, Scheurich, and Briggs presented the concept of weak ordering [10] in which coherence of caches is enforced only at user-defined synchronization points.
Reference: [17] <author> Pete Keleher, Alan L. Cox, and Willy Zwaenepoel. </author> <title> Lazy release consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21. </pages> <publisher> IEEE, </publisher> <year> 1992. </year>
Reference-contexts: An eager implementation requires that the effect of o 1 be observable (by every process) when wu (`) is executed regardless of whether rl (`) and o 2 will be performed later; a lazy implementation <ref> [17] </ref> requires that the effect of o 1 be observable when rl (`) is executed regardless of whether o 2 will be performed later; a demand-driven implementation requires that the effect of o 1 be observable when o 2 is performed. <p> Upon acquiring the lock, process p j waits for the required number of messages before proceeding. Eager and lazy implementations of lock/unlock operations are similar to the eager and lazy implementations of release consistency <ref> [17] </ref>. Both eager and lazy implementations do not take into account whether data is actually accessed subsequently. Thus, unnecessary data transmission may occur in some executions. A demand-driven implementation alleviates these problems by only ensuring the required updates to be delivered prior to subsequent accesses.
Reference: [18] <author> Pete Keleher, Sandhya Dwarkadas, Alan Cox, and Willy Zwaenepoel. </author> <title> Memo: Distributed shared memory on standard workstations and operating systems. </title> <type> Technical Report COMP TR93-206, </type> <institution> Rice University, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: The definitions of weak ordering [1, 10] and release consistency [14] iden 1 This is not to be confused with the Parallel RAM computational model [11]. tify a limited kind of synchronization by labeling mem-ory operations. Software implementations of these consistency models replace labels by lock operations <ref> [7, 18] </ref>. Some of the other memory models do not provide any synchronization operations [3, 6, 23]. The mixed consistency model combines two different abstractions of DSMs: PRAM and causal memory. PRAM admits efficient implementation as suggested by Lipton and Sandberg [23]. <p> In these systems, explicit lock and unlock operations play the role of acquire and release instructions. The Munin system [8] classifies shared variables based on their patterns of accesses and uses this information to implement release consistency. The Memo system <ref> [18] </ref> develops a more efficient implementation of release consistency by delaying updates until they are needed. This is referred to as lazy release consistency . As a part of the Midway system, Bershad et al. [7] restrict release consistency by explicitly associating synchronization variables with critical sections.
Reference: [19] <author> Leslie Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 28(9) </volume> <pages> 690-691, </pages> <month> September </month> <year> 1979. </year>
Reference-contexts: Several definitions have been put forth in the literature for relaxing this strong coupling; in general, the degree of coupling is closely related to the latency of memory access and to the cost of maintaining consistency. Sequential consistency <ref> [19] </ref> weakens atomicity by not enforcing the ordering of operations across processes. For parallel applications where the final results of a computation are the only concern, sequential consistency suffices as the net effect is the same as that of using atomic memory. <p> It requires that even in the presence of concurrent access, the shared variables should behave as if each concurrent access occurred atomically [20, 25]. This idea was later generalized to arbitrary shared objects and termed linearizability by Her-lihy and Wing [16]. Sequential consistency <ref> [19] </ref> weakens the requirement of atomicity by not enforcing that the ordering of non-overlapping operations be maintained in the equivalent global history. Dubois, Scheurich, and Briggs presented the concept of weak ordering [10] in which coherence of caches is enforced only at user-defined synchronization points.
Reference: [20] <author> Leslie Lamport. </author> <title> On interprocess communication: Parts I and II. </title> <journal> Distributed Computing, </journal> <volume> 1(2) </volume> <pages> 77-101, </pages> <year> 1986. </year>
Reference-contexts: It requires that even in the presence of concurrent access, the shared variables should behave as if each concurrent access occurred atomically <ref> [20, 25] </ref>. This idea was later generalized to arbitrary shared objects and termed linearizability by Her-lihy and Wing [16]. Sequential consistency [19] weakens the requirement of atomicity by not enforcing that the ordering of non-overlapping operations be maintained in the equivalent global history.
Reference: [21] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W. Weber, et al. </author> <title> The Stanford DASH multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Based on the above classification scheme, they propose a new correctness condition called release consistency that permits greater concurrency than before. More recently, Gibbons and Merritt [15] have presented a generalization of release consistency in which shared accesses are non-blocking. Release consistency, though originally proposed for the DASH architecture <ref> [21] </ref>, has also been adopted in software implementations of DSM. In these systems, explicit lock and unlock operations play the role of acquire and release instructions. The Munin system [8] classifies shared variables based on their patterns of accesses and uses this information to implement release consistency.
Reference: [22] <author> Kai Li and Paul Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: Sequential consistency has been widely adopted as the correctness criterion by hardware architects and implementors of distributed shared memory (DSM) <ref> [22] </ref>. However, even sequential consistency imposes very strict requirements and inhibits many optimizations such as pipelined writes and out-of-order reads. Consequently, a number of weaker consistency conditions have been proposed recently.
Reference: [23] <author> Richard J. Lipton and Jonathan S. Sandberg. </author> <title> PRAM: A scalable shared memory. </title> <type> Technical Report CS-TR-180-88, </type> <institution> Princeton University, Department of Computer Science, </institution> <month> September </month> <year> 1988. </year>
Reference-contexts: Consequently, a number of weaker consistency conditions have been proposed recently. These include weak ordering [10], release consistency [14], hybrid consistency [6], causal memory [3], and pipelined random access memory (PRAM 1 ) <ref> [23] </ref>. This paper proposes a new programming model called mixed consistency for DSM. The architecture that we target consists of a set of processors, each equipped with its own memory, interconnected by a message passing network. <p> Software implementations of these consistency models replace labels by lock operations [7, 18]. Some of the other memory models do not provide any synchronization operations <ref> [3, 6, 23] </ref>. The mixed consistency model combines two different abstractions of DSMs: PRAM and causal memory. PRAM admits efficient implementation as suggested by Lipton and Sandberg [23]. <p> Some of the other memory models do not provide any synchronization operations [3, 6, 23]. The mixed consistency model combines two different abstractions of DSMs: PRAM and causal memory. PRAM admits efficient implementation as suggested by Lipton and Sandberg <ref> [23] </ref>. Causal memory on the other hand expresses the causality constraints of a user program very naturally and may simplify the design of parallel programs [3]. For programming convenience, our model also provides special synchronization primitives such as read locks, write locks, barriers, and await statements. <p> However, we also incorporate PRAM accesses and await statements that can be used to capture the producer/consumer paradigm in an efficient manner. Pipelined random access memory (PRAM) <ref> [23] </ref>, introduced by Lipton and Sandberg, uses a fully replicated representation of shared data objects. Every read operation returns the value of the local copy. <p> A memory system is sequentially consistent if it admits only sequentially consistent histories. Though sequential consistency is the most prevalent programming model offered by hardware designers, it has stringent consistency requirements that lead to a high access latency <ref> [23] </ref>. A number of weaker consistency require ments have been proposed in the literature. As a part of the mixed consistency model, we focus on causal memory and PRAM. To define a causal read operation, we need to define the causality observable to a process p i . <p> Second, the definition can be easily generalized to maintain causality across an arbitrary group of processes; PRAM reads and causal reads form the two end points of the spectrum. Finally, in the absence of synchronization operations, this definition reduces to the original idea due to Lipton and Sandberg <ref> [23] </ref>. We could have defined PRAM reads so that they also preserve the operation orderings on account of previous causal operations. This would have been similar to the formalization of weak operations in hybrid consistency [6] and ordinary operations in release consistency [14].
Reference: [24] <author> N.K. Madsen. </author> <title> Divergence preserving discrete surface integral methods for Maxwell's curl equations using non-orthogonal unstructured grids. </title> <type> Technical Report 92.04, </type> <institution> RIACS, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: The first two examples that we consider are iterative solution of linear equations [3] and computation of electromagnetic fields <ref> [24] </ref>. The computations in these examples can be decomposed into a set of phases so that updates made within a phase are made available in the subsequent phases. PRAM and causal reads along with barriers are used to solve these problems. <p> phase [i]; x [i] := temp [i]; updated [i] := phase [i]; await updated [i] = phase [i]; endwhile; Handshaking (Causal Memory) 5.2 Computation of Electromagnetic Fields The second application we consider is the problem of computing the electric field (E-field) and the magnetic field (H-field) in a certain space <ref> [24] </ref>. E-field and H-field values are sampled at different points of this space and stored in variables called E-nodes and H-nodes respectively. Each process p i holds a partition of E-nodes and H-nodes and requires read access to adjoining nodes in neighboring partitions.
Reference: [25] <author> J. Misra. </author> <title> Axioms for memory access in asynchronous hardware systems. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 8(1) </volume> <pages> 142-153, </pages> <month> January </month> <year> 1986. </year>
Reference-contexts: It requires that even in the presence of concurrent access, the shared variables should behave as if each concurrent access occurred atomically <ref> [20, 25] </ref>. This idea was later generalized to arbitrary shared objects and termed linearizability by Her-lihy and Wing [16]. Sequential consistency [19] weakens the requirement of atomicity by not enforcing that the ordering of non-overlapping operations be maintained in the equivalent global history. <p> For simplicity, we assume that all write operations are associated with distinct values. This assumption is also made by Misra <ref> [25] </ref> and Ahamad et al. [3]. The definition of the synchronization order 7! appears in the next subsection. 3.1 Synchronization Operations The effect of a synchronization operation type s on a history is captured by its synchronization order 7! s .
Reference: [26] <author> J. Misra. </author> <title> Loosely-coupled processes. </title> <editor> In E.H.L. Aarts et.al, editor, </editor> <booktitle> Parallel Architectures and Languages Eu-rope, </booktitle> <volume> volume II, </volume> <pages> pages 1-26, </pages> <year> 1991. </year>
Reference-contexts: The above definition is similar to the idea of forward commutativity by Weihl [29], which is used to develop concurrency control protocols for abstract objects, and to the idea of loosely-coupled processes by Misra <ref> [26] </ref>. It is clear from the definition that any pair of operations on different memory objects commute. Furthermore, any pair of read operations commute and operations that are never enabled simultaneously commute.
Reference: [27] <author> Edward Eric Rothberg. </author> <title> Exploiting the Memory Hierarchy in Sequential and Parallel Sparse Cholesky Factorization. </title> <type> PhD thesis, </type> <institution> Stanford University, Department of Computer Science, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: Basically, the problem is to factorize an input symmetric sparse matrix A as a matrix product L fi L T where L is a lower triangular matrix. A symbolic factorization is first carried out to build a dependency tree of the columns <ref> [27] </ref>. A column k depends on column j if the values of column j are used to update column k. Each column j is associated with a count initialized to the number of columns on which it depends.
Reference: [28] <author> Ambuj K. Singh. </author> <title> A framework for programming using non-atomic variables. </title> <booktitle> In Proceedings of the 8th International Parallel Processing Symposium, </booktitle> <year> 1994. </year>
Reference-contexts: Such conditions can be useful for a programmer and also for a compiler, which can exploit these conditions to speed up a computation transparently from the programmer. Similar conditions have also been investigated for release consistency [13] and causal memory <ref> [4, 28] </ref>. The discussion of these conditions appears in Section 4. In Section 5, we consider a number of examples that illustrate the applicability of our model. These include the solution of linear equations, computation of electromagnetic fields, and Cholesky factorization of sparse matrices. <p> This establishes the desired condition that h = h 3 ; o; h 4 is a sequential history. 2 The above theorem extends Singh's results for causal memory in <ref> [28] </ref> by considering synchronization operations. Ahamad et al. [4] have also developed similar conditions for causal memory with await statements and semaphore operations. Next, we discuss some consequences of the theorem.
Reference: [29] <author> William E. Weihl. </author> <title> The impact of recovery on concur-rency control. </title> <booktitle> In Proceedings of the 8th ACM Annual Symposium on Principles of Database Systems, </booktitle> <pages> pages 259-269. </pages> <publisher> ACM, </publisher> <year> 1989. </year>
Reference-contexts: The above definition is similar to the idea of forward commutativity by Weihl <ref> [29] </ref>, which is used to develop concurrency control protocols for abstract objects, and to the idea of loosely-coupled processes by Misra [26]. It is clear from the definition that any pair of operations on different memory objects commute.
References-found: 29

