URL: http://www.ics.uci.edu/~pedrod/air95.ps.gz
Refering-URL: http://www.ics.uci.edu/~mlearn/MLPapers.html
Root-URL: 
Email: pedrod@ics.uci.edu  
Title: Context-Sensitive Feature Selection for Lazy Learners  
Author: Pedro Domingos 
Keyword: Lazy learning, feature selection, nearest neighbor, induction, machine learning  
Address: Irvine, California 92697, U.S.A.  
Affiliation: Department of Information and Computer Science University of California, Irvine  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. W. </author> <year> (1989). </year> <title> Incremental, Instance-Based Learning of Independent and Graded Concept Descriptions. </title> <booktitle> In Proceedings of The Sixth International Workshop on Machine Learning, </booktitle> <pages> 387-391. </pages> <address> Ithaca, NY: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: More flexible approaches employ one weight per feature value (Nosofsky, Clark & Shin, 1989; Stanfill & Waltz, 1986), one weight per feature per class <ref> (Aha, 1989) </ref>, or a combination of the two (Creecy et al, 1992), and thus exhibit a moderate degree of context sensitivity.
Reference: <author> Aha, D. W. </author> <year> (1992). </year> <title> Generalizing from Case Studies: A Case Study. </title> <booktitle> In Proceedings of The Ninth International Workshop on Machine Learning, </booktitle> <pages> 1-10. </pages> <address> Aberdeen, Scotland: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Another question is whether the feature difference estimate used effectively corresponds to the context dependency we seek to measure, and thus whether the results obtained are meaningful. These two problems were investigated by carrying out experiments in artificial domains. As argued in <ref> (Aha, 1992) </ref> and (Schaffer, 1989), more robust and general conclusions will be reached if whole classes of domains are considered, instead of the few individual cases typically used in the machine learning literature. <p> The most elaborate algorithms have in effect one weight per feature per instance, and are consequently 19 fully context-sensitive; these weights can be assigned at classification time (Atkeson, Moore & Schaal, 1996) or at learning time <ref> (Aha & Goldstone, 1992) </ref>. Seen as a 0-1 feature weighting algorithm, RC falls into this last category. 9 Concluding remarks This article introduced a new feature selection algorithm for lazy learners.
Reference: <author> Aha, D. W. & Bankert, R. L. </author> <year> (1994). </year> <title> Feature Selection for Case-Based Classification of Cloud Types: An Empirical Comparison. </title> <booktitle> In Proceedings of The 1994 AAAI Workshop on Case-Based Reasoning, </booktitle> <pages> 106-112. </pages> <address> Seattle, WA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Several algorithms have been proposed for this purpose (see (Kittler, 1986) for a survey), of which two of the most widely known are forward sequential search (FSS) and backward sequential search (BSS) (Devijver & Kittler, 1982). Many variations of these exist (e.g., <ref> (Aha & Bankert, 1994) </ref>). Their use can have a large positive impact on accuracy. However, all of these algorithms have the common characteristic that they ignore the fact that some features may be relevant only in context (i.e., given the values of other features). <p> The wrapper strategy has been found to often yield the best results <ref> (Aha & Bankert, 1994) </ref>, and this is attributable to the fact that its learning bias is that of the classifier itself, avoiding a possible mismatch between the feature selection and classification biases. <p> that RC's higher performance is indeed at least partly due do its context sensitivity, and, pending further evidence, that RC is the feature selection algorithm of choice among the three studied, when feature relevance is significantly context-dependent. 8 Related work Variations of FSS and BSS are described and evaluated in <ref> (Aha & Bankert, 1994) </ref>.
Reference: <author> Aha, D. W. & Goldstone, R. L. </author> <year> (1992). </year> <title> Concept Learning and Flexible Weighting. </title> <booktitle> In Proceedings of The Fourteenth Annual Conference of the Cognitive Science Society, </booktitle> <pages> 534-539. </pages> <address> Evanston, IL: </address> <publisher> Lawrence Erlbaum. </publisher>
Reference-contexts: Another question is whether the feature difference estimate used effectively corresponds to the context dependency we seek to measure, and thus whether the results obtained are meaningful. These two problems were investigated by carrying out experiments in artificial domains. As argued in <ref> (Aha, 1992) </ref> and (Schaffer, 1989), more robust and general conclusions will be reached if whole classes of domains are considered, instead of the few individual cases typically used in the machine learning literature. <p> The most elaborate algorithms have in effect one weight per feature per instance, and are consequently 19 fully context-sensitive; these weights can be assigned at classification time (Atkeson, Moore & Schaal, 1996) or at learning time <ref> (Aha & Goldstone, 1992) </ref>. Seen as a 0-1 feature weighting algorithm, RC falls into this last category. 9 Concluding remarks This article introduced a new feature selection algorithm for lazy learners.
Reference: <author> Aha, D. W., Kibler, D. & Albert, M. K. </author> <year> (1991). </year> <title> Instance-Based Learning Algorithms. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 37-66. </pages>
Reference-contexts: Lazy approaches, in contrast, delay this generalization until classification time; it is performed implicitly when a new example is compared to the stored instances and the class of the nearest one (s) is assigned to it. Lazy learners, also known as instance-based <ref> (Aha, 1991) </ref>, memory-based (Stanfill & Waltz, 1986), exemplar-based (Salzberg, 1991), case-based (Kolodner, 1993) and others, have several advantages when compared to eager methods like decision-tree (Quinlan, 1993) and rule induction (Clark & Niblett, 1989).
Reference: <author> Almuallim, H. & Dietterich, T. G. </author> <year> (1991). </year> <title> Learning with Many Irrelevant Features. </title> <booktitle> In Proceedings of The Ninth National Conference on Artificial Intelligence, </booktitle> <pages> 547-552. </pages> <address> Menlo Park, CA: </address> <note> AAAI Press. 21 Atkeson, </note> <author> C. G., Moore, A. W. & Schaal, S. </author> <year> (1996). </year> <title> Locally Weighted Learning. </title> <journal> Artificial Intelligence Review. </journal> <note> This issue. </note>
Reference: <author> Cain, T., Pazzani, M. J. & Silverstein, G. </author> <year> (1991). </year> <title> Using Domain Knowledge to Influence Similarity Judgments. </title> <booktitle> In Proceedings of The Case-Based Reasoning Workshop, </booktitle> <pages> 191-199. </pages> <address> Washington, DC: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Cardie, C. </author> <year> (1993). </year> <title> Using Decision Trees to Improve Case-Based Learning. </title> <booktitle> In Proceedings of The Tenth International Conference on Machine Learning, </booktitle> <pages> 25-32. </pages> <address> Amherst, MA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Caruana, R. & Freitag, D. </author> <year> (1994). </year> <title> Greedy Attribute Selection. </title> <booktitle> In Proceedings of The Eleventh International Conference on Machine Learning, </booktitle> <pages> 28-36. </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Clark, P. & Niblett, T. </author> <year> (1989). </year> <title> The CN2 Induction Algorithm. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 261-283. </pages>
Reference-contexts: Lazy learners, also known as instance-based (Aha, 1991), memory-based (Stanfill & Waltz, 1986), exemplar-based (Salzberg, 1991), case-based (Kolodner, 1993) and others, have several advantages when compared to eager methods like decision-tree (Quinlan, 1993) and rule induction <ref> (Clark & Niblett, 1989) </ref>. They are conceptually simple, and yet able to form complex decision boundaries in the instance space even when relatively little information is available. <p> In this case all paths through the tree contain the same set of features, and so the level of context sensitivity is similar to that of BSS. Decision-tree (Quinlan, 1993) and rule induction algorithms <ref> (Clark & Niblett, 1989) </ref> per se can be regarded as performing context-sensitive feature selection, and deriving most of their power from it.
Reference: <author> Cost, S. & Salzberg, S. </author> <year> (1993). </year> <title> A Weighted Nearest Neighbor Algorithm for Learning with Symbolic Features. </title> <journal> Machine Learning, </journal> <volume> 10 </volume> <pages> 57-78. </pages>
Reference-contexts: They are conceptually simple, and yet able to form complex decision boundaries in the instance space even when relatively little information is available. They combine naturally with analogical reasoning, apply easily to numeric domains, and with appropriate distance measures can also outperform other approaches in symbolic ones <ref> (Cost & Salzberg, 1993) </ref>. Special cases that may be missed by abstraction-forming approaches can be retained and recognized. Learning is often simple to perform, because it involves mainly storing the examples, possibly with some selection and indexing. Lazy learners have some shortcomings, however. <p> Thus SVDM incorporates into lazy learning some of the information used by eager and Bayesian classifiers. Versions 3 of this measure have been found to produce large improvements in accuracy compared to overlap in some symbolic domains <ref> (Cost & Salzberg, 1993) </ref>. SVDM differs from Cost and Salzberg's MVDM in that the latter also includes weights for the instances being compared.
Reference: <author> Creecy, R. H., Masand, B. M., Smith, S. J. & Waltz, D. L. </author> <year> (1992). </year> <title> Trading MIPS and Memory for Knowledge Engineering. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 48-63. </pages>
Reference-contexts: More flexible approaches employ one weight per feature value (Nosofsky, Clark & Shin, 1989; Stanfill & Waltz, 1986), one weight per feature per class (Aha, 1989), or a combination of the two <ref> (Creecy et al, 1992) </ref>, and thus exhibit a moderate degree of context sensitivity. In the case of continuous features, it is also possible to take into account the relative values of the feature in the instance and the example being classified, resulting in directional weights (Ricci & Avesani, 1995).
Reference: <author> DeGroot, M. H. </author> <year> (1986). </year> <title> Probability and Statistics, Second Edition. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: The t test is appropriate because the accuracies being compared, being means of random samples, are normally distributed by the central limit theorem <ref> (DeGroot, 1986) </ref>, and the variances are unknown and also being estimated; the one-tailed test is preferred over the two-tailed one because the goal is to determine in each case whether RC is better than the context-free algorithm, not whether the two are simply different. <p> BSS have only a probability of occurrence of 1/1000. This results in very high confidence that RC is a more accurate algorithm than FSS and BSS on the population of domains from which the 24 used are drawn. Line four shows the result of a Wilcoxon signed-ranks test <ref> (DeGroot, 1986) </ref>, a more sensitive procedure that also takes into account the relative magnitudes of the differences observed, though not their absolute values; a large difference in accuracy is considered more significant than a small one.
Reference: <author> Devijver, P. A. & Kittler, J. </author> <year> (1982). </year> <title> Pattern Recognition: A Statistical Approach. </title> <address> Engle-wood Cliffs, NJ: Prentice/Hall. </address>
Reference-contexts: Several algorithms have been proposed for this purpose (see (Kittler, 1986) for a survey), of which two of the most widely known are forward sequential search (FSS) and backward sequential search (BSS) <ref> (Devijver & Kittler, 1982) </ref>. Many variations of these exist (e.g., (Aha & Bankert, 1994)). Their use can have a large positive impact on accuracy. <p> Beyond the pattern recognition approaches surveyed in <ref> (Devijver & Kittler, 1982) </ref> and (Kittler, 1986), many methods for feature selection have been proposed in the artificial intelligence literature in recent years (Almuallim & Dietterich, 1991; Kira & Rendell, 1992; Schlimmer, 1993; Vafaie & DeJong, 1993; Caruana & Freitag, 1994; John et al., 1994; Skalak, 1994). 1 With 100 examples,
Reference: <author> Domingos, P. </author> <year> (1995). </year> <title> The RISE 2.0 System: A Case Study in Multistrategy Learning. </title> <institution> TR-95-2, Department of Information and Computer Science, University of California at Irvine, </institution> <address> Irvine, CA. </address>
Reference-contexts: In this domain, most features appear to be globally irrelevant; simply assigning all test examples to the most frequent class, ignoring all feature information, produces higher accuracy than all three algorithms (and also than decision-tree and rule learners, as was found in a separate study <ref> (Domingos, 1995) </ref>). FSS and BSS correctly discard most of the features. However, because the dataset is small (90 examples) and noisy (as evinced by the fact that it contains inconsistent examples) RC has difficulty detecting the global irrelevance of features, and retains most of them for most instances.
Reference: <author> Domingos, P. </author> <year> (1996). </year> <title> Unifying Instance-Based and Rule-Based Induction. </title> <journal> Machine Learning, </journal> <volume> 24 </volume> <pages> 141-168. </pages>
Reference-contexts: RISE is a full-fledged rule learner; starting with one rule per example, it searches for an optimal rule set, taking rule interactions during the induction process into account, allowing deletion of rules, and allowing generalization of numeric values to intervals. In <ref> (Domingos, 1996) </ref> it is compared with ID3 and CN2 on a large number of domains, and found to achieve significantly higher accuracy than either in most datasets, at the cost of increased memory usage, and with comparable running times.
Reference: <author> Holte, R. C. </author> <year> (1993). </year> <title> Very Simple Classification Rules Perform Well on Most Commonly Used Datasets. </title> <journal> Machine Learning, </journal> <volume> 11 </volume> <pages> 63-91. </pages>
Reference-contexts: BSS, and 3% vs. FSS). This is consistent with Holte's observation that, for some datasets in the UCI repository, accuracies within a small range of the best recorded values can be obtained using only the single most relevant feature <ref> (Holte, 1993) </ref>. If RC, BSS and FSS all incorporate the "best" features, then their accuracies should not be expected to differ by more than this amount. The number of features that each algorithm selects on average is also an indication of how the algorithms' behavior differs.
Reference: <author> John, G. H., Kohavi, R. & Pfleger, K. </author> <year> (1994). </year> <title> Irrelevant Features and the Subset Selection Problem. </title> <booktitle> In Proceedings of The Eleventh International Conference on Machine Learning, </booktitle> <pages> 121-129. </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: If Eval () is a heuristic measure, the feature selection algorithm acts as a filter, extracting features to be used later by the main algorithm; if it is the actual accuracy, it acts as a wrapper around that algorithm <ref> (John, Kohavi & Pfleger, 1994) </ref>. The wrapper strategy has been found to often yield the best results (Aha & Bankert, 1994), and this is attributable to the fact that its learning bias is that of the classifier itself, avoiding a possible mismatch between the feature selection and classification biases.
Reference: <author> Kelly, J. D. & Davis, L. </author> <year> (1991). </year> <title> A Hybrid Genetic Algorithm for Classification. </title> <booktitle> In Proceedings of The Twelfth International Joint Conference on Artificial Intelligence, </booktitle> <pages> 645-650. </pages> <address> Sydney: </address> <publisher> Morgan Kaufmann. 22 Kibler, </publisher> <editor> D. & Aha, D. W. </editor> <year> (1987). </year> <title> Learning Representative Exemplars of Concepts: An Initial Case Study. </title> <booktitle> In Proceedings of The Fourth International Workshop on Machine Learning, </booktitle> <pages> 24-30, </pages> <address> Irvine, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kira, A. & Rendell, L. A. </author> <year> (1992). </year> <title> A Practical Approach to Feature Selection. </title> <booktitle> In Proceedings of The Ninth International Workshop on Machine Learning, </booktitle> <pages> 249-256. </pages> <address> Aberdeen, Scotland: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kittler, J. </author> <year> (1986). </year> <title> Feature Selection and Extraction. </title> <editor> In Young, T. Y. & Fu, K. S. (eds.) </editor> <booktitle> Handbook of Pattern Recognition and Image Processing. </booktitle> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference-contexts: A natural solution to this problem is identifying the irrelevant features, and discarding them before storing the examples for future use. Several algorithms have been proposed for this purpose (see <ref> (Kittler, 1986) </ref> for a survey), of which two of the most widely known are forward sequential search (FSS) and backward sequential search (BSS) (Devijver & Kittler, 1982). Many variations of these exist (e.g., (Aha & Bankert, 1994)). Their use can have a large positive impact on accuracy. <p> Beyond the pattern recognition approaches surveyed in (Devijver & Kittler, 1982) and <ref> (Kittler, 1986) </ref>, many methods for feature selection have been proposed in the artificial intelligence literature in recent years (Almuallim & Dietterich, 1991; Kira & Rendell, 1992; Schlimmer, 1993; Vafaie & DeJong, 1993; Caruana & Freitag, 1994; John et al., 1994; Skalak, 1994). 1 With 100 examples, 20 runs with 9 values
Reference: <author> Kolodner, J. </author> <year> (1993). </year> <title> Case-Based Reasoning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Lazy learners, also known as instance-based (Aha, 1991), memory-based (Stanfill & Waltz, 1986), exemplar-based (Salzberg, 1991), case-based <ref> (Kolodner, 1993) </ref> and others, have several advantages when compared to eager methods like decision-tree (Quinlan, 1993) and rule induction (Clark & Niblett, 1989). They are conceptually simple, and yet able to form complex decision boundaries in the instance space even when relatively little information is available.
Reference: <author> Langley, P. & Sage, S. </author> <year> (1994). </year> <title> Oblivious Decision Trees and Abstract Cases. </title> <booktitle> In Proceedings of The 1994 AAAI Workshop on Case-Based Reasoning, </booktitle> <pages> 113-117. </pages> <address> Seattle, CA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Although each path through the tree represents a context-dependent set of relevant features, this information is discarded, and only the unstructured set of all the features used in the tree is passed to the lazy component. Another lazy-learner feature selection method, also based on decision trees, is described in <ref> (Langley & Sage, 1994) </ref>. In this case all paths through the tree contain the same set of features, and so the level of context sensitivity is similar to that of BSS.
Reference: <author> Lee, C. </author> <year> (1994). </year> <title> An Instance-Based Learning Method for Databases: An Information Theoretic Approach. </title> <booktitle> In Proceedings of The Ninth European Conference on Machine Learning, </booktitle> <pages> 387-390. </pages> <address> Catania, Italy: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Mohri, T. & Tanaka, H. </author> <year> (1994). </year> <title> An Optimal Weighting Criterion of Case Indexing for Both Numeric and Symbolic Attributes. </title> <booktitle> In Proceedings of The 1994 AAAI Workshop on Case-Based Reasoning, </booktitle> <pages> 123-127. </pages> <address> Seattle, WA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Murphy, P. M. </author> <year> (1995). </year> <title> UCI Repository of Machine Learning Databases. Machine-Readable Data Repository, </title> <institution> Department of Information and Computer Science, University of California at Irvine, </institution> <address> Irvine, CA. </address>
Reference-contexts: The core of the study that follows will thus be to correlate the feature difference D with the differential accuracy of RC and the context-free algorithms. An empirical study was conducted using 24 datasets from the UCI repository <ref> (Murphy, 1995) </ref>. These datasets were chosen so as to provide a wide variety of application areas, sizes, combinations of feature types, and difficulty as measured by the accuracy achieved on them by current algorithms.
Reference: <author> Niblett, T. </author> <year> (1987). </year> <title> Constructing Decision Trees in Noisy Domains. </title> <booktitle> In Proceedings of The Second European Working Session on Learning, </booktitle> <pages> 67-78. </pages> <address> Bled, Yugoslavia: Sigma. </address>
Reference-contexts: The accuracy of an instance is the fraction of the examples it won that were indeed of its class. Because this tends to over-estimate the accuracy of instances that win very few examples, the Laplace-corrected accuracy <ref> (Niblett, 1987) </ref> is used instead: LAcc (I) = N corr (I) + 1 N won (I) + C where I is any instance, C is the number of classes, N won (I) is the total number or examples won by I, and N corr (I) is the number of examples that
Reference: <author> Nosofsky, R. M., Clark, S. E. & Shin, H. J. </author> <year> (1989). </year> <title> Rules and Exemplars in Categorization, Identification, and Recognition. Journal of Experimental Psychology: Learning, Memory, </title> <journal> and Cognition, </journal> <volume> 15 </volume> <pages> 282-304. </pages>
Reference: <author> Pagallo, G. & Haussler, D. </author> <year> (1990). </year> <title> Boolean Feature Discovery in Empirical Learning. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 71-99. </pages>
Reference-contexts: Because they search in a general-to-specific direction, adding one feature at a time, when seen as feature selectors they are most similar to FSS, and indeed have similar shortcomings with regard to detecting feature interactions <ref> (Pagallo & Haussler, 1990) </ref>. It makes sense then to also view RC as a prototype specific-to-general rule induction algorithm, and expect it to have the same advantages relative to algorithms like ID3 and CN2 that BSS has with respect to FSS.
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Lazy learners, also known as instance-based (Aha, 1991), memory-based (Stanfill & Waltz, 1986), exemplar-based (Salzberg, 1991), case-based (Kolodner, 1993) and others, have several advantages when compared to eager methods like decision-tree <ref> (Quinlan, 1993) </ref> and rule induction (Clark & Niblett, 1989). They are conceptually simple, and yet able to form complex decision boundaries in the instance space even when relatively little information is available. <p> Another lazy-learner feature selection method, also based on decision trees, is described in (Langley & Sage, 1994). In this case all paths through the tree contain the same set of features, and so the level of context sensitivity is similar to that of BSS. Decision-tree <ref> (Quinlan, 1993) </ref> and rule induction algorithms (Clark & Niblett, 1989) per se can be regarded as performing context-sensitive feature selection, and deriving most of their power from it.
Reference: <author> Ricci, F. & Avesani, P. </author> <year> (1995). </year> <title> Learning a Local Similarity Metric for Case-Based Reasoning. </title> <booktitle> In Proceedings of The First International Conference on Case-Based Reasoning, </booktitle> <pages> 301-312. </pages> <address> Sesimbra, Portugal: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: In the case of continuous features, it is also possible to take into account the relative values of the feature in the instance and the example being classified, resulting in directional weights <ref> (Ricci & Avesani, 1995) </ref>. The most elaborate algorithms have in effect one weight per feature per instance, and are consequently 19 fully context-sensitive; these weights can be assigned at classification time (Atkeson, Moore & Schaal, 1996) or at learning time (Aha & Goldstone, 1992).
Reference: <author> Salzberg, S. </author> <year> (1991). </year> <title> A Nearest Hyperrectangle Learning Method. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 251-276. </pages>
Reference-contexts: Lazy learners, also known as instance-based (Aha, 1991), memory-based (Stanfill & Waltz, 1986), exemplar-based <ref> (Salzberg, 1991) </ref>, case-based (Kolodner, 1993) and others, have several advantages when compared to eager methods like decision-tree (Quinlan, 1993) and rule induction (Clark & Niblett, 1989). They are conceptually simple, and yet able to form complex decision boundaries in the instance space even when relatively little information is available.
Reference: <author> Schaffer, C. </author> <year> (1989). </year> <title> Analysis of Artificial Data Sets. </title> <booktitle> In Proceedings of The Second Inter--national Symposium on Artificial Intelligence, </booktitle> <pages> 607-617. </pages> <address> Monterrey, Mexico: </address> <publisher> McGraw-Hill. </publisher>
Reference-contexts: Another question is whether the feature difference estimate used effectively corresponds to the context dependency we seek to measure, and thus whether the results obtained are meaningful. These two problems were investigated by carrying out experiments in artificial domains. As argued in (Aha, 1992) and <ref> (Schaffer, 1989) </ref>, more robust and general conclusions will be reached if whole classes of domains are considered, instead of the few individual cases typically used in the machine learning literature.
Reference: <author> Schlimmer, J. C. </author> <year> (1993). </year> <title> Efficiently Inducing Determinations: A Complete and Systematic Search Algorithm that Uses Optimal Pruning. </title> <booktitle> In Proceedings of The Tenth International Conference on Machine Learning, </booktitle> <pages> 284-290. </pages> <address> Amherst, MA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Skalak, D. B. </author> <year> (1992). </year> <title> Representing Cases as Knowledge Sources that Apply Local Similarity Metrics. </title> <booktitle> In Proceedings of The Fourteenth Annual Conference of the Cognitive Science Society, </booktitle> <pages> 325-330. </pages> <address> Evanston, IL: </address> <publisher> Lawrence Erlbaum. </publisher>
Reference: <author> Skalak, D. B. </author> <year> (1994). </year> <title> Prototype and Feature Selection by Sampling and Random Mutation Hill Climbing Algorithms. </title> <booktitle> In Proceedings of The Eleventh International Conference on Machine Learning, </booktitle> <pages> 293-301. </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Stanfill, C. & Waltz, D. </author> <year> (1986). </year> <title> Toward Memory-Based Reasoning. </title> <journal> Communications of the ACM, </journal> <volume> 29 </volume> <pages> 1213-1228. </pages>
Reference-contexts: Lazy approaches, in contrast, delay this generalization until classification time; it is performed implicitly when a new example is compared to the stored instances and the class of the nearest one (s) is assigned to it. Lazy learners, also known as instance-based (Aha, 1991), memory-based <ref> (Stanfill & Waltz, 1986) </ref>, exemplar-based (Salzberg, 1991), case-based (Kolodner, 1993) and others, have several advantages when compared to eager methods like decision-tree (Quinlan, 1993) and rule induction (Clark & Niblett, 1989).
Reference: <author> Vafaie, H. & De Jong, K. </author> <year> (1993). </year> <title> Robust Feature Selection Algorithms. </title> <booktitle> In Proceedings of The Fifth IEEE International Conference on Tools for Artificial Intelligence, </booktitle> <pages> 356-363. </pages> <address> Boston, MA: </address> <publisher> IEEE Computer Society Press. </publisher> <pages> 24 </pages>
References-found: 38

