URL: http://www.cs.washington.edu/homes/mock/papers/generals.ps
Refering-URL: http://www.cs.washington.edu/homes/mock/papers.html
Root-URL: 
Title: Designing Intermediate Representations for Optimization PhD General Exam Report  
Author: Markus U. Mock 
Date: December 19, 1997  
Abstract: Compilers analyze programs to perform code-improving transformations. Analysis and transformation are carried out on intermediate representations of the program. The particular intermediate representation used has an important impact on what optimizations can be performed effectively and efficiently. This paper examines the design of intermediate representations as it relates to program optimization. It identifies important themes in the design of intermediate representation such as data and control dependence and the increasing importance of the representation of resource information for the generation of efficient code. Furthermore, the paper identifies design goals for an intermediate representation to support optimization for compilation at run time. 
Abstract-found: 1
Intro-found: 1
Reference: [AEBK94] <author> Wolfgang Ambrosch, M. Anton Ertl, Felix Beer, and Andreas Krall. </author> <title> Dependence-conscious global register allocation. </title> <editor> In Jurg Gutknecht, editor, </editor> <booktitle> 24 Programming Languages and System Architectures, volume 782 of Lecture Notes in Computer Science, </booktitle> <pages> pages 125-136. </pages> <publisher> Springer Verlag, </publisher> <month> March </month> <year> 1994. </year>
Reference-contexts: Finally, the graph is colored with R colors, where R is the number of available registers, so that any two adjacent nodes have different colors. Then each object is allocated to the register that has the same color as it does. The minimal interference graph (MIF) <ref> [AEBK94] </ref> combines a local (basic block) data dependence graph and the interference graph to do register allocation. Using a local dependence graph is an alternative to using webs when deciding which objects are candidates for allocation.
Reference: [AK87] <author> Randy Allen and Ken Kennedy. </author> <title> Automatic translation of Fortran programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: Parallelization and Vectorization One optimization, whose validity can be read directly from the PDG, is parallelization. Two nodes in the PDG can be executed in parallel if they are control dependent on the same (region) node and have no data dependences between them. Allen and Kennedy <ref> [AK87] </ref> describe an automatic way to transform Fortran77 programs to vector code that can be efficiently executed on vector computers. Their algorithm is based on data and control dependences. The PDG represents these dependences explicitly and jointly, so that their algorithm can be applied directly. <p> Nodes that are not contained in a cycle of both data and control dependence, can be vectorized. A derivation of that condition and details of the algorithm can be found in <ref> [AK87, ZC91] </ref>. Code Motion Code motion takes an expression (inside a loop) that yields the same result independent of the number of times the loop is executed and places it outside of the loop.
Reference: [APC + 96] <author> Joel Auslander, Matthai Philipose, Craig Chambers, Susan J. Eggers, and Brian N. Bershad. </author> <title> Fast, effective dynamic compilation. </title> <booktitle> In Proceedings of the ACM SIGPLAN '96 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 149-159, </pages> <address> Philadelphia, Pennsylvania, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: Consequently, the use of different IRs is desirable, employing a more complex representation enabling more powerful but likely more expensive optimizations for regions where they are expected to give substantial payoff, and a cheaper representation for regions where only simple optimizations are desired or possible. Initial ex 19 periences <ref> [APC + 96, GMP + 97] </ref> with dynamic compilation have indicated that dynamic register allocation is important for dynamically generated code. This was especially evident for interpreter-like applications where dynamic compilation efficiently removed interpretation overhead but failed to register-allocate data structures manipulated by the interpreter. <p> Template-based dynamic compilation schemes such as <ref> [APC + 96, CN96, GMP + 97] </ref> produce code templates at compile time that contain holes. Holes represent runtime constants that are filled in (stitched) at dynamic compilation time before executing the stitched code.
Reference: [ASU86] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers. Principles, Techniques and Tools. </booktitle> <publisher> Addison-Wesley Publishing, </publisher> <year> 1986. </year>
Reference-contexts: A basic block is a maximal sequence of instructions that can be entered only at the first of them and exited only from the last of them. Instructions are typically represented in some variant of three address code <ref> [ASU86] </ref>, where instructions are of the form x := y op z. Figure 1 shows an example C program and the corresponding CFG. The primary advantage of the CFG representation is that construction of and code generation from the CFG is easy, however, it has a number of serious shortcomings. <p> The primary advantage of the CFG representation is that construction of and code generation from the CFG is easy, however, it has a number of serious shortcomings. First of all, little information is represented explicitly: loop structure needs to be discovered, typically by computing dominator trees <ref> [ASU86] </ref>; the flow of data values needs to be computed from the CFG, e.g., by computing du-chains (cf. section 3.1) that link definitions of variables to all their uses. <p> Moreover, the commonly used bit vector representation of program properties used in iterative data flow analysis <ref> [ASU86] </ref> on the CFG is often wasteful. Sparse representations that represent, for instance, only the set of constants actually used in a program vs. all possible constants, do generally use less space and are also faster. Another problem of CFG-based analyses is that their effectiveness may depend on programmer-chosen names. <p> Natural loops are identified by searching for back edges tail ! head, i.e., edges whose head dominates their tail, and adding all nodes that can reach tail without going through head <ref> [ASU86] </ref>. IDFA then discovers other program properties by defining (for each property) a lattice and flow functions for the statements in the program and iterates over the graph until convergence [ASU86]. <p> head, i.e., edges whose head dominates their tail, and adding all nodes that can reach tail without going through head <ref> [ASU86] </ref>. IDFA then discovers other program properties by defining (for each property) a lattice and flow functions for the statements in the program and iterates over the graph until convergence [ASU86]. The second approach, called interval analysis, includes a series of methods that analyze the overall structure of the analyzed routine and that decompose it into nested regions, called intervals. The nesting structure of intervals forms a tree, called control tree.
Reference: [BEH91] <author> D. G. Bradlee, S. J. Eggers, and R. R. Henry. </author> <title> Integrated register allocation and instruction scheduling for RISCs. </title> <booktitle> In Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Operating System (ASPLOS), </booktitle> <volume> volume 26, </volume> <pages> pages 122-131, </pages> <address> New York, NY, April 1991. </address> <publisher> ACM Press. </publisher>
Reference-contexts: Conversely, if scheduling is done first, the scheduler might keep too many values in symbolic registers resulting in excessive spill costs. 9 Bradlee et al. <ref> [BEH91] </ref> have tried to assess the impact of this phase ordering problem on code quality.
Reference: [BGS95] <author> David A. Berson, Rajiv Gupta, and Mary Lou Soffa. GURRR: </author> <title> A global unified resource requirements representation. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 30(3) </volume> <pages> 23-34, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: Since there are no practical experiences with the algorithm, the impact of this interaction is hard to estimate. 6.3 Explicit Resource Representation While the representations discussed in section 6.2 combine dependence information and the interference graph to improve instruction scheduling, the global unified resource requirements representation (GURRR) <ref> [BGS95] </ref> proposed by Berson et al. merges dependence information and information about machine resources relevant to register allocation and scheduling, namely registers and functional units, directly into one unified intermediate representation. The GURRR graph is based on a PDG in SSA form.
Reference: [BMO90] <author> Robert A. Ballance, Arthur B. Maccabe, and Karl J. Ottenstein. </author> <title> The Program Dependence Web: A representation supporting control-, data-, and demand-driven interpretation of imperative languages. </title> <booktitle> In ACM SIG-PLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 257-271, </pages> <address> White Plains, N.Y., </address> <month> June </month> <year> 1990. </year>
Reference-contexts: This is because the -functions are not referentially transparent, i.e., their result depends both on the input values and on control flow since they effectively select the value of one incoming edge, which may be different each time the merge is reached during execution. Gated single assignment form (GSA) <ref> [BMO90] </ref> and thinned gated single assignment form (TGSA) [Hav93] have been proposed to overcome this defect by replacing -functions by gating functions that capture the control conditions that determine which of the definitions reaching a given -functions will supply the value. <p> It also overcomes the restriction to reducible flow graphs 5 that is present in the program dependence web (PDW) that 5 A (control) flow graph G = (N; E) is said to be reducible or well-structured if and only if E can be 7 uses GSA <ref> [BMO90] </ref>. 4 Representing Control Dependence Analyzing the flow of control in a program is important for two reasons. First, it is indispensable for a number of optimizations that depend on the discovery of certain control structures in the program, particularly loops. <p> Conditional expressions are transformed to fl-nodes, 12 which take the conditional expression as one special input. Depending on its value, one of the other input values of the fl-function is selected; it is similar to the gating functions in gated single assignment form <ref> [BMO90] </ref> and its variation thinned gated single assignment form [Hav93]. Since control dependence is replaced by data dependence on the special values true and false at fl-nodes, the resulting graph is a data-flow machine like representation of the program's computation.
Reference: [CFR + 91] <author> Ron Cytron, Jeanne Ferrante, Barry K. Rosen, Mark N. Wegman, and F. Kenneth Zadeck. </author> <title> Efficiently computing static single assignment form and the control dependence graph. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(4) </volume> <pages> 451-490, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Because exactly one definition reaches each use of a variable, many analyses and optimizations such as constant propagation, value numbering, invariant code motion and removal, strength reduction and partial redundancy elimination are simplified or rendered more effective. In addition, SSA form can be computed efficiently <ref> [CFR + 91] </ref> with a worst case time bound of O (N 2 ), where N is the number of nodes in the CFG. In contrast to its simple construction, efficient code generation from SSA form is more difficult.
Reference: [CHK86] <author> Deborah S. Coutant, Carol L. Hammond, and Jon W. Kelly. </author> <title> Compilers for the new generation of Hewlett-Packard computers. </title> <booktitle> In Digest of Papers. COMPCON Spring 86. Forty IEEE Computer Society International Conference Proceedings, </booktitle> <pages> pages 48-61, </pages> <address> San Jose, CA, USA, </address> <month> mar </month> <year> 1986. </year>
Reference-contexts: The TIL-compiler for the ML language [TMC + 96] uses several different levels of representation and successively compiles the source program to lower levels of IR. On each level particular aspects of the computation are exposed and optimized. In the HP PA-RISC compilers <ref> [CHK86, Hol97] </ref> two different IRs are used for optimization for a software engineering reason. The compilers were originally targeted for the stack-based HP 3000 processor for which the original IR language Ucode produced by the front ends was appropriate.
Reference: [CLZ86] <author> Ron Cytron, Andy Lowry, and Kenneth Zadeck. </author> <title> Code motion of control structures in high-level languages. </title> <booktitle> In Conference Record of the Thirteenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 70-85. </pages> <publisher> ACM, ACM, </publisher> <month> January </month> <year> 1986. </year>
Reference-contexts: This also requires that du-and ud-chains be updated due to the change in structure of the CFG. The PDG-based code motion algorithm instead uses the data and control dependences directly. <ref> [FOW87, CLZ86] </ref>. In addition, performing the code motion transformation is simpler in the PDG since no preheader node needs to be created, i.e., the PDG can be updated incrementally after the code motion transformation.
Reference: [CN96] <author> Charles Consel and Fran~cois Noel. </author> <title> A general approach for run-time special ization and its application to C. </title> <booktitle> In Conference Record of POPL '96: The 23 rd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, </booktitle> <pages> pages 145-156, </pages> <address> St. Petersburg Beach, Florida, </address> <month> 21-24 January </month> <year> 1996. </year>
Reference-contexts: Template-based dynamic compilation schemes such as <ref> [APC + 96, CN96, GMP + 97] </ref> produce code templates at compile time that contain holes. Holes represent runtime constants that are filled in (stitched) at dynamic compilation time before executing the stitched code.
Reference: [CP95] <author> Cliff Click and Michael Paleczny. </author> <title> A simple graph-based intermediate rep resentation. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 30(3) </volume> <pages> 35-49, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: Gated single assignment form (GSA) [BMO90] and thinned gated single assignment form (TGSA) [Hav93] have been proposed to overcome this defect by replacing -functions by gating functions that capture the control conditions that determine which of the definitions reaching a given -functions will supply the value. Click <ref> [CP95] </ref> uses these ideas to design the simple graph-based IR (SGIR), which, in contrast to SSA form, is directly executable.
Reference: [FOW87] <author> Jeanne Ferrante, Karl J. Ottenstein, and Joe D. Warren. </author> <title> The program dependence graph and its use in optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: The PST can be used to compute control dependences in time proportional to the number of edges of the CFG. This improves a previous result <ref> [FOW87] </ref> that showed how control dependences can be computed in O (N E). The main application of the PST is its use in program analysis. Similar to intervals in elimination methods, the decomposition into SESE regions speeds up program analysis. <p> An especially important practical use of the unified dependence representation is automatic program restructuring, which aims to make programs written in sequential programming languages, such as Fortran, executable on parallel or vector computers. 8 The program dependence graph (PDG) <ref> [FOW87] </ref> is one unified representation particularly designed for the parallelization and vectorization of Fortran programs. It consists of two components: the data dependence graph (DDG) and the control dependence graph (CDG). <p> This also requires that du-and ud-chains be updated due to the change in structure of the CFG. The PDG-based code motion algorithm instead uses the data and control dependences directly. <ref> [FOW87, CLZ86] </ref>. In addition, performing the code motion transformation is simpler in the PDG since no preheader node needs to be created, i.e., the PDG can be updated incrementally after the code motion transformation.
Reference: [GBF97] <author> Rajiv Gupta, David A. Berson, and Jesse Z. Fang. </author> <title> Resource-sensitive profile directed data flow analysis for code optimization. </title> <booktitle> In Proceedings of MICRO-30: 30th Annual IEEE/ACM International Symposium on Microarchitec-ture, page ??, 1997. </booktitle> <pages> 25 </pages>
Reference-contexts: GURRR can be used to assess more precisely if the code produced by transformations has the required resources available in order to improve performance, and perform transformations only when the resources are sufficient to expect an improvement in code quality. Gupta et al. <ref> [GBF97] </ref> use the same idea to do program optimizations guided both by profiles and by resource-sensitivity. Their algorithms do code sinking or hoisting only when sufficient resources are available at the locations to which instruction would get moved.
Reference: [GMP + ] <author> Brian Grant, Markus Mock, Matthai Philipose, Craig Chambers, and Susan Eggers. DyC: </author> <title> An expressive annotation-based dynamic compiler for C. </title> <note> Submitted to PLDI '98. </note>
Reference-contexts: when carried out at dynamic compile time, will rewrite instructions to use registers rather than memory locations for variables. 11 The meaning of the annotations in the interpreter program and the algorithm used to infer what values are run-time constant, and how code is generated, is described in detail in <ref> [GMP + ] </ref>. 22 instruction action r1 := load y REMOVE.y r2 := load z REMOVE.z r3 := r1 + r2 OP1.y OP2.z RESULT.x store x := r3 REMOVE.x For example, the code in figure 13 would be annotated as shown.
Reference: [GMP + 97] <author> Brian Grant, Markus Mock, Matthai Philipose, Craig Chambers, and Susan Eggers. </author> <title> Annotation-directed run-time specialization in C. In Partial Evaluation and Semantics-Based Program Manipulation, </title> <address> Amsterdam, The Neth-erlands, </address> <month> June </month> <year> 1997, </year> <pages> pages 163-178. </pages> <address> New York: </address> <publisher> ACM, </publisher> <year> 1997. </year>
Reference-contexts: Consequently, the use of different IRs is desirable, employing a more complex representation enabling more powerful but likely more expensive optimizations for regions where they are expected to give substantial payoff, and a cheaper representation for regions where only simple optimizations are desired or possible. Initial ex 19 periences <ref> [APC + 96, GMP + 97] </ref> with dynamic compilation have indicated that dynamic register allocation is important for dynamically generated code. This was especially evident for interpreter-like applications where dynamic compilation efficiently removed interpretation overhead but failed to register-allocate data structures manipulated by the interpreter. <p> Template-based dynamic compilation schemes such as <ref> [APC + 96, CN96, GMP + 97] </ref> produce code templates at compile time that contain holes. Holes represent runtime constants that are filled in (stitched) at dynamic compilation time before executing the stitched code.
Reference: [Hav93] <author> Paul Havlak. </author> <title> Construction of thinned gated single-assignment form. </title> <booktitle> In 1993 Workshop on Languages and Compilers for Parallel Computing, number 768 in Lecture Notes in Computer Science, </booktitle> <pages> pages 477-499, </pages> <address> Portland, Ore., </address> <month> August </month> <year> 1993. </year> <title> Berlin: </title> <publisher> Springer Verlag. </publisher>
Reference-contexts: Gated single assignment form (GSA) [BMO90] and thinned gated single assignment form (TGSA) <ref> [Hav93] </ref> have been proposed to overcome this defect by replacing -functions by gating functions that capture the control conditions that determine which of the definitions reaching a given -functions will supply the value. <p> Depending on its value, one of the other input values of the fl-function is selected; it is similar to the gating functions in gated single assignment form [BMO90] and its variation thinned gated single assignment form <ref> [Hav93] </ref>. Since control dependence is replaced by data dependence on the special values true and false at fl-nodes, the resulting graph is a data-flow machine like representation of the program's computation. For code generation purposes, a demand program dependence graph (dPDG) is computed that replaces the PDG's control dependence graph.
Reference: [Hol97] <author> Anne M. Holler. </author> <title> Compiler optimizations for the PA-8000. </title> <booktitle> In Digest of Papers. COMPCON Spring 97. Forty-Second IEEE Computer Society International Conference Proceedings, </booktitle> <pages> pages 87-94, </pages> <address> San Jose, CA, USA, </address> <month> feb </month> <year> 1997. </year>
Reference-contexts: The TIL-compiler for the ML language [TMC + 96] uses several different levels of representation and successively compiles the source program to lower levels of IR. On each level particular aspects of the computation are exposed and optimized. In the HP PA-RISC compilers <ref> [CHK86, Hol97] </ref> two different IRs are used for optimization for a software engineering reason. The compilers were originally targeted for the stack-based HP 3000 processor for which the original IR language Ucode produced by the front ends was appropriate.
Reference: [JP93] <author> Richard Johnson and Keshav Pingali. </author> <title> Dependence-based program analysis. </title> <booktitle> In ACM SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 78-89, </pages> <address> Albuquerque, N.M., </address> <month> June </month> <year> 1993. </year>
Reference-contexts: On the right, region nodes have been added for each control region; the edges linking basic blocks to region nodes are shown in dotted style. 5.2 Combining SSA Form and Dependence Information An alternative combination of data dependence and control dependence information is the dependence flow graph (DFG) <ref> [JP93] </ref>, which represents control by hierarchical SESE regions (cf. section 4.2) and data dependence by a modified version of SSA form.
Reference: [JPP94] <author> R. Johnson, D. Pearson, and K. Pingali. </author> <title> The program structure tree: Com puting control regions in linear time. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 29(6) </volume> <pages> 171-185, </pages> <year> 1994. </year>
Reference-contexts: Johnson et al. <ref> [JPP94] </ref> define the program structure tree (PST) that represents the program's control structure hierarchically in a tree of single-entry single-exit (SESE) regions.
Reference: [Muc97] <author> Steven S. Muchnick. </author> <title> Advanced Compiler Design & Implementation. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1997. </year>
Reference-contexts: Dead code elimination is just one example of an optimization that can be performed efficiently using du-and ud-chains <ref> [Muc97] </ref>. Dead code elimination tries to remove dead (unnecessary) instructions from the program. An instruction ins is dead if it computes only values that are not used on any executable path from ins. <p> a DAG (directed acyclic graph) in which each node can be reached from the entry node, and the edges in E b are all backedges, where a backedge is defined as an edge e = (v 1 ; v 2 ) whose head v 2 dominates its tail v 1 <ref> [Muc97] </ref>. 6 Loop inversion transforms a while loop into a repeat loop, i.e., it moves the loop-closing test before the body of the loop to the end of the body. <p> Then, the complement of this graph is computed by removing all 10 Simply the s i could be selected, however, a better choice are names (also called webs in <ref> [Muc97] </ref>), which yield better allocation. Names or webs are maximal unions of intersecting du-chains and offer the advantage, that the same variable used for different purposes in a routine (a typical example would be i) can be allocated to different registers. <p> Using the s i directly would allocate the different uses of i to the same register, which is overly restrictive. Details about the use of webs can be found in <ref> [Muc97] </ref>. 15 edges and adding edges between those nodes, that are not presently connected by edges. The resulting graph represents false dependences between the vertices, since nodes connected by an edge can be scheduled together, those that are not connected by an edge, cannot be scheduled together.
Reference: [OOH + 95] <author> Kevin O'Brien, Kathryn M. O'Brien, Martin Hopkins, Arvin Shepherd, and Ron Unrau. XIL and YIL: </author> <title> The intermediate languages of TOBEY. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 30(3) </volume> <pages> 71-82, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: In practice, there is no single best answer to all of these questions and IRs vary widely in their design choices. For instance, the IBM XL compilers <ref> [OOH + 95] </ref> use a fairly low-level IR, called XIL, to do all but array-oriented and loop optimizations.
Reference: [Pin93] <author> S. S. Pinter. </author> <title> Register allocation with instruction scheduling: a new ap proach. </title> <booktitle> In ACM, editor, Proceedings of the ACM SIGPLAN '93 Conference on Programming Language Design and Implementation: </booktitle> <address> Albuquerque, New Mexico, </address> <month> June 23-25, </month> <year> 1993, </year> <journal> volume 28(6) of ACM SIGPLAN Notices, </journal> <pages> pages 248-257, </pages> <address> New York, NY, USA, June 1993. </address> <publisher> ACM Press. </publisher>
Reference-contexts: Dependence-conscious coloring using the MIF, which is simply the interference graph computed on the basis of data dependences (instead of webs or all the symbolic registers), reduced antidependences in the experiments performed by Ambrosch et al., however, no data on actual improvement in program performance is reported. Pinter <ref> [Pin93] </ref> presents the parallel interference graph (PIG) as a unified IR to achieve integrated register allocation and instruction scheduling.
Reference: [SGL96] <author> Vugranam C. Sreedhar, Guang R. Gao, and Yong-Fong Lee. </author> <title> A new frame work for exhaustive and incremental data flow analysis using DJ graphs. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 31(5) </volume> <pages> 278-290, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Johnson et al. call such a reduced graph the quick propagation graph (QPG), which is typically much smaller than the original CFG and hence faster to analyze. Sreedhar et al. <ref> [SGL96] </ref> present a variation of data flow analysis by elimination based on DJ-graphs. In DJ-graphs the flow graph edges are classified into edges that are also edges in the dominator tree (D-edges) and other edges, called J (oin)-edges.
Reference: [TMC + 96] <author> D. Tarditi, G. Morrisett, P. Cheng, C. Stone, R. Harper, and P. Lee. </author> <title> TIL: A type-directed optimizing compiler for ML. </title> <booktitle> In Proceedings of the ACM SIGPLAN '96 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 181-192, </pages> <address> Philadelphia, Pennsylvania, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: For these, XIL is transformed into YIL, in which array operations and loop statements that are present in the source code are represented explicitly to perform loop optimizations such as unrolling and other transformations to exploit locality of reference. The TIL-compiler for the ML language <ref> [TMC + 96] </ref> uses several different levels of representation and successively compiles the source program to lower levels of IR. On each level particular aspects of the computation are exposed and optimized.
Reference: [Wal86] <author> D. W. Wall. </author> <title> Global register allocation at link time. </title> <booktitle> In SIGPlan '86 Sym posium on Compiler Construction, </booktitle> <pages> pages 264-275, </pages> <address> Palo Alto, CA, </address> <month> June </month> <year> 1986. </year> <institution> Association for Computing Machinery, SIGPlan. </institution>
Reference-contexts: Obviously, allocating the symbolic registers r1, r2 and r31 used in the interpreted program to real registers, would make the code much more efficient. 7.2.2 Register Actions Wall <ref> [Wal86] </ref> presents an approach for doing cheap interprocedural register allocation at link time.
Reference: [WCES94] <author> Daniel Weise, Roger F. Crew, Michael Ernst, and Bjarne Steensgaard. </author> <title> Value dependence graphs: Representation without taxation. </title> <booktitle> In Conference Record of POPL '94: 21st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, </booktitle> <pages> pages 297-310, </pages> <address> Portland, Oregon, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: Furthermore, the decomposition into regions lends itself to program restructuring optimizations designed to expose parallelism. 5.3 Control as Data Dependence The value dependence graph (VDG) <ref> [WCES94] </ref> is another modification of the PDG designed to simplify program analysis and transformation. Like the program structure tree or the data flow graph it decomposes the original CFG into SESE regions. Control flow is converted to data flow in two steps.
Reference: [ZC91] <author> Hans Zima and Barbara Chapman. </author> <title> Supercompilers for parallel and vector computers. </title> <publisher> ACM Press, Addison-Wesley, </publisher> <year> 1991. </year> <month> 26 </month>
Reference-contexts: Before we show how some IRs try to capture it more directly we'll first briefly review the different kind of dependences we find in programs. A thorough treatment and applications to program parallelization and vectorization can be found in <ref> [ZC91] </ref>. Data dependence is a constraint that arises from the flow of data between statements in a program. Consider the example in figure 3: statement S4 depends on statement S3 and cannot be executed before it, since it uses the value of d which is set in statement S3. <p> Nodes that are not contained in a cycle of both data and control dependence, can be vectorized. A derivation of that condition and details of the algorithm can be found in <ref> [AK87, ZC91] </ref>. Code Motion Code motion takes an expression (inside a loop) that yields the same result independent of the number of times the loop is executed and places it outside of the loop.
References-found: 28

