URL: http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume2/cohen95a.ps.Z
Refering-URL: http://www.cs.washington.edu/research/jair/abstracts/cohen95a.html
Root-URL: 
Email: wcohen@research.att.com  
Title: Pac-Learning Recursive Logic Programs: Efficient Algorithms  
Author: William W. Cohen 
Address: 600 Mountain Avenue, Murray Hill, NJ 07974 USA  
Affiliation: AT&T Bell Laboratories  
Note: Journal of Artificial Intelligence Research 2 (1995) 501-539 Submitted 9/94; published 5/95  
Abstract: We present algorithms that learn certain classes of function-free recursive logic programs in polynomial time from equivalence queries. In particular, we show that a single k-ary recursive constant-depth determinate clause is learnable. Two-clause programs consisting of one learnable recursive clause and one constant-depth determinate non-recursive clause are also learnable, if an additional "basecase" oracle is assumed. These results immediately imply the pac-learnability of these classes. Although these classes of learnable recursive programs are very constrained, it is shown in a companion paper that they are maximally general, in that generalizing either class in any natural way leads to a compu-tationally difficult learning problem. Thus, taken together with its companion paper, this paper establishes a boundary of efficient learnability for recursive logic programs.
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D., Lapointe, S., Ling, C. X., & Matwin, S. </author> <year> (1994). </year> <title> Inverting implication with small training sets. In Machine Learning: </title> <institution> ECML-94 Catania, Italy. </institution> <note> Springer-Verlag. Lecture Notes in Computer Science # 784. </note>
Reference-contexts: Forced simulation is a simple and analytically tractable alternative to other methods for generalizing recursive programs against examples, such as n-th root finding (Muggleton, 1994), sub-unification <ref> (Aha, Lapointe, Ling, & Matwin, 1994) </ref> and recursive anti-unification (Idestam-Almquist, 1993), but it has been only rarely used in experimental ILP systems (Ling, 1991). The paper is organized as follows. <p> However, while both of these techniques have obvious applications in learning, both are extremely expensive in the worst case. The CRUSTACEAN system <ref> (Aha et al., 1994) </ref> uses inverting implication in constrained settings to learn certain restricted classes of recursive programs. The class of programs efficiently learned by this system is not formally well-understood, but it appears to be similar to the classes analyzed by Frazier and Page.
Reference: <author> Angluin, D. </author> <year> (1988). </year> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2 (4). </volume>
Reference-contexts: The largest learnable class we identify that requires extra "hints" is the class of constant-depth determinate programs consisting of a single nonrecursive base clause and a single recursive clause from the class described above. All of our results are proved in the model of identification from equivalence queries <ref> (Angluin, 1988, 1989) </ref>, which is somewhat stronger than pac-learnability. Identification from equivalence queries requires that the target concept be exactly identified, in polynomial time, and using only a polynomial number of equivalence queries. <p> We will first review the necessary definitions for a standard learning model, the model of learning from equivalence queries <ref> (Angluin, 1988, 1989) </ref>, and discuss its relationship to other learning models. We will then introduce an extension to this model which is necessary for analyzing ILP problems. 2.3.1 Identification From Equivalence Queries Let X be a set. We will call X the domain, and call the elements of X instances. <p> bounded by poly (n t ; n e ), where n t = jjCjj and n e is the size of the largest counterexample seen so far, or 0 if no equivalence queries have been made. 2.3.2 Relation to Pac-Learnability The model of identification from equivalence queries has been well-studied <ref> (Angluin, 1988, 1989) </ref>.
Reference: <author> Angluin, D. </author> <year> (1989). </year> <title> Equivalence queries and approximate fingerprints. </title> <booktitle> In Proceedings of the 1989 Workshop on Computational Learning Theory Santa Cruz, </booktitle> <address> California. </address>
Reference: <author> Bergadano, F., & Gunetti, D. </author> <year> (1993). </year> <title> An interactive system to learn functional logic programs. </title> <booktitle> In Proceedings of the 13th International Joint Conference on Artificial Intelligence Chambery, France. 536 Pac-Learning Recursive Logic Programs: Efficient Algorithms Biermann, A. </booktitle> <year> (1978). </year> <title> The inference of regular lisp programs from examples. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 8 (8). </volume>
Reference: <author> Cohen, W. W. </author> <year> (1993a). </year> <title> A pac-learning algorithm for a restricted class of recursive logic programs. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence Washington, </booktitle> <address> D.C. </address>
Reference-contexts: A final prior paper which may be of interest presents some experimental results with a Prolog implementation of a variant of the Force2 algorithm <ref> (Cohen, 1993a) </ref>. This paper shows that forced simulation can be the basis of a learning program that outperforms state-of-the art heuristic methods such as FOIL (Quin-lan, 1990; Quinlan & Cameron-Jones, 1993) in learning from randomly chosen examples. 7. <p> We have also shown the soundness and efficiency of several instances of generalization by forced simulation. This method may have applications in practical learning systems. The Force2 algorithm compares quite well experimentally with modern ILP systems on learning problems from the restricted class that it can identify <ref> (Cohen, 1993a) </ref>; thus sound learning methods like Force2 might be useful as a filter before a more general ILP system like FOIL (Quinlan, 1990; Quinlan & Cameron-Jones, 1993). Alternatively, forced simulation could be used in heuristic programs.
Reference: <author> Cohen, W. W. </author> <year> (1993b). </year> <title> Pac-learning non-recursive Prolog clauses. </title> <note> To appear in Artificial Intelligence. </note>
Reference-contexts: Finally, we wish to direct the reader to several pieces of our own research that are relevant. As noted above, a companion paper exists which presents negative learnability results for several natural generalizations of the language kd-MaxRecLang (Cohen, 1995). Another related paper investigates the learnability of non-recursive Prolog programs <ref> (Cohen, 1993b) </ref>; this paper also contains a number of negative results which strongly motivate the restriction of constant-depth determinacy. A final prior paper which may be of interest presents some experimental results with a Prolog implementation of a variant of the Force2 algorithm (Cohen, 1993a).
Reference: <author> Cohen, W. W. </author> <year> (1993c). </year> <title> Rapid prototyping of ILP systems using explicit bias. </title> <booktitle> In Proceedings of the 1993 IJCAI Workshop on Inductive Logic Programming Chambery, </booktitle> <address> France. </address>
Reference: <author> Cohen, W. W. </author> <year> (1994). </year> <title> Pac-learning nondeterminate clauses. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence Seattle, </booktitle> <address> WA. </address>
Reference: <author> Cohen, W. W. </author> <year> (1995). </year> <title> Pac-learning recursive logic programs: negative results. </title> <journal> Journal of AI Research, </journal> <volume> 2, </volume> <pages> 541-573. </pages>
Reference-contexts: We then discuss related work and conclude. Although the learnable class of programs is large enough to include some well-known automatic logic programming benchmarks, it is extremely restricted. In a companion paper <ref> (Cohen, 1995) </ref>, we provide a number of negative results, showing that relaxing any of these restrictions leads to difficult learning problems: in particular, learning problems that are either as hard as learning DNF (an open problem in computational learning theory), or as hard as cracking certain presumably secure cryptographic schemes. <p> It is natural to ask if it is possible to learn a complete recursive program by simultaneously learning both a recursive clause, and its associated nonrecursive base case. In general, this is not possible, as is demonstrated elsewhere <ref> (Cohen, 1995) </ref>. However, there are several cases in which the positive result can be extended to two-clause programs. 5. <p> Theorem 8 Let d-Depth-2-Clause be the set of 2-clause programs consisting of one clause in d-DepthLinRec and one clause in d-DepthNonRec. For any constants a and d the language family d-Depth-2-Clause [DB; aDetDEC] is uniformly identifiable from equivalence and basecase queries. Proof: Omitted. A companion paper <ref> (Cohen, 1995) </ref> shows that something like the basecase oracle is necessary: in particular, without any "hints" about the base clause, learning a two-clause linear recursive program is as hard as learning boolean DNF. However, there are several situations in which the basecase oracle can be dispensed with. Case 1. <p> For example, one might generalize the language to allow an arbitrary number of recursive clauses, or to include clauses that are not determinate. These generalizations might very well be pac-learnable|given the results that we have presented so far. However, a companion paper <ref> (Cohen, 1995) </ref> presents a series of negative results showing that most natural generalizations of kd-MaxRecLang are not efficiently learnable, and further that kd-MaxRecLang itself is not efficiently learnable without the basecase oracle. <p> This system cannot, however, make use of background knowledge. Finally, we wish to direct the reader to several pieces of our own research that are relevant. As noted above, a companion paper exists which presents negative learnability results for several natural generalizations of the language kd-MaxRecLang <ref> (Cohen, 1995) </ref>. Another related paper investigates the learnability of non-recursive Prolog programs (Cohen, 1993b); this paper also contains a number of negative results which strongly motivate the restriction of constant-depth determinacy. <p> Alternatively, forced simulation could be used in heuristic programs. For 533 Cohen example, although forced simulation for programs with many recursive clauses is nondeterministic and hence potentially inefficient, one could introduce heuristics that would make the forced simulation efficient, at the cost of completeness. A companion paper <ref> (Cohen, 1995) </ref> shows that the positive results of this paper are not likely to be improved: either eliminating the basecase oracle for the language above or learning two recursive clauses simultaneously is as hard as learning DNF, and learning n linear recursive determinate clauses, one n-ary recursive determinate clause, or one
Reference: <author> De Raedt, L., & Bruynooghe, M. </author> <year> (1992). </year> <title> Interactive concept-learning and constructive induction by analogy. </title> <journal> Machine Learning, </journal> <volume> 8 (2). </volume>
Reference-contexts: The idea of learning by repeated generalization is an old one; in particular, previous methods exist for learning a definite clause by generalizing a highly-specific one. For example, CLINT <ref> (De Raedt & Bruynooghe, 1992) </ref> generalizes a "starting clause" guided by queries made to the user; PROGOL (Srinivasan, Muggleton, King, & Sternberg, 1994) guides a top-down generalization process with a known bottom clause; and Rouveirol (1994) describes a method for generalizing bottom clauses created by saturation.
Reference: <author> De Raedt, L., & Dzeroski, S. </author> <year> (1994). </year> <title> First-order jk-clausal theories are PAC-learnable. </title>
Reference: <editor> In Wrobel, S. (Ed.), </editor> <booktitle> Proceedings of the Fourth International Workshop on Inductive Logic Programming Bad Honnef/Bonn, </booktitle> <address> Germany. </address>
Reference: <author> De Raedt, L., Lavrac, N., & Dzeroski, S. </author> <year> (1993). </year> <title> Multiple predicate learning. </title> <booktitle> In Proceedings of the Third International Workshop on Inductive Logic Programming Bled, </booktitle> <pages> Slovenia. </pages>
Reference-contexts: Forced simulation is a simple and analytically tractable alternative to other methods for generalizing recursive programs against examples, such as n-th root finding (Muggleton, 1994), sub-unification (Aha, Lapointe, Ling, & Matwin, 1994) and recursive anti-unification <ref> (Idestam-Almquist, 1993) </ref>, but it has been only rarely used in experimental ILP systems (Ling, 1991). The paper is organized as follows. After presenting some preliminary definitions, we begin by presenting (primarily for pedagogical reasons) a procedure for identifying from equivalence queries a single non-recursive constant-depth determinate clause.
Reference: <author> Dzeroski, S., Muggleton, S., & Russell, S. </author> <year> (1992). </year> <title> Pac-learnability of determinate logic programs. </title> <booktitle> In Proceedings of the 1992 Workshop on Computational Learning Theory Pittsburgh, </booktitle> <address> Pennsylvania. </address>
Reference-contexts: By way of an introduction to this technique, we will consider a learning algorithm for non-recursive constant-depth clauses. While this result is presented primarily for pedagogical reasons, it may be of interest on its own: it is independent of previous proofs of the pac-learnability of this class <ref> (Dzeroski et al., 1992) </ref>, and it is also somewhat more rigorous than previous proofs. Although the details and analysis of the algorithm for non-recursive clauses are somewhat involved, the basic idea behind the algorithm is quite simple. <p> This is a natural extension of the Force1 NR learning algorithm to recursive clauses|in fact an algorithm based on similar ideas has been been previously conjectured to pac-learn closed recursive constant-depth determinate clauses <ref> (Dzeroski et al., 1992) </ref>. Unfortunately, this algorithm can fail to return a clause that is consistent with a positive counterexample. To illustrate this, consider the following example. Example. Consider using the extension of Force1 NR described above to learn following target program: append (Xs,Ys,Zs) 2. <p> Previous results also exist on the pac-learnability of nonrecursive constant-depth determinate programs, and on the pac-learnability of recursive constant-depth determinate programs in a model that also allows membership and subset queries <ref> (Dzeroski et al., 1992) </ref>. The basis for the intelligent search used in our learning algorithms is the technique of forced simulation. This method finds the least implicant of a clause C that covers an extended instance e.
Reference: <author> Frazier, M., & Page, C. D. </author> <year> (1993a). </year> <title> Learnability in inductive logic programming: Some basic results and techniques. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence Washington, </booktitle> <address> D.C. </address>
Reference: <author> Frazier, M., & Page, C. D. </author> <year> (1993b). </year> <title> Learnability of recursive, non-determinate theories: Some basic results and techniques. </title> <booktitle> In Proceedings of the Third International Workshop on Inductive Logic Programming Bled, </booktitle> <pages> Slovenia. </pages>
Reference: <author> Gold, M. </author> <year> (1967). </year> <title> Language identification in the limit. </title> <journal> Information and Control, </journal> <volume> 10. </volume>
Reference-contexts: If the requirement of efficiency is relaxed far enough, very general positive results can be obtained using very simple learning algorithms. For example, in model of learnability in the limit <ref> (Gold, 1967) </ref>, any language that is both recursively enumerable and decidable (which includes all of Datalog) can be learned by a simple enumeration procedure; in the model of U-learnability (Muggleton & Page, 1994) any language that is polynomially enumerable and polynomially decidable can be learned by enumeration.
Reference: <author> Hirsh, H. </author> <year> (1992). </year> <title> Polynomial-time learning with version spaces. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence San Jose, </booktitle> <address> California. </address> <publisher> MIT Press. </publisher>
Reference-contexts: This set of clauses could then be used to tractably encode the version space of all consistent programs, using the [S; N ] representation for version spaces <ref> (Hirsh, 1992) </ref>. 5. Extending the Learning Algorithm We will now consider a number of ways in which the result of Theorem 5 can be extended. 5.1 The Equality-Predicate and Unique-Mode Assumptions Theorem 5 shows that the language family d-DepthLinRec [DB = ; aDetDEC =1 ] is identifiable from equivalence queries.
Reference: <author> Idestam-Almquist, P. </author> <year> (1993). </year> <title> Generalization under implication by recursive anti-unification. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning Amherst, </booktitle> <address> Massachusetts. </address> <publisher> Morgan Kaufmann. </publisher> <editor> 537 Cohen King, R. D., Muggleton, S., Lewis, R. A., & Sternberg, M. J. E. </editor> <year> (1992). </year> <title> Drug design by machine learning: the use of inductive logic programming to model the structure-activity relationships of trimethoprim analogues binding to dihydrofolate reductase. </title> <booktitle> Proceedings of the National Academy of Science, </booktitle> <pages> 89. </pages>
Reference: <author> Lavrac, N., & Dzeroski, S. </author> <year> (1992). </year> <title> Background knowledge and declarative bias in inductive concept learning. </title> <editor> In Jantke, K. P. (Ed.), </editor> <title> Analogical and Inductive Inference: </title> <booktitle> International Workshop AII'92. </booktitle> <publisher> Springer Verlag, </publisher> <address> Daghstuhl Castle, Germany. </address> <booktitle> Lectures in Artificial Intelligence Series #642. </booktitle>
Reference: <author> Ling, C. </author> <year> (1991). </year> <title> Inventing necessary theoretical terms in scientific discovery and inductive logic programming. </title> <type> Tech. rep. 301, </type> <institution> University of Western Ontario. </institution>
Reference-contexts: Forced simulation is a simple and analytically tractable alternative to other methods for generalizing recursive programs against examples, such as n-th root finding (Muggleton, 1994), sub-unification (Aha, Lapointe, Ling, & Matwin, 1994) and recursive anti-unification (Idestam-Almquist, 1993), but it has been only rarely used in experimental ILP systems <ref> (Ling, 1991) </ref>. The paper is organized as follows. After presenting some preliminary definitions, we begin by presenting (primarily for pedagogical reasons) a procedure for identifying from equivalence queries a single non-recursive constant-depth determinate clause.
Reference: <author> Ling, C. </author> <year> (1992). </year> <title> Logic program synthesis from good examples. In Inductive Logic Programming. </title> <publisher> Academic Press. </publisher>
Reference: <author> Lloyd, J. W. </author> <year> (1987). </year> <title> Foundations of Logic Programming: Second Edition. </title> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Background In this section we will present the technical background necessary to state our results. We will assume, however, that the reader is familiar with the basic elements of logic programming; readers without this background are referred to one of the standard texts, for example <ref> (Lloyd, 1987) </ref>. 2.1 Logic Programs Our treatment of logic programs is standard, except that we will usually consider the body of a clause to be an ordered set of literals.
Reference: <author> Muggleton, S. </author> <year> (1994). </year> <note> Inverting implication. To appear in Artificial Intelligence. </note>
Reference-contexts: Interestingly, the learning algorithms analyzed are different from most existing ILP learning methods; they all employ an unusual method of generalizing examples called forced simulation. Forced simulation is a simple and analytically tractable alternative to other methods for generalizing recursive programs against examples, such as n-th root finding <ref> (Muggleton, 1994) </ref>, sub-unification (Aha, Lapointe, Ling, & Matwin, 1994) and recursive anti-unification (Idestam-Almquist, 1993), but it has been only rarely used in experimental ILP systems (Ling, 1991). The paper is organized as follows. <p> The idea of learning by repeated generalization is an old one; in particular, previous methods exist for learning a definite clause by generalizing a highly-specific one. For example, CLINT (De Raedt & Bruynooghe, 1992) generalizes a "starting clause" guided by queries made to the user; PROGOL <ref> (Srinivasan, Muggleton, King, & Sternberg, 1994) </ref> guides a top-down generalization process with a known bottom clause; and Rouveirol (1994) describes a method for generalizing bottom clauses created by saturation. <p> For example, in model of learnability in the limit (Gold, 1967), any language that is both recursively enumerable and decidable (which includes all of Datalog) can be learned by a simple enumeration procedure; in the model of U-learnability <ref> (Muggleton & Page, 1994) </ref> any language that is polynomially enumerable and polynomially decidable can be learned by enumeration. The most similar previous work is that of Frazier and Page (1993a, 1993b). They analyze the learnability from equivalence queries of recursive programs with function symbols but without background knowledge.
Reference: <author> Muggleton, S., & De Raedt, L. </author> <year> (1994). </year> <title> Inductive logic programming: Theory and methods. </title> <journal> Journal of Logic Programming, </journal> <volume> 19/20 (7), </volume> <pages> 629-679. </pages>
Reference-contexts: Interestingly, the learning algorithms analyzed are different from most existing ILP learning methods; they all employ an unusual method of generalizing examples called forced simulation. Forced simulation is a simple and analytically tractable alternative to other methods for generalizing recursive programs against examples, such as n-th root finding <ref> (Muggleton, 1994) </ref>, sub-unification (Aha, Lapointe, Ling, & Matwin, 1994) and recursive anti-unification (Idestam-Almquist, 1993), but it has been only rarely used in experimental ILP systems (Ling, 1991). The paper is organized as follows. <p> The idea of learning by repeated generalization is an old one; in particular, previous methods exist for learning a definite clause by generalizing a highly-specific one. For example, CLINT (De Raedt & Bruynooghe, 1992) generalizes a "starting clause" guided by queries made to the user; PROGOL <ref> (Srinivasan, Muggleton, King, & Sternberg, 1994) </ref> guides a top-down generalization process with a known bottom clause; and Rouveirol (1994) describes a method for generalizing bottom clauses created by saturation. <p> For example, in model of learnability in the limit (Gold, 1967), any language that is both recursively enumerable and decidable (which includes all of Datalog) can be learned by a simple enumeration procedure; in the model of U-learnability <ref> (Muggleton & Page, 1994) </ref> any language that is polynomially enumerable and polynomially decidable can be learned by enumeration. The most similar previous work is that of Frazier and Page (1993a, 1993b). They analyze the learnability from equivalence queries of recursive programs with function symbols but without background knowledge.
Reference: <author> Muggleton, S., & Feng, C. </author> <year> (1992). </year> <title> Efficient induction of logic programs. In Inductive Logic Programming. </title> <publisher> Academic Press. </publisher>
Reference-contexts: Datalog is relatively easy to analyze. There is a close connection between Datalog and the restrictions imposed by certain practical learning systems, such FOIL (Quinlan, 1990; Quinlan & Cameron-Jones, 1993), FOCL (Pazzani & Kibler, 1992), and GOLEM <ref> (Muggleton & Feng, 1992) </ref>. Finally, using extended instances addresses the following technical problem. The learning problems considered in this paper involve restricted classes of logic programs. <p> The following program, which computes C , is determinate and of depth two. choose (A,B,C) zero (B) ^ one (C). choose (A,B,C) decrement (B,D) ^ decrement (A,E) ^ 505 Cohen multiply (B,C,G) ^ divide (G,A,F) ^ choose (E,D,F). The program GOLEM <ref> (Muggleton & Feng, 1992) </ref> learns constant-depth determinate programs, and related restrictions have been adopted by several other practical learning systems (Quinlan, 1991; Lavrac & Dzeroski, 1992; Cohen, 1993c).
Reference: <author> Muggleton, S., King, R. D., & Sternberg, M. J. E. </author> <year> (1992). </year> <title> Protein secondary structure prediction using logic-based machine learning. </title> <journal> Protein Engineering, </journal> <volume> 5 (7), </volume> <pages> 647-657. </pages>
Reference-contexts: Datalog is relatively easy to analyze. There is a close connection between Datalog and the restrictions imposed by certain practical learning systems, such FOIL (Quinlan, 1990; Quinlan & Cameron-Jones, 1993), FOCL (Pazzani & Kibler, 1992), and GOLEM <ref> (Muggleton & Feng, 1992) </ref>. Finally, using extended instances addresses the following technical problem. The learning problems considered in this paper involve restricted classes of logic programs. <p> The following program, which computes C , is determinate and of depth two. choose (A,B,C) zero (B) ^ one (C). choose (A,B,C) decrement (B,D) ^ decrement (A,E) ^ 505 Cohen multiply (B,C,G) ^ divide (G,A,F) ^ choose (E,D,F). The program GOLEM <ref> (Muggleton & Feng, 1992) </ref> learns constant-depth determinate programs, and related restrictions have been adopted by several other practical learning systems (Quinlan, 1991; Lavrac & Dzeroski, 1992; Cohen, 1993c).
Reference: <author> Muggleton, S., & Page, C. D. </author> <year> (1994). </year> <title> A learnability model for universal representations. </title>
Reference-contexts: Interestingly, the learning algorithms analyzed are different from most existing ILP learning methods; they all employ an unusual method of generalizing examples called forced simulation. Forced simulation is a simple and analytically tractable alternative to other methods for generalizing recursive programs against examples, such as n-th root finding <ref> (Muggleton, 1994) </ref>, sub-unification (Aha, Lapointe, Ling, & Matwin, 1994) and recursive anti-unification (Idestam-Almquist, 1993), but it has been only rarely used in experimental ILP systems (Ling, 1991). The paper is organized as follows. <p> The idea of learning by repeated generalization is an old one; in particular, previous methods exist for learning a definite clause by generalizing a highly-specific one. For example, CLINT (De Raedt & Bruynooghe, 1992) generalizes a "starting clause" guided by queries made to the user; PROGOL <ref> (Srinivasan, Muggleton, King, & Sternberg, 1994) </ref> guides a top-down generalization process with a known bottom clause; and Rouveirol (1994) describes a method for generalizing bottom clauses created by saturation. <p> For example, in model of learnability in the limit (Gold, 1967), any language that is both recursively enumerable and decidable (which includes all of Datalog) can be learned by a simple enumeration procedure; in the model of U-learnability <ref> (Muggleton & Page, 1994) </ref> any language that is polynomially enumerable and polynomially decidable can be learned by enumeration. The most similar previous work is that of Frazier and Page (1993a, 1993b). They analyze the learnability from equivalence queries of recursive programs with function symbols but without background knowledge.
Reference: <editor> In Wrobel, S. (Ed.), </editor> <booktitle> Proceedings of the Fourth International Workshop on Inductive Logic Programming Bad Honnef/Bonn, </booktitle> <address> Germany. </address>
Reference: <author> Muggleton, S. H. (Ed.). </author> <year> (1992). </year> <title> Inductive Logic Programming. </title> <publisher> Academic Press. </publisher>
Reference-contexts: Datalog is relatively easy to analyze. There is a close connection between Datalog and the restrictions imposed by certain practical learning systems, such FOIL (Quinlan, 1990; Quinlan & Cameron-Jones, 1993), FOCL (Pazzani & Kibler, 1992), and GOLEM <ref> (Muggleton & Feng, 1992) </ref>. Finally, using extended instances addresses the following technical problem. The learning problems considered in this paper involve restricted classes of logic programs. <p> The following program, which computes C , is determinate and of depth two. choose (A,B,C) zero (B) ^ one (C). choose (A,B,C) decrement (B,D) ^ decrement (A,E) ^ 505 Cohen multiply (B,C,G) ^ divide (G,A,F) ^ choose (E,D,F). The program GOLEM <ref> (Muggleton & Feng, 1992) </ref> learns constant-depth determinate programs, and related restrictions have been adopted by several other practical learning systems (Quinlan, 1991; Lavrac & Dzeroski, 1992; Cohen, 1993c).
Reference: <author> Nienhuys-Cheng, S., & Polman, M. </author> <year> (1994). </year> <title> Sample pac-learnability in model inference. In Machine Learning: </title> <institution> ECML-94 Catania, Italy. </institution> <note> Springer-Verlag. Lecture notes in Computer Science # 784. </note>
Reference: <author> Page, C. D., & Frisch, A. M. </author> <year> (1992). </year> <title> Generalization and learnability: A study of constrained atoms. In Inductive Logic Programming. </title> <publisher> Academic Press. </publisher>
Reference: <author> Pazzani, M., & Kibler, D. </author> <year> (1992). </year> <title> The utility of knowledge in inductive learning. </title> <journal> Machine Learning, </journal> <volume> 9 (1). </volume>
Reference-contexts: Datalog is relatively easy to analyze. There is a close connection between Datalog and the restrictions imposed by certain practical learning systems, such FOIL (Quinlan, 1990; Quinlan & Cameron-Jones, 1993), FOCL <ref> (Pazzani & Kibler, 1992) </ref>, and GOLEM (Muggleton & Feng, 1992). Finally, using extended instances addresses the following technical problem. The learning problems considered in this paper involve restricted classes of logic programs.
Reference: <author> Quinlan, J. R., & Cameron-Jones, R. M. </author> <year> (1993). </year> <title> FOIL: A midterm report. </title> <editor> In Brazdil, P. B. (Ed.), </editor> <booktitle> Machine Learning: </booktitle> <address> ECML-93 Vienna, Austria. </address> <note> Springer-Verlag. Lecture notes in Computer Science # 667. </note>
Reference: <author> Quinlan, J. R. </author> <year> (1990). </year> <title> Learning logical definitions from relations. Machine Learning, 5 (3). 538 Pac-Learning Recursive Logic Programs: Efficient Algorithms Quinlan, </title> <editor> J. R. </editor> <booktitle> (1991). Determinate literals in inductive logic programming. In Proceedings of the Eighth International Workshop on Machine Learning Ithaca, </booktitle> <address> New York. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Rouveirol, C. </author> <year> (1994). </year> <title> Flattening and saturation: two representation changes for generalization. </title> <journal> Machine Learning, </journal> <volume> 14 (2). </volume>
Reference-contexts: Specifically, this oracle would provide for any fact f a polynomial upper bound on the depth of the proof for f in the target program. Finally we note that if efficient (but non-ground) background knowledge is allowed, then function symbols always can be removed via flattening <ref> (Rouveirol, 1994) </ref>. This transformation also preserves determinacy, although it may increase depth|in general, the depth of a flattened clause depends also on term depth in the original clause.
Reference: <author> Shapiro, E. </author> <year> (1982). </year> <title> Algorithmic Program Debugging. </title> <publisher> MIT Press. </publisher>
Reference: <author> Srinivasan, A., Muggleton, S. H., King, R. D., & Sternberg, M. J. E. </author> <year> (1994). </year> <title> Mutagenesis: ILP experiments in a non-determinate biological domain. </title> <editor> In Wrobel, S. (Ed.), </editor> <booktitle> Proceedings of the Fourth International Workshop on Inductive Logic Programming Bad Honnef/Bonn, </booktitle> <address> Germany. </address>
Reference-contexts: The idea of learning by repeated generalization is an old one; in particular, previous methods exist for learning a definite clause by generalizing a highly-specific one. For example, CLINT (De Raedt & Bruynooghe, 1992) generalizes a "starting clause" guided by queries made to the user; PROGOL <ref> (Srinivasan, Muggleton, King, & Sternberg, 1994) </ref> guides a top-down generalization process with a known bottom clause; and Rouveirol (1994) describes a method for generalizing bottom clauses created by saturation.
Reference: <author> Stahl, I., Tausend, B., & Wirth, R. </author> <year> (1993). </year> <title> Two methods for improving inductive logic programming. </title> <booktitle> In Proceedings of the 1993 European Conference on Machine Learning Vienna, </booktitle> <address> Austria. </address>
Reference-contexts: Thus Case 3 above seems likely to cover a large fraction of the automatic logic programming programs of practical interest. We also note that heuristics have been proposed for finding such basecase decision rules automatically using typing restrictions <ref> (Stahl, Tausend, & Wirth, 1993) </ref>. 5.5 Combining the Results Finally, we note that all of the extensions described above are compatible.
Reference: <author> Summers, P. D. </author> <year> (1977). </year> <title> A methodology for LISP program construction from examples. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 24 (1), </volume> <pages> 161-175. </pages>
Reference: <author> Valiant, L. G. </author> <year> (1984). </year> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27 (11). </volume>
Reference: <author> Zelle, J. M., & Mooney, R. J. </author> <year> (1994). </year> <title> Inducing deterministic Prolog parsers from treebanks: a machine learning approach. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence Seattle, </booktitle> <address> Washington. </address> <publisher> MIT Press. </publisher> <pages> 539 </pages>
References-found: 42

