URL: http://www.cs.helsinki.fi/~tirri/isis96.ps.gz
Refering-URL: http://www.cs.helsinki.fi/~tirri/publications.html
Root-URL: 
Title: Comparing Bayesian Model Class Selection Criteria by Discrete Finite Mixtures  
Author: Petri T. Kontkanen Petri J. Myllymaki Henry R. Tirri 
Keyword: Bayesian model selection, Finite mixture models, BIC, AIC, K-means algorithm. Area of Interest: Bayesian Methodology, Concept Formation and Classification.  
Note: FIN-00014 University of Helsinki, Finland  Pp. 364-374 in Information, Statistics and Induction in Science (Proceedings of the ISIS'96 Conference in Melbourne, Australia, August 1996), edited by D.L.Dowe, K.B.Korb, and J.J.Oliver. World Scientific, Singapore 1996.  
Address: P.O.Box  Science  
Affiliation: Complex Systems Computation Group (CoSCo) ph: +358-9-708-44173  Department of Computer  
Email: Petri.Kontkanen@cs.Helsinki.FI  Petri.Myllymaki@cs.Helsinki.FI  Henry.Tirri@cs.Helsinki.FI  
Phone: 26,  fax: +358-9-708-44213  
Web: URL: http://www.cs.Helsinki.FI/research/cosco/  
Abstract: We investigate the problem of computing the posterior probability of a model class, given a data sample and a prior distribution for possible parameter settings. By a model class we mean a group of models which all share the same parametric form. In general this posterior may be very hard to compute for high-dimensional parameter spaces, which is usually the case with real-world applications. In the literature several methods for computing the posterior approximately have been proposed, but the quality of the approximations may depend heavily on the size of the available data sample. In this work we are interested in testing how well the approximative methods perform in real-world problem domains. In order to conduct such a study, we have chosen the model family of finite mixture distributions. With certain assumptions, we are able to derive the model class posterior analytically for this model family. We report a series of model class selection experiments on real-world data sets, where the true posterior and the approximations are compared. The empirical results support the hypothesis that the approximative techniques can provide good estimates of the true posterior, especially when the sample size grows large. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Akaike. </author> <title> Information theory and an extension of the maximum likelihood principle. In B.N. </title> <editor> Petrox and F. Caski, editors, </editor> <booktitle> Proceedings of the Second International Symposium on Information Theory, </booktitle> <pages> pages 267-281, </pages> <address> Budapest, </address> <year> 1973. </year> <note> Akademiai Kiado. </note>
Reference-contexts: In the experimental part of this study (Section 5), we compute the complete data evidence integral for different model classes by using the derived analytical formula, and compare the results to the results obtained by Bayesian Information Criterion (BIC) [23] and Akaike Information Criterion (AIC) <ref> [1] </ref>. It should be noted that we do not suggest using the complete data evidence for practical model selection tasks | this integral is used here only as a tool for evaluating the quality of the BIC and AIC approximations. <p> Here we are interested in investigating empirically how well such approximations perform. For this study we have chosen two well known approximation methods: the Bayesian information criterion (BIC) [23], also known as the Schwarz criterion, and the Akaike information criterion (AIC) <ref> [1] </ref>. The BIC criterion can also been given a formulation in the MDL setting, as showed in [21]. <p> The AIC approximation is even simpler: log P (D) log P (Dj ^ fi) d: For justifications of these approximations, see e.g. <ref> [11, 23, 1] </ref>. When evaluating the quality of the approximation methods described above, we are faced with a dilemma if the correct value of the evidence can not be computed in feasible time, as is the case with natural datasets used in our tests.
Reference: [2] <author> H. Bozdogan. </author> <title> On the information-based measure of covariance complexity and its applications to the evaluation of mul-tivariate linear models. </title> <booktitle> Communica 372 373 tions in Statistics Theory and Methods, </booktitle> <volume> 19(1) </volume> <pages> 221-278, </pages> <year> 1990. </year>
Reference-contexts: In the literature there exist several methods for computing the evidence approximately (see e.g., <ref> [24, 21, 11, 3, 2] </ref>), but the quality of these approximations depend heavily on the number of data samples available. In this work we are interested in the performance of the approximative methods for computing the evidence in real-world problem domains.
Reference: [3] <author> P. Cheeseman and J. Stutz. </author> <title> Bayesian classification (AutoClass): Theory and results. </title> <editor> In U. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, chapter 6. </booktitle> <publisher> AAAI Press, </publisher> <address> Menlo Park, </address> <year> 1996. </year>
Reference-contexts: In the literature there exist several methods for computing the evidence approximately (see e.g., <ref> [24, 21, 11, 3, 2] </ref>), but the quality of these approximations depend heavily on the number of data samples available. In this work we are interested in the performance of the approximative methods for computing the evidence in real-world problem domains. <p> Interestingly, a third evidence approximation, the Cheeseman-Stutz approximation used in the AutoClass system <ref> [3] </ref>, suggests a useful framework for evaluating the approximations: the incomplete data evidence (1) can be approximated by using the complete data evidence P (D; Z), P (D) P (D; Z j ^ fi) 368 369 where Z denotes the conditional expectations of the missing data, given the observed data and
Reference: [4] <author> D.M. Chickering and D. Heckerman. </author> <title> Efficient approximations for the marginal likelihood of incomplete data given a Bayesian network. </title> <type> Technical Report MSR-TR-96-08, </type> <institution> Microsoft Research, Advanced Technology Division, </institution> <year> 1996. </year>
Reference-contexts: Consequently, we are faced with a serious practical dilemma as the exact evaluation of the approximation methods is impossible in practice since the correct value of the evidence can not be computed in feasible time. As in <ref> [13, 4] </ref>, the above problem can be circumvented by using synthetic data, in which case the correct value is "implicitly known", as we can control the number of mixing distributions used for generating data.
Reference: [5] <author> A.P. Dempster, N.M. Laird, and D.B. Rubin. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> 39(1) </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: In [14] it is shown how the MAP parameters for a given finite mixture model class can be estimated from sufficient statistics of the unknown latent variable by using the well-known Expectation-Maximization (EM) algorithm <ref> [5] </ref>. However, in this study we are not only interested in computing the MAP parameter values, but need also a good approximation of the actual missing values of the latent variable in order to determine the integral to be used for our tests. <p> In Section 4 we discuss how to obtain good estimates of Z and ^ fi with a fast variant of the Expectation-Maximization (EM) algorithm <ref> [5] </ref>. In order to determine the complete data evidence P (D; Z), P (D; Z) = P (D; Z j fi)P (fi) dfi;(3) we need to assign a prior distribution to the parameters. <p> Consequently, some clustering algorithm has to be used. One of the most well-known clustering algorithms is the K-means algorithm [6], which is a simple variant of the more general Expectation-Maximization (EM) algorithm <ref> [5] </ref>. It is an iterative procedure, which finds a locally optimal clustering. The intuitive idea behind K-means is very simple. It initially chooses a random clustering, and then estimates the parameters of the optimality criterion assuming the current clustering to be the correct one.
Reference: [6] <author> R.O. Duda and P.E. Hart. </author> <title> Pattern classification and scene analysis. </title> <publisher> John Wiley, </publisher> <year> 1973. </year>
Reference-contexts: In Section 4 we show how to use the K-means algorithm <ref> [6] </ref>, a variant of EM, for this purpose. <p> The number of all possible clusterings is exponential (K N ), thus an exhaustive search for determining this clustering is computationally infeasible. Consequently, some clustering algorithm has to be used. One of the most well-known clustering algorithms is the K-means algorithm <ref> [6] </ref>, which is a simple variant of the more general Expectation-Maximization (EM) algorithm [5]. It is an iterative procedure, which finds a locally optimal clustering. The intuitive idea behind K-means is very simple.
Reference: [7] <author> B.S. </author> <title> Everitt and D.J. Hand. Finite Mixture Distributions. </title> <publisher> Chapman and Hall, </publisher> <address> London, </address> <year> 1981. </year>
Reference-contexts: In this work we are interested in the performance of the approximative methods for computing the evidence in real-world problem domains. As the model family for this study, we have chosen the family of finite mixture distributions <ref> [7, 26] </ref> with multivalued discrete variables. The main reason for choosing this family is due to the fact that, with certain assumptions, we are able to provide an analytical solution for computing the evidence, as derived in Section 3. <p> Thus the instantiation space is divided into K local regions called clusters, each of which consists of the data vectors generated by the corresponding mechanism. An appropriate statistical model for the case is a finite mixture <ref> [7, 26] </ref>.
Reference: [8] <author> A. Gelman, J. Carlin, H. Stern, and D. Rubin. </author> <title> Bayesian Data Analysis. </title> <publisher> Chapman & Hall, </publisher> <year> 1995. </year>
Reference-contexts: 1 Introduction In our work we are motivated by the need to construct computational models for decision support applications. For this model construction process we adopt the Bayesian approach (see e.g., <ref> [8] </ref>), which offers a solid theoretical framework for combining the statistical model learning approach with the knowledge acquisition approach.
Reference: [9] <author> M. Gyllenberg, T. Koski, and M. Ver-laan. </author> <title> Classification of binary vectors by stochastic complexity. </title> <type> Technical Report A5, </type> <institution> University of Turku, Institute for Applied Mathematics, </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: The analytical method for computing the evidence of finite mixture classes can be seen as an extension of the work in <ref> [9] </ref>, where a similar result was derived for finite mixtures with only binary variables and uniform prior distributions for the parameters. In our case, the variables can be multivalued, and the prior distributions are assumed to come from the family of Dirichlet distributions.
Reference: [10] <author> D. Heckerman, D. Geiger, </author> <title> and D.M. Chickering. Learning Bayesian networks: The combination of knowledge and statistical data. </title> <journal> Machine Learning, </journal> <volume> 20(3) </volume> <pages> 197-243, </pages> <month> September </month> <year> 1995. </year>
Reference-contexts: In our case, the variables can be multivalued, and the prior distributions are assumed to come from the family of Dirichlet distributions. This result for multi-valued finite mixtures was derived independently from <ref> [10] </ref>, where a similar result is given for a more general family of distributions expressed as Bayesian networks [19, 18].
Reference: [11] <author> R.E. Kass and A.E. Raftery. </author> <title> Bayes factors. </title> <type> Technical Report 254, </type> <institution> Department of Statistics, University of Wash-ington, </institution> <year> 1994. </year>
Reference-contexts: In the literature there exist several methods for computing the evidence approximately (see e.g., <ref> [24, 21, 11, 3, 2] </ref>), but the quality of these approximations depend heavily on the number of data samples available. In this work we are interested in the performance of the approximative methods for computing the evidence in real-world problem domains. <p> The AIC approximation is even simpler: log P (D) log P (Dj ^ fi) d: For justifications of these approximations, see e.g. <ref> [11, 23, 1] </ref>. When evaluating the quality of the approximation methods described above, we are faced with a dilemma if the correct value of the evidence can not be computed in feasible time, as is the case with natural datasets used in our tests.
Reference: [12] <author> H. Kitano. </author> <title> Challenges of massive parallelism. </title> <booktitle> In Proc. of IJCAI-93, the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 813-834, </pages> <address> Chambery, France, August 1993. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: K is usually small compared to the sample size N , and thus the prediction computation can be performed very efficiently 1 . The finite mixture model also conforms interestingly to the intuitively appealing memory-based reasoning paradigm (see e.g., <ref> [12] </ref>), and it can be seen to offer a Bayesian solution to the important case matching and adaptation problems in such domains [25].
Reference: [13] <author> P. Kontkanen, P. Myllymaki, and H. Tirri. </author> <title> Unsupervised Bayesian learning of discrete finite mixtures. </title> <type> Manuscript, </type> <note> submitted for publication. </note>
Reference-contexts: Consequently, we are faced with a serious practical dilemma as the exact evaluation of the approximation methods is impossible in practice since the correct value of the evidence can not be computed in feasible time. As in <ref> [13, 4] </ref>, the above problem can be circumvented by using synthetic data, in which case the correct value is "implicitly known", as we can control the number of mixing distributions used for generating data.
Reference: [14] <author> P. Kontkanen, P. Myllymaki, and H. Tirri. </author> <title> Constructing Bayesian finite mixture models by the EM algorithm. </title> <type> Technical Report C-1996-9, </type> <institution> University of Helsinki, Department of Computer Science, </institution> <month> February </month> <year> 1996. </year>
Reference-contexts: In this work we use the complete data integral which is closest to the incomplete data integral, corresponding to the single latent variable value assignment with the highest probability. In <ref> [14] </ref> it is shown how the MAP parameters for a given finite mixture model class can be estimated from sufficient statistics of the unknown latent variable by using the well-known Expectation-Maximization (EM) algorithm [5].
Reference: [15] <author> P. Myllymaki and H. Tirri. </author> <title> Bayesian case-based reasoning with neural networks. </title> <booktitle> In Proceedings of the IEEE International Conference on Neural Networks, </booktitle> <volume> volume 1, </volume> <pages> pages 422-427, </pages> <address> San Francisco, March 1993. </address> <publisher> IEEE, </publisher> <address> Piscataway, NJ. </address>
Reference-contexts: number of clusters can safely be assumed to be bounded by N , since otherwise the sample size is clearly too small for the learning problem in question. 1 If massively parallel hardware is available, the computations can be made even faster since the algorithms can be parallelized quite easily <ref> [16, 15] </ref>.
Reference: [16] <author> P. Myllymaki and H. Tirri. </author> <title> Massively parallel case-based reasoning with probabilistic similarity metrics. </title> <editor> In S. Wess, K.-D. Althoff, and M Richter, editors, </editor> <booktitle> Topics in Case-Based Reasoning, volume 837 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 144-154. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: number of clusters can safely be assumed to be bounded by N , since otherwise the sample size is clearly too small for the learning problem in question. 1 If massively parallel hardware is available, the computations can be made even faster since the algorithms can be parallelized quite easily <ref> [16, 15] </ref>.
Reference: [17] <author> P. Myllymaki and H. Tirri. </author> <title> Constructing computationally efficient Bayesian models via unsupervised clustering. </title> <editor> In A.Gammerman, editor, </editor> <booktitle> Probabilistic Reasoning and Bayesian Belief Networks, </booktitle> <pages> pages 237-248. </pages> <publisher> Alfred Waller Publishers, </publisher> <address> Suffolk, </address> <year> 1995. </year>
Reference-contexts: Consequently, it should be observed that as finite mixture models can be regarded as a special case of Bayesian networks (see e.g. <ref> [17] </ref>), the formula for computing the evidence can also be obtained by adapting the corresponding general result for Bayesian networks.
Reference: [18] <author> R.E. </author> <title> Neapolitan. Probabilistic Reasoning in Expert Systems. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, NY, </address> <year> 1990. </year> <pages> 373 374 </pages>
Reference-contexts: This result for multi-valued finite mixtures was derived independently from [10], where a similar result is given for a more general family of distributions expressed as Bayesian networks <ref> [19, 18] </ref>. Consequently, it should be observed that as finite mixture models can be regarded as a special case of Bayesian networks (see e.g. [17]), the formula for computing the evidence can also be obtained by adapting the corresponding general result for Bayesian networks.
Reference: [19] <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1988. </year>
Reference-contexts: This result for multi-valued finite mixtures was derived independently from [10], where a similar result is given for a more general family of distributions expressed as Bayesian networks <ref> [19, 18] </ref>. Consequently, it should be observed that as finite mixture models can be regarded as a special case of Bayesian networks (see e.g. [17]), the formula for computing the evidence can also be obtained by adapting the corresponding general result for Bayesian networks.
Reference: [20] <author> A. Raftery. </author> <title> Approximate Bayes factors and accounting for model uncertainty in generalized linear models. </title> <type> Technical Report 255, </type> <institution> Department of Statistics, University of Washington, </institution> <year> 1993. </year>
Reference-contexts: The recent successful empirical results [25] with public domain classification problems clearly demonstrate that discrete finite mixtures can provide superior prediction performance with respect to many commonly used model families including decision trees and neural networks. In most earlier similar studies (see e.g., <ref> [22, 20] </ref>), the model family used has either been too restricted for extending the results to real-world domains, or too general to allow an exact solution to be used for the comparisons.
Reference: [21] <author> J. Rissanen. </author> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> World Scientific Publishing Company, </publisher> <address> New Jersey, </address> <year> 1989. </year>
Reference-contexts: Assuming uniform priors for the model classes, this probability is proportional to an integral where the integration is performed over all possible parameters settings. This integral, usually called the evidence, or equivalently the stochastic complexity (defined as minus logarithm of the evidence) in the information theoretical approaches (see e.g., <ref> [21] </ref>), may be very hard to compute in the case of real world applications, where the parameter space is high-dimensional. <p> In the literature there exist several methods for computing the evidence approximately (see e.g., <ref> [24, 21, 11, 3, 2] </ref>), but the quality of these approximations depend heavily on the number of data samples available. In this work we are interested in the performance of the approximative methods for computing the evidence in real-world problem domains. <p> For this study we have chosen two well known approximation methods: the Bayesian information criterion (BIC) [23], also known as the Schwarz criterion, and the Akaike information criterion (AIC) [1]. The BIC criterion can also been given a formulation in the MDL setting, as showed in <ref> [21] </ref>. The BIC approximation is derived by expanding the logarithm of the integrand of the evidence (1) around the posterior mode ^ fi, which gives log P (D) log P (Dj ^ fi) 2 where d is the number of parameters.
Reference: [22] <author> S. Rosenkranz. </author> <title> The Bayes factors for model evaluation in hierarchical Poisson model for area counts. </title> <type> PhD thesis, </type> <institution> Department of Biostatistics, University of Washington, </institution> <year> 1992. </year>
Reference-contexts: The recent successful empirical results [25] with public domain classification problems clearly demonstrate that discrete finite mixtures can provide superior prediction performance with respect to many commonly used model families including decision trees and neural networks. In most earlier similar studies (see e.g., <ref> [22, 20] </ref>), the model family used has either been too restricted for extending the results to real-world domains, or too general to allow an exact solution to be used for the comparisons.
Reference: [23] <author> G. Schwarz. </author> <title> Estimating the dimension of a model. </title> <journal> Annals of Statistics, </journal> <volume> 6 </volume> <pages> 461-464, </pages> <year> 1978. </year>
Reference-contexts: In the experimental part of this study (Section 5), we compute the complete data evidence integral for different model classes by using the derived analytical formula, and compare the results to the results obtained by Bayesian Information Criterion (BIC) <ref> [23] </ref> and Akaike Information Criterion (AIC) [1]. It should be noted that we do not suggest using the complete data evidence for practical model selection tasks | this integral is used here only as a tool for evaluating the quality of the BIC and AIC approximations. <p> Solving the evidence formula (1) analytically is intractable in practice, but several methods exist for computing approximations of the evidence. Here we are interested in investigating empirically how well such approximations perform. For this study we have chosen two well known approximation methods: the Bayesian information criterion (BIC) <ref> [23] </ref>, also known as the Schwarz criterion, and the Akaike information criterion (AIC) [1]. The BIC criterion can also been given a formulation in the MDL setting, as showed in [21]. <p> The AIC approximation is even simpler: log P (D) log P (Dj ^ fi) d: For justifications of these approximations, see e.g. <ref> [11, 23, 1] </ref>. When evaluating the quality of the approximation methods described above, we are faced with a dilemma if the correct value of the evidence can not be computed in feasible time, as is the case with natural datasets used in our tests.
Reference: [24] <author> L. Tierney and J. Kadane. </author> <title> Accurate approximations for posterior moments and marginal densities. </title> <journal> J. Amer. Statist. Ass., </journal> <volume> 81 </volume> <pages> 82-86, </pages> <year> 1986. </year>
Reference-contexts: In the literature there exist several methods for computing the evidence approximately (see e.g., <ref> [24, 21, 11, 3, 2] </ref>), but the quality of these approximations depend heavily on the number of data samples available. In this work we are interested in the performance of the approximative methods for computing the evidence in real-world problem domains.
Reference: [25] <author> H. Tirri, P. Kontkanen, and P. Myl-lymaki. </author> <title> Probabilistic instance-based learning. </title> <editor> In L. Saitta, editor, </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference (to appear). </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1996. </year>
Reference-contexts: However, it should be noted that although an analytical form for the evidence can be found in this case, this does not indicate that finite mixtures are unrealistically simple models. The recent successful empirical results <ref> [25] </ref> with public domain classification problems clearly demonstrate that discrete finite mixtures can provide superior prediction performance with respect to many commonly used model families including decision trees and neural networks. <p> It should be noted that we do not suggest using the complete data evidence for practical model selection tasks | this integral is used here only as a tool for evaluating the quality of the BIC and AIC approximations. The datasets used here are the same as in <ref> [25] </ref>, where the predictive power of finite mixture models was empirically evaluated and com 366 367 pared to the performance of alternative models and model construction techniques. <p> The finite mixture model also conforms interestingly to the intuitively appealing memory-based reasoning paradigm (see e.g., [12]), and it can be seen to offer a Bayesian solution to the important case matching and adaptation problems in such domains <ref> [25] </ref>. A recent empirical study [25] shows that finite mixture models perform very well in real-world classification problems. 3 A Bayesian criterion for determining the model class In the Bayesian framework, the optimal number of mixing distributions (clusters) can be determined by evaluating the posterior probability for each model class M <p> The finite mixture model also conforms interestingly to the intuitively appealing memory-based reasoning paradigm (see e.g., [12]), and it can be seen to offer a Bayesian solution to the important case matching and adaptation problems in such domains <ref> [25] </ref>. A recent empirical study [25] shows that finite mixture models perform very well in real-world classification problems. 3 A Bayesian criterion for determining the model class In the Bayesian framework, the optimal number of mixing distributions (clusters) can be determined by evaluating the posterior probability for each model class M K given the data: P <p> Iterate until convergence. The global maximum can be found stochastic ally, i.e., by starting K-means sufficiently many times from different starting points. 5 Empirical results In our experiments, we used five public domain classification datasets 2 , which were all included in the recent empirical study <ref> [25] </ref>.
Reference: [26] <author> D.M. Titterington, A.F.M. Smith, and U.E. Makov. </author> <title> Statistical Analysis of Finite Mixture Distributions. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: In this work we are interested in the performance of the approximative methods for computing the evidence in real-world problem domains. As the model family for this study, we have chosen the family of finite mixture distributions <ref> [7, 26] </ref> with multivalued discrete variables. The main reason for choosing this family is due to the fact that, with certain assumptions, we are able to provide an analytical solution for computing the evidence, as derived in Section 3. <p> Thus the instantiation space is divided into K local regions called clusters, each of which consists of the data vectors generated by the corresponding mechanism. An appropriate statistical model for the case is a finite mixture <ref> [7, 26] </ref>.
Reference: [27] <author> C.S. Wallace and P.R. Freeman. </author> <title> Estimation and inference by compact coding. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> 49(3) </volume> <pages> 240-265, </pages> <year> 1987. </year>
Reference-contexts: It should be noted that using two phases in model construction is in contrast to the Minimum Message Length (MML) approach <ref> [27] </ref>, which is based on a one-phase process for finding a single, maximum a posterior probability model. In the approach described above, the goodness of a model class can be determined by evaluating the posterior probability of the given class.
Reference: [28] <author> S.S Wilks. </author> <title> Mathematical statistics. </title> <publisher> John Wiley, </publisher> <year> 1962. </year> <month> 374 </month>
Reference-contexts: and ki appear dis-jointly in (4), we can decompose the integ ral (3) into a product of integrals and thus we get Z K Y ff k dff k=1 i=1 l=1 f kil + kil 1 These integrals have the form of Dirichlet integrals with a well-known solution (see e.g., <ref> [28] </ref>, p.178), so (5) equals C k=1 (h k + k ) P K k=1 i=1 l=1 (f kil + kil ) P n i (6) k=1 k N + k=1 k K Y (h k + k ) k=1 i=1 ( l=1 kil ) P n i n i Y
References-found: 28

