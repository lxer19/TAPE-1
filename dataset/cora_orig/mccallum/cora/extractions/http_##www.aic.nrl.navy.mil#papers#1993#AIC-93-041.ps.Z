URL: http://www.aic.nrl.navy.mil/papers/1993/AIC-93-041.ps.Z
Refering-URL: http://www.aic.nrl.navy.mil/~aha/pub-details.html
Root-URL: 
Title: Case-Based Sonogram Classification  
Author: David Aha Patrick Harrison 
Date: October, 1993  
Web: hlastnamei.aic.nrl.navy.mil  
Address: Code 5510 Washington, D.C. 20375  
Affiliation: Navy Center for Applied Research in AI Naval Research Laboratory,  
Abstract: This report replicates and extends results reported by Naval Air Warfare Center (NAWC) personnel on the automatic classification of sonar images. They used novel case-based reasoning systems in their empirical studies, but did not obtain comparative analyses using standard classification algorithms. Therefore, the quality of the NAWC results were unknown. We replicated the NAWC studies and also tested several other classifiers (i.e., both case-based and otherwise) from the machine learning literature. These comparisons and their ramifications are detailed in this paper. Next, we investigated Fala and Walker's two suggestions for future work (i.e., on combining their similarity functions and on an alternative case representation). Finally, we describe several ways to incorporate additional domain-specific knowledge when applying case-based classifiers to similar tasks. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. W. </author> <year> (1989). </year> <title> Incremental, Instance-Based Learning of Independent and Graded Concept Descriptions. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning. </booktitle> <address> Ithaca, NY: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 387-391. </pages>
Reference: <author> Aha, D. W. </author> <year> (1991). </year> <title> Incremental Constructive Induction: An Instance-Based Approach. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning. </booktitle> <address> Evanston, IL: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 117-121. </pages> <note> 12 Aha and Harrison Aha, </note> <author> D. W. </author> <year> (1992). </year> <title> Generalizing from Case Studies: A Case Study. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning. </booktitle> <address> Aberdeen, Scotland: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 1-10. </pages>
Reference-contexts: Thus, only 20 of the 21 cases were used in their experiments, which left only four classes represented by cases in the dataset. The nearest-neighbor algorithm has been extensively analyzed in the literature on pattern recognition (Dasarathy 1991) and machine learning <ref> (Aha, Kibler, and Albert 1991) </ref>, where it is viewed as an instance-based learning (IBL) algorithm. For the purposes of this report, IBL algorithms can be thought of as consisting of the following three functions: 1. <p> In the machine learning literature, several algorithms implementing feature construction and constructive induction have been used to modify the given case representation (Birnbaum and Collins 1991). While few such algorithms have been described for use with case-based classifiers <ref> (e.g., Aha 1991) </ref>, constructive induction algorithms have greatly improved classification behavior on a limited set of applications.
Reference: <author> Aha, D. W. </author> <year> (1990). </year> <title> A Study of Instance-Based Learning Algorithms for Supervised Learning Tasks: Mathematical, Empirical, </title> <type> and Psychological Evaluations (Technical Report 90-42). </type> <institution> Irvine, CA: University of California, Department of Information and Computer Science. </institution>
Reference: <author> Aha, D. W., and Goldstone, R. L. </author> <year> (1992). </year> <title> Concept Learning and Flexible Weighting. </title> <booktitle> In Proceedings of the Fourteenth Annual Conference of the Cognitive Science Society. Bloomington, IN: </booktitle> <publisher> Lawrence Erlbaum, </publisher> <pages> 534-539. </pages>
Reference-contexts: The Hamming distance function performed as well as Hits. 4. The machine learning algorithms fared poorly because they perform comparatively well only with larger-sized databases. Previous research also suggests that C4.5 and CN2 may not work well when the data is completely numeric <ref> (Aha 1992) </ref>. As usual, Backprop performed well primarily because we tested it on a large number of values for its many parameters. <p> Studying this requires varying one component of the case-based classifier while controlling the selection of the other components. For example, we would like to understand the comparative limitations of case-based classifiers and other approaches. This is partially addressed elsewhere <ref> (Aha 1992) </ref>, but these experiments are beyond the scope of this report. 5. INCORPORATING DOMAIN KNOWLEDGE Although it is comforting that perfect classification accuracy could be achieved on this sonogram dataset via a combination of the Matches and Misses algorithms, we do not know whether this result will scale up.
Reference: <author> Aha, D. W., Kibler, D., and Albert, M. K. </author> <year> (1991). </year> <title> Instance-Based Learning Algorithms. </title> <booktitle> Machine Learning 6, </booktitle> <pages> 37-66. </pages>
Reference-contexts: Thus, only 20 of the 21 cases were used in their experiments, which left only four classes represented by cases in the dataset. The nearest-neighbor algorithm has been extensively analyzed in the literature on pattern recognition (Dasarathy 1991) and machine learning <ref> (Aha, Kibler, and Albert 1991) </ref>, where it is viewed as an instance-based learning (IBL) algorithm. For the purposes of this report, IBL algorithms can be thought of as consisting of the following three functions: 1. <p> In the machine learning literature, several algorithms implementing feature construction and constructive induction have been used to modify the given case representation (Birnbaum and Collins 1991). While few such algorithms have been described for use with case-based classifiers <ref> (e.g., Aha 1991) </ref>, constructive induction algorithms have greatly improved classification behavior on a limited set of applications.
Reference: <author> Bakiri, G. </author> <year> (1991). </year> <title> Converting English Text to Speech: A Machine Learning Approach. </title> <type> Doctoral dissertation, </type> <institution> Department of Computer Science, Oregon State University, Corvallis, Oregon. </institution>
Reference-contexts: Several weight-learning methods have been proposed, including algorithms based on incremental training (Salzberg 1990; Aha 1989), genetic algorithms (Kelly and Davis 1991), decision trees (Kibler and Aha 1987; Cardie 1993), information theory <ref> (Bakiri 1991) </ref>, ones for symbolic-valued attributes (Stanfill and Waltz 1986), and several others. All of these algorithms can be run in a knowledge-poor fashion. However, knowledge-intensive algorithms are often more appropriate, especially when only a small amount of data is available for an application with a large instance space.
Reference: <author> Bareiss, R. </author> <year> (1989). </year> <title> The Experimental Evaluation of a Case-Based Learning Apprentice. </title> <booktitle> In Proceedings of a Case-Based Reasoning Workshop. </booktitle> <address> Pensacola Beach, FL: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 162-167. </pages>
Reference: <author> Birnbaum, L. A., and Collins, G. C. (Ed.). </author> <year> (1991). </year> <booktitle> Proceedings of the Eighth International Workshop on Machine Learning. </booktitle> <address> Evanston, IL: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: A final alternative is to have the algorithm itself propose alternative case representations. In the machine learning literature, several algorithms implementing feature construction and constructive induction have been used to modify the given case representation <ref> (Birnbaum and Collins 1991) </ref>. While few such algorithms have been described for use with case-based classifiers (e.g., Aha 1991), constructive induction algorithms have greatly improved classification behavior on a limited set of applications.
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A., and Stone, C. J. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <address> Belmont, CA: </address> <publisher> Wadsworth International Group. </publisher>
Reference: <author> Cain, T., Pazzani, M. J., and Silverstein, G. </author> <year> (1991). </year> <title> Using Domain Knowledge to Influence Similarity Judgement. </title> <booktitle> In Proceedings of the Case-Based Reasoning Workshop. </booktitle> <address> Washington, DC: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 191-202. </pages>
Reference: <author> Cardie, C. </author> <year> (1993). </year> <title> Using Decision Trees to Improve Case-Based Learning. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning. </booktitle> <address> Amherst, MA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 25-32. </pages>
Reference: <author> Clark, P. E., and Boswell, R. </author> <year> (1991). </year> <title> Rule Induction with CN2: Some Recent Improvements. </title> <booktitle> In Proceedings of the Fifth European Working Session on Learning. Porto, </booktitle> <address> Portugal: </address> <publisher> Springer-Verlag, </publisher> <pages> 151-163. </pages>
Reference: <author> Clark, P., Feng, C., and Matwin, S. </author> <year> (1993). </year> <title> Design for a Case-Based Expert-System for Remote Sensing (Technical Report 93-08). </title> <institution> University of Ottawa, Department of Computer Science. </institution>
Reference: <author> Clark, P. E., and Niblett, T. </author> <year> (1989). </year> <title> The CN2 Induction Algorithm. </title> <booktitle> Machine Learning 3, </booktitle> <pages> 261-284. </pages>
Reference: <author> Cover, T. M., and Hart, P. E. </author> <year> (1967). </year> <title> Nearest Neighbor Pattern Classification. </title> <journal> IEEE Trans. Inf. Theory 13, </journal> <pages> 21-27. </pages>
Reference: <author> Cover, T. M. </author> <year> (1968). </year> <title> Estimation by the Nearest Neighbor Rule. </title> <journal> IEEE Trans. Inf. Theory 14, </journal> <pages> 50-55. </pages>
Reference: <author> Dasarathy, B. V. (Ed.). </author> <year> (1991). </year> <title> Nearest Neighbor(NN) Norms: NN Pattern Classification Techniques. </title> <publisher> Los Alamitos, </publisher> <address> CA: </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Thus, only 20 of the 21 cases were used in their experiments, which left only four classes represented by cases in the dataset. The nearest-neighbor algorithm has been extensively analyzed in the literature on pattern recognition <ref> (Dasarathy 1991) </ref> and machine learning (Aha, Kibler, and Albert 1991), where it is viewed as an instance-based learning (IBL) algorithm. For the purposes of this report, IBL algorithms can be thought of as consisting of the following three functions: 1.
Reference: <author> Duda, R. O., and Hart, P. E. </author> <year> (1973). </year> <title> Pattern Classification and Scene Analysis. </title> <address> New York, NY: </address> <publisher> Wiley. </publisher>
Reference: <author> Fala, G. and Walker, J. </author> <year> (1993). </year> <title> Using Case-Based Reasoning to Automate Acoustic Submarine Classification: Detailed Summary of Technical Progress. In ONR Computer Science Division Program Summary, Fiscal Year 1992. Case-Based Sonar Classification 13 Fix, </title> <editor> E., and Hodges, J. L., Jr. </editor> <year> (1951). </year> <title> Discriminatory Analysis, Nonparametric Discrimination, </title> <type> Consistency Properties (Technical Report 4). </type> <institution> Randolph Field, TX: United States Air Force, School of Aviation Medicine. </institution>
Reference: <author> Hintzman, D. L. </author> <year> (1988). </year> <title> Judgments of Frequency and Recognition Memory in a Multiple-Trace Memory Model. </title> <journal> Psychological Review 95, </journal> <pages> 528-551. </pages>
Reference: <author> Jabbour, K., Riveros, J. F. V., Landsbergen, D., and Meyer W. </author> <year> (1987). </year> <title> ALFA: Automated Load Forecasting Assistant. </title> <booktitle> In Proceedings of the 1987 IEEE Power Engineering Society Summer Meeting. </booktitle> <address> San Francisco, CA. </address>
Reference: <author> Kelly, J. D., Jr., and Davis, L. </author> <year> (1991). </year> <title> A Hybrid Genetic Algorithm for Classification. </title> <booktitle> In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence. </booktitle> <address> Sydney, Australia: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 645-650. </pages>
Reference-contexts: Using this function, features with weights of zero are effectively ignored during similarity computations, whereas features whose weights are high have the most impact on determining similarity. Several weight-learning methods have been proposed, including algorithms based on incremental training (Salzberg 1990; Aha 1989), genetic algorithms <ref> (Kelly and Davis 1991) </ref>, decision trees (Kibler and Aha 1987; Cardie 1993), information theory (Bakiri 1991), ones for symbolic-valued attributes (Stanfill and Waltz 1986), and several others. All of these algorithms can be run in a knowledge-poor fashion.
Reference: <author> Kibler, D., and Aha, D. W. </author> <year> (1987). </year> <title> Learning Representative Exemplars of Concepts: An Initial Case Study. </title> <booktitle> In Proceedings of the Fourth International Workshop on Machine Learning. </booktitle> <address> Irvine, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 24-30. </pages>
Reference: <author> Nosofsky, R. M. </author> <year> (1986). </year> <title> Attention, Similarity, </title> <booktitle> and the Identification-Categorization Relationship. </booktitle>
Reference: <author> J. </author> <title> Experimental Psychology: </title> <booktitle> General 15, </booktitle> <pages> 39-57. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We also tested three common machine learning algorithms: a decision tree inducer named C4.5 1 <ref> (Quinlan 1993) </ref>, a decision rule inducer named CN2 2 (Clark and Niblett 1989; Clark and Boswell 1991), and the Backpropagation algorithm 3 (Rumelhart, McClelland, and the PDP Research Group 1986).
Reference: <author> Reed, S. K. </author> <year> (1972). </year> <title> Pattern Recognition and Categorization. </title> <booktitle> Cognitive Psychology 3, </booktitle> <pages> 382-407. </pages>
Reference: <author> Rumelhart D. E., McClelland, J. L., </author> <title> and The PDP Research Group (Eds.), </title> <booktitle> (1986). Parallel Distributed Processing: Explorations in the Microstructure of Cognition (Vol. 1). </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: We also tested three common machine learning algorithms: a decision tree inducer named C4.5 1 (Quinlan 1993), a decision rule inducer named CN2 2 (Clark and Niblett 1989; Clark and Boswell 1991), and the Backpropagation algorithm 3 <ref> (Rumelhart, McClelland, and the PDP Research Group 1986) </ref>. Table 2 summarizes the results for the algorithms alongside the results from the original study and the baseline results for guessing randomly or always guessing the most frequent class in the dataset. <p> This coarse coding representation <ref> (Rumelhart, McClelland, and The PDP Research Group 1986) </ref> modifies the original representation via an averaging process. Each feature in this representation corresponds to a sequence of frequency boundaries rather than to a single boundary.
Reference: <author> Salzberg, S. L. </author> <year> (1990). </year> <title> Learning with Nested Generalized Exemplars. </title> <address> Boston, MA: </address> <publisher> Kluwer. </publisher>
Reference: <author> Schaffer, C. </author> <year> (1993). </year> <title> Overfitting Avoidance as Bias. </title> <booktitle> Machine Learning 10, </booktitle> <pages> 113-152. </pages>
Reference: <author> Sebestyen, G. S. </author> <year> (1962). </year> <title> Decision-Making Processes in Pattern Recognition. </title> <address> New York, NY: </address> <publisher> Macmillan. </publisher>
Reference: <author> Shavlik, J. W., Mooney, R. J., and Towell, G. G. </author> <year> (1991). </year> <title> Symbolic and Neural Learning Algorithms: An Experimental Comparison. </title> <booktitle> Machine Learning 5, </booktitle> <pages> 111-145. </pages>
Reference: <author> Shepard, B. </author> <year> (1983). </year> <title> An Appraisal of a Decision Tree Approach to Image Classification. </title> <booktitle> In Proceedings of the Eighth International Joint Conference on Artificial Intelligence. </booktitle> <address> Karlsruhe, West Germany: </address> <publisher> William Kaufmann, </publisher> <pages> 473-475. </pages>
Reference: <author> Shepard, R. N. </author> <year> (1987). </year> <title> Toward a Universal Law of Generalization for Psychological Science. </title> <booktitle> Science 237, </booktitle> <pages> 1317-1323. </pages>
Reference: <author> Stanfill, C., and Waltz, D. </author> <year> (1986). </year> <title> Toward Memory-Based Reasoning. </title> <journal> Commun. Assoc. Computing Machinery 29, </journal> <pages> 1213-1228. </pages>
Reference-contexts: Several weight-learning methods have been proposed, including algorithms based on incremental training (Salzberg 1990; Aha 1989), genetic algorithms (Kelly and Davis 1991), decision trees (Kibler and Aha 1987; Cardie 1993), information theory (Bakiri 1991), ones for symbolic-valued attributes <ref> (Stanfill and Waltz 1986) </ref>, and several others. All of these algorithms can be run in a knowledge-poor fashion. However, knowledge-intensive algorithms are often more appropriate, especially when only a small amount of data is available for an application with a large instance space.
Reference: <author> Turney, P. D. </author> <year> (1993). </year> <title> Exploiting Context when Learning to Classify. </title> <booktitle> In Proceedings of the Euro-pean Conference on Machine Learning. </booktitle> <address> Vienna, Austria: </address> <publisher> Springer-Verlag, </publisher> <pages> 402-407. </pages>
Reference: <author> Tversky, A. </author> <year> (1977). </year> <title> Features of Similarity. </title> <journal> Psychological Review 84, </journal> <pages> 327-352. </pages>
Reference: <author> Utgoff, P. E. </author> <year> (1986). </year> <title> Machine Learning of Inductive Bias. </title> <address> Hingham, MA: </address> <publisher> Kluwer. </publisher>
Reference: <author> Walker, J. </author> <year> (1993). </year> <type> Personal communication. </type>
Reference: <author> Weiss, S. M., and Kulikowski, C. A. </author> <year> (1991). </year> <title> Computer Systems that Learn: Classification and Prediction Methods from Statistics, Neural Nets, </title> <booktitle> Machine Learning, and Expert Systems. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Since this function can map many frequencies to a single frequency boundary, the 128 feature values constituting each case were non-negative integers. Fala and Walker used the standard leave-one-out strategy <ref> (Weiss and Kulikowski 1991) </ref> to evaluate the classification accuracy of three novel variants of the nearest-neighbor algorithm (Fix and Hodges 1951; Cover and Hart 1967; Duda and Hart 1973).
References-found: 40

