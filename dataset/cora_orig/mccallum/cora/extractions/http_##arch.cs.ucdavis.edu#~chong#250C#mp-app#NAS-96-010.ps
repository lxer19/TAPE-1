URL: http://arch.cs.ucdavis.edu/~chong/250C/mp-app/NAS-96-010.ps
Refering-URL: http://arch.cs.ucdavis.edu/~chong/250C/mp-app/
Root-URL: http://www.cs.ucdavis.edu
Keyword: IBM SP2/66 MHz SGI Power Challenge Array/90 MHz Cray Research T3D Intel Paragon  
Address: Moffett Field, CA, 94035-1000.  Palo Alto, CA.  
Affiliation: NASA Ames Research Center,  Sterling Software,  
Note: MRJ, Inc. This work is supported through NASA Contract NAS 2-14303.  This work is supported through NASA Contract NAS 2-13210.  
Abstract: The NAS Parallel Benchmarks 2.1 Results William Saphir fl Alex Woo y and Maurice Yarrow z npb@nas.nasa.gov Report NAS-96-010, August, 1996 Abstract We present performance results for version 2.1 of the NAS Parallel Bench marks (NPB) on the following architectures: 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Bailey, D. H.; Barszcz, E.; Barton, J. T.; Browning, D. S.; Carter, R. L.; Dagum, L.; Fatoohi, R. A.; Frederickson, P. O.; Lasinski, T. A.; Schreiber, R. S.; Simon, H. D.; Venkatakrishnam, V.; and Weeratunga, S. K.: </author> <title> The NAS Parallel Benchmarks, </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> Vol. 5, No. 3, </volume> <month> (Fall </month> <year> 1991), </year> <pages> pp. 63-73. </pages>
Reference-contexts: 1 Introduction The NAS Parallel Benchmarks (NPBs) <ref> [1, 2] </ref> are a widely-recognized suite of benchmarks originally designed to compare the performance of highly parallel computers with that of traditional supercomputers. The NPBs are specified algorithmically, and are implemented mainly by computer vendors, using techniques and optimizations appropriate to their specific computers.
Reference: [2] <author> Bailey, D. H.; Barton, J.T.; Lasinski, T. A.; and Simon, H. D., eds: </author> <title> "The NAS Parallel Benchmarks," </title> <type> NASA Technical Memorandum 103863, </type> <institution> NASA Ames Research Center, Moffett Field, </institution> <address> CA, 94035-1000, July 1993 http://www.nas.nasa.gov/NAS/NPB/. </address>
Reference-contexts: 1 Introduction The NAS Parallel Benchmarks (NPBs) <ref> [1, 2] </ref> are a widely-recognized suite of benchmarks originally designed to compare the performance of highly parallel computers with that of traditional supercomputers. The NPBs are specified algorithmically, and are implemented mainly by computer vendors, using techniques and optimizations appropriate to their specific computers.
Reference: [3] <author> Bailey, D. H.; Harris, T.; Saphir, W.; van der Wijngaart, R.; Woo, A.; and Yarrow, M., </author> <title> "The NAS Parallel Benchmarks 2.0,"NASA Technical Report NAS-95-020, </title> <institution> NASA Ames Research Center, Moffett Field, </institution> <address> CA, 94035-1000, </address> <note> December 1995 http://www.nas.nasa.gov/NAS/NPB/. </note>
Reference-contexts: Performance results based on these optimized proprietary implementations are submitted to NAS by vendors and are reported in a periodic NAS Technical Report [4]. The vendor-optimized NPB implementations will be referred to as NPB 1. In late 1995, NAS announced NPB 2 <ref> [3] </ref>, a set of specific NPB implementations, based on Fortran 77 and MPI [5]. NPB 2 implementations are intended to be run with little or no tuning, in contrast to NPB 1 implementations, which have been highly optimized by vendors for specific architectures.
Reference: [4] <author> Bailey, D. H.; and Saini, S., </author> <title> "The NAS Parallel Benchmarks Results 12-95,"NASA Technical Report NAS-95-021, </title> <institution> NASA Ames Research Center, Moffett Field, </institution> <address> CA, 94035-1000, </address> <note> December 1995 http://www.nas.nasa.gov/NAS/NPB/. </note>
Reference-contexts: The NPBs are specified algorithmically, and are implemented mainly by computer vendors, using techniques and optimizations appropriate to their specific computers. Performance results based on these optimized proprietary implementations are submitted to NAS by vendors and are reported in a periodic NAS Technical Report <ref> [4] </ref>. The vendor-optimized NPB implementations will be referred to as NPB 1. In late 1995, NAS announced NPB 2 [3], a set of specific NPB implementations, based on Fortran 77 and MPI [5].
Reference: [5] <author> Message Passing Interface Forum: </author> <title> MPI: A Message|Passing Interface Standard, </title> <note> Version 1.1, July, 1995, http://www.mcs.anl.gov/mpi/mpi-report-1.1/mpi-report.html. </note>
Reference-contexts: The vendor-optimized NPB implementations will be referred to as NPB 1. In late 1995, NAS announced NPB 2 [3], a set of specific NPB implementations, based on Fortran 77 and MPI <ref> [5] </ref>. NPB 2 implementations are intended to be run with little or no tuning, in contrast to NPB 1 implementations, which have been highly optimized by vendors for specific architectures. NPB 2 implementations are designed for computers with hierarchical cache-based memories.
Reference: [6] <author> Dongarra, J. J.: </author> <title> The LINPACK Benchmark: An Explanation. </title> <booktitle> SuperComputing, Spring 1988, </booktitle> <pages> pp. 10-14. </pages>
Reference-contexts: NPB 1 results are meaningful and can be compared because every vendor has had equal opportunity to optimize, and because the pencil-and-paper nature of the benchmarks eliminates architectural bias. NPB 1 results, while far more meaningful than the "not to be exceeded" peak performance and LINPACK <ref> [6] </ref> rates, generally represent upper limits on what can be achieved for the type of calculations performed by the NPBs. In contrast, NPB 2 implementations are run essentially "out of the box" with no tuning 1 .
Reference: [7] <author> Bruno, J; Cappello, P.R.: </author> <title> Implementing the Beam and Warming Method on the Hypercube. </title> <booktitle> Proceedings of the 3ed conference on Hypercube Concurrent Computers and Applications, </booktitle> <address> Pasadena, CA, </address> <month> Jan 19-20, </month> <year> 1988 </year>
Reference-contexts: Parallelism in the solution phase appears both in the data (solutions along parallel grid lines can be computed independently, since they have been decoupled by approximate factorization) and through GE pipelining. To solve these equations, both SP and BT use the multipartition <ref> [7] </ref> method. Each processor is assigned several non-overlapping sub-blocks (partitions) of points. This domain decomposition provides near-perfect load balance, requires only few data movements of moderate size along partition boundaries, and has no pipeline fill delay. Tradeoffs between this and other methods are described in [8].
Reference: [8] <author> R.F. Van der Wijngaart, </author> <title> Efficient implementation of a 3-dimensional ADI method on the iPSC/860 , Supercomputing '93, </title> <address> Portland, OR, </address> <month> November 15-19, </month> <year> 1993 </year> <month> 27 </month>
Reference-contexts: Each processor is assigned several non-overlapping sub-blocks (partitions) of points. This domain decomposition provides near-perfect load balance, requires only few data movements of moderate size along partition boundaries, and has no pipeline fill delay. Tradeoffs between this and other methods are described in <ref> [8] </ref>. Both SP and BT have two phases in each iteration. The first phase is the computation of the right hand side, which is identical in the two codes. It 14 involves compact local difference stencils only. The matrix solution phases proceed as described above.
References-found: 8

