URL: http://www.csc.calpoly.edu/~tpederse/emnlp97-final.ps.gz
Refering-URL: http://www.csc.calpoly.edu/~tpederse/pubs.html
Root-URL: http://www.csc.calpoly.edu
Email: fpedersen,rbruceg@seas.smu.edu  
Title: Distinguishing Word Senses in Untagged Text  
Author: Ted Pedersen and Rebecca Bruce 
Address: Dallas, TX 75275-0112  
Affiliation: Department of Computer Science and Engineering Southern Methodist University  
Date: August 1997, Providence, RI  
Note: Appears in the Proceedings of the Second Conference on Empirical Methods in Natural Language Processing,  
Abstract: This paper describes an experimental comparison of three unsupervised learning algorithms that distinguish the sense of an ambiguous word in untagged text. The methods described in this paper, McQuitty's similarity analysis, Ward's minimum-variance method, and the EM algorithm, assign each instance of an ambiguous word to a known sense definition based solely on the values of automatically identifiable features in text. These methods and feature sets are found to be more successful in disambiguating nouns rather than adjectives or verbs. Overall, the most accurate of these procedures is McQuitty's similarity analysis in combination with a high dimensional feature set. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Baum, L. </author> <year> 1972. </year> <title> An inequality and associated maximization technique in statistical estimation for probabilistic functions of a Markov process. </title> <editor> In O. Shisha, editor, Inequalities, </editor> <volume> volume 3. </volume> <publisher> Academic Press, </publisher> <address> New York, NY, </address> <pages> pages 1-8. </pages>
Reference-contexts: The results presented are preliminary but show an accuracy percentage in the mid-nineties when applied to Dixon, a name found to be quite ambiguous. It should be noted that the EM algorithm relates to a large body of work in speech processing. The Baum-Welch forward-backward algorithm <ref> (Baum, 1972) </ref> is a specialized form of the EM algorithm that assumes the underlying parametric model is a hidden Markov model.
Reference: <author> Black, E. </author> <year> 1988. </year> <title> An experiment in computational discrimination of English word senses. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 32(2) </volume> <pages> 185-194. </pages>
Reference-contexts: 88 191 279 enhance 354 634 988 442 825 1267 Ward - 722 correct Discovered Actual assist enhance assist 119 160 279 enhance 344 644 988 463 804 1267 EM - 763 correct 7 Related Work Word-sense disambiguation has more commonly been cast as a problem in supervised learning (e.g., <ref> (Black, 1988) </ref>, (Yarowsky, 1992), (Yarowsky, 1993), (Leacock, Towell, and Voorhees, 1993), (Bruce and Wiebe, 1994), (Mooney, 1996), (Ng and Lee, 1996), (Pedersen, Bruce, and Wiebe, 1997), (Pedersen and Bruce, 1997a)). However, all of these methods require that manually sense tagged text be available to train the algorithm.
Reference: <author> Bruce, R. and J. Wiebe. </author> <year> 1994. </year> <title> Word-sense disambiguation using decomposable models. </title> <booktitle> In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 139-146. </pages>
Reference-contexts: - 722 correct Discovered Actual assist enhance assist 119 160 279 enhance 344 644 988 463 804 1267 EM - 763 correct 7 Related Work Word-sense disambiguation has more commonly been cast as a problem in supervised learning (e.g., (Black, 1988), (Yarowsky, 1992), (Yarowsky, 1993), (Leacock, Towell, and Voorhees, 1993), <ref> (Bruce and Wiebe, 1994) </ref>, (Mooney, 1996), (Ng and Lee, 1996), (Pedersen, Bruce, and Wiebe, 1997), (Pedersen and Bruce, 1997a)). However, all of these methods require that manually sense tagged text be available to train the algorithm. For most domains such text is not available and is expensive to create.
Reference: <author> Bruce, R., J. Wiebe, and T. Pedersen. </author> <year> 1996. </year> <title> The measure of a model. </title> <booktitle> In Proceedings of the Conference on Empirical Methods in Natural Language Processing, </booktitle> <pages> pages 101-112. </pages>
Reference-contexts: Implicit in Ward's method is the assumption that the sample comes from a mixture of normal distributions. While NLP data is typically not well characterized by a normal distribution (see, e.g. (Zipf, 1935), <ref> (Pedersen, Kayaalp, and Bruce, 1996) </ref>), there is evidence that our data, when represented by a dissimilarity matrix, can be adequately characterized by a normal distribution. <p> With the exception of line, each ambiguous word is tagged with a single sense defined in the Longman Dictionary of Contemporary English (LDOCE) (Procter, 1978). The data for the 12 words tagged using LDOCE senses are described in more detail in <ref> (Bruce, Wiebe, and Ped-ersen, 1996) </ref>. The line data comes from both the ACL/DCI WSJ corpus and the American Printing House for the Blind corpus. Each occurrence of line is tagged with a single sense defined in WordNet (Miller, 1995).
Reference: <author> Dempster, A., N. Laird, and D. Rubin. </author> <year> 1977. </year> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> 39 </volume> <pages> 1-38. </pages>
Reference: <author> Devijver, P. and J. Kittler. </author> <year> 1982. </year> <title> Pattern Classification: A Statistical Approach. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ. </address>
Reference-contexts: The distance between two points in a multi-dimensional space can be measured using any of a wide variety of metrics (see, e.g. <ref> (Devijver and Kittler, 1982) </ref>). Observations are grouped in the manner that minimizes the distance between the members of each class. Ward's and McQuitty's method are agglomerative clustering algorithms that differ primarily in how they compute the distance between clusters.
Reference: <author> Duda, R. and P. Hart. </author> <year> 1973. </year> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <address> New York, NY. </address>
Reference-contexts: In order to use the EM algorithm, the parametric form of the model representing the data must be known. In these experiments, we assume that the model form is the Naive Bayes <ref> (Duda and Hart, 1973) </ref>. In this model, all features are conditionally independent given the value of the classification feature, i.e., the sense of the ambiguous word. <p> In future work, we will expand all of the above types of features and employ techniques to reduce dimensionality along the lines suggested in <ref> (Duda and Hart, 1973) </ref> and (Gale, Church, and Yarowsky, 1995). 6 Experimental Results deviation of disambiguation over 25 random trials for each combination of word, feature set and learning algorithm.
Reference: <author> Gale, W., K. Church, and D. Yarowsky. </author> <year> 1992. </year> <title> A method for disambiguating word senses in a large corpus. </title> <journal> Computers and the Humanities, </journal> <volume> 26 </volume> <pages> 415-439. </pages>
Reference-contexts: In this model, all features are conditionally independent given the value of the classification feature, i.e., the sense of the ambiguous word. This assumption is based on the suc cess of the Naive Bayes model when applied to su-pervised word-sense disambiguation (e.g. <ref> (Gale, Church, and Yarowsky, 1992) </ref>, (Leacock, Towell, and Voorhees, 1993), (Mooney, 1996), (Pedersen, Bruce, and Wiebe, 1997), (Pedersen and Bruce, 1997a)). There are two potential problems when using the EM algorithm. First, it is computationally expensive and convergence can be slow for problems with large numbers of model parameters.
Reference: <author> Gale, W., K. Church, and D. Yarowsky. </author> <year> 1995. </year> <title> Discrimination decisions for 100,000 dimensional spaces. </title> <journal> Journal of Operations Research, </journal> <volume> 55 </volume> <pages> 323-344. </pages>
Reference-contexts: In future work, we will expand all of the above types of features and employ techniques to reduce dimensionality along the lines suggested in (Duda and Hart, 1973) and <ref> (Gale, Church, and Yarowsky, 1995) </ref>. 6 Experimental Results deviation of disambiguation over 25 random trials for each combination of word, feature set and learning algorithm. <p> Subject-specific neighborhoods are composed of words having at least one sense marked with that subject code. 7.3 EM algorithm The only other application of the EM algorithm to word-sense disambiguation is described in <ref> (Gale, Church, and Yarowsky, 1995) </ref>. There the EM algorithm is used as part of a supervised learning algorithm to distinguish city names from people's names. A narrow window of context, one or two words to either side, was found to perform better than wider windows.
Reference: <author> Geman, S. and D. Geman. </author> <year> 1984. </year> <title> Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 6 </volume> <pages> 721-741. </pages>
Reference-contexts: Second, if the likelihood function is very irregular it may always converge to a local maxima and not find the global maximum. In this case, an alternative is to use the more computationally expensive method of Gibbs Sampling <ref> (Geman and Geman, 1984) </ref>. 3.1 Description At the heart of the EM Algorithm lies the Q-function.
Reference: <author> Guthrie, J., L. Guthrie, Y. Wilks, and H. Aidine-jad. </author> <year> 1991. </year> <title> Subject-dependent co-occurrence and word sense disambiguation. </title> <booktitle> In Proceedings of the 29th Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 146-152, </pages> <address> Berkeley, CA, </address> <month> June. </month>
Reference-contexts: The "expanded" sense definitions are then compared to the context of an ambiguous word, and the sense-definition with the greatest number of word overlaps with the context is selected as correct. <ref> (Guthrie et al., 1991) </ref> propose that neighborhoods be subject dependent. They suggest that a word should potentially have different neighborhoods corresponding to the different LDOCE subject code.
Reference: <author> Hearst, M. </author> <year> 1991. </year> <title> Noun homograph disambiguation using local context in large text corpora. </title> <booktitle> In Proceedings of the 7th Annual Conference of the UW Centre for the New OED and Text Research: Using Corpora, </booktitle> <publisher> Oxford. </publisher>
Reference-contexts: An early example of such an approach is described in <ref> (Hearst, 1991) </ref>. A supervised learning algorithm is trained with a small amount of manually sense tagged text and applied to a held out test set. Those examples in the test set that are most confidently disambiguated are added to the training sample.
Reference: <author> Jelinek, F. </author> <year> 1990. </year> <title> Self-organized language modeling for speech recognition. </title> <editor> In Waibel and Lee, editors, </editor> <booktitle> Readings in Speech Recognition. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: The Baum-Welch forward-backward algorithm (Baum, 1972) is a specialized form of the EM algorithm that assumes the underlying parametric model is a hidden Markov model. The Baum-Welch forward-backward algorithm has been used extensively in speech recognition (e.g. (Levinson, Rabiner, and Sondhi, 1983), (Kupiec, 1992)), <ref> (Jelinek, 1990) </ref>). 8 Conclusions Supervised learning approaches to word-sense disambiguation fall victim to the knowledge acquisition bottleneck. The creation of sense tagged text sufficient to serve as a training sample is expensive and time consuming.
Reference: <author> Kiss, G. </author> <year> 1973. </year> <title> Grammatical word classes: A learning process and its simulation. </title> <journal> Psychology of Learning and Motivation, </journal> <volume> 7 </volume> <pages> 1-41. </pages>
Reference-contexts: This may help in dealing with very skewed distributions of senses since we currently select collocations based simply on frequency. 7.2 Clustering Clustering has most often been applied in natural language processing as a method for inducing syntactic or semantically related groupings of words (e.g., (Rosenfeld, Huang, and Schneider, 1969), <ref> (Kiss, 1973) </ref>, (Ritter and Kohonen, 1989), (Pereira, Tishby, and Lee, 1993), (Schutze, 1993), (Resnik, 1995a)). An early application of clustering to word-sense disambiguation is described in (Schutze, 1992). There words are represented in terms of the co-occurrence statistics of four letter sequences.
Reference: <author> Kupiec, J. </author> <year> 1992. </year> <title> Robust part-of-speech tagging using a hidden Markov model. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 6 </volume> <pages> 225-243. </pages>
Reference-contexts: The Baum-Welch forward-backward algorithm (Baum, 1972) is a specialized form of the EM algorithm that assumes the underlying parametric model is a hidden Markov model. The Baum-Welch forward-backward algorithm has been used extensively in speech recognition (e.g. (Levinson, Rabiner, and Sondhi, 1983), <ref> (Kupiec, 1992) </ref>), (Jelinek, 1990)). 8 Conclusions Supervised learning approaches to word-sense disambiguation fall victim to the knowledge acquisition bottleneck. The creation of sense tagged text sufficient to serve as a training sample is expensive and time consuming.
Reference: <author> Leacock, C., G. Towell, and E. Voorhees. </author> <year> 1993. </year> <title> Corpus-based statistical sense resolution. </title> <booktitle> In Proceedings of the ARPA Workshop on Human Language Technology, </booktitle> <pages> pages 260-265, </pages> <month> March. </month>
Reference-contexts: In this model, all features are conditionally independent given the value of the classification feature, i.e., the sense of the ambiguous word. This assumption is based on the suc cess of the Naive Bayes model when applied to su-pervised word-sense disambiguation (e.g. (Gale, Church, and Yarowsky, 1992), <ref> (Leacock, Towell, and Voorhees, 1993) </ref>, (Mooney, 1996), (Pedersen, Bruce, and Wiebe, 1997), (Pedersen and Bruce, 1997a)). There are two potential problems when using the EM algorithm. First, it is computationally expensive and convergence can be slow for problems with large numbers of model parameters. <p> 988 442 825 1267 Ward - 722 correct Discovered Actual assist enhance assist 119 160 279 enhance 344 644 988 463 804 1267 EM - 763 correct 7 Related Work Word-sense disambiguation has more commonly been cast as a problem in supervised learning (e.g., (Black, 1988), (Yarowsky, 1992), (Yarowsky, 1993), <ref> (Leacock, Towell, and Voorhees, 1993) </ref>, (Bruce and Wiebe, 1994), (Mooney, 1996), (Ng and Lee, 1996), (Pedersen, Bruce, and Wiebe, 1997), (Pedersen and Bruce, 1997a)). However, all of these methods require that manually sense tagged text be available to train the algorithm.
Reference: <author> Levinson, S., L. Rabiner, and M. Sondhi. </author> <year> 1983. </year> <title> An introduction to the application of the theory of probabilistic functions of a Markov process to automatic speech recognition. </title> <journal> Bell System Technical Journal, </journal> <volume> 62 </volume> <pages> 1035-1074. </pages>
Reference-contexts: The Baum-Welch forward-backward algorithm (Baum, 1972) is a specialized form of the EM algorithm that assumes the underlying parametric model is a hidden Markov model. The Baum-Welch forward-backward algorithm has been used extensively in speech recognition (e.g. <ref> (Levinson, Rabiner, and Sondhi, 1983) </ref>, (Kupiec, 1992)), (Jelinek, 1990)). 8 Conclusions Supervised learning approaches to word-sense disambiguation fall victim to the knowledge acquisition bottleneck. The creation of sense tagged text sufficient to serve as a training sample is expensive and time consuming.
Reference: <author> Li, X., S. Szpakowicz, and S. Matwin. </author> <year> 1995. </year> <title> A WordNet-based algorithm for word sense disambiguation. </title> <booktitle> In Proceedings of the 14th International Joint Conference on Artificial Intelligence, </booktitle> <address> Montreal, </address> <month> August. </month>
Reference-contexts: Other clustering approaches to word-sense disambiguation have been based on measures of semantic distance defined with respect to a semantic network such as WordNet. Measures of semantic distance are based on the path length between concepts in a network and are used to group semantically similar concepts (e.g. <ref> (Li, Szpakowicz, and Matwin, 1995) </ref>). (Resnik, 1995b) provides an information theoretic definition of semantic distance based on WordNet. (McDonald et al., 1990) apply another clustering approach to word-sense disambiguation (also see (Wilks et al., 1990)).
Reference: <author> Marcus, M., B. Santorini, and M. Marcinkiewicz. </author> <year> 1993. </year> <title> Building a large annotated corpus of English: The Penn Treebank. </title> <journal> Computational Linguistics, </journal> <volume> 19(2) </volume> <pages> 313-330. </pages>
Reference-contexts: The words disambiguated and their sense distributions are shown in Figure 3. All data, with the exception of the data for line, come from the ACL/DCI Wall Street Journal corpus <ref> (Marcus, Santorini, and Marcinkiewicz, 1993) </ref>. With the exception of line, each ambiguous word is tagged with a single sense defined in the Longman Dictionary of Contemporary English (LDOCE) (Procter, 1978).
Reference: <author> McDonald, J., T. Plate, , and R. Schvaneveldt. </author> <year> 1990. </year> <title> Using pathfinder to extract semantic information from text. </title> <editor> In R. Schvaneveldt, editor, </editor> <title> Pathfinder Associative Networks: Studies in Knowledge Organization. </title> <publisher> Ablex, </publisher> <address> Norwood, NJ. </address>
Reference-contexts: Measures of semantic distance are based on the path length between concepts in a network and are used to group semantically similar concepts (e.g. (Li, Szpakowicz, and Matwin, 1995)). (Resnik, 1995b) provides an information theoretic definition of semantic distance based on WordNet. <ref> (McDonald et al., 1990) </ref> apply another clustering approach to word-sense disambiguation (also see (Wilks et al., 1990)). They use co-occurrence data gathered from the machine-readable version of LDOCE to define neighborhoods of related words. Conceptually, the neighborhood of a word is a type of equivalence class.
Reference: <author> McQuitty, L. </author> <year> 1966. </year> <title> Similarity analysis by reciprocal pairs for discrete and continuous data. </title> <booktitle> Educational and Psychological Measurement, </booktitle> <volume> 26 </volume> <pages> 825-831. </pages>
Reference-contexts: The object of unsupervised learning is to determine the class membership of each observation (i.e. each object to be classified), in a sample without using training examples of correct classifications. We discuss three algorithms, McQuitty's similarity analysis <ref> (McQuitty, 1966) </ref>, Ward's minimum-variance method (Ward, 1963) and the EM algorithm (Demp-ster, Laird, and Rubin, 1977), that can be used to distinguish among the known senses of an ambiguous word without the aid of disambiguated examples.
Reference: <author> Miller, G. </author> <year> 1995. </year> <title> WordNet: A lexical database. </title> <journal> Communications of the ACM, </journal> <volume> 38(11) </volume> <pages> 39-41, </pages> <month> November. </month>
Reference-contexts: The line data comes from both the ACL/DCI WSJ corpus and the American Printing House for the Blind corpus. Each occurrence of line is tagged with a single sense defined in WordNet <ref> (Miller, 1995) </ref>. This data is described in more detail in (Lea-cock, Towell, and Voorhees, 1993). Every experiment utilizes all of the sentences available for each word. The number of sentences available per word is shown as "total count" in Figure 3.
Reference: <author> Mooney, R. </author> <year> 1996. </year> <title> Comparative experiments on disambiguating word senses: An illustration of the role of bias in machine learning. </title> <booktitle> In Proceedings of the Conference on Empirical Methods in Natural Language Processing, </booktitle> <pages> pages 82-91, </pages> <month> May. </month>
Reference-contexts: This assumption is based on the suc cess of the Naive Bayes model when applied to su-pervised word-sense disambiguation (e.g. (Gale, Church, and Yarowsky, 1992), (Leacock, Towell, and Voorhees, 1993), <ref> (Mooney, 1996) </ref>, (Pedersen, Bruce, and Wiebe, 1997), (Pedersen and Bruce, 1997a)). There are two potential problems when using the EM algorithm. First, it is computationally expensive and convergence can be slow for problems with large numbers of model parameters. <p> Actual assist enhance assist 119 160 279 enhance 344 644 988 463 804 1267 EM - 763 correct 7 Related Work Word-sense disambiguation has more commonly been cast as a problem in supervised learning (e.g., (Black, 1988), (Yarowsky, 1992), (Yarowsky, 1993), (Leacock, Towell, and Voorhees, 1993), (Bruce and Wiebe, 1994), <ref> (Mooney, 1996) </ref>, (Ng and Lee, 1996), (Pedersen, Bruce, and Wiebe, 1997), (Pedersen and Bruce, 1997a)). However, all of these methods require that manually sense tagged text be available to train the algorithm. For most domains such text is not available and is expensive to create.
Reference: <author> Ng, H.T. and H.B. Lee. </author> <year> 1996. </year> <title> Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach. </title> <booktitle> In Proceedings of the 34th Annual Meeting of the Society for Computational Linguistics, </booktitle> <pages> pages 40-47. </pages>
Reference-contexts: enhance assist 119 160 279 enhance 344 644 988 463 804 1267 EM - 763 correct 7 Related Work Word-sense disambiguation has more commonly been cast as a problem in supervised learning (e.g., (Black, 1988), (Yarowsky, 1992), (Yarowsky, 1993), (Leacock, Towell, and Voorhees, 1993), (Bruce and Wiebe, 1994), (Mooney, 1996), <ref> (Ng and Lee, 1996) </ref>, (Pedersen, Bruce, and Wiebe, 1997), (Pedersen and Bruce, 1997a)). However, all of these methods require that manually sense tagged text be available to train the algorithm. For most domains such text is not available and is expensive to create.
Reference: <author> Pedersen, T. and R. Bruce. </author> <year> 1997a. </year> <title> A new supervised learning algorithm for word sense disambiguation. </title> <booktitle> In Proceedings of the Fourteenth National Conference on Artificial Intelligence, </booktitle> <address> Providence, RI, </address> <month> July. </month>
Reference-contexts: This assumption is based on the suc cess of the Naive Bayes model when applied to su-pervised word-sense disambiguation (e.g. (Gale, Church, and Yarowsky, 1992), (Leacock, Towell, and Voorhees, 1993), (Mooney, 1996), (Pedersen, Bruce, and Wiebe, 1997), <ref> (Pedersen and Bruce, 1997a) </ref>). There are two potential problems when using the EM algorithm. First, it is computationally expensive and convergence can be slow for problems with large numbers of model parameters. <p> 463 804 1267 EM - 763 correct 7 Related Work Word-sense disambiguation has more commonly been cast as a problem in supervised learning (e.g., (Black, 1988), (Yarowsky, 1992), (Yarowsky, 1993), (Leacock, Towell, and Voorhees, 1993), (Bruce and Wiebe, 1994), (Mooney, 1996), (Ng and Lee, 1996), (Pedersen, Bruce, and Wiebe, 1997), <ref> (Pedersen and Bruce, 1997a) </ref>). However, all of these methods require that manually sense tagged text be available to train the algorithm. For most domains such text is not available and is expensive to create.
Reference: <author> Pedersen, T. and R. Bruce. </author> <year> 1997b. </year> <title> Unsupervised text mining. </title> <type> Technical Report 97-CSE-9, </type> <institution> Southern Methodist University, </institution> <month> June. </month>
Reference-contexts: In previous unsupervised experiments with interest, using a modified version of Feature Set A, we were able to achieve an increase of 36 percentage points over the accuracy of the majority classifier when the 3 classes were evenly distributed in the sample <ref> (Pedersen and Bruce, 1997b) </ref>. Here, our best performance using a larger sample with a natural distribution of senses is only an increase of 20 percentage points over the accuracy of the majority classifier.
Reference: <author> Pedersen, T., R. Bruce, and J. Wiebe. </author> <year> 1997. </year> <title> Sequential model selection for word sense disambiguation. </title> <booktitle> In Proceedings of the Fifth Conference on Applied Natural Language Processing, </booktitle> <pages> pages 388-395, </pages> <address> Washington, DC, </address> <month> April. </month>
Reference-contexts: This assumption is based on the suc cess of the Naive Bayes model when applied to su-pervised word-sense disambiguation (e.g. (Gale, Church, and Yarowsky, 1992), (Leacock, Towell, and Voorhees, 1993), (Mooney, 1996), <ref> (Pedersen, Bruce, and Wiebe, 1997) </ref>, (Pedersen and Bruce, 1997a)). There are two potential problems when using the EM algorithm. First, it is computationally expensive and convergence can be slow for problems with large numbers of model parameters. <p> 279 enhance 344 644 988 463 804 1267 EM - 763 correct 7 Related Work Word-sense disambiguation has more commonly been cast as a problem in supervised learning (e.g., (Black, 1988), (Yarowsky, 1992), (Yarowsky, 1993), (Leacock, Towell, and Voorhees, 1993), (Bruce and Wiebe, 1994), (Mooney, 1996), (Ng and Lee, 1996), <ref> (Pedersen, Bruce, and Wiebe, 1997) </ref>, (Pedersen and Bruce, 1997a)). However, all of these methods require that manually sense tagged text be available to train the algorithm. For most domains such text is not available and is expensive to create.
Reference: <author> Pedersen, T., M. Kayaalp, and R. Bruce. </author> <year> 1996. </year> <title> Significant lexical relationships. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 455-460, </pages> <address> Portland, OR, </address> <month> August. </month>
Reference-contexts: Implicit in Ward's method is the assumption that the sample comes from a mixture of normal distributions. While NLP data is typically not well characterized by a normal distribution (see, e.g. (Zipf, 1935), <ref> (Pedersen, Kayaalp, and Bruce, 1996) </ref>), there is evidence that our data, when represented by a dissimilarity matrix, can be adequately characterized by a normal distribution.
Reference: <author> Pereira, F., N. Tishby, and L. Lee. </author> <year> 1993. </year> <title> Distributional clustering of English words. </title> <booktitle> In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 183-190, </pages> <address> Columbus, OH. </address>
Reference-contexts: very skewed distributions of senses since we currently select collocations based simply on frequency. 7.2 Clustering Clustering has most often been applied in natural language processing as a method for inducing syntactic or semantically related groupings of words (e.g., (Rosenfeld, Huang, and Schneider, 1969), (Kiss, 1973), (Ritter and Kohonen, 1989), <ref> (Pereira, Tishby, and Lee, 1993) </ref>, (Schutze, 1993), (Resnik, 1995a)). An early application of clustering to word-sense disambiguation is described in (Schutze, 1992). There words are represented in terms of the co-occurrence statistics of four letter sequences.
Reference: <editor> Procter, P., editor. </editor> <year> 1978. </year> <title> Longman Dictionary of Contemporary English. </title> <publisher> Longman Group Ltd., </publisher> <address> Es-sex, UK. </address>
Reference-contexts: All data, with the exception of the data for line, come from the ACL/DCI Wall Street Journal corpus (Marcus, Santorini, and Marcinkiewicz, 1993). With the exception of line, each ambiguous word is tagged with a single sense defined in the Longman Dictionary of Contemporary English (LDOCE) <ref> (Procter, 1978) </ref>. The data for the 12 words tagged using LDOCE senses are described in more detail in (Bruce, Wiebe, and Ped-ersen, 1996). The line data comes from both the ACL/DCI WSJ corpus and the American Printing House for the Blind corpus.
Reference: <author> Resnik, P. </author> <year> 1995a. </year> <title> Disambiguating noun groupings with respect to WordNet senses. </title> <booktitle> In Proceedings of the Third Workshop on Very Large Corpora, </booktitle> <publisher> MIT, </publisher> <month> June. </month>
Reference-contexts: currently select collocations based simply on frequency. 7.2 Clustering Clustering has most often been applied in natural language processing as a method for inducing syntactic or semantically related groupings of words (e.g., (Rosenfeld, Huang, and Schneider, 1969), (Kiss, 1973), (Ritter and Kohonen, 1989), (Pereira, Tishby, and Lee, 1993), (Schutze, 1993), <ref> (Resnik, 1995a) </ref>). An early application of clustering to word-sense disambiguation is described in (Schutze, 1992). There words are represented in terms of the co-occurrence statistics of four letter sequences.
Reference: <author> Resnik, P. </author> <year> 1995b. </year> <title> Using information content to evaluate semantic similarity in a taxonomy. </title> <booktitle> In Proceedings of the 14th International Joint Conference on Artificial Intelligence, </booktitle> <address> Montreal, </address> <month> August. </month>
Reference-contexts: Measures of semantic distance are based on the path length between concepts in a network and are used to group semantically similar concepts (e.g. (Li, Szpakowicz, and Matwin, 1995)). <ref> (Resnik, 1995b) </ref> provides an information theoretic definition of semantic distance based on WordNet. (McDonald et al., 1990) apply another clustering approach to word-sense disambiguation (also see (Wilks et al., 1990)). They use co-occurrence data gathered from the machine-readable version of LDOCE to define neighborhoods of related words.
Reference: <author> Ritter, H. and T. Kohonen. </author> <year> 1989. </year> <title> Self-organizing semantic maps. </title> <journal> Biological Cybernetics, </journal> <volume> 62 </volume> <pages> 241-254. </pages>
Reference-contexts: help in dealing with very skewed distributions of senses since we currently select collocations based simply on frequency. 7.2 Clustering Clustering has most often been applied in natural language processing as a method for inducing syntactic or semantically related groupings of words (e.g., (Rosenfeld, Huang, and Schneider, 1969), (Kiss, 1973), <ref> (Ritter and Kohonen, 1989) </ref>, (Pereira, Tishby, and Lee, 1993), (Schutze, 1993), (Resnik, 1995a)). An early application of clustering to word-sense disambiguation is described in (Schutze, 1992). There words are represented in terms of the co-occurrence statistics of four letter sequences.
Reference: <author> Rosenfeld, A., H. Huang, and V. Schneider. </author> <year> 1969. </year> <title> An application of cluster detection to text and picture processing. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 15 </volume> <pages> 672-681. </pages>
Reference-contexts: This may help in dealing with very skewed distributions of senses since we currently select collocations based simply on frequency. 7.2 Clustering Clustering has most often been applied in natural language processing as a method for inducing syntactic or semantically related groupings of words (e.g., <ref> (Rosenfeld, Huang, and Schneider, 1969) </ref>, (Kiss, 1973), (Ritter and Kohonen, 1989), (Pereira, Tishby, and Lee, 1993), (Schutze, 1993), (Resnik, 1995a)). An early application of clustering to word-sense disambiguation is described in (Schutze, 1992). There words are represented in terms of the co-occurrence statistics of four letter sequences.
Reference: <author> Schutze, H. </author> <year> 1992. </year> <title> Dimensions of meaning. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <pages> pages 787-796, </pages> <address> Minneapolis, MN. </address>
Reference-contexts: An early application of clustering to word-sense disambiguation is described in <ref> (Schutze, 1992) </ref>. There words are represented in terms of the co-occurrence statistics of four letter sequences. <p> The set of context vectors for the word to be disambiguated are then clustered, and the clusters are manually sense tagged. The features used in this work are complex and difficult to interpret and it isn't clear that this complexity is required. (Yarowsky, 1995) compares his method to <ref> (Schutze, 1992) </ref> and shows that for four words the former performs significantly better in distinguishing between two senses. Other clustering approaches to word-sense disambiguation have been based on measures of semantic distance defined with respect to a semantic network such as WordNet.
Reference: <author> Schutze, H. </author> <year> 1993. </year> <title> Word space. </title> <editor> In S. Hanson, J. Cowan, and C. Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5. </booktitle> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: since we currently select collocations based simply on frequency. 7.2 Clustering Clustering has most often been applied in natural language processing as a method for inducing syntactic or semantically related groupings of words (e.g., (Rosenfeld, Huang, and Schneider, 1969), (Kiss, 1973), (Ritter and Kohonen, 1989), (Pereira, Tishby, and Lee, 1993), <ref> (Schutze, 1993) </ref>, (Resnik, 1995a)). An early application of clustering to word-sense disambiguation is described in (Schutze, 1992). There words are represented in terms of the co-occurrence statistics of four letter sequences.
Reference: <author> Ward, J. </author> <year> 1963. </year> <title> Hierarchical grouping to optimize an objective function. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 58 </volume> <pages> 236-244. </pages>
Reference-contexts: The object of unsupervised learning is to determine the class membership of each observation (i.e. each object to be classified), in a sample without using training examples of correct classifications. We discuss three algorithms, McQuitty's similarity analysis (McQuitty, 1966), Ward's minimum-variance method <ref> (Ward, 1963) </ref> and the EM algorithm (Demp-ster, Laird, and Rubin, 1977), that can be used to distinguish among the known senses of an ambiguous word without the aid of disambiguated examples.
Reference: <author> Wilks, Y., D. Fass, C. Guo, J. McDonald, T. Plate, and B. Slator. </author> <year> 1990. </year> <title> Providing machine tractable dictionary tools. </title> <editor> In J. Pustejovsky, editor, </editor> <title> Theoretical and Computational Issues in Lexical Semantics. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: based on the path length between concepts in a network and are used to group semantically similar concepts (e.g. (Li, Szpakowicz, and Matwin, 1995)). (Resnik, 1995b) provides an information theoretic definition of semantic distance based on WordNet. (McDonald et al., 1990) apply another clustering approach to word-sense disambiguation (also see <ref> (Wilks et al., 1990) </ref>). They use co-occurrence data gathered from the machine-readable version of LDOCE to define neighborhoods of related words. Conceptually, the neighborhood of a word is a type of equivalence class.
Reference: <author> Yarowsky, D. </author> <year> 1992. </year> <title> Word-sense disambiguation using statistical models of Roget's categories trained on large corpora. </title> <booktitle> In Proceedings of the 14th International Conference on Computational Linguistics (COLING-92), </booktitle> <pages> pages 454-460, </pages> <address> Nantes, France, </address> <month> July. </month>
Reference-contexts: In this model, all features are conditionally independent given the value of the classification feature, i.e., the sense of the ambiguous word. This assumption is based on the suc cess of the Naive Bayes model when applied to su-pervised word-sense disambiguation (e.g. <ref> (Gale, Church, and Yarowsky, 1992) </ref>, (Leacock, Towell, and Voorhees, 1993), (Mooney, 1996), (Pedersen, Bruce, and Wiebe, 1997), (Pedersen and Bruce, 1997a)). There are two potential problems when using the EM algorithm. First, it is computationally expensive and convergence can be slow for problems with large numbers of model parameters. <p> 279 enhance 354 634 988 442 825 1267 Ward - 722 correct Discovered Actual assist enhance assist 119 160 279 enhance 344 644 988 463 804 1267 EM - 763 correct 7 Related Work Word-sense disambiguation has more commonly been cast as a problem in supervised learning (e.g., (Black, 1988), <ref> (Yarowsky, 1992) </ref>, (Yarowsky, 1993), (Leacock, Towell, and Voorhees, 1993), (Bruce and Wiebe, 1994), (Mooney, 1996), (Ng and Lee, 1996), (Pedersen, Bruce, and Wiebe, 1997), (Pedersen and Bruce, 1997a)). However, all of these methods require that manually sense tagged text be available to train the algorithm.
Reference: <author> Yarowsky, D. </author> <year> 1993. </year> <title> One sense per collocation. </title> <booktitle> In Proceedings of the ARPA Workshop on Human Language Technology, </booktitle> <pages> pages 266-271. </pages>
Reference-contexts: 354 634 988 442 825 1267 Ward - 722 correct Discovered Actual assist enhance assist 119 160 279 enhance 344 644 988 463 804 1267 EM - 763 correct 7 Related Work Word-sense disambiguation has more commonly been cast as a problem in supervised learning (e.g., (Black, 1988), (Yarowsky, 1992), <ref> (Yarowsky, 1993) </ref>, (Leacock, Towell, and Voorhees, 1993), (Bruce and Wiebe, 1994), (Mooney, 1996), (Ng and Lee, 1996), (Pedersen, Bruce, and Wiebe, 1997), (Pedersen and Bruce, 1997a)). However, all of these methods require that manually sense tagged text be available to train the algorithm. <p> Experiments with 11 other words using collocation seeds result in an average accuracy of 96 percent. While (Yarowsky, 1995) does not discuss distinguishing more than 2 senses of a word, there is no immediate reason to doubt that the "one sense per collocation" rule <ref> (Yarowsky, 1993) </ref> would still hold for a larger number of senses. In future work we will evaluate using the "one sense per collocation" rule to seed our various methods.
Reference: <author> Yarowsky, D. </author> <year> 1995. </year> <title> Unsupervised word sense disambiguation rivaling supervised methods. </title> <booktitle> In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 189-196, </pages> <address> Cambridge, MA. </address>
Reference-contexts: In future work, we will expand all of the above types of features and employ techniques to reduce dimensionality along the lines suggested in (Duda and Hart, 1973) and <ref> (Gale, Church, and Yarowsky, 1995) </ref>. 6 Experimental Results deviation of disambiguation over 25 random trials for each combination of word, feature set and learning algorithm. <p> A supervised learning algorithm is trained with a small amount of manually sense tagged text and applied to a held out test set. Those examples in the test set that are most confidently disambiguated are added to the training sample. A more recent bootstrapping approach is described in <ref> (Yarowsky, 1995) </ref>. This algorithm requires a small number of training examples to serve as a seed. There are a variety of options discussed for automatically selecting seeds; one is to identify col-locations that uniquely distinguish between senses. For plant, the collocations manufacturing plant and living plant make such a distinction. <p> Based on 106 examples of manufacturing plant and 82 examples of living plant this algorithm is able to distinguish between two senses of plant for 7,350 examples with 97 percent accuracy. Experiments with 11 other words using collocation seeds result in an average accuracy of 96 percent. While <ref> (Yarowsky, 1995) </ref> does not discuss distinguishing more than 2 senses of a word, there is no immediate reason to doubt that the "one sense per collocation" rule (Yarowsky, 1993) would still hold for a larger number of senses. <p> The set of context vectors for the word to be disambiguated are then clustered, and the clusters are manually sense tagged. The features used in this work are complex and difficult to interpret and it isn't clear that this complexity is required. <ref> (Yarowsky, 1995) </ref> compares his method to (Schutze, 1992) and shows that for four words the former performs significantly better in distinguishing between two senses. Other clustering approaches to word-sense disambiguation have been based on measures of semantic distance defined with respect to a semantic network such as WordNet. <p> Subject-specific neighborhoods are composed of words having at least one sense marked with that subject code. 7.3 EM algorithm The only other application of the EM algorithm to word-sense disambiguation is described in <ref> (Gale, Church, and Yarowsky, 1995) </ref>. There the EM algorithm is used as part of a supervised learning algorithm to distinguish city names from people's names. A narrow window of context, one or two words to either side, was found to perform better than wider windows.
Reference: <author> Zipf, G. </author> <year> 1935. </year> <title> The Psycho-Biology of Language. </title> <publisher> Houghton Mi*in, </publisher> <address> Boston, MA. </address>
Reference-contexts: Implicit in Ward's method is the assumption that the sample comes from a mixture of normal distributions. While NLP data is typically not well characterized by a normal distribution (see, e.g. <ref> (Zipf, 1935) </ref>, (Pedersen, Kayaalp, and Bruce, 1996)), there is evidence that our data, when represented by a dissimilarity matrix, can be adequately characterized by a normal distribution. <p> Here, our best performance using a larger sample with a natural distribution of senses is only an increase of 20 percentage points over the accuracy of the majority classifier. Because skewed distributions are common in lexical work <ref> (Zipf, 1935) </ref>, they are an important consideration in formulating disambiguation experiments. In future work, we will investigate procedures for feature selection that are more sensitive to minority classes.
References-found: 42

