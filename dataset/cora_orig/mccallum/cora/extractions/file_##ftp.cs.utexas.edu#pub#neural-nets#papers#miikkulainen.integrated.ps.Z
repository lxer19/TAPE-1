URL: file://ftp.cs.utexas.edu/pub/neural-nets/papers/miikkulainen.integrated.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/nn/pages/publications/abstracts.html
Root-URL: http://www.cs.utexas.edu
Email: risto@cs.utexas.edu  
Title: Integrated Connectionist Models: Building AI Systems on Subsymbolic Foundations  
Author: Risto Miikkulainen 
Address: Austin, Austin, TX 78712-1188  
Affiliation: Department of Computer Sciences The University of Texas at  
Abstract-found: 0
Intro-found: 1
Reference: <author> Alvarado, S., Dyer, M. G., and Flowers, M. </author> <year> 1990. </year> <title> Argument representation for editorial text. </title> <booktitle> Knowledge-Based Systems 3, </booktitle> <pages> 87-107. </pages>
Reference: <author> Baddeley, A. D. </author> <year> 1976. </year> <title> The psychology of memory. </title> <publisher> Basic Books, </publisher> <address> New York. </address>
Reference: <author> Caramazza, A. </author> <year> 1988. </year> <title> Some aspects of language processing revealed through the analysis of acquired aphasia: The lexical system. </title> <booktitle> Annual Review of Neuroscience 11, </booktitle> <pages> 395-421. </pages>
Reference: <author> Coltheart, M., Patterson, K., and Marshall, J. C. (eds.) </author> <year> 1988. </year> <title> Deep dyslexia, second edition. </title> <publisher> Routledge and Kegan Paul, </publisher> <address> London; Boston. </address>
Reference-contexts: These types of errors are common in human deep dyslexia as well <ref> (Coltheart et al., 1988) </ref>. 6 FIGURE 2: The lexicon. The lexical input symbol DOG is translated into the semantic representation of the concept dog. The representations are vectors of gray-scale values between 0.0 and 1.0, stored in the weights of the units.
Reference: <author> Cullingford, R. E. </author> <year> 1978. </year> <title> Script application: Computer understanding of newspaper stories. </title> <type> Ph.D thesis, </type> <institution> Department of Computer Science, Yale University, New Haven, </institution> <type> Connecticut. (Technical Report 116). </type>
Reference: <author> Dolan, C. P. </author> <year> 1989. </year> <title> Tensor manipulation networks: Connectionist and symbolic approaches to comprehension, learning and planning. </title> <type> Ph.D thesis, </type> <institution> Computer Science Department, University of California, Los Angeles. </institution> <note> (Technical Report UCLA-AI-89-06). </note>
Reference: <author> Dyer, M. G. </author> <year> 1983. </year> <title> In-depth understanding: A computer model of integrated processing for narrative comprehension. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts. </address>
Reference: <author> Dyer, M. G. </author> <year> 1991. </year> <title> Symbolic neuroengineering for natural language processing: A multilevel research approach. </title> <editor> In: Barnden, J. A., and Pollack, J. B. (eds.), </editor> <title> High-level connectionist models, </title> <publisher> Ablex, </publisher> <address> Norwood, New Jersey, </address> <pages> pp. 32-86. </pages>
Reference-contexts: Each module performs a specific subtask, such as parsing a sentence or generating an answer to a question. All these modules have the same basic architecture. The FGREP mechanism (Forming Global Representations with Extended backPropagation) <ref> (Miikkulainen and Dyer, 1991) </ref> is based on a basic three-layer backward error propagation network, with the I/O representation patterns stored in an external lexicon 1 (Figure 3). The input and output layers of the network are divided into assemblies. <p> If there is not enough information to fill a role, the most likely filler is selected and maintained throughout paraphrase generation. Thus, DISCERN performs plausible role bindings|an essential task in high-level inferencing and one that has been postulated to be very difficult for parallel distributed processing systems to achieve <ref> (Dyer, 1991) </ref>. The system extracts the appropriate inferences automatically, based on statistical correlations in the input examples. This differs from the symbolic models of script processing (Schank and Abelson, 1977; Cullingford, 1978; Dyer, 1983), where the inferences are based on handcrafted rules and representations of the script.
Reference: <author> Dyer, M. G., Cullingford, R. E., and Alvarado, S. </author> <year> 1987. </year> <title> Scripts. </title> <editor> In: Shapiro, S. C. (ed.), </editor> <booktitle> Encyclopedia of artificial intelligence, </booktitle> <publisher> Wiley, </publisher> <address> New York, </address> <pages> pp. 980-994. </pages>
Reference: <author> Elman, J. L. </author> <year> 1990. </year> <title> Finding structure in time. </title> <booktitle> Cognitive Science 14, </booktitle> <pages> 179-211. </pages>
Reference: <author> Elman, J. L. </author> <year> 1991. </year> <title> Incremental learning, or The importance of starting small. </title> <booktitle> In: Proceedings of the 13th Annual Conference of the Cognitive Science Society, </booktitle> <publisher> Erlbaum, </publisher> <address> Hillsdale, New Jersey, </address> <pages> pp. 443-448. </pages>
Reference: <author> Feldman, J. A. </author> <year> 1989. </year> <title> Neural representation of conceptual knowledge. </title> <editor> In: Nadel, L., Cooper, L. A., Culicover, P., and Harnish, R. M. (eds.), </editor> <title> Neural connections, mental computation, </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <pages> pp. 68-103. </pages>
Reference: <author> Fodor, J. A. </author> <year> 1983. </year> <title> Modularity of mind: An essay on faculty psychology. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts. </address>
Reference: <author> Harris, C. L., and Elman, J. L. </author> <year> 1989. </year> <title> Representing variable information with simple recurrent networks. </title> <booktitle> In: Proceedings of the 11th Annual Conference of the Cognitive Science Society, </booktitle> <publisher> Erlbaum, </publisher> <address> Hillsdale, New Jersey, </address> <pages> pp. 635-642. </pages>
Reference: <author> Hinton, G. E. </author> <year> 1990. </year> <title> Mapping part-whole hierarchies into connectionist networks. </title> <booktitle> Artificial Intelligence 46, </booktitle> <pages> 47-75. </pages>
Reference: <author> Kohonen, T. </author> <year> 1989. </year> <title> Self-organization and associative memory, third edition. </title> <publisher> Springer, </publisher> <address> Berlin; Heidelberg; New York. </address>
Reference-contexts: The semantic representations stand for distinct meanings. They are developed automatically by the system while it is learning the processing task. The lexicon (Miikkulainen, 1990a) stores the lexical and semantic representations and translates between them (Figure 2). It is implemented as two feature maps <ref> (Kohonen, 1989, 1990) </ref>, one lexical and the other semantic. Words whose lexical forms are similar, such as BALL and DOLL, are represented by nearby units in the lexical map. In the semantic map, words with similar semantic content, such as predator and prey, are mapped near each other.
Reference: <author> Kohonen, T. </author> <year> 1990. </year> <title> The self-organizing map. </title> <booktitle> Proceedings of the IEEE 78, </booktitle> <pages> 1464-1480. </pages>
Reference: <author> Kolodner, J. L. </author> <year> 1984. </year> <title> Retrieval and organizational strategies in conceptual memory: A computer model. </title> <publisher> Erlbaum, </publisher> <address> Hillsdale, New Jersey. </address> <note> 19 Laird, </note> <author> J. E., Newell, A., and Rosenbloom, P. S. </author> <year> 1987. </year> <title> SOAR: An architecture for general intelligence. </title> <booktitle> Artificial Intelligence 33, </booktitle> <pages> 1-64. </pages>
Reference-contexts: Symbolic systems have been quite successful, for example, in modeling in-depth natural language processing (Dyer, 1983; Schank and Abelson, 1977), episodic memory <ref> (Kolodner, 1984) </ref>, and problem solving (Laird et al., 1987; Newell, 1991). However, there are two major issues that the symbolic approach does not address: the statistical (intuitive) nature of certain cognitive processes, and the physical implementation of the cognitive system.
Reference: <author> Lebowitz, M. </author> <year> 1980. </year> <title> Generalization and memory in an integrated understanding system. </title> <type> Ph.D thesis, </type> <institution> Department of Computer Science, Yale University, </institution> <address> New Haven, Connecticut. </address> <note> Research Report 186. </note>
Reference: <author> McCarthy, R. A., and Warrington, E. K. </author> <year> 1990. </year> <title> Cognitive neuropsychology: A clinical introduction. </title> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference: <author> McClelland, J. L., Rumelhart, D. E., </author> <title> and the PDP Research Group 1986. Parallel distributed processing: Explorations in the microstructure of cognition, Vol. 2: Psychological and biological models. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts. </address>
Reference-contexts: In many cases, the problem is reduced to learning a simple mapping. This suits modeling isolated low-level tasks, such as learning past tense forms of verbs <ref> (Rumelhart and McClelland, 1986) </ref> or pronunciation of words (Sejnowski and Rosenberg, 1987). However, modeling higher-level cognitive tasks with simple pattern transformation networks has been infeasible, for three reasons: 1. High-level tasks are often composites of distinct subtasks.
Reference: <author> Miikkulainen, R. </author> <year> 1990a. </year> <title> A distributed feature map model of the lexicon. </title> <booktitle> In: Proceedings of the 12th Annual Conference of the Cognitive Science Society, </booktitle> <publisher> Erlbaum, </publisher> <address> Hillsdale, New Jersey, </address> <pages> pp. 447-454. </pages>
Reference-contexts: They remain fixed throughout the training and performance of DISCERN. The semantic representations stand for distinct meanings. They are developed automatically by the system while it is learning the processing task. The lexicon <ref> (Miikkulainen, 1990a) </ref> stores the lexical and semantic representations and translates between them (Figure 2). It is implemented as two feature maps (Kohonen, 1989, 1990), one lexical and the other semantic. Words whose lexical forms are similar, such as BALL and DOLL, are represented by nearby units in the lexical map.
Reference: <author> Miikkulainen, R. </author> <year> 1990b. </year> <title> A PDP architecture for processing sentences with relative clauses. </title>
Reference-contexts: The third problem, that of dynamic inferencing (Touretzky, 1991), is evident in such areas as processing relative clauses. A sentence-processing network may be able to generalize to different versions of the same sentence structure, but not to new recursive clause structures <ref> (Miikkulainen, 1990b) </ref>. It cannot make inferences by dynamically combining processing knowledge it has previously seen only in separate situations. All these three problems originate from the use of distributed neural networks as statistical pattern transformers.
Reference: <editor> In: Karlgren, H. (ed.), </editor> <booktitle> Proceedings of the 13th International Conference on Computational Linguistics, </booktitle> <address> Yliopistopaino, Helsinki, Finland, </address> <pages> pp. 201-206. </pages>
Reference: <author> Miikkulainen, R. </author> <year> 1990c. </year> <title> Script recognition with hierarchical feature maps. </title> <booktitle> Connection Science 2, </booktitle> <pages> 83-101. </pages>
Reference-contexts: This particular input story representation is classified as an instance of the restaurant script (top level) and fancy-restaurant track (middle level), with role bindings customer=John, food=lobster, restaurant=MaMaison, tip=big (i.e., JLMB, bottom level). 7 Episodic memory The episodic memory in DISCERN is a hierarchical feature map system <ref> (Miikkulainen, 1990c) </ref> combined with the trace feature map mechanism (Miikkulainen, 1992).
Reference: <author> Miikkulainen, R. </author> <year> 1992. </year> <title> Trace feature map: A model of episodic associative memory. </title> <booktitle> Biological Cybernetics 67, </booktitle> <pages> 273-282. </pages>
Reference-contexts: is classified as an instance of the restaurant script (top level) and fancy-restaurant track (middle level), with role bindings customer=John, food=lobster, restaurant=MaMaison, tip=big (i.e., JLMB, bottom level). 7 Episodic memory The episodic memory in DISCERN is a hierarchical feature map system (Miikkulainen, 1990c) combined with the trace feature map mechanism <ref> (Miikkulainen, 1992) </ref>. The map hierarchy provides the organization for the memory, and the trace feature map technique implements storage and retrieval of memory traces. 7.1 Map hierarchy The feature map hierarchy is a pyramid organized according to the hierarchical taxonomy of script-based stories (Figure 5).
Reference: <author> Miikkulainen, R. </author> <year> 1993. </year> <title> Subsymbolic natural language processing: An integrated model of scripts, lexicon, and memory. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts. </address>
Reference: <author> Miikkulainen, R., and Dyer, M. G. </author> <year> 1989. </year> <title> A modular neural network architecture for sequential paraphrasing of script-based stories. </title> <booktitle> In: Proceedings of the International Joint Conference on Neural Networks (Washington, DC), Vol. II, IEEE, </booktitle> <address> Piscataway, New Jersey, </address> <pages> pp. 49-56. </pages>
Reference: <author> Miikkulainen, R., and Dyer, M. G. </author> <year> 1991. </year> <title> Natural language processing with modular neural networks and distributed lexicon. </title> <booktitle> Cognitive Science 15, </booktitle> <pages> 343-399. </pages>
Reference-contexts: Each module performs a specific subtask, such as parsing a sentence or generating an answer to a question. All these modules have the same basic architecture. The FGREP mechanism (Forming Global Representations with Extended backPropagation) <ref> (Miikkulainen and Dyer, 1991) </ref> is based on a basic three-layer backward error propagation network, with the I/O representation patterns stored in an external lexicon 1 (Figure 3). The input and output layers of the network are divided into assemblies.
Reference: <author> Minsky, M. </author> <year> 1985. </year> <title> Society of mind. </title> <publisher> Simon & Schuster, </publisher> <address> New York. </address>
Reference: <author> Newell, A. </author> <year> 1980. </year> <title> Physical symbol systems. </title> <booktitle> Cognitive Science 4, </booktitle> <pages> 135-183. </pages>
Reference-contexts: Artificial Intelligence and Neural Networks: Steps toward Principled Integration. New York: Academic Press. 2 Why subsymbolic AI? Symbolic artificial intelligence is founded on the hypothesis that symbol manipulation is both necessary and sufficient for intelligence <ref> (Newell, 1980) </ref>. Symbolic systems have been quite successful, for example, in modeling in-depth natural language processing (Dyer, 1983; Schank and Abelson, 1977), episodic memory (Kolodner, 1984), and problem solving (Laird et al., 1987; Newell, 1991).
Reference: <author> Newell, A. </author> <year> 1991. </year> <title> Unified theories of cognition. </title> <publisher> Harvard University Press, </publisher> <address> Cambridge, Mas-sachusetts. </address>
Reference: <author> Pollack, J. B. </author> <year> 1987. </year> <title> Cascaded back-propagation on dynamic connectionist networks. </title> <booktitle> In: Proceedings of the Ninth Annual Conference of the Cognitive Science Society, </booktitle> <publisher> Erlbaum, </publisher> <address> Hillsdale, New Jersey, </address> <pages> pp. 391-404. </pages>
Reference: <author> Pollack, J. B. </author> <year> 1990. </year> <title> Recursive distributed representations. </title> <booktitle> Artificial Intelligence 46, </booktitle> <pages> 77-105. </pages>
Reference: <author> Postman, L. </author> <year> 1971. </year> <title> Transfer, interference and forgetting. </title> <editor> In: Kling, J. W., and Riggs, L. A. (eds.), </editor> <booktitle> Woodworth and Schlosberg's experimental psychology, third edition, </booktitle> <publisher> Holt, Rinehart and Winston, </publisher> <address> New York, </address> <pages> pp. 1019-1132. </pages>
Reference: <author> Reeves, J. F. </author> <year> 1991. </year> <title> Computational morality: A process model of belief conflict and resolution for story understanding. </title> <type> Ph.D thesis, </type> <institution> Computer Science Department, University of California, Los Angeles. </institution> <note> (Technical Report UCLA-AI-91-05). 20 Rumelhart, </note> <author> D. E., Hinton, G. E., and McClelland, J. L. </author> <year> 1986a. </year> <title> A general framework for parallel distributed processing. </title> <editor> In: Rumelhart, D. E., and McClelland, J. L. (eds.), </editor> <booktitle> Parallel distributed processing: Explorations in the microstructure of cognition, </booktitle> <volume> Vol. 1: </volume> <booktitle> Foundations, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <pages> pp. 45-76. </pages>
Reference: <author> Rumelhart, D. E., Hinton, G. E., and Williams, R. J. </author> <year> 1986b. </year> <title> Learning internal representations by error propagation. </title> <editor> In: Rumelhart, D. E., and McClelland, J. L. (eds.), </editor> <booktitle> Parallel distributed processing: Explorations in the microstructure of cognition, </booktitle> <volume> Vol. 1: </volume> <booktitle> Foundations, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <pages> pp. 318-362. </pages>
Reference-contexts: A routing network forms each input pattern and the corresponding teaching pattern by concatenating the semantic lexicon entries of the input and teaching items. The network learns the processing task by adapting the connection weights according to the standard backpropagation procedure <ref> (Rumelhart et al., 1986b, pp. 327-329) </ref>. At the end of each cycle, the current input representations are modified at the input layer based on the error signal.
Reference: <author> Rumelhart, D. E., and McClelland, J. L. </author> <year> 1986. </year> <title> On learning past tenses of English verbs. </title>
Reference-contexts: In many cases, the problem is reduced to learning a simple mapping. This suits modeling isolated low-level tasks, such as learning past tense forms of verbs <ref> (Rumelhart and McClelland, 1986) </ref> or pronunciation of words (Sejnowski and Rosenberg, 1987). However, modeling higher-level cognitive tasks with simple pattern transformation networks has been infeasible, for three reasons: 1. High-level tasks are often composites of distinct subtasks.
Reference: <editor> In: Rumelhart, D. E., and McClelland, J. L. (eds.), </editor> <booktitle> Parallel distributed processing: explorations in the microstructure of cognition, </booktitle> <volume> Vol. </volume> <month> 2: </month> <title> Psychological and biological models, </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <pages> pp. 216-271. </pages>
Reference: <editor> Rumelhart, D. E., McClelland, J. L., </editor> <booktitle> and the PDP Research Group 1986c. Parallel distributed processing: Explorations in the microstructure of cognition, </booktitle> <volume> Vol. 1: </volume> <booktitle> Foundations. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts. </address>
Reference: <author> St. John, M. F. </author> <year> 1992. </year> <title> The story gestalt: A model of knowledge-intensive processes in text comprehension. </title> <booktitle> Cognitive Science 16, </booktitle> <pages> 271-306. </pages>
Reference: <author> St. John, M. F., and McClelland, J. L. </author> <year> 1990. </year> <title> Learning and applying contextual constraints in sentence comprehension. </title> <booktitle> Artificial Intelligence 46, </booktitle> <pages> 217-258. </pages>
Reference: <author> Schank, R. C., and Abelson, R. P. </author> <year> 1977. </year> <title> Scripts, plans, goals, and understanding: An inquiry into human knowledge structures. </title> <publisher> Erlbaum, </publisher> <address> Hillsdale, New Jersey. </address>
Reference: <author> Sejnowski, T. J., and Rosenberg, C. R. </author> <year> 1987. </year> <title> Parallel networks that learn to pronounce English text. </title> <booktitle> Complex Systems 1, </booktitle> <pages> 145-168. </pages>
Reference-contexts: In many cases, the problem is reduced to learning a simple mapping. This suits modeling isolated low-level tasks, such as learning past tense forms of verbs (Rumelhart and McClelland, 1986) or pronunciation of words <ref> (Sejnowski and Rosenberg, 1987) </ref>. However, modeling higher-level cognitive tasks with simple pattern transformation networks has been infeasible, for three reasons: 1. High-level tasks are often composites of distinct subtasks. They consist of several interacting subprocesses, such as parsing language, generating language, memory storage, memory retrieval, and reasoning.
Reference: <author> Shallice, T. </author> <year> 1988. </year> <title> From neuropsychology to mental structure. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, United Kingdom. </address>
Reference: <author> Simon, H. A. </author> <year> 1969. </year> <booktitle> The sciences of the artificial. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts. </address>
Reference: <author> Smolensky, P. </author> <year> 1988. </year> <title> On the proper treatment of connectionism. </title> <booktitle> Behavioral and Brain Sciences 11, </booktitle> <pages> 1-74. </pages>
Reference: <author> Smolensky, P. </author> <year> 1990. </year> <title> Tensor product variable binding and the representation of symbolic structures in connectionist systems. </title> <booktitle> Artificial Intelligence 46, </booktitle> <pages> 159-216. </pages>
Reference: <author> Touretzky, D. S. </author> <year> 1991. </year> <title> Connectionism and compositional semantics. </title> <editor> In: Barnden, J. A., and Pollack, J. B. (eds.), </editor> <title> High-level connectionist models, </title> <publisher> Ablex, </publisher> <address> Norwood, New Jersey, </address> <pages> pp. 17-31. </pages>
Reference-contexts: John, 1992). Exceptions are simply overridden. The network has no representation for all-or-none role bindings, and as a result it cannot process truly novel inputs according to a symbolic-like higher-level rule. The third problem, that of dynamic inferencing <ref> (Touretzky, 1991) </ref>, is evident in such areas as processing relative clauses. A sentence-processing network may be able to generalize to different versions of the same sentence structure, but not to new recursive clause structures (Miikkulainen, 1990b).
References-found: 49

