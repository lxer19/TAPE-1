URL: ftp://ai.iit.nrc.ca/pub/iit-papers/NRC-35058.ps.Z
Refering-URL: http://ai.iit.nrc.ca/cgi-bin/ftpsearch/?turney
Root-URL: 
Email: turney@ai.iit.nrc.ca  
Title: Exploiting Context When Learning to Classify  
Author: Peter D. Turney 
Address: Canada, Ottawa, Ontario, Canada, K1A 0R6  
Affiliation: Knowledge Systems Laboratory, Institute for Information Technology National Research Council  
Abstract: This paper addresses the problem of classifying observations when features are context-sensitive, specifically when the testing set involves a context that is different from the training set. The paper begins with a precise definition of the problem, then general strategies are presented for enhancing the performance of classification algorithms on this type of problem. These strategies are tested on two domains. The first domain is the diagnosis of gas turbine engines. The problem is to diagnose a faulty engine in one context, such as warm weather, when the fault has previously been seen only in another context, such as cold weather. The second domain is speech recognition. The problem is to recognize words spoken by a new speaker, not represented in the training set. For both domains, exploiting context results in substantially more accurate classification. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> D.W. Aha, D. Kibler, and M.K. Albert: </author> <title> Instance-based learning algorithms, </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 37-66, </pages> <year> 1991. </year>
Reference-contexts: General strategies for exploiting contextual information are given in Section 3. The strategies are tested on two domains. Section 4 shows how contextual information can improve the diagnosis of faults in an aircraft gas turbine engine. The classification algorithms used on the engine data were instance-based learning (IBL) <ref> [1, 2, 3] </ref> and multivariate linear regression (MLR) [4]. Both algorithms benefit from contextual information. Section 5 shows how context can be used to improve speech recognition. The speech recognition data were classified using IBL and cascade-correlation [5]. Again, both algorithms benefit from exploiting context. <p> One set was used as the training set and the other as the testing set, then the sets were swapped and the process was repeated. Thus the sample size for testing purposes is 242. The data were analyzed using two classification algorithms, a form of instance-based learning (IBL) <ref> [1, 2, 3] </ref> and multivariate linear regression (MLR) [4]. IBL and MLR were also used to preprocess the data by contextual normalization [7].
Reference: 2. <author> D. Kibler, D.W. Aha, and M.K. Albert: </author> <title> Instance-based prediction of real-valued attributes, </title> <journal> Computational Intelligence, </journal> <volume> 5, </volume> <pages> 51-57, </pages> <year> 1989. </year>
Reference-contexts: General strategies for exploiting contextual information are given in Section 3. The strategies are tested on two domains. Section 4 shows how contextual information can improve the diagnosis of faults in an aircraft gas turbine engine. The classification algorithms used on the engine data were instance-based learning (IBL) <ref> [1, 2, 3] </ref> and multivariate linear regression (MLR) [4]. Both algorithms benefit from contextual information. Section 5 shows how context can be used to improve speech recognition. The speech recognition data were classified using IBL and cascade-correlation [5]. Again, both algorithms benefit from exploiting context. <p> One set was used as the training set and the other as the testing set, then the sets were swapped and the process was repeated. Thus the sample size for testing purposes is 242. The data were analyzed using two classification algorithms, a form of instance-based learning (IBL) <ref> [1, 2, 3] </ref> and multivariate linear regression (MLR) [4]. IBL and MLR were also used to preprocess the data by contextual normalization [7].
Reference: 3. <author> B.V. Dasarathy: </author> <title> Nearest Neighbor Pattern Classification Techniques, (edited collection), </title> <publisher> Los Alamitos, </publisher> <address> CA: </address> <publisher> IEEE Press, </publisher> <year> 1991. </year>
Reference-contexts: General strategies for exploiting contextual information are given in Section 3. The strategies are tested on two domains. Section 4 shows how contextual information can improve the diagnosis of faults in an aircraft gas turbine engine. The classification algorithms used on the engine data were instance-based learning (IBL) <ref> [1, 2, 3] </ref> and multivariate linear regression (MLR) [4]. Both algorithms benefit from contextual information. Section 5 shows how context can be used to improve speech recognition. The speech recognition data were classified using IBL and cascade-correlation [5]. Again, both algorithms benefit from exploiting context. <p> One set was used as the training set and the other as the testing set, then the sets were swapped and the process was repeated. Thus the sample size for testing purposes is 242. The data were analyzed using two classification algorithms, a form of instance-based learning (IBL) <ref> [1, 2, 3] </ref> and multivariate linear regression (MLR) [4]. IBL and MLR were also used to preprocess the data by contextual normalization [7].
Reference: 4. <author> N.R. Draper and H. Smith: </author> <title> Applied Regression Analysis, (second edition), </title> <address> New York, NY: </address> <publisher> John Wiley & Sons, </publisher> <year> 1981. </year>
Reference-contexts: The strategies are tested on two domains. Section 4 shows how contextual information can improve the diagnosis of faults in an aircraft gas turbine engine. The classification algorithms used on the engine data were instance-based learning (IBL) [1, 2, 3] and multivariate linear regression (MLR) <ref> [4] </ref>. Both algorithms benefit from contextual information. Section 5 shows how context can be used to improve speech recognition. The speech recognition data were classified using IBL and cascade-correlation [5]. Again, both algorithms benefit from exploiting context. <p> Thus the sample size for testing purposes is 242. The data were analyzed using two classification algorithms, a form of instance-based learning (IBL) [1, 2, 3] and multivariate linear regression (MLR) <ref> [4] </ref>. IBL and MLR were also used to preprocess the data by contextual normalization [7].
Reference: 5. <author> S.E. Fahlman and C. Lebiere: </author> <title> The Cascade-Correlation Learning Architecture, </title> <type> (technical report), </type> <institution> CMU-CS-90-100, Pittsburgh, PA: Carnegie-Mellon University, </institution> <year> 1991. </year>
Reference-contexts: The classification algorithms used on the engine data were instance-based learning (IBL) [1, 2, 3] and multivariate linear regression (MLR) [4]. Both algorithms benefit from contextual information. Section 5 shows how context can be used to improve speech recognition. The speech recognition data were classified using IBL and cascade-correlation <ref> [5] </ref>. Again, both algorithms benefit from exploiting context. The work presented here is compared with related work by other researchers in Section 6. Future work is discussed in Section 7. Finally, Section 8 presents the conclusion. <p> These results show that there is a form of synergy here, since the sum of the improvements of each strategy used separately is less than the improvement of the three strategies used together ( % vs. %). The three strategies were also tested with cascade-correlation <ref> [5] </ref>. Because of the time required for training the cascade-correlation algorithm, results were gathered for only two cases: With no preprocessing, cascade-correlation correctly classified 216 observations (47%). With preprocessing by all three strategies, cascade-correlation correctly classified 236 observations (51%). <p> Acknowledgments The gas turbine engine data and engine expertise were provided by the Engine Laboratory of the NRC, with funding from DND. The vowel data were obtained from the University of California data repository (ftp ics.uci.edu, directory /pub/machine-learning-databases) [10]. The cascade-correlation <ref> [5] </ref> software was obtained from Carnegie-Mellon University (ftp pt.cs.cmu.edu, directory /afs/cs/project/connect/code). The author wishes to thank Rob Wylie and Peter Clark of the NRC and two anonymous referees of ECML-93 for their helpful comments on this paper.
Reference: 6. <author> A.J. Katz, M.T. Gately, and D.R. Collins: </author> <title> Robust classifiers without robust features, </title> <journal> Neural Computation, </journal> <volume> 2, </volume> <pages> 472-479, </pages> <year> 1990. </year>
Reference-contexts: When is unknown, it is often possible to use background knowledge to distinguish primary, contextual, and irrelevant features. Examples of this use of background knowledge will be presented later in the paper. 3 Strategies for Exploiting Context Katz et al. <ref> [6] </ref> list four strategies for using contextual information when classifying: 1. Contextual normalization: The contextual features can be used to normalize the context-sensitive primary features, prior to classification. The intent is to process context-sensitive features in a way that reduces their sensitivity to the context. 2. <p> With preprocessing by all three strategies, cascade-correlation correctly classified 236 observations (51%). This shows that contextual information can be of benefit for both neural networks and nearest neighbor pattern recognition. 6 Related Work The work described here is most closely related to <ref> [6] </ref>. However, [6] did not give a precise definition of the distinction between contextual features (their terminology: parameters or global features) and primary features (their terminology: features). They examined only contextual classifier selection, using neural networks to classify images, with context such as lighting. <p> With preprocessing by all three strategies, cascade-correlation correctly classified 236 observations (51%). This shows that contextual information can be of benefit for both neural networks and nearest neighbor pattern recognition. 6 Related Work The work described here is most closely related to <ref> [6] </ref>. However, [6] did not give a precise definition of the distinction between contextual features (their terminology: parameters or global features) and primary features (their terminology: features). They examined only contextual classifier selection, using neural networks to classify images, with context such as lighting.
Reference: 7. <author> P.D. Turney and M. </author> <title> Halasz: Contextual normalization applied to aircraft gas turbine engine diagnosis, (in press), </title> <journal> Journal of Applied Intelligence, </journal> <year> 1993. </year>
Reference-contexts: The observations fall in eight classes: seven classes of deliberately implanted faults and a healthy class <ref> [7] </ref>. The amount of thrust produced by an engine is a primary feature for diagnosing faults in the engine. The exterior air temperature is a contextual feature, since the engines performance is sensitive to the exterior air temperature. <p> Thus the sample size for testing purposes is 242. The data were analyzed using two classification algorithms, a form of instance-based learning (IBL) [1, 2, 3] and multivariate linear regression (MLR) [4]. IBL and MLR were also used to preprocess the data by contextual normalization <ref> [7] </ref>. <p> The values of and were estimated using IBL and MLR, trained with healthy observations (spanning a range of ambient conditions) <ref> [7] </ref>. Table 1 (derived from Table 5 in [7]) shows the results of this experiment. For IBL, the average score without contextual normalization is 42% and the average score with contextual normalization is 55%, an improvement of 13%. For MLR, the improvement is 7%. <p> The values of and were estimated using IBL and MLR, trained with healthy observations (spanning a range of ambient conditions) <ref> [7] </ref>. Table 1 (derived from Table 5 in [7]) shows the results of this experiment. For IBL, the average score without contextual normalization is 42% and the average score with contextual normalization is 55%, an improvement of 13%. For MLR, the improvement is 7%. <p> For IBL, the average score without contextual normalization is 42% and the average score with contextual normalization is 55%, an improvement of 13%. For MLR, the improvement is 7%. According to the Student t-test, contextual normalization is significantly better than all of the alternatives that were examined <ref> [7] </ref>. 5 Speech Recognition This section examines strategies 1, 2, and 5: contextual normalization, contextual expansion, and contextual weighting. The problem is to recognize a vowel spoken by an arbitrary speaker.
Reference: 8. <author> D. Deterding: </author> <title> Speaker Normalization for Automatic Speech Recognition, </title> <type> (Ph.D. thesis), </type> <institution> Cambridge, UK: University of Cambridge, Department of Engineering, </institution> <year> 1989. </year>
Reference-contexts: The problem is to recognize a vowel spoken by an arbitrary speaker. There are ten continuous primary features (derived from spectral data) and two discrete contextual features (the speakers identity and sex). The observations fall in eleven classes (eleven different vowels) <ref> [8] </ref>. For speech recognition, spectral data is a primary feature for recognizing a vowel. The sex of the speaker is a contextual feature, since we can achieve better recognition by exploiting the fact that a mans voice tends to sound different from a womans voice. <p> This work is also related to work in speech recognition on speaker normalization <ref> [8] </ref>. However, the work on speaker normalization tends to be specific to speech recognition.
Reference: 9. <author> A.J. Robinson: </author> <title> Dynamic Error Propagation Networks, </title> <type> (Ph.D. thesis), </type> <institution> Cambridge, UK: University of Cambridge, Department of Engineering, </institution> <year> 1989. </year>
Reference-contexts: Each of the eleven vowels is spoken six times by each speaker. The training set is from four male and four female speakers ( observations). The testing set is from four new male and three new female speakers ( observations). Using a wide variety of neural network algorithms, Robinson <ref> [9] </ref> achieved accuracies ranging from 33% to 55% correct on the testing set. The mean score was 49%, with a standard deviation of 6%.
Reference: 10. <author> P.M. Murphy and D.W. Aha: </author> <title> UCI Repository of Machine Learning Databases, </title> <address> Irvine, CA: </address> <institution> University of California, Department of Information and Computer Science, </institution> <year> 1991. </year>
Reference-contexts: Acknowledgments The gas turbine engine data and engine expertise were provided by the Engine Laboratory of the NRC, with funding from DND. The vowel data were obtained from the University of California data repository (ftp ics.uci.edu, directory /pub/machine-learning-databases) <ref> [10] </ref>. The cascade-correlation [5] software was obtained from Carnegie-Mellon University (ftp pt.cs.cmu.edu, directory /afs/cs/project/connect/code). The author wishes to thank Rob Wylie and Peter Clark of the NRC and two anonymous referees of ECML-93 for their helpful comments on this paper.
References-found: 10

