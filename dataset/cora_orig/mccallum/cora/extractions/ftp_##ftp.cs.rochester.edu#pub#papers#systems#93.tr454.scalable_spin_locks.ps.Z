URL: ftp://ftp.cs.rochester.edu/pub/papers/systems/93.tr454.scalable_spin_locks.ps.Z
Refering-URL: http://www.cs.rochester.edu/u/scott/synchronization/pubs.html
Root-URL: 
Title: Scalable Spin Locks for Multiprogrammed Systems  
Author: Robert W. Wisniewski, Leonidas Kontothanassis, and Michael L. Scott 
Note: This work was supported in part by NSF Institutional Infrastructure award number CDA-8822724, NSF grant number CCR-9005633, and ONR research contract number N00014-92-J-1801 (in conjunction with the ARPA Research in Information Science and Technology|High Performance Computing, Software Science and Technology program, ARPA Order No. 8930).  
Date: September 1993  
Address: Rochester, NY 14627-0226  Rochester, New York 14627  
Affiliation: Computer Science Department University of Rochester  The University of Rochester Computer Science Department  
Abstract: Synchronization primitives for large scale multiprocessors need to provide low latency and low contention to achieve good performance. Queue-based locks (implemented in software with fetch and instructions) can greatly reduce contention and improve overall performance by arranging for processors to spin only on local locations. Unfortunately, queued locks exhibit poor behavior in the presence of multiprogramming: a process near the end of the queue, in addition to waiting for any process that is preempted during its critical section, must also wait for any preempted processes ahead of it in the queue. To solve this problem, we present two queue-based locks that recover from in-queue preemption. The first lock employs the kernel interface of the NYU Symunix project. The second employs an extended interface that shares information in both directions across the user-kernel boundary, resulting in simpler code and better performance. We describe experiments with these locks in both real and synthetic applications on Silicon Graphics and Kendall Square multiprocessors. Results demonstrate the feasibility of high performance software locks with multiprogramming on scalable systems, and show competitive behavior on smaller, bus based machines. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. E. Anderson. </author> <title> The Performance of Spin Lock Alternatives for Shared-Memory Multiprocessors. </title> <journal> IEEETPDS, </journal> <volume> 1(1) </volume> <pages> 6-16, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: To address the first of these problems, several researchers have devised queue-based locks in which every process spins on a different, local location, essentially eliminating contention <ref> [1, 7, 12] </ref>. To address the second problem, several operating systems have incorporated schemes in which applications communicate with the kernel scheduler to prevent [6] or recover from [3] preemption in a critical section, or to avoid entering a critical section when preemption is imminent [11]. <p> When two processes spin on the same location, coherence operations or remote memory references (depending on machine type) create substantial amounts of contention for memory and for the processor-memory interconnect. The key to good performance is therefore to minimize active sharing. 1 The queue-based spin locks of Anderson <ref> [1] </ref> and of Graunke and Thakkar [7] minimize active sharing on coherently-cached machines by arranging for every waiting processor to spin on a different element of an array. Each element of the array lies in a separate cache line, which migrates to the spinning processor.
Reference: [2] <author> T. E. Anderson. </author> <title> Operating System Support for High Performance Multiprocessing. </title> <editor> Ph. D. dissertation, TR 91-08-10, UWASHCSED, </editor> <month> August </month> <year> 1991. </year>
Reference-contexts: A growing body of evidence <ref> [2, 5, 10, 15, 16] </ref> suggests that throughput is maximized by a processor-partitioned environment in which each application runs exactly one process per processor. In such an environment, or in one that employs coscheduling, all the processes that share a given lock will always run simultaneously.
Reference: [3] <author> T. E. Anderson, B. N. Bershad, E. D. Lazowska, and H. M. Levy. </author> <title> Scheduler Activations: Effective Kernel Support for the User-Level Management of Parallelism. </title> <journal> TOCS, </journal> <volume> 10(1) </volume> <pages> 53-79, </pages> <month> February </month> <year> 1992. </year> <booktitle> Originally presented at the Thirteenth SOSP, </booktitle> <month> 13-16 October </month> <year> 1991. </year>
Reference-contexts: To address the second problem, several operating systems have incorporated schemes in which applications communicate with the kernel scheduler to prevent [6] or recover from <ref> [3] </ref> preemption in a critical section, or to avoid entering a critical section when preemption is imminent [11]. What has not been clear from previous work is how to solve both problems at once. <p> The various mechanisms for dealing with preemption can all be applied in a straightforward manner to programs using (test and )test and set locks, resulting in good performance. Their application to programs using queue-based locks is much less straightforward. None of <ref> [3] </ref>, [6], and [11] discusses queue-based locks, and [12] explicitly recommends non-queue-based locks for multipro-grammed environments. <p> Also, existing on-line algorithms are designed for scenarios in which any waiting process can acquire a lock, and are not directly applicable to queue-based locks. To address the possibility of preemption, several researchers have invented forms of synchronization-sensitive scheduling. The Scheduler Activation proposal of Anderson et al. <ref> [3] </ref> allows a parallel application to recover from untimely preemption. When a processor is taken away from an application, another processor in the same application is given a software interrupt, informing it of the preemp tion.
Reference: [4] <author> D. L. Black. </author> <title> Scheduling Support for Concurrency and Parallelism in the Mach Operating System. </title> <journal> IEEEC, </journal> <volume> 23(5) </volume> <pages> 35-43, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The second processor can then perform a context switch to the preempted process if desired, e.g. to push it through its critical section. In a similar vein, Black's work on Mach <ref> [4] </ref> allows a process to suggest to the scheduler that it be descheduled in favor of some specific other process.
Reference: [5] <author> M. Crovella, P. Das, C. Dubnicki, T. LeBlanc, and E. Markatos. </author> <title> Multiprogramming on Multiprocessors. </title> <booktitle> In PROC of the Third SPDP, </booktitle> <pages> pages 590-597, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: A growing body of evidence <ref> [2, 5, 10, 15, 16] </ref> suggests that throughput is maximized by a processor-partitioned environment in which each application runs exactly one process per processor. In such an environment, or in one that employs coscheduling, all the processes that share a given lock will always run simultaneously.
Reference: [6] <author> J. Edler, J. Lipkis, and E. Schonberg. </author> <title> Process Management for Highly Parallel UNIX Systems. </title> <booktitle> In PROC of the USENIX Workshop on Unix and Supercomputers, </booktitle> <address> Pittsburgh, PA, </address> <month> 26-27 September </month> <year> 1988. </year>
Reference-contexts: To address the second problem, several operating systems have incorporated schemes in which applications communicate with the kernel scheduler to prevent <ref> [6] </ref> or recover from [3] preemption in a critical section, or to avoid entering a critical section when preemption is imminent [11]. What has not been clear from previous work is how to solve both problems at once. <p> The various mechanisms for dealing with preemption can all be applied in a straightforward manner to programs using (test and )test and set locks, resulting in good performance. Their application to programs using queue-based locks is much less straightforward. None of [3], <ref> [6] </ref>, and [11] discusses queue-based locks, and [12] explicitly recommends non-queue-based locks for multipro-grammed environments. <p> We discuss related work in more detail in section 2, touching on scalable spin locks, multiprogramming of multiprocessors, and synchronization-sensitive scheduling. We present our algorithms in section 3, with a focus on scheduler interfaces resembling that of Symunix <ref> [6] </ref>, and locks resembling those of Mellor-Crummey and Scott [12]. We present empirical results in section 4, comparing the performance of a variety of locks on a variety of workloads, applications, and machines. <p> If lock data structures indicate which process holds the lock, a process that spins "too long" may guess that the lock holder is not running, and can offer to give it its processor. Rather than recover from untimely preemption, the Symunix system of Edler et al. <ref> [6] </ref> and the Psyche system of Marsh et al. [11] provide mechanisms to avoid or prevent it. The Symunix scheduler allows a process to request that it not be preempted during a critical section, and will honor that request, within reason.
Reference: [7] <author> G. Graunke and S. Thakkar. </author> <title> Synchronization Algorithms for Shared-Memory Multiprocessors. </title> <journal> IEEEC, </journal> <volume> 23(6) </volume> <pages> 60-69, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: To address the first of these problems, several researchers have devised queue-based locks in which every process spins on a different, local location, essentially eliminating contention <ref> [1, 7, 12] </ref>. To address the second problem, several operating systems have incorporated schemes in which applications communicate with the kernel scheduler to prevent [6] or recover from [3] preemption in a critical section, or to avoid entering a critical section when preemption is imminent [11]. <p> The key to good performance is therefore to minimize active sharing. 1 The queue-based spin locks of Anderson [1] and of Graunke and Thakkar <ref> [7] </ref> minimize active sharing on coherently-cached machines by arranging for every waiting processor to spin on a different element of an array. Each element of the array lies in a separate cache line, which migrates to the spinning processor.
Reference: [8] <author> M. Herlihy. </author> <title> Wait-Free Synchronization. </title> <journal> TOPLAS, </journal> <volume> 13(1) </volume> <pages> 124-149, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: The ability to implement our locks testifies to the flexibility of fetch and instructions. The ease with which such instructions can be implemented, and their utility in other areas (e.g. wait-free data structures <ref> [8] </ref>) makes them a very attractive alternative to special-purpose synchronization hardware. The native locks of the KSR1, for example, are faster than the Smart-Q lock, but not by very much.
Reference: [9] <author> A. R. Karlin, K. Li, M. S. Manasse, and S. Owicki. </author> <title> Empirical Studies of Competitive Spinning for a Shared-Memory Multiprocessor. </title> <booktitle> In PROC of the Thirteenth SOSP, </booktitle> <pages> pages 41-55, </pages> <address> Pacific Grove, CA, </address> <month> 13-16 October </month> <year> 1991. </year>
Reference-contexts: Applications that use locks to protect both long and short critical sections can use on-line adaptive algorithms to guess whether it is better to spin or reschedule in a given situation <ref> [9] </ref>. The possibility of preemption, however, introduces very high variance in the apparent length of critical 2 sections, and makes past behavior an unreliable predictor of whether to spin or block.
Reference: [10] <author> S. T. Leutenegger and M. K. Vernon. </author> <title> Performance of Multiprogrammed Multiprocessor Scheduling Algorithms. </title> <booktitle> In PROC of the 1990 MMCS, </booktitle> <address> Boulder, CO, </address> <month> 22-25 May </month> <year> 1990. </year>
Reference-contexts: A growing body of evidence <ref> [2, 5, 10, 15, 16] </ref> suggests that throughput is maximized by a processor-partitioned environment in which each application runs exactly one process per processor. In such an environment, or in one that employs coscheduling, all the processes that share a given lock will always run simultaneously.
Reference: [11] <author> B. D. Marsh, M. L. Scott, T. J. LeBlanc, and E. P. Markatos. </author> <title> First-Class User-Level Threads. </title> <booktitle> In PROC of the Thirteenth SOSP, </booktitle> <pages> pages 110-121, </pages> <address> Pacific Grove, CA, </address> <month> 14-16 October </month> <year> 1991. </year>
Reference-contexts: To address the second problem, several operating systems have incorporated schemes in which applications communicate with the kernel scheduler to prevent [6] or recover from [3] preemption in a critical section, or to avoid entering a critical section when preemption is imminent <ref> [11] </ref>. What has not been clear from previous work is how to solve both problems at once. The various mechanisms for dealing with preemption can all be applied in a straightforward manner to programs using (test and )test and set locks, resulting in good performance. <p> The various mechanisms for dealing with preemption can all be applied in a straightforward manner to programs using (test and )test and set locks, resulting in good performance. Their application to programs using queue-based locks is much less straightforward. None of [3], [6], and <ref> [11] </ref> discusses queue-based locks, and [12] explicitly recommends non-queue-based locks for multipro-grammed environments. <p> Rather than recover from untimely preemption, the Symunix system of Edler et al. [6] and the Psyche system of Marsh et al. <ref> [11] </ref> provide mechanisms to avoid or prevent it. The Symunix scheduler allows a process to request that it not be preempted during a critical section, and will honor that request, within reason.
Reference: [12] <author> J. M. Mellor-Crummey and M. L. Scott. </author> <title> Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors. </title> <journal> TOCS, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: To address the first of these problems, several researchers have devised queue-based locks in which every process spins on a different, local location, essentially eliminating contention <ref> [1, 7, 12] </ref>. To address the second problem, several operating systems have incorporated schemes in which applications communicate with the kernel scheduler to prevent [6] or recover from [3] preemption in a critical section, or to avoid entering a critical section when preemption is imminent [11]. <p> The various mechanisms for dealing with preemption can all be applied in a straightforward manner to programs using (test and )test and set locks, resulting in good performance. Their application to programs using queue-based locks is much less straightforward. None of [3], [6], and [11] discusses queue-based locks, and <ref> [12] </ref> explicitly recommends non-queue-based locks for multipro-grammed environments. <p> We discuss related work in more detail in section 2, touching on scalable spin locks, multiprogramming of multiprocessors, and synchronization-sensitive scheduling. We present our algorithms in section 3, with a focus on scheduler interfaces resembling that of Symunix [6], and locks resembling those of Mellor-Crummey and Scott <ref> [12] </ref>. We present empirical results in section 4, comparing the performance of a variety of locks on a variety of workloads, applications, and machines. <p> To release the lock, it toggles the state of its own (permanently associated) element, which will in turn be polled by the next processor to acquire the lock. The queue-based spin lock of Mellor-Crummey and Scott <ref> [12] </ref> represents its queue with a distributed linked list instead of an array. Much as in Graunke and Thakkar's lock, each waiting processor uses a fetch and store operation to obtain the address of the list element (if any) associated with the previous holder of the lock.
Reference: [13] <author> J. P. Singh, W. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <journal> CAN, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: The first was a synthetic program that allowed us to extensively explore the parameter space. To verify the results we obtained from the synthetic program, two real programs were run|Cholesky from the SPLASH suite <ref> [13] </ref> and a multiprocessor version of Quicksort. The two programs were good candidates for examining the effectiveness of the locks since this is the only form of synchronization they use.
Reference: [14] <author> M. S. Squillante. </author> <title> Issues in Shared-Memory Multiprocessor Scheduling: A Performance Evaluation. </title> <editor> Ph. D. dissertation, TR 90-10-04, UWASHCSED, </editor> <month> October </month> <year> 1990. </year>
Reference-contexts: Alternatively, taking our lead from Black's work, we could have the releaserer of a lock give its processor to its successor in the lock queue, if that successor were currently blocked. The drawback of this approach is that it entails additional context switches, and violates processor affinity <ref> [14] </ref>. 4 Experiments and Results We studied each lock implementation on three different programs. The first was a synthetic program that allowed us to extensively explore the parameter space.
Reference: [15] <author> A. Tucker and A. Gupta. </author> <title> Process Control and Scheduling Issues for Multiprogrammed Shared-Memory Multiprocessors. </title> <booktitle> In PROC of the Twelfth SOSP, </booktitle> <pages> pages 159-166, </pages> <address> Litchfield Park, AZ, </address> <month> 3-6 December </month> <year> 1989. </year>
Reference-contexts: A growing body of evidence <ref> [2, 5, 10, 15, 16] </ref> suggests that throughput is maximized by a processor-partitioned environment in which each application runs exactly one process per processor. In such an environment, or in one that employs coscheduling, all the processes that share a given lock will always run simultaneously.
Reference: [16] <author> J. Zahorjan and C. McCann. </author> <title> Processor Scheduling in Shared Memory Multiprocessors. </title> <booktitle> In PROC of the 1990 MMCS, </booktitle> <pages> pages 214-225, </pages> <address> Boulder, CO, </address> <month> 22-25 May </month> <year> 1990. </year> <month> 14 </month>
Reference-contexts: A growing body of evidence <ref> [2, 5, 10, 15, 16] </ref> suggests that throughput is maximized by a processor-partitioned environment in which each application runs exactly one process per processor. In such an environment, or in one that employs coscheduling, all the processes that share a given lock will always run simultaneously.
References-found: 16

