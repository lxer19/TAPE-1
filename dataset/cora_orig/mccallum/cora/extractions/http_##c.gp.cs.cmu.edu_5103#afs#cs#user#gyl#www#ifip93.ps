URL: http://c.gp.cs.cmu.edu:5103/afs/cs/user/gyl/www/ifip93.ps
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs/user/gyl/www/publications.html
Root-URL: http://www.cs.cmu.edu
Title: on Architectures and Compilation Techniques for Fine and Medium Grain Parallelism Jan. 1993, Orlando, FL.
Author: Ali-Reza Adl-Tabatabai Thomas Gross Guei-Yuan Lueh and James Reinders 
Address: Pittsburgh, PA 15213-3891  
Affiliation: 1 School of Computer Science, Carnegie Mellon University,  
Note: To appear in the Proceedings of IFIP WG 10.3 (Concurrent Sytems) Working Conference  2 Intel Corporation, SSD, 5200 NE Elam Young Parkway Hillsboro, OR 97124-6497  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A. V. Aho, R. Sethi, and Ullman J. D. </author> <title> Compilers. </title> <publisher> Addison-Wesley, </publisher> <year> 1986. </year>
Reference-contexts: The IR of a program has been subject to a variety of optimizations and transformations (e.g., see <ref> [1] </ref> and [11] for descriptions of such optimizations). The intermediate representation is typically a graph (e.g., DAGs, or trees), where edges indicate data or control dependences. In our system, the IR consists of trees in which tree edges indicate flow dependences.
Reference: [2] <author> S. Borkar, R. Cohn, G. Cox, S. Gleason, T. Gross, H. T. Kung, M. Lam, B. Moore, C. Peterson, J. Pieper, L. Rankin, P. S. Tseng, J. Sutton, J. Urbanski, and J. Webb. </author> <title> iWarp: An integrated solution to high-speed parallel computing. </title> <booktitle> In Proceedings of Supercomputing '88, </booktitle> <pages> pages 330-339, </pages> <address> Orlando, Florida, </address> <month> November </month> <year> 1988. </year> <journal> IEEE Computer Society and ACM SIGARCH. </journal>
Reference: [3] <author> S. Borkar, R. Cohn, G. Cox, T. Gross, H. T. Kung, M. Lam, M. Levine, B. Moore, W. Moore, C. Peterson, J. Susman, J. Sutton, J. Urbanski, and J. Webb. </author> <title> Supporting systolic and memory communication in iWarp. </title> <booktitle> In Proc. 17th Intl. Symposium on Computer Architecture, </booktitle> <pages> pages 70-81. </pages> <publisher> ACM, </publisher> <month> May </month> <year> 1990. </year>
Reference: [4] <author> G. J. Chaitin. </author> <title> Register allocation and spilling via graph coloring. </title> <booktitle> In Proc. of the SIGPLAN 1982 Symposium on Compiler Construction, </booktitle> <pages> pages 98-105, </pages> <month> June </month> <year> 1982. </year> <journal> In SIGPLAN Notices, v. </journal> <volume> 17, </volume> <editor> n. </editor> <volume> 6. </volume>
Reference-contexts: The code scheduling phase places the machine operations into a sequence of machine instructions, using software pipelining for inner loops and straight-line code compaction for other regions. Register assignment builds live ranges of virtual registers and assigns physical registers to virtual registers using Chaitin-style graph coloring <ref> [4] </ref>. Coalescing eliminates register move operations. Spills and reloads are added to the schedule for live ranges that are spilled. Finally, the code emission phase turns the schedule into assembly code. This phase generates prologue and epilogue code for pipelined loops and assigns functional units to machine operations. <p> Thus certain code templates are grouped into pre-scheduled sequences of operations that are scheduled as a unit by the scheduler. Note that this copy eliminating optimization is different from coalescing <ref> [4] </ref>. Coalescing is performed after scheduling as part of register assignment. Pre-scheduling eliminates extraneous copies prior to scheduling. Hence, pre-scheduling shortens the critical path and decreases the resource requirements of the operations to be scheduled, allowing the code scheduler to produce a better schedule.
Reference: [5] <author> R. Cohn, T. Gross, M. Lam, and P. S. Tseng. </author> <title> Architecture and compiler tradeoffs for a wide instruction word microprocessor. </title> <booktitle> In Proc. Third. Intl. Conf. on Architectural Support for Programming Languages and Operating Systems (ASPLOS III), </booktitle> <pages> pages 2-14, </pages> <address> Boston, </address> <month> Apr </month> <year> 1989. </year> <month> ACM/IEEE. </month>
Reference-contexts: Each processor consists of a computation agent (similar to a microprocessor) that is closely coupled to a communication agent [2,3]. The computation agent is controlled by a long instruction word <ref> [5] </ref> and provides the source of examples for this paper. Similar issues arise when using software pipelining for other processors with instruction level parallelism (although the details may be different).
Reference: [6] <author> Intel Corp. </author> <title> iWarp Microprocessor (Part Number 318153). </title> <address> Hillsboro, OR., </address> <year> 1991. </year> <title> Technical Information, Order Number 281006. </title>
Reference: [7] <author> J. D. Dehnert, P. Y. Hsu, and J. P. Bratt. </author> <title> Overlapped loop support in the Cydra 5. </title> <booktitle> In Proc. Third. Intl. Conf. on Architectural Support for Programming Languages and Operating Systems (ASPLOS III), </booktitle> <pages> pages 26-38, </pages> <address> Boston, </address> <month> Apr </month> <year> 1989. </year> <month> ACM/IEEE. </month>
Reference-contexts: 1 Introduction Software pipelining is an attractive method to schedule code for processors that exhibit instruction-level parallelism such as pipelined, super-scalar, and (V)LIW machines. It has been implemented for a variety of processors ( e.g. FPS-164 [10], Warp [9], Cydra-5 <ref> [7] </ref>), and a number of pipelining algorithms have been described in the literature. Software pipelining produces a schedule so that the executions of multiple loop iterations are overlapped.
Reference: [8] <author> S. C. Johnson. </author> <title> A Tour Through the Portable C Compiler. </title> <institution> Bell Laboratories, </institution> <month> January </month> <year> 1981. </year> <title> in Documents for UNIX, </title> <journal> Vol. </journal> <volume> 2. </volume>
Reference-contexts: Code generation is organized into four phases that are run in series, with no backtracking among the phases. The code selection phase maps IR operations to machine operations. The code selector is based on a simple non-backtracking recursive pattern matcher that matches tree patterns against sub-trees in the IR <ref> [8] </ref>. Each tree pattern contains a sequence of machine operations called a code template. A code template includes the resource requirements (such as functional units or instruction fields) of its machine operations. The machine operations produced by this phase reference virtual registers that are allocated from an infinite pool.
Reference: [9] <author> M. Lam. </author> <title> Software pipelining: An effective scheduling technique for VLIW machines. </title> <booktitle> In Proc. of the ACM SIGPLAN '88 Conf. on Programming Language Design and Implementation, </booktitle> <pages> pages 318-328, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: 1 Introduction Software pipelining is an attractive method to schedule code for processors that exhibit instruction-level parallelism such as pipelined, super-scalar, and (V)LIW machines. It has been implemented for a variety of processors ( e.g. FPS-164 [10], Warp <ref> [9] </ref>, Cydra-5 [7]), and a number of pipelining algorithms have been described in the literature. Software pipelining produces a schedule so that the executions of multiple loop iterations are overlapped.
Reference: [10] <author> Roy F. Touzeau. </author> <title> A Fortran compiler for the FPS-164 scientific computer. </title> <booktitle> In ACM SIGPLAN 84 Symp on Compiler Construction, </booktitle> <pages> pages 48-57. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1984. </year>
Reference-contexts: 1 Introduction Software pipelining is an attractive method to schedule code for processors that exhibit instruction-level parallelism such as pipelined, super-scalar, and (V)LIW machines. It has been implemented for a variety of processors ( e.g. FPS-164 <ref> [10] </ref>, Warp [9], Cydra-5 [7]), and a number of pipelining algorithms have been described in the literature. Software pipelining produces a schedule so that the executions of multiple loop iterations are overlapped.
Reference: [11] <author> M.J. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> October </month> <year> 1982. </year>
Reference-contexts: The IR of a program has been subject to a variety of optimizations and transformations (e.g., see [1] and <ref> [11] </ref> for descriptions of such optimizations). The intermediate representation is typically a graph (e.g., DAGs, or trees), where edges indicate data or control dependences. In our system, the IR consists of trees in which tree edges indicate flow dependences.
References-found: 11

