URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-255.ps.Z
Refering-URL: http://www.cs.gatech.edu/classes/cs7322_97_spring/papers/papers.html
Root-URL: 
Email: fsandy,picard,stang@media.mit.edu  
Title: Storage and Retrieval Image and Video Databases II,  Photobook: Content-Based Manipulation of Image Databases  
Author: A. Pentland, R. W. Picard, S. Sclaroff 
Affiliation: The Media Laboratory Massachusetts Institute of Technology  
Date: Fall 1995  June 12, 1995  
Note: Appeared: SPIE  No. 2185, Feb 6-10, 1994, San Jose To Appear: International Journal of Computer Vision,  
Pubnum: Perceptual Computing Section,  
Abstract: M.I.T. Media Laboratory Perceptual Computing Technical Report No. 255, Nov. 1993 Abstract We describe the Photobook system, which is a set of interactive tools for browsing and searching images and image sequences. These query tools differ from those used in standard image databases in that they make direct use of the image content rather than relying on text annotations. Direct search on image content is made possible by use of semantics-preserving image compression, which reduces images to a small set of perceptually-significant coefficients. We discuss three types of Photobook descriptions in detail: one that allows search based on appearance, one that uses 2-D shape, and a third that allows search based on textural properties. These image content descriptions can be combined with each other and with text-based descriptions to provide a sophisticated browsing and search capability. In this paper we demonstrate Photobook on databases containing images of people, video keyframes, hand tools, fish, texture swatches, and 3-D medical data. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Adelson and J. Bergen, </author> <title> "The Plenoptic Function and the Elements of Early Vision," </title> <editor> in: M. Landy and J. A. Movshon, </editor> <title> (eds) Computational Models of Visual Processing, </title> <publisher> MIT Press (1991). </publisher>
Reference-contexts: These two types of semantic indexing | using texture-like descriptions of "stuff" and using object-like descriptions of "things" | constitute the two basic types of image search operation in our system. These two types of description seem to be fundamentally different in human vision <ref> [1] </ref>, and correspond roughly to the distinction between mass nouns and count nouns in language. Note that both types of image query can operate on the same image primitives (e.g., the energy in different bandpass filters) but they differ in how they group these primitives for comparison.
Reference: [2] <editor> ACM SIGIR. </editor> <booktitle> Proceedings of International Conference on Multimedia Information Systems, </booktitle> <address> Singapore, </address> <year> 1991. </year>
Reference-contexts: Thus in our view the image database problem requires development of semantics-preserving image compression methods. 2.1 Comparison with Other Approaches In recent years there has been a growing interest in the image database problem <ref> [2, 25] </ref>. The first proposed solutions were intended for engineering drawings, and typically assumed that hand preprocessing had fully "predigested" them into meaningful parts and functional features [8, 9, 28, 29]. We feel that this requirement is acceptable for things like CAD drawings, but not for general imagery.
Reference: [3] <author> D. Ballard and C. Brown. </author> <title> Computer Vision. </title> <publisher> Prentice Hall, </publisher> <year> 1982 </year>
Reference: [4] <author> E. Binaghi, I. Gagliardi, and R. Schettini. </author> <title> "Indexing and fuzzy logic-based retrieval of color images." In Visual Database Systems, II, </title> <journal> IFIP Transactions A-7, </journal> <pages> pages 79-92. </pages>
Reference-contexts: We feel that this requirement is acceptable for things like CAD drawings, but not for general imagery. More recently, researchers have proposed a variety of image indexing methods, based on shape [10, 23, 24, 26, 27, 33], color <ref> [4, 50, 22] </ref>, or combinations of such indices [35, 13]. The general approach is to calculate some approximately invariant statistic, like a color histogram or invariants of shape moments, and use that to stratify or partition the image database.
Reference: [5] <author> W. E. Blanz, D. Petkovic, and J. L. Sanz. </author> <title> Algorithms and Architectures for Machine Vision. </title> <editor> ed. C.H. Chen, </editor> <publisher> Marcel Decker Inc., </publisher> <year> 1989. </year>
Reference: [6] <author> T. Breuel, </author> <title> Indexing for Recognition from a Large Model Base, M.I.T. </title> <note> Artificial Intelligence Laboratory Memo 1108, </note> <month> August </month> <year> 1990 </year>
Reference-contexts: There are two reasons for adopting this approach. The first is that a 2-D matching approach can be trained directly from image data; it does not require a 3-D model. The second reason is that the 2-D approach has lower computational complexity than 3-D methods. Breuel <ref> [6] </ref>, for instance, has proven that only O (ffi 2 ) 2-D aspects are needed to cover the entire 3-D viewing sphere with a 2-D matching error bounded by ffi radians (0 &lt; ffi &lt; 1).
Reference: [7] <author> P. Brodatz. </author> <title> Textures: A Photographic Album for Artists and Designers. </title> <publisher> Dover, </publisher> <address> New York, </address> <year> 1966. </year>
Reference-contexts: Computer code implementing this proceedure, together with technical reports providing additional detail, is available by anonymous FTP from whitechapel.media.mit.edu. 6.2 Database experiments The illustrations here are from experiments run on the Brodatz database, which consists of 1008 non-overlapping texture patches cropped from all 112 images of the Brodatz Album <ref> [7] </ref>. Each Bro-datz texture provides nine 128 fi 128 subimages in 8-bit gray levels. This collection of natural textures exhibits large variety, including many inhomogeneous patterns not usually included in texture studies formed from small subsets of the Brodatz collection.
Reference: [8] <author> C. C. Chang and S. Y. Lee. </author> <title> "Retrieval of similar pictures on pictorial databases." </title> <journal> Pattern recognition, </journal> <volume> 24(7):675- 680, </volume> <year> 1991. </year>
Reference-contexts: The first proposed solutions were intended for engineering drawings, and typically assumed that hand preprocessing had fully "predigested" them into meaningful parts and functional features <ref> [8, 9, 28, 29] </ref>. We feel that this requirement is acceptable for things like CAD drawings, but not for general imagery.
Reference: [9] <author> C-C. Chang and T-C. Wu. </author> <title> "Retrieving the most similar symbolic pictures from pictorial databases." </title> <booktitle> Information Processing and Management, </booktitle> <volume> 28(5) </volume> <pages> 581-588, </pages> <year> 1992. </year>
Reference-contexts: The first proposed solutions were intended for engineering drawings, and typically assumed that hand preprocessing had fully "predigested" them into meaningful parts and functional features <ref> [8, 9, 28, 29] </ref>. We feel that this requirement is acceptable for things like CAD drawings, but not for general imagery.
Reference: [10] <author> Z. Chen and S-Y. Ho. </author> <title> "Computer vision for robust 3D aircraft recognition with fast library search." </title> <journal> Pattern Recognition, </journal> <volume> 24(5): </volume> <pages> 375-390, </pages> <year> 1991. </year>
Reference-contexts: We feel that this requirement is acceptable for things like CAD drawings, but not for general imagery. More recently, researchers have proposed a variety of image indexing methods, based on shape <ref> [10, 23, 24, 26, 27, 33] </ref>, color [4, 50, 22], or combinations of such indices [35, 13]. The general approach is to calculate some approximately invariant statistic, like a color histogram or invariants of shape moments, and use that to stratify or partition the image database.
Reference: [11] <author> T. Darrell and A. Pentland, </author> <title> "Robust Estimation of a Multi-Layer Motion Representation", </title> <booktitle> in Proceedings IEEE Workshop on Visual Motion, </booktitle> <pages> pp. 173-177, </pages> <year> 1991. </year> <note> longer version available as M.I.T. Media Laboratory Perceptual Computing Technical Report No. 163 </note>
Reference-contexts: The first solution is to use motion and color to pull out foreground objects. We have found that this sort of figure-ground segmentation can be done reliably and efficiently by use of clustering in conjunction with optical flow <ref> [11, 55] </ref> and/or color difference information [12]. This provides us with good "cut-outs" of foreground objects, as is illustrated in Similarly, we can analyze the appearance, motion, and texture of the background, and insert this information into our database. <p> We are also fortunate that, at least in the case of video, it is relatively easy to use motion and color changes to help find things and stuff of interest. This was illustrated by Figure 2, and is discussed more fully in references <ref> [11, 55, 12] </ref>. However, we can also draw on our framework of semantics-preserving compression to address this problem. Recall that our basic approach for representing specific classes of interest is to use a prototype (s) and the smallest possible set of parametric variations or deformations.
Reference: [12] <author> T. Darrell, P. Maes, B. Blumberg, and A. Pent-land, </author> <title> "A Novel Environment for Situated Vision and Behavior," </title> <booktitle> IEEE Workshop on Visual Behaviors pp. </booktitle> <pages> 68-72, </pages> <address> Seattle, WA., </address> <month> June 19, </month> <year> 1994. </year>
Reference-contexts: The first solution is to use motion and color to pull out foreground objects. We have found that this sort of figure-ground segmentation can be done reliably and efficiently by use of clustering in conjunction with optical flow [11, 55] and/or color difference information <ref> [12] </ref>. This provides us with good "cut-outs" of foreground objects, as is illustrated in Similarly, we can analyze the appearance, motion, and texture of the background, and insert this information into our database. <p> This figure shows a system that extracts the outlines of people in view; a geometric analysis of the outline is then used to label position of head, hands, and feet. This system runs at 20 frames/second without special hardware, and has been tested on more than 2,000 people <ref> [12] </ref>. age descriptions available to it. In this paper we will discuss appearance-specific descriptions ("Appearance Photobook") applied to face and keyframe databases, texture descriptions ("Texture Photo-book") applied to texture-swatch and keyframe databases, and shape descriptions ("Shape Pho-tobook") applied to hand-tool and fish databases. <p> We are also fortunate that, at least in the case of video, it is relatively easy to use motion and color changes to help find things and stuff of interest. This was illustrated by Figure 2, and is discussed more fully in references <ref> [11, 55, 12] </ref>. However, we can also draw on our framework of semantics-preserving compression to address this problem. Recall that our basic approach for representing specific classes of interest is to use a prototype (s) and the smallest possible set of parametric variations or deformations.
Reference: [13] <author> S. Smoliar, and H. Zhang, </author> <title> "Content-Based Video Indexing and Retrieval," </title> <journal> IEEE Multimedia Magazine, </journal> <volume> Vol. 1, No. 2, </volume> <pages> pp. 62-72, </pages> <year> 1994. </year>
Reference-contexts: We feel that this requirement is acceptable for things like CAD drawings, but not for general imagery. More recently, researchers have proposed a variety of image indexing methods, based on shape [10, 23, 24, 26, 27, 33], color [4, 50, 22], or combinations of such indices <ref> [35, 13] </ref>. The general approach is to calculate some approximately invariant statistic, like a color histogram or invariants of shape moments, and use that to stratify or partition the image database. <p> Such partitioning allows users to limit the search space when looking for a particular image, and has proven to be quite useful for small image databases <ref> [35, 13] </ref>. The difference between these methods and ours is that they emphasize computing a discriminant that can reject many false matches, whereas ours can encode the image data to the accuracy required to retain"all" of its perceptually salient aspects.
Reference: [14] <author> R. Duda and P. </author> <title> Hart Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1973. </year>
Reference: [15] <author> J. </author> <title> Francos "Orthogonal Decompositions of 2-D Random Fields and their Applications for 2-D Spectral Estimation", </title> <booktitle> Signal Processing and its Applications, </booktitle> <pages> pp. 287-327, </pages> <editor> N.K. Bose and C.R. Rao (eds.), </editor> <publisher> North-Holland., </publisher> <year> 1993. </year>
Reference-contexts: It is also desirable that the model parameters correspond to semantic attributes of patterns, such as periodicity or randomness. Picard and Liu [43] have therefore developed a new model based on the Wold decomposition for regular stationary stochastic processes in 2-D images <ref> [15] </ref>. If an image is assumed to be a homogeneous 2-D discrete random field, then the 2-D Wold-like decomposition is a sum of three mutually orthogonal components: a harmonic field, a generalized-evanescent field, and a purely-indeterministic field. <p> The Wold decomposition transforms textures into three orthogonal components: harmonic, evanescent, and random. The upper three textures illustrate these components; below each texture is shown its DFT magnitude. ples in the "past" of (m; n) where the implied spatial ordering is with respect to the non-symmetric half-plane (NSHP) neighborhood <ref> [15] </ref>. If the innovation field fu (m; n) = y (m; n) ^y (m; n)g vanishes, then fy (m; n)g is deterministic; else, it is regular. If fy (m; n)g is regular and spans the same Hilbert space as its innovation field then it is purely-indeterministic.
Reference: [16] <author> P. Gast, </author> <title> "Integrating Eigenpicture Analysis with an Image Database," M.I.T. </title> <type> Bachelors Thesis, </type> <institution> Computer Science and Electrical Engineering Deptartment, Advisor: </institution> <address> Alex Pentland, </address> <year> 1993. </year>
Reference-contexts: complexity is an important consideration for image database applications. 3 Photobook Photobook is a computer system that allows the user to browse large image databases quickly and efficiently, using both text annotation information in an AI database and by having the computer search the images directly based on their content <ref> [38, 16, 42] </ref>.
Reference: [17] <author> W. I. Grosky, P. Neo, and R. Mehrotra. </author> <title> "A pictorial index mechanism for model-based matching." </title> <journal> Data and Knowledge Engineering, </journal> <volume> 8 </volume> <pages> 309-327, </pages> <year> 1992 </year>
Reference: [18] <author> K. Haase, "FRAMER: </author> <title> A Portable Persistent Representation Library," </title> <booktitle> Proceedings of the AAAI Workshop on AI in Systems and Support, Am. Asso. for AI, </booktitle> <year> 1993. </year>
Reference-contexts: This subset selection is accomplished by searching text annotations using an object-oriented, memory-based AI database called Framer <ref> [18, 19] </ref>. Photobook then presents the user with the first screenful of these images (see Figure 3); the rest of the images can be viewed by "paging" through them one screen at a time. <p> Photobook can also handle combinations of these descriptors, e.g., shape and appearance, which we will illustrate using 3-D data of human brain ventricles. It can also handle complex functions of text annotations, via functionality of the Framer knowledge representation language <ref> [18, 19] </ref>. Obvious applications for "Appearance Photo-book" as applied to face databases include customs, security, and criminal investigation. A different application would be a dating service where individuals could browse a database of prospective partners based on their looks as well as biographical data.
Reference: [19] <author> K. Haase, </author> <title> "AI in Service and Support: Bridging the Gap", Haase, </title> <booktitle> Proceedings of Am. Asso. AI, </booktitle> <year> 1993. </year>
Reference-contexts: This subset selection is accomplished by searching text annotations using an object-oriented, memory-based AI database called Framer <ref> [18, 19] </ref>. Photobook then presents the user with the first screenful of these images (see Figure 3); the rest of the images can be viewed by "paging" through them one screen at a time. <p> Photobook can also handle combinations of these descriptors, e.g., shape and appearance, which we will illustrate using 3-D data of human brain ventricles. It can also handle complex functions of text annotations, via functionality of the Framer knowledge representation language <ref> [18, 19] </ref>. Obvious applications for "Appearance Photo-book" as applied to face databases include customs, security, and criminal investigation. A different application would be a dating service where individuals could browse a database of prospective partners based on their looks as well as biographical data.
Reference: [20] <author> H. Helson and D. Lowdenslager, </author> <title> "Prediction Theory and Fourier Series in Several Variables.II", </title> <journal> Acta Mathmatica, </journal> <volume> Vol. 196, </volume> <pages> pp. 175-213, </pages> <year> 1962. </year>
Reference-contexts: Corresponding essentially to these three cases, the Wold theorem for a 2-D random field, fy (m; n)g, (m; n) 2 Z 2 yields a decomposition into three processes <ref> [20] </ref>. This decomposition can be formulated as a linear prediction problem. Let ^y (m; n) be the projection of y (m; n) on the Hilbert space spanned by all the sam 12 (a) Fig. 5.
Reference: [21] <author> K. Hirata and T. Kato. </author> <title> "Query by visual example," </title> <booktitle> In Advances in Database Technology EDBT '92, Third International Conference on Extending Database Technology, </booktitle> <address> Vienna, Aus-tria, March 1992. </address> <publisher> Springer-Verlag. </publisher>
Reference: [22] <author> M. Ioka. </author> <title> "A method of defining the similarity of images on the basis of color information," </title> <type> Technical Report RT-003 0, </type> <institution> IBM Tokyo Research Lab, </institution> <year> 1989. </year>
Reference-contexts: We feel that this requirement is acceptable for things like CAD drawings, but not for general imagery. More recently, researchers have proposed a variety of image indexing methods, based on shape [10, 23, 24, 26, 27, 33], color <ref> [4, 50, 22] </ref>, or combinations of such indices [35, 13]. The general approach is to calculate some approximately invariant statistic, like a color histogram or invariants of shape moments, and use that to stratify or partition the image database.
Reference: [23] <author> M. A. Ireton and C. S. Xydeas. </author> <title> "Classification of shape for content retrieval of images in a multimedia database," </title> <booktitle> In Sixth International Conference on Digital Processing of Signals in Communications, </booktitle> <pages> pages 111-116, </pages> <address> Loughbor-ough, UK, 2-6 Sept., 1990. </address> <publisher> IEE. </publisher>
Reference-contexts: We feel that this requirement is acceptable for things like CAD drawings, but not for general imagery. More recently, researchers have proposed a variety of image indexing methods, based on shape <ref> [10, 23, 24, 26, 27, 33] </ref>, color [4, 50, 22], or combinations of such indices [35, 13]. The general approach is to calculate some approximately invariant statistic, like a color histogram or invariants of shape moments, and use that to stratify or partition the image database.
Reference: [24] <author> H. V. Jagadish. </author> <title> "A retrieval technique for similar shapes," </title> <booktitle> In International Conference on Management of Data, SIGMOD 91, </booktitle> <pages> pages 208-217, </pages> <address> Denver CO, </address> <month> May </month> <year> 1991. </year> <note> ACM. </note>
Reference-contexts: We feel that this requirement is acceptable for things like CAD drawings, but not for general imagery. More recently, researchers have proposed a variety of image indexing methods, based on shape <ref> [10, 23, 24, 26, 27, 33] </ref>, color [4, 50, 22], or combinations of such indices [35, 13]. The general approach is to calculate some approximately invariant statistic, like a color histogram or invariants of shape moments, and use that to stratify or partition the image database.
Reference: [25] <author> R. Jain and W. </author> <title> Niblack. </title> <booktitle> NSF workshop on visual information management, </booktitle> <month> February </month> <year> 1992. </year>
Reference-contexts: Thus in our view the image database problem requires development of semantics-preserving image compression methods. 2.1 Comparison with Other Approaches In recent years there has been a growing interest in the image database problem <ref> [2, 25] </ref>. The first proposed solutions were intended for engineering drawings, and typically assumed that hand preprocessing had fully "predigested" them into meaningful parts and functional features [8, 9, 28, 29]. We feel that this requirement is acceptable for things like CAD drawings, but not for general imagery.
Reference: [26] <author> T. Kato, T. Kurita, H. Shimogaki, T. Mizutori, and K. Fujimura. </author> <title> "A cognitive approach to visual interaction. </title> <booktitle> In International Conference of Multimedia Information Systems," </booktitle> <volume> MIS '91, </volume> <pages> pages 109-120. </pages> <institution> ACM and National University of Singapore, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: We feel that this requirement is acceptable for things like CAD drawings, but not for general imagery. More recently, researchers have proposed a variety of image indexing methods, based on shape <ref> [10, 23, 24, 26, 27, 33] </ref>, color [4, 50, 22], or combinations of such indices [35, 13]. The general approach is to calculate some approximately invariant statistic, like a color histogram or invariants of shape moments, and use that to stratify or partition the image database.
Reference: [27] <author> Y. Lamdan and H. J. Wolfson. </author> <title> "Geometric hashing: A general and efficient model-based recognition scheme," </title> <booktitle> In 2nd International Conference on Computer Vision (ICCV), </booktitle> <pages> pages 238-249, </pages> <address> Tampa, Florida, 1988. </address> <publisher> IEEE. </publisher>
Reference-contexts: We feel that this requirement is acceptable for things like CAD drawings, but not for general imagery. More recently, researchers have proposed a variety of image indexing methods, based on shape <ref> [10, 23, 24, 26, 27, 33] </ref>, color [4, 50, 22], or combinations of such indices [35, 13]. The general approach is to calculate some approximately invariant statistic, like a color histogram or invariants of shape moments, and use that to stratify or partition the image database.
Reference: [28] <author> S-Y. Lee and F-J. Hsu. </author> <title> "2D C-string: A new spatial knowledge representation for image database systems," </title> <journal> Pattern Recognition, </journal> <volume> 23(10) </volume> <pages> 1077-1087, </pages> <year> 1990. </year>
Reference-contexts: The first proposed solutions were intended for engineering drawings, and typically assumed that hand preprocessing had fully "predigested" them into meaningful parts and functional features <ref> [8, 9, 28, 29] </ref>. We feel that this requirement is acceptable for things like CAD drawings, but not for general imagery.
Reference: [29] <author> S-Y. Lee and F-J. Hsu. </author> <title> "Spatial reasoning and similarity retrieval of images using 2D c-string knowledge representation," </title> <journal> Pattern Recognition, </journal> <volume> 25(3) </volume> <pages> 305-318, </pages> <year> 1992. </year>
Reference-contexts: The first proposed solutions were intended for engineering drawings, and typically assumed that hand preprocessing had fully "predigested" them into meaningful parts and functional features <ref> [8, 9, 28, 29] </ref>. We feel that this requirement is acceptable for things like CAD drawings, but not for general imagery.
Reference: [30] <author> A Lippman. </author> <title> "Semantic bandwidth compression," Picture Coding Symposium, </title> <year> 1981. </year>
Reference-contexts: Consequently, we believe that the key to solving the image database problem is semantics-preserving image compression: compact representations that preserve essential image similarities. This concept is related to some of the "semantic bandwidth compression" ideas put forth in the context of image compression <ref> [30] </ref> [31] [46] [40]. Image coding has utilized semantics primarily through efforts to compute a compact image representation by exploiting knowledge about the content of the image.
Reference: [31] <author> P. McLean, </author> <title> "Structured Video Coding," M.I.T. </title> <type> Masters Thesis, Advisor: </type> <institution> Andrew Lipp-man, </institution> <year> 1989. </year>
Reference-contexts: Consequently, we believe that the key to solving the image database problem is semantics-preserving image compression: compact representations that preserve essential image similarities. This concept is related to some of the "semantic bandwidth compression" ideas put forth in the context of image compression [30] <ref> [31] </ref> [46] [40]. Image coding has utilized semantics primarily through efforts to compute a compact image representation by exploiting knowledge about the content of the image.
Reference: [32] <author> J. Mao and A. Jain, </author> <title> "Texture Classification and Segmentation using Multiresolution Simultaneous Autoregressive Models", </title> <journal> Pattern Recognition, </journal> <volume> Vol. 25, No. 2, </volume> <pages> pp 173-188, </pages> <year> 1992. </year>
Reference-contexts: Only the textures which possess the same number of main orientations are subsequently compared by examining the Wold complexity component. The complexity component is modeled by use of a multiscale simultaneous autoregressive (SAR) model, whose parameters are estimated using the process of Mao and Jain <ref> [32] </ref>. The SAR parameters of different textures are compared using the Mahalanobis distance measure.
Reference: [33] <author> R. Mehrotra and W. I. Grosky. </author> <title> "Shape matching utilizing indexed hypotheses generation and testing," </title> <journal> IEEE Transactions of Robotics and Automation, </journal> <volume> 5(1) </volume> <pages> 70-77, </pages> <year> 1989. </year>
Reference-contexts: We feel that this requirement is acceptable for things like CAD drawings, but not for general imagery. More recently, researchers have proposed a variety of image indexing methods, based on shape <ref> [10, 23, 24, 26, 27, 33] </ref>, color [4, 50, 22], or combinations of such indices [35, 13]. The general approach is to calculate some approximately invariant statistic, like a color histogram or invariants of shape moments, and use that to stratify or partition the image database.
Reference: [34] <author> B. Moghaddam and A. Pentland, </author> <title> "Face recognition using view-based and modular eigenspaces for Identification And Inspection of Humans, </title> " <booktitle> SPIE Conf. on Automatic Systems, </booktitle> <address> San Diego, </address> <month> July </month> <year> 1994 </year>
Reference-contexts: Although not a real-time process on current workstations, this computation is sufficiently efficient to be incorporated in the image preprocessing step. We first used this approach for finding faces [53], and have now applied it to finding a wide variety of "things" (including eyes, cars, roads, etc. <ref> [34] </ref>). Multiple texture models can also be used to find "stuff" such as sky, trees, buildings, etc. [44]. Finally, it should be remarked that this framework for searching images is based on 2-D matching of appearance, rather than matching of 3-D properties. There are two reasons for adopting this approach. <p> Photo-book uses multiple examples to make an improved estimate of the parameter's probability distribution function (PDF). We have experimented with allowing the user to provide both positive and negative examples, and with characterization of arbitrary PDFs <ref> [34, 44] </ref>, although the current interface only supports updating the parameter's mean from multiple positive examples. 5 (a) (b) Fig. 2. Using motion and color information, we can separate foreground objects from background. <p> This is accomplished by determining which set of eigenimages provides the best encoding of the image; the same approach is also used to detect occurrences of these models in the image. The details of this procedure are described in references <ref> [39, 34] </ref> and discussed in Section 7.2. Note that because this approach is view-based, we must have separate models if we want to describe appearance from different points of view. <p> The search process can be made surprisingly efficient by appropriately arranging the order in which we calculate the ! k . We have used this approach both for finding a wide variety of "things" (including eyes, cars, roads, etc. <ref> [34] </ref>) and "stuff" (including sky, trees, buildings, etc. [44]). For further detail see references [34, 44]. <p> We have used this approach both for finding a wide variety of "things" (including eyes, cars, roads, etc. [34]) and "stuff" (including sky, trees, buildings, etc. [44]). For further detail see references <ref> [34, 44] </ref>.
Reference: [35] <author> W. Niblack, R. Barber, W. Equitz, M. Flick-ner, E. Glasman, D. Petkovic, and P. Yanker. </author> <title> "The QBIC project: Querying image s by content using color, texture, and shape," </title> <booktitle> In IS & T/SPIE 1993 International Symposium on Electronic Imaging: Science & Technology,, Conference 1908, Storage and Retrieval for Image and Video Databases, </booktitle> <month> February </month> <year> 1993. </year>
Reference-contexts: We feel that this requirement is acceptable for things like CAD drawings, but not for general imagery. More recently, researchers have proposed a variety of image indexing methods, based on shape [10, 23, 24, 26, 27, 33], color [4, 50, 22], or combinations of such indices <ref> [35, 13] </ref>. The general approach is to calculate some approximately invariant statistic, like a color histogram or invariants of shape moments, and use that to stratify or partition the image database. <p> Such partitioning allows users to limit the search space when looking for a particular image, and has proven to be quite useful for small image databases <ref> [35, 13] </ref>. The difference between these methods and ours is that they emphasize computing a discriminant that can reject many false matches, whereas ours can encode the image data to the accuracy required to retain"all" of its perceptually salient aspects.
Reference: [36] <author> J. Martin, A. Pentland, and R. </author> <title> Kikinis "Shape Analysis of Brain Structures using Physical and Experimental Modes," </title> <booktitle> IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pp. 752-755, </pages> <address> Seattle, WA., </address> <month> June </month> <year> 1994. </year>
Reference-contexts: Using this shape-and-appearance approach allowed us to more accurately characterize subtle shape differences such as occur between Alzheimer's disease and normal pressure hydrocephalus disease <ref> [36] </ref>. 7.2 Detection and Preprocessing No matter how well one can describe appearance, shape, and texture, there is still the question of finding the things (or stuff) to be described. <p> Fig. 10. An examples of using texture to sort keyframes from a video database; sorting is by similarity of the periodic components of texture in each of the RGB channels. 19 Fig. 11. Combining shape and appearance descriptions to search NMR data, from reference <ref> [36] </ref>. 20 scription, we use example images to calculate a mean image and eigenimages u k . These define a small parametric space which contains most of the variation among the training images.
Reference: [37] <author> A. Pentland and S. </author> <title> Sclaroff "Closed-Form Solutions For Physically Based Shape Modeling and Recognition." </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> Vol. 13, No. 7, </volume> <pages> pp. 715-730. </pages>
Reference-contexts: For an in-depth description of this formulation, readers are directed to <ref> [37, 47, 48] </ref>. To compare two FEM shape representations, we deform one elastic shape model to align it with the other. <p> We can now rewrite Equation 2 in terms of these generalized or eigenmode displacements, obtaining a decoupled system of equations: ~ U + 2 ~ U = T R; (5) allowing for closed-form solution to equilibrium problems such as shape fitting <ref> [37] </ref>. Code for these operations (for the case of simple 3-D objects only), together with technical reports providing additional detail, is available by anonymous FTP from whitechapel.media.mit.edu. Whenever a new object is entered into Shape Photobook, the first step is to compute its M, K, and matrices. <p> These nodal displacements can then be transformed into eigenmode amplitudes by the relation ~ U = T U. Such a set of eigenmode amplitudes can be used directly for object recognition and comparison <ref> [37] </ref>, exactly as the eigenimage amplitudes were used in Appearance Photobook. As in that case we need use only a few coefficients to obtain an accurate encoding of the shape; discarding high-frequency eigenmodes also tends to make our comparisons robust to noise and local shape variations.
Reference: [38] <author> A. Pentland, R. Picard, G. Davenport, R. Welsh, </author> <title> "The BT/MIT Project on Advanced Image Tools for Telecommunications: An Overview," </title> <booktitle> ImageCom `93, 2nd International Conference on Image Communications, </booktitle> <address> Bordeaux, France, 23-25 March, </address> <year> 1993. </year>
Reference-contexts: complexity is an important consideration for image database applications. 3 Photobook Photobook is a computer system that allows the user to browse large image databases quickly and efficiently, using both text annotation information in an AI database and by having the computer search the images directly based on their content <ref> [38, 16, 42] </ref>.
Reference: [39] <author> A. Pentland, B. Moggadam, and T. Starner, </author> <title> "View-Based and Modular Eigenspaces for 23 Face Recognition," </title> <booktitle> IEEE Conf. Computer Vi--sion and Pattern Recognition, </booktitle> <pages> pp. 84-90, </pages> <address> Seat-tle, WA, </address> <month> June </month> <year> 1994 </year>
Reference-contexts: This is accomplished by determining which set of eigenimages provides the best encoding of the image; the same approach is also used to detect occurrences of these models in the image. The details of this procedure are described in references <ref> [39, 34] </ref> and discussed in Section 7.2. Note that because this approach is view-based, we must have separate models if we want to describe appearance from different points of view. <p> This subspace can be approximated by use of the Karhunen-Loeve expansion, e.g., the eigenvectors of the autocorrelation matrix. For face imagery we refer to this subspace as "face space" and the eigenvectors as "eigenfaces" or "eigenfeatures" <ref> [53, 39] </ref>. Let the training set of images be 1 ; 2 ; 3 ; ::: M . The average of the set is defined by = 1 P M n=1 n . Each training image differs from the average by the vector i = i . <p> Note that at the lower right is still another image of this same person...but wearing sunglasses. Photobook's performance on this database was evaluated on a random sample of 200 images, and recognition accuracy was found to be 95%, while verification accuracy was above 99% <ref> [39] </ref>. on a second face database, assembled by the Army Research Laboratory at Ft. Belvoir, which contains substantial variations in scale, position, and head orientation.
Reference: [40] <author> R. W. </author> <title> Picard "Random Field Texture Coding," </title> <booktitle> Society for Information Display International Symposium Digest, </booktitle> <volume> Vol XXIII, </volume> <month> May </month> <year> 1992, </year> <pages> pages 685-688. </pages>
Reference-contexts: Consequently, we believe that the key to solving the image database problem is semantics-preserving image compression: compact representations that preserve essential image similarities. This concept is related to some of the "semantic bandwidth compression" ideas put forth in the context of image compression [30] [31] [46] <ref> [40] </ref>. Image coding has utilized semantics primarily through efforts to compute a compact image representation by exploiting knowledge about the content of the image.
Reference: [41] <author> R. W. Picard and M. Gorkani. </author> <title> "Finding perceptually dominant orientations in natural textures." Spatial Vision Vol. </title> <journal> 8, </journal> <volume> No. 2, </volume> <pages> pp. 221-253, </pages> <year> 1994. </year>
Reference-contexts: This stage approximates the finding of the two less salient dimensions identified in the study of Rao and Lohse, the directional and complexity components. The number of dominant orientations is estimated via steerable filtering and a decision process based on thresholding orientation histograms, as described in <ref> [41] </ref>. Only the textures which possess the same number of main orientations are subsequently compared by examining the Wold complexity component. The complexity component is modeled by use of a multiscale simultaneous autoregressive (SAR) model, whose parameters are estimated using the process of Mao and Jain [32].
Reference: [42] <author> R. W. Picard and T. Kabir. </author> <title> "Finding similar patterns in large image databases." </title> <booktitle> Proc. ICASSP, Minneapolis, MN, </booktitle> <volume> Vol. V, </volume> <pages> pp. 161-164, </pages> <year> 1993. </year>
Reference-contexts: complexity is an important consideration for image database applications. 3 Photobook Photobook is a computer system that allows the user to browse large image databases quickly and efficiently, using both text annotation information in an AI database and by having the computer search the images directly based on their content <ref> [38, 16, 42] </ref>.
Reference: [43] <author> R. W. Picard and F. Liu, </author> <title> "A new Wold ordering for image similarity," </title> <booktitle> IEEE Conf. on ASSP, </booktitle> <address> Adelaide, Australia, </address> <month> April, </month> <year> 1994. </year>
Reference-contexts: The model is successful if distances between its parameters correspond to ordering images by their perceptual similarity. It is also desirable that the model parameters correspond to semantic attributes of patterns, such as periodicity or randomness. Picard and Liu <ref> [43] </ref> have therefore developed a new model based on the Wold decomposition for regular stationary stochastic processes in 2-D images [15].
Reference: [44] <author> R. W. Picard and T. P. Minka, </author> <title> "Vision Texture for Annotation" ACM/Springer-Verlag Journal of Multimedia Systems, </title> <note> to appear. </note>
Reference-contexts: We first used this approach for finding faces [53], and have now applied it to finding a wide variety of "things" (including eyes, cars, roads, etc. [34]). Multiple texture models can also be used to find "stuff" such as sky, trees, buildings, etc. <ref> [44] </ref>. Finally, it should be remarked that this framework for searching images is based on 2-D matching of appearance, rather than matching of 3-D properties. There are two reasons for adopting this approach. <p> Photo-book uses multiple examples to make an improved estimate of the parameter's probability distribution function (PDF). We have experimented with allowing the user to provide both positive and negative examples, and with characterization of arbitrary PDFs <ref> [34, 44] </ref>, although the current interface only supports updating the parameter's mean from multiple positive examples. 5 (a) (b) Fig. 2. Using motion and color information, we can separate foreground objects from background. <p> The search process can be made surprisingly efficient by appropriately arranging the order in which we calculate the ! k . We have used this approach both for finding a wide variety of "things" (including eyes, cars, roads, etc. [34]) and "stuff" (including sky, trees, buildings, etc. <ref> [44] </ref>). For further detail see references [34, 44]. <p> We have used this approach both for finding a wide variety of "things" (including eyes, cars, roads, etc. [34]) and "stuff" (including sky, trees, buildings, etc. [44]). For further detail see references <ref> [34, 44] </ref>. <p> We provide the user with primitives such as model-specific detection and perceptual similarity, and use relevance feedback to learn what relations are valid in this particular context <ref> [44] </ref>. 8 Conclusion The Photobook system is a set of interactive tools for browsing and searching images and image sequences. The key idea behind this suite of tools is semantics-preserving image compression, which reduces images to a small set of perceptually-significant coefficients.
Reference: [45] <author> A. R. Rao and G. L. Lohse, </author> <title> "Towards a Texture Naming System: Identifying Relevant Dimensions of Texture," </title> <booktitle> IEEE Conf. on Visualization 1993, </booktitle> <address> San Jose, CA. </address>
Reference-contexts: The motivation for choosing a Wold-based model, in addition to its significance in random field theory, is its interesting relationship to independent psychophysical findings of perceptual similarity. Noteworthy is a recent study by Rao and Lohse where humans grouped patterns according to perceived similarity <ref> [45] </ref>. The three most important similarity dimensions identified in this study were repetitiveness, directionality, and complexity. These dimensions might be considered the perceptual equivalents of the harmonic, evanescent, and indeterministic components, respectively, in the Wold decomposition.
Reference: [46] <author> L. Sirovich, and M. Kirby, </author> <title> "Low-dimensional procedure for the characterization of human faces," </title> <journal> J. Opt. Soc. Am. A, </journal> <volume> Vol. 4, No. 3, </volume> <month> March </month> <year> 1987, </year> <pages> 519-524. </pages>
Reference-contexts: Consequently, we believe that the key to solving the image database problem is semantics-preserving image compression: compact representations that preserve essential image similarities. This concept is related to some of the "semantic bandwidth compression" ideas put forth in the context of image compression [30] [31] <ref> [46] </ref> [40]. Image coding has utilized semantics primarily through efforts to compute a compact image representation by exploiting knowledge about the content of the image. <p> We need a computa-tionally feasible method to find these eigenvectors. Fortunately we can determine the eigenvectors by first solving a much smaller M by M matrix problem, and taking linear combinations of the resulting vectors <ref> [46, 53] </ref>. Code for this calculation, together with technical reports providing additional detail, is available by anonymous FTP from whitechapel.media.mit.edu.
Reference: [47] <author> S. Sclaroff and A. Pentland, </author> <title> "A finite-element framework for correspondence and matching," </title> <booktitle> 4th International Conference on Computer Vision, </booktitle> <pages> pp. 308-313, </pages> <address> May 11-14, 1993, Berlin, Germany. </address>
Reference-contexts: Perhaps the major difference in how the shape and appearance codes are used in Photobook is the preprocessing to align the shapes. This preprocessing is developed in detail in references <ref> [47, 48] </ref> and discussed in Section 7.2. 5.1 Eigenmode Representations In Shape Photobook an object's shape representation is based on the eigenvectors of its physical model. <p> Before obtaining these eigenvectors, we first build a physical model for the shape using the finite element method. Interpolation functions are developed that allow continuous material properties, such as mass and stiffness, to be integrated across the region of interest. In <ref> [47] </ref> we introduced a new finite element formulation that uses Gaussian basis functions as FEM interpolants; this allows us to use the data itself to define the deformable object, by building stiffness and mass matrices that use the positions of image feature points as the finite element nodes. <p> For an in-depth description of this formulation, readers are directed to <ref> [37, 47, 48] </ref>. To compare two FEM shape representations, we deform one elastic shape model to align it with the other. <p> Normally this is done once when a new object is entered into Shape Photobook, and the correspondences stored. This process is called modal matching, and is discussed in references <ref> [47, 48] </ref>. Given these correspondences, we can then recover the eigenmode deformations ~ U that deform the matched points on one object to their corresponding positions on a prototype object. <p> For instance, for the tool and fish databases, the background was sufficiently simple that grey-level thresholding yielded a good outline of the shape. The point-to-point correspondences were determined automatically, as described in references <ref> [47, 48] </ref>. We are also fortunate that, at least in the case of video, it is relatively easy to use motion and color changes to help find things and stuff of interest. This was illustrated by Figure 2, and is discussed more fully in references [11, 55, 12].
Reference: [48] <author> S. Sclaroff and A. Pentland, </author> <title> "Modal Matching for Correspondence and Recognition," </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <note> to appear. Also available as: M.I.T. Media Laboratory Perceptual Computing Technical Note No. 304. [49] , R. </note> <author> Sriram, J. M. Francos and W. A. Pearl-man, </author> <title> "Texture coding Using a Wold Decomposition Model," </title> <booktitle> sl Proc. 12th IAPR Int. Conf. Pat. Rec., </booktitle> <address> Jerusalem, Israel, </address> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: Perhaps the major difference in how the shape and appearance codes are used in Photobook is the preprocessing to align the shapes. This preprocessing is developed in detail in references <ref> [47, 48] </ref> and discussed in Section 7.2. 5.1 Eigenmode Representations In Shape Photobook an object's shape representation is based on the eigenvectors of its physical model. <p> For an in-depth description of this formulation, readers are directed to <ref> [37, 47, 48] </ref>. To compare two FEM shape representations, we deform one elastic shape model to align it with the other. <p> Normally this is done once when a new object is entered into Shape Photobook, and the correspondences stored. This process is called modal matching, and is discussed in references <ref> [47, 48] </ref>. Given these correspondences, we can then recover the eigenmode deformations ~ U that deform the matched points on one object to their corresponding positions on a prototype object. <p> For instance, for the tool and fish databases, the background was sufficiently simple that grey-level thresholding yielded a good outline of the shape. The point-to-point correspondences were determined automatically, as described in references <ref> [47, 48] </ref>. We are also fortunate that, at least in the case of video, it is relatively easy to use motion and color changes to help find things and stuff of interest. This was illustrated by Figure 2, and is discussed more fully in references [11, 55, 12].
Reference: [50] <author> M. Swain and D. Ballard, </author> <title> "Color indexing". </title> <journal> Int. J. of Computer Vision, </journal> <volume> 7(1) </volume> <pages> 11-32, </pages> <year> 1991. </year>
Reference-contexts: We feel that this requirement is acceptable for things like CAD drawings, but not for general imagery. More recently, researchers have proposed a variety of image indexing methods, based on shape [10, 23, 24, 26, 27, 33], color <ref> [4, 50, 22] </ref>, or combinations of such indices [35, 13]. The general approach is to calculate some approximately invariant statistic, like a color histogram or invariants of shape moments, and use that to stratify or partition the image database.
Reference: [51] <author> S. Tanaka, M. Shima, J. Shibayama, and A. Maeda. </author> <title> "Retrieval method for an image database based on topographical structure." </title> <booktitle> In Applic. of Digital Image Processing, </booktitle> <volume> Vol. 1153, </volume> <pages> pages 318-327. SPIE, </pages> <year> 1989. </year>
Reference: [52] <institution> Discrete Random Signals and Statistical Signal Processing, </institution> <address> C. </address> <publisher> W. Therrien, Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ 1992. </address>
Reference-contexts: Wold for 1-D random processes. This theorem states that any random process can be written as the sum of two processes, one that can be predicted by a linear filter with zero mean-squared error (deterministic), and one which is regular <ref> [52] </ref> (indeterministic). Moreover, these two processes will be mutually orthogonal. In terms of the 1-D spectrum, these two processes correspond to the discrete part of the spectrum and the continuous part of the spectrum, respectively.
Reference: [53] <author> M. Turk and A. Pentland, </author> <title> "Eigenfaces for Recognition", </title> <journal> Journal of Cognitive Neuro-science, </journal> <month> May </month> <year> 1991. </year>
Reference-contexts: This allows us to detect instances of models by asking how well they can describe each part of the image. Although not a real-time process on current workstations, this computation is sufficiently efficient to be incorporated in the image preprocessing step. We first used this approach for finding faces <ref> [53] </ref>, and have now applied it to finding a wide variety of "things" (including eyes, cars, roads, etc. [34]). Multiple texture models can also be used to find "stuff" such as sky, trees, buildings, etc. [44]. <p> This subspace can be approximated by use of the Karhunen-Loeve expansion, e.g., the eigenvectors of the autocorrelation matrix. For face imagery we refer to this subspace as "face space" and the eigenvectors as "eigenfaces" or "eigenfeatures" <ref> [53, 39] </ref>. Let the training set of images be 1 ; 2 ; 3 ; ::: M . The average of the set is defined by = 1 P M n=1 n . Each training image differs from the average by the vector i = i . <p> We need a computa-tionally feasible method to find these eigenvectors. Fortunately we can determine the eigenvectors by first solving a much smaller M by M matrix problem, and taking linear combinations of the resulting vectors <ref> [46, 53] </ref>. Code for this calculation, together with technical reports providing additional detail, is available by anonymous FTP from whitechapel.media.mit.edu.
Reference: [54] <author> K. Wakimoto, M. Shima, S. Tanaka, and A. Maeda. </author> <title> "An intelligent user interface to an image database using a figure interpretation method." </title> <booktitle> In 9th Int. Conference on Pattern Recognition, </booktitle> <volume> volume 2, </volume> <pages> pages 516-991, </pages> <year> 1990. </year>
Reference: [55] <author> J. Y. A. Wang and E. H. Adelson, </author> <title> "Layered Representation for Motion Analysis" IEEE CVPR '93. </title> <note> Longer version available as: M.I.T. Media Laboratory Perceptual Computing Technical Report No. 228. 24 </note>
Reference-contexts: The first solution is to use motion and color to pull out foreground objects. We have found that this sort of figure-ground segmentation can be done reliably and efficiently by use of clustering in conjunction with optical flow <ref> [11, 55] </ref> and/or color difference information [12]. This provides us with good "cut-outs" of foreground objects, as is illustrated in Similarly, we can analyze the appearance, motion, and texture of the background, and insert this information into our database. <p> We are also fortunate that, at least in the case of video, it is relatively easy to use motion and color changes to help find things and stuff of interest. This was illustrated by Figure 2, and is discussed more fully in references <ref> [11, 55, 12] </ref>. However, we can also draw on our framework of semantics-preserving compression to address this problem. Recall that our basic approach for representing specific classes of interest is to use a prototype (s) and the smallest possible set of parametric variations or deformations.
References-found: 54

