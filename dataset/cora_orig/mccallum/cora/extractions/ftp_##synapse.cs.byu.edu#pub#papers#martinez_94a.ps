URL: ftp://synapse.cs.byu.edu/pub/papers/martinez_94a.ps
Refering-URL: ftp://synapse.cs.byu.edu/pub/papers/details.html
Root-URL: 
Title: Priority ASOCS  ASOCS models have two significant advantages over other learning models:  
Author: Tony R. Martinez Douglas M. Campbell Brent W. Hughes 
Keyword: ASOCS, self-organizing, neural networks, parallel, rule-based, learning.  
Address: Provo, Utah 84602  
Affiliation: Computer Science Department Brigham Young University  
Note: In Journal of Artificial Neural Networks, vol. 1, no. 3, pp. 403-429, 1994.  1 Introduction  
Abstract: This paper presents an ASOCS (Adaptive Self-Organizing Concurrent System) model for massively parallel processing of incrementally defined rule systems in such areas as adaptive logic, robotics, logical inference, and dynamic control. An ASOCS is an adaptive network composed of many simple computing elements operating asynchronously and in parallel. An ASOCS can operate in either a data processing mode or a learning mode. During data processing mode, an ASOCS acts as a parallel hardware circuit. During learning mode, an ASOCS incorporates a rule expressed as a Boolean conjunction in a distributed fashion in time logarithmic in the number of rules. This paper proposes a learning algorithm and architecture for Priority ASOCS. This new ASOCS model uses rules with priorities. The new model has significant learning time and space complexity improvements over previous models. Non-von Neumann architectures such as neural networks attack the word-at-a-time bottleneck of traditional computing systems [1]. Neural networks learn input-output mappings using highly distributed processing and memory [10,11,12]. Their numerous simple processing elements with modifiable weighted links permit a high degree of parallelism. A typical neural network has fixed topology. It learns by modifying weighted links between nodes. A new class of connectionist architectures has been proposed called ASOCS (Adaptive Self-Organizing Concurrent Systems) [4,5]. ASOCS models support efficient computation through self-organized learning and parallel execution. Learning is done through the incremental presentation of rules and/or examples. ASOCS models learn by modifying their topology. Data types include Boolean and multi-state variables; recent models support analog variables. The model incorporates rules into an adaptive logic network in a parallel and self organizing fashion. In processing mode, ASOCS supports fully parallel execution on actual inputs according to the learned rules. The adaptive logic network acts as a parallel hardware circuit during execution, mapping n input boolean vectors into m output boolean vectors, in a combinatoric fashion. The overall philosophy of ASOCS follows the high level goals of current neural network models. However, the mechanisms of learning and execution vary significantly. The ASOCS logic network is topologically dynamic with the network growing to efficiently fit the specific application. Current ASOCS models are based on digital nodes. ASOCS also supports use of symbolic and heuristic learning mechanisms, thus combining the parallelism and distributed nature of connectionist computing with the potential power of AI symbolic learning. A proof of concept ASOCS chip has been developed [2]. 
Abstract-found: 1
Intro-found: 0
Reference: 10. <author> Minsky, M. and Papert, S., </author> <title> Perceptrons, an Introduction to Computational Geometry. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA 1969. </address>
Reference: 11. <author> Rosenblatt, F., </author> <title> Principles of Neurodynamics. </title> <publisher> Spartan Books, </publisher> <address> Washington D. C. </address> <year> 1962. </year> <month> 22 </month>
Reference: 12. <author> Rumelhart, D. and McClelland J., </author> <title> Parallel Distributed Processing: Explorations in the Microstructure of Cognition. 1, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA 1986. </address>
References-found: 3

