URL: http://www.dcs.ex.ac.uk/~jamie/acl98.ps.Z
Refering-URL: http://www.dcs.ex.ac.uk/~jamie/
Root-URL: http://www.dcs.ex.ac.uk
Email: jamie@dcs.ex.ac.uk, pclane@dcs.ex.ac.uk  
Title: A Connectionist Architecture for Learning to Parse  
Author: James Henderson and Peter Lane 
Address: Exeter EX4 4PT, United Kingdom  
Affiliation: Dept of Computer Science, Univ of Exeter  
Abstract: We present a connectionist architecture and demonstrate that it can learn syntactic parsing from a corpus of parsed text. The architecture can represent syntactic constituents, and can learn generalizations over syntactic constituents, thereby addressing the sparse data problems of previous connectionist architectures. We apply these Simple Synchrony Networks to mapping sequences of word tags to parse trees. After training on parsed samples of the Brown Corpus, the networks achieve precision and recall on constituents that approaches that of statistical methods for this task. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Eugene Charniak. </author> <title> forthcoming. Statistical techniques for natural language parsing. </title> <journal> AI Magazine. </journal>
Reference: <author> Jeffrey L. Elman. </author> <year> 1991. </year> <title> Distributed representations, simple recurrent networks, and grammatical structure. </title> <journal> Machine Learning, </journal> <volume> 7 </volume> <pages> 195-225. </pages>
Reference: <author> Jerry A. Fodor and Zenon W. Pylyshyn. </author> <year> 1988. </year> <title> Connectionism and cognitive architecture: A critical analysis. </title> <journal> Cognition, </journal> <volume> 28 </volume> <pages> 3-71. </pages>
Reference: <editor> R. Garside, G. Leech, and G. Sampson (eds). </editor> <year> 1987. </year> <title> The Computational Analysis of English: a corpus-based approach. </title> <publisher> Longman Group UK Limited. </publisher>
Reference-contexts: This section describes the conversion of the Susanne corpus sentences and the precision/recall evaluation functions. We begin by describing the part of speech tags, which form the input to the network. The tags in the Susanne scheme are a detailed extension of the tags used in the Lancaster-Leeds Treebank <ref> (see Garside et al, 1987) </ref>. For the experiments described below the simpler Lancaster-Leeds scheme is used. Each tag is a two or three letter sequence, e.g. `John' would be encoded `NP', the articles `a' and `the' are encoded `AT', and verbs such as `is' encoded `VBZ'.
Reference: <author> James Henderson. </author> <year> 1996. </year> <title> A connectionist architecture with inherent systematicity. </title> <booktitle> In Proceedings of the Eighteenth Conference of the Cognitive Science Society, </booktitle> <pages> pages 574-579, </pages> <address> La Jolla, CA. </address>
Reference: <author> I. Melcuk. </author> <year> 1988. </year> <title> Dependency Syntax: Theory and Practice. </title> <publisher> SUNY Press. </publisher>
Reference-contexts: The same is done for other such constructions, which include adjective, noun, determiner and prepositional phrases. This move is not linguistically unmotivated, since the result is equivalent to a form of dependency grammar <ref> (Melcuk, 1988) </ref>, which have a long linguistic tradition. The constructions are also well defined enough 3 We acknowledge the roles of the Economic and Social Research Council (UK) as sponsor and the University of Sussex as grantholder in providing the Susanne corpus used in the experiments described in this paper.
Reference: <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams. </author> <year> 1986. </year> <title> Learning internal representations by error propagation. </title> <editor> In D. E. Rumelhart and J. L. Mc-Clelland, editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Vol 1. </volume> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: In the next section of this paper we present the proposed connectionist architecture, Simple Synchrony Networks (SSNs). SSNs are a natural extension of Simple Recurrent Networks (SRNs) (El-man, 1991), which are in turn a natural extension of Multi-Layered Perceptrons (MLPs) <ref> (Rumelhart et al., 1986) </ref>. SRNs are an improvement over MLPs because they generalize what they have learned over words in different sentence positions. SSNs are an improvement over SRNs because the use of TSVB gives them the additional ability to generalize over constituents in different structural positions. <p> SRNs are a standard connectionist method for processing sequences. As the name implies, SSNs are one way of extending SRNs with TSVB. 2.1 Simple Recurrent Networks Simple Recurrent Networks (Elman, 1991) are a simple extension of the most popular form of connectionist network, Multi-Layered Perceptrons (MLPs) <ref> (Rumelhart et al., 1986) </ref>. MLPs are popular because they can approximate any finite mapping, and because training them with the Backpropagation learning algorithm (Rumelhart et al., 1986) has been demonstrated to be effective in a wide variety of applications. <p> SRNs with TSVB. 2.1 Simple Recurrent Networks Simple Recurrent Networks (Elman, 1991) are a simple extension of the most popular form of connectionist network, Multi-Layered Perceptrons (MLPs) <ref> (Rumelhart et al., 1986) </ref>. MLPs are popular because they can approximate any finite mapping, and because training them with the Backpropagation learning algorithm (Rumelhart et al., 1986) has been demonstrated to be effective in a wide variety of applications. Like MLPs, SRNs consist of a finite set of units which are connected by weighted links, as illustrated in figure 1. The output of a unit is simply a scalar activation value. <p> This context is the state of the network. A number of algorithms exist for training such networks with loops in their flow of activation (called recurrence), for example Backpropagation Through Time <ref> (Rumelhart et al., 1986) </ref>. The most important characteristic of any learning-based model is the way it generalizes from the examples it is trained on to novel testing examples. In this regard there is a crucial difference between SRNs and MLPs, namely that SRNs generalize across sequence positions.
Reference: <author> Geoffrey Sampson. </author> <year> 1995. </year> <title> English for the Computer. </title> <publisher> Oxford University Press, Oxford, </publisher> <address> UK. </address>
Reference: <author> Lokendra Shastri and Venkat Ajjanagadde. </author> <year> 1993. </year> <title> From simple associations to systematic reasoning: A connectionist representation of rules, variables, and dynamic bindings using temporal synchrony. </title> <journal> Behavioral and Brain Sciences, </journal> <volume> 16 </volume> <pages> 417-451. </pages>
References-found: 9

