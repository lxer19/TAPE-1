URL: http://www.fzi.de/prost/people/weisbrod/ieee-nn97.ps.gz
Refering-URL: http://www.fzi.de/prost/people/weisbrod.html
Root-URL: http://www.fzi.de
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> S. Geva and J. Sitte, </author> <title> "A cartpole experiment benchmark for trainable controllers," </title> <journal> IEEE Control Systems, </journal> <pages> pp. 40-51, </pages> <month> Octo-ber </month> <year> 1993. </year>
Reference-contexts: Then any sensory information may get mapped to more than one control action. Further generalizing the notion of a (crisp) relation into a fuzzy relation ~ C : I fi O ! <ref> [0; 1] </ref>, we are finally able to differentiate the degree of applicability of several control actions.
Reference: [2] <author> H.R. Berenji and P. Khedkar, </author> <title> "Learning and tuning fuzzy logic controllers through reinforcements," </title> <journal> in IEEE Trans. Neural Networks, 1992, </journal> <volume> vol. 3, </volume> <pages> pp. 724-740. </pages>
Reference-contexts: The second task is the local decision of which action should be applied in a concrete situation. This division of the learning task into a critic and a controller module is already known in literature in several variations and with different objectives see e.g. [11], [12], <ref> [2] </ref>. Since this approach perfectly matches the modularization into a neural network and a fuzzy relation derived from our requirements above, we decided to adapt the critic/controller paradigm to fit our needs.
Reference: [3] <author> D. Nauck, F. Klawonn, and R. Kruse, </author> <title> "Combining neural networks and fuzzy controllers," </title> <booktitle> in Fuzzy Logic in Artificial Intelligence (FLAI93), </booktitle> <editor> E.P. Klement and W. Slany, </editor> <booktitle> Eds., </booktitle> <pages> pp. 35-46. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1993. </year>
Reference: [4] <author> S.M. Sulzberger, N.N. Tschichold-Gurman, and S.J. Vestli, </author> <title> "Fun: Optimization of fuzzy rule based systems using neural networks," </title> <booktitle> in Proc. IEEE Int. Conf. on Neural Networks, </booktitle> <address> San Francisco, </address> <year> 1993, </year> <pages> pp. 312-316. </pages>
Reference: [5] <author> R. Jang, </author> <title> Adaptive Network Fuzzy Inference System, </title> <type> Ph.D. thesis, </type> <institution> University of California, Berkeley, </institution> <year> 1992. </year>
Reference: [6] <author> H. Bersini, P.J. Nordvick, and A. Bonarini, </author> <title> "A simple direct adaptive fuzzy controller derived from its neural equivalent," </title> <booktitle> in 3rd IEEE Int. Conf. on Fuzzy Systems, </booktitle> <year> 1993, </year> <pages> pp. 345-350. </pages>
Reference: [7] <author> P.Y. Glorennec, </author> <title> "A neuro fuzzy inference system designed for implementation on a neural chip," </title> <booktitle> in Proc. of the 2nd Int. Conf. on Fuzzy Logic and Neural Networks, </booktitle> <address> Iiuzuka, Japan, </address> <year> 1993, </year> <pages> pp. 209-212. </pages>
Reference: [8] <author> L. Wang, </author> <title> Adaptive Fuzzy Systems and Control, </title> <publisher> Prentice-Hall, </publisher> <year> 1994. </year>
Reference: [9] <author> W. Eppler, </author> <title> Prestructuring of Neural Networks with Fuzzy Logic (in German), </title> <type> Ph.D. thesis, </type> <institution> University of Karlsruhe, Germany, </institution> <year> 1993. </year>
Reference: [10] <author> A. G. Barto, S. J. Bradtke, and S. P. Singh, </author> <title> "Learning to act using real-time dynamic programming," </title> <journal> Artificial Intelligence, </journal> , <volume> no. 72, </volume> <pages> pp. 81-138, </pages> <year> 1995. </year> <note> RIEDMILLER, SPOTT, WEISBROD: FYNESSE 16 </note>
Reference-contexts: A modified variant of a Dynamic Programming method therefore suggests that the optimization procedure is only applied to those situations, that are actually visited during a control trial. This approach is called Real Time Dynamic Programming <ref> [10] </ref> and will be discussed in further detail in section V-A. The idea is to repeatedly execute control trials from different starting situations and thereby improving the knowledge about the consequences of the current policy. <p> Recently, there is a growing interest to apply methods founded by the theory of Dynamic Programming (DP) [15] to solve learning problems of the above kind [16], [17], <ref> [10] </ref>, [18], [19], [20]. DP methods offer a solid mathematical foundation with the goal of finding optimal solutions for temporal optimization problems. Thus they constitute an ideally suited framework for the class of learning problems that we want to solve by the Fynesse control architecture.
Reference: [11] <author> A. G. Barto, R. S. Sutton, and C. W. Anderson, </author> <title> "Neuron-like adaptive elements that can solve difficult learning control problems," </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> vol. 13, </volume> <pages> pp. 834-846, </pages> <year> 1983. </year>
Reference-contexts: The second task is the local decision of which action should be applied in a concrete situation. This division of the learning task into a critic and a controller module is already known in literature in several variations and with different objectives see e.g. <ref> [11] </ref>, [12], [2]. Since this approach perfectly matches the modularization into a neural network and a fuzzy relation derived from our requirements above, we decided to adapt the critic/controller paradigm to fit our needs.
Reference: [12] <author> R. S. Sutton, </author> <title> "First results with dyna, an integrated architecture for learning, planning and reaction," in Neural Networks for Control, </title> <editor> W. T. Miller, R. S. Sutton, and P. J. Werbos, Eds. </editor> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: The second task is the local decision of which action should be applied in a concrete situation. This division of the learning task into a critic and a controller module is already known in literature in several variations and with different objectives see e.g. [11], <ref> [12] </ref>, [2]. Since this approach perfectly matches the modularization into a neural network and a fuzzy relation derived from our requirements above, we decided to adapt the critic/controller paradigm to fit our needs.
Reference: [13] <author> J. Weisbrod, </author> <title> "A combined approach to fuzzy reasoning," </title> <booktitle> in 3rd European Congress on Fuzzy and Intelligent Technologies (EU-FIT'96), Aachen, 1996, </booktitle> <volume> vol. 1, </volume> <pages> pp. 554-557, </pages> <publisher> ELITE foundation. </publisher>
Reference-contexts: The uncertainty of the strategy may be measured by the fuzziness of the relation, but it is not possible to detect the stage of learning, i.e. locating situations we did not learn to handle yet. In <ref> [13] </ref>, [14] a concept was introduced that allows an explicit representation of ignorance and inconsistent knowledge. The idea is the use of two fuzzy relations, each attached with its own semantics: one for positive and another for negative knowledge.
Reference: [14] <author> M. Spott and J. Weisbrod, </author> <title> "A new approach to the adaptation of fuzzy relations," </title> <booktitle> in Proc. of EUFIT96, Aachen, Germany, 1996, </booktitle> <volume> vol. 2, </volume> <pages> pp. 782-786. </pages>
Reference-contexts: The uncertainty of the strategy may be measured by the fuzziness of the relation, but it is not possible to detect the stage of learning, i.e. locating situations we did not learn to handle yet. In [13], <ref> [14] </ref> a concept was introduced that allows an explicit representation of ignorance and inconsistent knowledge. The idea is the use of two fuzzy relations, each attached with its own semantics: one for positive and another for negative knowledge. <p> The greater the difference between support and possibility is, the less we know. If the support overshoots the possibility then we have detected inconsistent knowledge that will be transformed into ignorance. These adaptation algorithms including the treatment of ignorance and inconsistent knowledge is explained in detail in <ref> [14] </ref>. B.4 Interpretation of fuzzy control relation In this paper, our understanding of the interpretation of the fuzzy control relation qual is an approximation by the aggregation of a set of fuzzy rules.
Reference: [15] <author> R. E. Bellman, </author> <title> Dynamic Programming, </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1957. </year>
Reference-contexts: Here, we focus on a special class of reinforcement problems, where the judgement over a decision comes with some delay, namely at the end of a trial. Recently, there is a growing interest to apply methods founded by the theory of Dynamic Programming (DP) <ref> [15] </ref> to solve learning problems of the above kind [16], [17], [10], [18], [19], [20]. DP methods offer a solid mathematical foundation with the goal of finding optimal solutions for temporal optimization problems. <p> Thus they constitute an ideally suited framework for the class of learning problems that we want to solve by the Fynesse control architecture. The ideas of dynamic programming are described in the following section. A.2 Dynamic Programming Dynamic programming was first introduced by Bellman <ref> [15] </ref> to solve a special sort of optimization tasks, where tem RIEDMILLER, SPOTT, WEISBROD: FYNESSE 8 poral relations play a central role.
Reference: [16] <author> C. J. Watkins, </author> <title> Learning from Delayed Rewards., </title> <type> Phd thesis, </type> <institution> Cambridge University, </institution> <year> 1989. </year>
Reference-contexts: Recently, there is a growing interest to apply methods founded by the theory of Dynamic Programming (DP) [15] to solve learning problems of the above kind <ref> [16] </ref>, [17], [10], [18], [19], [20]. DP methods offer a solid mathematical foundation with the goal of finding optimal solutions for temporal optimization problems. Thus they constitute an ideally suited framework for the class of learning problems that we want to solve by the Fynesse control architecture. <p> However, typically a model of the plant may not be available. A tricky solution of this problem is to represent the value of the costs for state/actions pairs x; u directly. This is the idea of the Q-Learning approach presented in <ref> [16] </ref>.
Reference: [17] <author> P. J. Werbos, </author> <title> "Overview of designs and capabilities," </title> <booktitle> in Neural networks for control, </booktitle> <pages> pp. 59-66. </pages> <publisher> MIT Press, Massachusetts, </publisher> <editor> T. Miller, R. S. Sutton & P. J. </editor> <address> Werbos edition, </address> <year> 1990. </year>
Reference-contexts: Recently, there is a growing interest to apply methods founded by the theory of Dynamic Programming (DP) [15] to solve learning problems of the above kind [16], <ref> [17] </ref>, [10], [18], [19], [20]. DP methods offer a solid mathematical foundation with the goal of finding optimal solutions for temporal optimization problems. Thus they constitute an ideally suited framework for the class of learning problems that we want to solve by the Fynesse control architecture.
Reference: [18] <author> D. P. Bertsekas and J. N. Tsitsiklis, </author> <title> Neuro Dynamic Programming, </title> <publisher> Athena Scientific, </publisher> <address> Belmont, Massachusetts, </address> <year> 1996. </year>
Reference-contexts: Recently, there is a growing interest to apply methods founded by the theory of Dynamic Programming (DP) [15] to solve learning problems of the above kind [16], [17], [10], <ref> [18] </ref>, [19], [20]. DP methods offer a solid mathematical foundation with the goal of finding optimal solutions for temporal optimization problems. Thus they constitute an ideally suited framework for the class of learning problems that we want to solve by the Fynesse control architecture.
Reference: [19] <author> M. Riedmiller, </author> <title> "Learning to control dynamic systems," </title> <booktitle> in Proceedings of the 13th. European Meeting on Cybernetics and Systems Research - 1996 (EMCSR '96), </booktitle> <editor> Robert Trappl, Ed., Vi-enna, </editor> <year> 1996. </year>
Reference-contexts: Recently, there is a growing interest to apply methods founded by the theory of Dynamic Programming (DP) [15] to solve learning problems of the above kind [16], [17], [10], [18], <ref> [19] </ref>, [20]. DP methods offer a solid mathematical foundation with the goal of finding optimal solutions for temporal optimization problems. Thus they constitute an ideally suited framework for the class of learning problems that we want to solve by the Fynesse control architecture.
Reference: [20] <author> M. Riedmiller, </author> <title> "Application of sequential reinforcement learning to control dynamic systems," </title> <booktitle> in IEEE Intenational Conference on Neural Networks (ICNN '96), </booktitle> <address> Washington, </address> <year> 1996. </year>
Reference-contexts: Recently, there is a growing interest to apply methods founded by the theory of Dynamic Programming (DP) [15] to solve learning problems of the above kind [16], [17], [10], [18], [19], <ref> [20] </ref>. DP methods offer a solid mathematical foundation with the goal of finding optimal solutions for temporal optimization problems. Thus they constitute an ideally suited framework for the class of learning problems that we want to solve by the Fynesse control architecture.
Reference: [21] <author> M. Riedmiller, </author> <title> Self learning neural controllers, </title> <type> Ph.D. thesis, </type> <institution> University of Karlsruhe, </institution> <year> 1996, </year> <note> in german. </note>
Reference-contexts: Unfortunately, closed loop control of dynamic systems is an ongoing process, and typically no such terminal state does exist. Instead, we formulate other conditions, under which convergence can be proven (for an exact specification of the assumptions and the proof the reader is referred to <ref> [21] </ref>): 1. there exists a set of states X + with zero immediate costs: 8x 2 X + 8u : r (x; u) = 0 2. for all other states immediate costs are positive: 8x =2 X + 8u : r (x; u) &gt; 0 3. with the available control signals
Reference: [22] <author> A. G. Barto, S. J. Bradtke, and S. P. Singh, </author> <title> "Learning to act using real-time dynamic programming," </title> <type> Tech. Rep., </type> <year> 1993. </year>
Reference-contexts: If we do not want to assume a finite number of states | for example because the current state information is a vector of continuous sensor values | we have to apply another technique. The Real Time Dynamic Programming approach proposed in <ref> [22] </ref> suggests to update only those states, that occur during a control trial. The idea is to start the plant in one of a finite set of random starting situations, which will be denoted by X 0 .
Reference: [23] <author> R. S. Sutton, </author> <title> "Learning to predict by the methods of temporal differences," </title> <journal> Machine Learning, </journal> , <volume> no. 3, </volume> <pages> pp. 9-44, </pages> <year> 1988. </year>
Reference-contexts: What makes this update formula special, is that the target value is not given externally, but is computed as the cost value of the successor state by the neural network itself. This special type of learning is therefore called Temporal Difference (TD)-learning <ref> [23] </ref>. A.6 Model based and model free approaches Finally, one word should be said concerning the use of an internal model. The pure value iteration idea as presented above assumes the evaluation of the states.
Reference: [24] <author> J. Weisbrod, </author> <title> "Fuzzy control revisited | why is it working?," </title> <booktitle> in Advances in Fuzzy Theory and Technology, </booktitle> <volume> Vol. </volume> <editor> III, P. P. Wang, </editor> <publisher> Ed., </publisher> <pages> pp. 219-244. </pages> <address> Bookwrights, Durham (NC), </address> <year> 1995. </year>
Reference-contexts: We concentrate on the logical interpretation with Mamdani-type (logical conjunction) and Goedel-type (logical implication) rules, as they play an important role in the theory of possibility (negative knowledge) and evidence (positive knowledge) <ref> [24] </ref>. As already mentioned in section IV-B.4 the degrees of freedom in the process of interpretation are the number of rules and the shape and location of the premises and conclusions. In our opinion the interpretation must fit RIEDMILLER, SPOTT, WEISBROD: FYNESSE 11 the users subjective idea of it.
Reference: [25] <author> E. Bolten and M. Spott, </author> <title> "Fuzzy rule extraction from fuzzy relations," </title> <booktitle> in Proc. of EUFIT97, </booktitle> <address> Aachen, Germany, </address> <year> 1997. </year>
Reference-contexts: We only expect a simple explanation of the controller's behavior that is qualitatively correct. difficult to distinguish different rules. For each conclusion U j the degree of membership of each u i 2 U has to be optimized with respect to approx. The method is explained in detail in <ref> [25] </ref>. VI. Control of a Chemical Plant A. Task description Fig. 2. x 2 vs. time | behavior of the uncontrolled chemical reactor The control of a chemical plant represents a challenging benchmark for nonlinear controller design.

References-found: 25

