URL: ftp://cns.brown.edu/nin/papers/bme_nc.ps.Z
Refering-URL: http://www.math.tau.ac.il/~nin/research.html
Root-URL: 
Email: Email: favni,ning@math.tau.ac.il  
Title: Boosted Mixture of Experts: An ensemble learning scheme  
Author: Ran Avnimelech Nathan Intrator 
Keyword: Boosting, Mixture of experts, Ensemble machines, Query by committee, Neural networks, Handwritten character recognition.  
Date: July 16, 1997  
Address: Ramat Aviv 69978, Tel-Aviv, Israel.  
Affiliation: Department of Computer Science Sackler Faculty of Exact Sciences Tel-Aviv University,  
Abstract: We present a new supervised learning procedure for ensemble machines, in which outputs of predictors, trained on different distributions, are combined by a dynamic classifier combination model. This procedure may be viewed either as a version of Mixture of Experts (Jacobs et al., 1991), applied to classification, or as a variant of the Boosting algorithm (Schapire, 1990). As a variant of the Mixture of Experts, it can be made appropriate for general classification and regression problems, by initializing the partition of the data-set to different experts in a boost-like manner. If viewed as a variant of the Boosting algorithm, its main gain is the use of a dynamic combination model for the outputs of the networks. Results are demonstrated on a synthetic example and on a digit recognition task from the NIST database and compared with classical ensemble approaches. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Boser, B. E., Guyon, I. M., and Vapnik, V. N. </author> <year> (1992). </year> <title> A training algorithm for optimal margin classifiers. </title> <booktitle> In Proc. 5th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 144-152. </pages> <publisher> ACM Press, </publisher> <address> New York, NY. </address>
Reference-contexts: The idea that maximizing the margin can improve the generalization error was first suggested and studied in (Vapnik, 1982). It it is the base for support-vector classifiers (Cortes and Vapnik, 1995) and optimal margin classifiers <ref> (Boser et al., 1992) </ref> methods which achieve low generalization error despite the use of functions with much more parameters than the size of the training set.
Reference: <author> Bottou, L., Cortes, C., Denker, J., Drucker, H., Guyon, I., Jackel, L., LeCun, Y., </author> <title> Sackinger, </title> <publisher> U. </publisher>
Reference: <author> M. E., Simard, P., and Vapnik, V. </author> <year> (1994). </year> <title> Comparision of classifier methods: A case study in handwritten digit recognition. </title> <booktitle> In Proceedings Int. Conf. on Pattern Recognition, </booktitle> <volume> volume 12, </volume> <pages> pages 77-82. </pages>
Reference: <author> Breiman, L. </author> <year> (1996a). </year> <title> Bagging predictors. </title> <journal> Machine Learning, </journal> <volume> 24 </volume> <pages> 123-140. </pages>
Reference-contexts: At the other extent, decision trees may be considered as selection-style ensembles of simpler tree-predictors. Ensembles, combining the output of trees trained on bootstrapped copies of the same data (bagging), effectively enhance performance <ref> (Breiman, 1996a) </ref>. Ensemble methods which encourage diverse training sets, may gain from such a method, if the data 7 partitions vary. Using a dynamic combination models makes the ensemble even more a selection--style ensemble. Therefore this approach is most appropriate for use with the BME algorithm.
Reference: <author> Breiman, L. </author> <year> (1996b). </year> <title> Bias, variance and arcing classifiers. </title> <type> Technical Report TR-460, </type> <institution> Department of Statistics, University of California, Berkeley. </institution>
Reference-contexts: A common explanation to the reduced error rates of ensembles, is "variance" reduction, e.g. <ref> (Breiman, 1996b) </ref>. Schapire et al. provide an alternative theoretical analysis: In order to analyze 8 the generalization error, they consider not only the training error, but also the confidence 2 of the classifications.
Reference: <author> Breiman, L. </author> <year> (1996c). </year> <title> Stacked regressions. </title> <journal> Machine Learning, </journal> <volume> 24 </volume> <pages> 49-64. </pages>
Reference: <author> Cortes and Vapnik (1995). </author> <title> Support-vector networks. </title> <journal> Machine Learning, </journal> <volume> 20. </volume>
Reference-contexts: This bound depends only on the value of the threshold, the VC-dimension of the individual classifiers and the training set size. The idea that maximizing the margin can improve the generalization error was first suggested and studied in (Vapnik, 1982). It it is the base for support-vector classifiers <ref> (Cortes and Vapnik, 1995) </ref> and optimal margin classifiers (Boser et al., 1992) methods which achieve low generalization error despite the use of functions with much more parameters than the size of the training set.
Reference: <author> Drucker, H., Cortes, C., Jackel, L., Lecun, Y., and Vapnik, V. </author> <year> (1994). </year> <title> Boosting and other ensemble methods. </title> <journal> Neural Computation, </journal> <volume> 6(6) </volume> <pages> 1289-1301. </pages>
Reference-contexts: The results indicate that the performance of an ensemble machine trained with the BME algorithm (and combined appropriately) is significantly better than a standard ensemble ("parallel machine"). The improvement rate is similar to that achieved using Boosting <ref> (Drucker et al., 1994) </ref>. It is most encouraging that this improvement rate is kept even for a high number of classifiers (20% error reduction for 10 classifiers).
Reference: <author> Drucker, H., Schapire, R., and Simard, P. </author> <year> (1993). </year> <title> Improving performance in neural networks using a boosting algorithm. </title> <editor> In Hanson, S. J., Cowan, J. D., and Giles, C. L., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 5, </volume> <pages> pages 42-49. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Still, boosting proved to be, not just a theoretical technique, but also a practical tool for enhancing performance. Drucker et al. demonstrated its advantage over a combination of independently trained classifiers (`parallel machine') on a handwritten recognition task <ref> (Drucker et al., 1993) </ref>. Recently, boosting achieved an extremely low error rate on the same problem (Bottou et al., 1994).
Reference: <author> Freund, Y. </author> <year> (1990). </year> <title> Boosting a weak learning algorithm by majority. </title> <booktitle> In 3rd annual workshop on computational learning theory, </booktitle> <pages> pages 202-216. </pages>
Reference-contexts: There have been various improvements to the original boosting algorithm; Freund suggested using a simpler structure for combining many sub-hypotheses: instead of having a tree of majority gates, all sub-hypotheses are presented to one majority gate <ref> (Freund, 1990) </ref>. AdaBoost (Freund and Schapire, 1995), is a more advanced algorithm, in which each pattern is assigned a different probability to appear in the training set presented to the new learner. This version also prefers a `flat' structure for combining the classifiers, rather than a hierarchical one.
Reference: <author> Freund, Y. and Schapire, R. </author> <year> (1995). </year> <title> A decision-theoretic generalization of on-line learning and an application to boosting. </title> <booktitle> In 2nd European Conference on Computational Learning Theory. </booktitle>
Reference-contexts: There have been various improvements to the original boosting algorithm; Freund suggested using a simpler structure for combining many sub-hypotheses: instead of having a tree of majority gates, all sub-hypotheses are presented to one majority gate (Freund, 1990). AdaBoost <ref> (Freund and Schapire, 1995) </ref>, is a more advanced algorithm, in which each pattern is assigned a different probability to appear in the training set presented to the new learner. This version also prefers a `flat' structure for combining the classifiers, rather than a hierarchical one.
Reference: <author> Freund, Y., Seung, H., Shamir, E., and Tishby, N. </author> <year> (1993). </year> <title> Information, prediction and query by comittee. </title> <editor> In Hanson, S. J., Cowan, J. D., and Giles, C. L., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 5, </volume> <pages> pages 483-490, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Hansen, L. K. and Salamon, P. </author> <year> (1990). </year> <title> Neural network ensembles. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 12(10) </volume> <pages> 993-1001. </pages> <note> 13 Ho, </note> <author> T., Hull, J., and Srihari, S. </author> <year> (1994). </year> <title> Decision combination in multiple classifier systems. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> 16(1):66-75. 
Reference: <author> Jacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E. </author> <year> (1991). </year> <title> Adaptive mixtures of local experts. </title> <journal> Neural Computation, </journal> <volume> 3(1) </volume> <pages> 79-87. </pages>
Reference-contexts: He applied these algorithms to decision trees (CARTs) on various data. Schwenk and Bengio applied AdaBoost to MLPs and auto-encoder based classifiers ("diabolo networks") on character recognition tasks. 3 The Mixture of Experts Learning Procedure The Adaptive mixture of local experts <ref> (Jacobs et al., 1991) </ref> is a learning procedure which achieves improved performance in certain problems by assigning different subtasks to different learners. Its basic idea is to concurrently train several `expert' classifiers (or regression estimators), and a gating function.
Reference: <author> Jordan, M. I. and Jacobs, R. A. </author> <year> (1992). </year> <title> Hierarchies of adaptive experts. </title> <editor> In Moody, J. E., Hanson, S. J., and Lippmann, R. P., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 4, </volume> <pages> pages 985-992. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: Further localization is achieved by giving higher learning rates to the better performing expert, on each pattern. This idea was later extended into a tree structure termed `Hierarchical Mixture of Experts' 3 (HME), in which experts may be built from lower level experts and gating functions <ref> (Jordan and Jacobs, 1992) </ref>. In later work, the EM algorithm was used for training the HME (Jordan and Jacobs, 1994). Waterhouse and Robinson describe how to gradually grow these recursive learning machines (Waterhouse and Robinson, 1996).
Reference: <author> Jordan, M. I. and Jacobs, R. A. </author> <year> (1994). </year> <title> Hierarchical Mixtures of Experts and the EM algorithm. </title> <journal> Neural Computation, </journal> <volume> 6(2) </volume> <pages> 181-214. </pages>
Reference-contexts: This idea was later extended into a tree structure termed `Hierarchical Mixture of Experts' 3 (HME), in which experts may be built from lower level experts and gating functions (Jordan and Jacobs, 1992). In later work, the EM algorithm was used for training the HME <ref> (Jordan and Jacobs, 1994) </ref>. Waterhouse and Robinson describe how to gradually grow these recursive learning machines (Waterhouse and Robinson, 1996). The Mixture of Experts procedure achieves superior generalization and fast learning when the learning task corresponds to different subtasks for distinct portions of the input space.
Reference: <author> Krogh, A. and Vedelsby, J. </author> <year> (1995). </year> <title> Neural network ensembles, cross validation, and active learning. </title> <editor> In Tesauro, G., Touretzky, D., and Leen, T., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 7, </volume> <pages> pages 231-238. </pages> <publisher> The MIT Press. </publisher>
Reference: <author> Meir, R. </author> <year> (1995). </year> <title> Bias, variance and the combination of least square estimators. </title> <editor> In Tesauro, G., Touretzky, D., and Leen, T., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 7, </volume> <pages> pages 295-302. </pages> <publisher> The MIT Press. </publisher>
Reference: <author> Perrone, M. P. and Cooper, L. N. </author> <year> (1993). </year> <title> When networks disagree: Ensemble method for neural networks. </title> <editor> In Mammone, R. J., editor, </editor> <title> Neural Networks for Speech and Image processing. </title> <publisher> Chapman-Hall. </publisher>
Reference-contexts: It is likely that the agreement between the different members of a committee is higher as the presented patterns are more similar to those in the committee's training set. This also follows the principle used in <ref> (Perrone and Cooper, 1993) </ref>. They suggest that in order to achieve an ensemble with minimum variance, the coefficient for each member should be inversely proportional to its variance (vs. the ground truth).
Reference: <author> Raviv, Y. and Intrator, N. </author> <year> (1996). </year> <title> Bootstrapping with noise: An effective regularization technique. </title> <journal> Connection Science, </journal> 8(3/4):355-372. 
Reference-contexts: More specifically, the different parts of the training set which are used to train individual classifiers are all drawn from the same distribution. This holds when different types of classifiers are used, in cross-validation (Meir, 1995; Krogh and Vedelsby, 1995), or when different noisy bootstrap copies are used <ref> (Raviv and Intrator, 1996) </ref>. A different approach is training the classifiers on different parts of the training set, partitioned in a manner such that their distributions differ. Such an approach, which is presented here, combines two algorithms: Boosting and Mixture of Experts. The remainder of the paper proceeds as follows.
Reference: <author> Schapire, R. </author> <year> (1990). </year> <title> The strength of weak learnability. </title> <journal> Machine Learning, </journal> <volume> 5(2) </volume> <pages> 197-227. </pages>
Reference-contexts: The empirical evaluation of the algorithm on a demonstration problem and on a character recognition task from the NIST database is reported in section 7. 2 Theory of Boosting The Boosting algorithm can improve the performance of learning machines <ref> (Schapire, 1990) </ref>. Its theoretic basis relies on a proof of the equivalence of the strong and weak PAC learning models.
Reference: <author> Schapire, R., Freund, Y., Bartlett, P., and Lee, W. </author> <year> (1997). </year> <title> Boosting the margin: A new explanation for the effectiveness of voting methods. In Machines That Learn - Snowbird. </title>
Reference-contexts: However, a recent analysis of model complexity presents an explanation to the reduced generalization error in learner combination, especially boosting <ref> (Schapire et al., 1997) </ref>. Schapire et al. follow previous works and concentrate on an unusual phenomenon: Ordinarily, as hypotheses become more and more complex, the generalization error eventually increases.
Reference: <author> Schwenk, H. and Bengio, Y. </author> <year> (1997). </year> <title> Adaptive boosting of neural networks for character recognition. </title> <type> Technical Report TR-1072, </type> <institution> Department d'Informatique et Recerche Operationnelle, Universite d'Montreal. </institution>
Reference: <author> Seung, H. S., Opper, M., and Sompolinsky, H. </author> <year> (1992). </year> <title> Query by comittee. </title> <booktitle> In Proceedings of the 5th Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 287-294. </pages>
Reference: <author> Shimshoni, Y. and Intrator, N. </author> <year> (1996). </year> <title> Classifying seismic signals by integrating ensembles of neural networks. </title> <editor> In Amari, S., Xu, L., Chan, L. W., King, I., and Leung, K. S., editors, </editor> <booktitle> Proceedings of ICONIP Hong Kong. Progress in Neural Information Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 84-90. </pages> <publisher> Springer. </publisher>
Reference-contexts: Estimating that value may be used to extract confidence information. Tresp and Taniguchi also suggest a unified approach, of which these two methods are extreme cases. Shimshoni and Intrator used base level ensembles of several similar estimators, as the different experts <ref> (Shimshoni and Intrator, 1996) </ref>. The variance within each base level ensemble indicates its confidence. A monotone function can be applied to the confidence measure to determine whether a soft or a hard partition of the data is to be used. <p> We assume that because of the different training sets, the members of each committee have different variances which vary in different regions of the input space. This follows the use of the internal variance in each committee, as an estimate to its error rate, in <ref> (Shimshoni and Intrator, 1996) </ref> 6.3 Complexity issues When discussing complex models used for learning, there is much interest in the model complexity (or capacity), as it has an effect on the generalization error.
Reference: <author> Tresp, V. and Taniguchi, M. </author> <year> (1995). </year> <title> Combining estimators using non-constant weighting function. </title> <editor> In Tesauro, G., Touretzky, D., and Leen, T., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 7. </volume> <publisher> MIT Press. </publisher>
Reference-contexts: The confidence should increase as the highest output is higher, and decrease as any of the other outputs is higher. Other confidence measures, reported in machine learning literature may also be used. Tresp and Taniguchi use various confidence measures of different predictors, in their combination model <ref> (Tresp and Taniguchi, 1995) </ref>. One confidence measure they use is the variance of the predictor as it is measured by the local sensitivity to changes in weights. Another approach they mention is assuming the different predictors were trained on different data sets (e.g. American vs.
Reference: <author> Vapnik, V. N. </author> <year> (1982). </year> <title> Estimation of Dependences Based on Empirical Data. </title> <address> Springer-Verlar, New York. </address> <note> 14 Waterhouse, </note> <author> S. R. and Cook, G. </author> <year> (1997). </year> <title> Ensemble methods for phoneme classification. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> volume 9. </volume>
Reference-contexts: The VC-dimension of a family of predictors is a function of the model dimensionality, which can be used as an upper bound to the difference between the error rate on the training set and the error rate on new patterns, as a function of the training set size <ref> (Vapnik, 1982) </ref>. This difference is due to the fact that the essence of the training process is minimizing the error on the training set. As the model has more free parameters, it may over-fit the training data while achieving inferior performance on unseen data. <p> This bound depends only on the value of the threshold, the VC-dimension of the individual classifiers and the training set size. The idea that maximizing the margin can improve the generalization error was first suggested and studied in <ref> (Vapnik, 1982) </ref>. It it is the base for support-vector classifiers (Cortes and Vapnik, 1995) and optimal margin classifiers (Boser et al., 1992) methods which achieve low generalization error despite the use of functions with much more parameters than the size of the training set.
Reference: <author> Waterhouse, S. R. and Robinson, A. J. </author> <year> (1996). </year> <title> Constructive Algorithms for Hierarchical Mixtures of Experts. </title> <editor> In Touretzky, D. S., Mozer, M. C., and Hasselmo, M. E., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 8. </volume> <publisher> MIT Press. </publisher>
Reference-contexts: In later work, the EM algorithm was used for training the HME (Jordan and Jacobs, 1994). Waterhouse and Robinson describe how to gradually grow these recursive learning machines <ref> (Waterhouse and Robinson, 1996) </ref>. The Mixture of Experts procedure achieves superior generalization and fast learning when the learning task corresponds to different subtasks for distinct portions of the input space.
Reference: <author> Wolpert, D. H. </author> <year> (1992). </year> <title> Stacked generalization. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 241-259. 15 </pages>
References-found: 29

