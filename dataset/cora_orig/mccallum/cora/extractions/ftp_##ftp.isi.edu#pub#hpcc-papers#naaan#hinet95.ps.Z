URL: ftp://ftp.isi.edu/pub/hpcc-papers/naaan/hinet95.ps.Z
Refering-URL: http://www.isi.edu/isi-technical-reports.html
Root-URL: http://www.isi.edu
Title: Improving PVM Performance Using ATOMIC User-Level Protocol  
Author: Hong Xu and Tom W. Fisher 
Address: Marina del Rey, CA 90292-6695  
Affiliation: Information Sciences Institute University of Southern California  
Abstract: Parallel virtual machine (PVM) software system provides a programming environment that allows a collection of networked workstations to appear as a single concurrent computational resource. The performance of parallel applications in this environment depends on the performance of reliable data transfers between tasks. In this paper, we improve PVM communication performance over the ATOMIC LAN, a gigabit per-second local area network. This is achieved by separating the PVM data-transmission path from the PVM control-message path and transmitting PVM data messages over a user-level application programming interface (API) provided by the Myrinet ATOMIC interface. The Myrinet-API, although significantly faster than the TCP/IP kernel stack, does not provide reliable communication. Therefore, a user-level protocol has been developed based on the Myrinet-API to offer reliable sequenced packet delivery. This protocol provides faster data transfers between PVM tasks over the ATOMIC LAN. Performance results obtained at USC/ISI demonstrate that our version of PVM can reach up to 140 Mbps throughput, which is 94% of the achievable network bandwidth over the ATOMIC LAN. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Beguelin, J. Dongarra, A. Geist, R. Manchek, S. Otto, and J. Walpole, </author> <title> "PVM: Experiences, current status and future direction," </title> <booktitle> in Supercomputing'93 Proceedings, </booktitle> <pages> pp. 765-766, </pages> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Over the past few years a rapid increase in the performance of microprocessors as well as in the speed of local area networks (LAN) has made workstation clusters a viable platform to solve computation-intensive problems. PVM <ref> [1, 2] </ref>, a message passing based software system, provides a programming environment that allows a collection of networked workstations to appear as a single parallel computational resource. PVM currently provides facilities for process control and data communication based on the TCP/IP network protocol suite [3].
Reference: [2] <author> A. Geist, A. Beguelin, J. Dongarra, W. Jiang, R. Manchek, and V. Sunderam, </author> <title> PVM 3 User's Guide and Reference manual. </title> <institution> Oak Ridge National Laboratory, Oak Ridge, Tennessee 37831, </institution> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: 1 Introduction Over the past few years a rapid increase in the performance of microprocessors as well as in the speed of local area networks (LAN) has made workstation clusters a viable platform to solve computation-intensive problems. PVM <ref> [1, 2] </ref>, a message passing based software system, provides a programming environment that allows a collection of networked workstations to appear as a single parallel computational resource. PVM currently provides facilities for process control and data communication based on the TCP/IP network protocol suite [3]. <p> The PVM system is composed of a daemon, known as pvmd in <ref> [2] </ref>, and a library of PVM interface routines. Pvmd is a user-level daemon process running on each host computer that is a part of a given PVM configuration. PVM interface library routines are used by each application process for data communication and process control. <p> The pvmd does not do any computation. There is only one pvmd running on each host computer while there may be multiple tasks running on the same host. As shown in Figure 2, in the general PVM service model <ref> [2] </ref>, a message sent from a local task to another task on another host is first, by default, routed to the local pvmd. The message is then forwarded by the local pvmd to the pvmd on the remote host. <p> To improve message passing performance, a task is allowed to talk to another remote task directly through a TCP/IP connection. Such a direct message passing scheme is known as message direct routing over TCP in <ref> [2] </ref>, or TCP path for short. No pvmd is involved in message passing using the TCP path. In the USC/ISI ATOMIC workstation cluster, communication latency is an average of 1.2 ms and the peak communication bandwidth is 11 Mbps when messages take the default route through respective pvmds. <p> The first two subsections describe the underlying packet flow on the host machine as well as how the overhead of memory copies is reduced. The last subsection presents the details of the protocol . Our implementation uses the same task-to-task communication modes as specified in PVM <ref> [2] </ref>. A receive (pvm recv ()) is a blocking receive which returns only if the data has been received. <p> Besides the expected packet size and the memory pointer (pointing to the allocated PVM buffer reserved for the expected packet), the request also contains the source PVM task ID and the PVM message tag <ref> [2] </ref> used to identify the requested PVM message. After the packet arrives at the host interface, the on-board processor will verify the source task ID and the message tag, and initiate the DMA operation to transfer the packet to the reserved memory.
Reference: [3] <author> J. B. Postel, </author> <title> Transmission Control Protocol. </title> <type> RFC 792, </type> <month> Sept. </month> <year> 1981. </year>
Reference-contexts: PVM [1, 2], a message passing based software system, provides a programming environment that allows a collection of networked workstations to appear as a single parallel computational resource. PVM currently provides facilities for process control and data communication based on the TCP/IP network protocol suite <ref> [3] </ref>. By taking advantage of popular TCP/IP implementations, PVM has become a fl This work is supported by the Advanced Research Projects Agency through Ft. Huachuaca contract #DABT63-93-C-0062 entitled "Netstation Architecture and Advanced Atomic Network".
Reference: [4] <author> R. Felderman, A. DeSchon, D. Cohen, and G. Finn, </author> <title> "ATOMIC: A high-speed local communication architecture," </title> <journal> Journal of High Speed Networks, </journal> <volume> vol. 3, no. 1, </volume> <pages> pp. 1-30, </pages> <year> 1994. </year>
Reference-contexts: In such situations, bulk data transfers frequently occur between the workstations, which can result in performance bottlenecks. Minimizing the communication overhead involved in PVM data transfers for both task parallel and data parallel applications requires an especially high-throughput low-latency form of reliable communication. The ATOMIC <ref> [4] </ref> LAN is a high-speed network that offers 640 Mbps bandwidth. The ATOMIC workstation cluster at USC/ISI consists of a collection of Sun SPARCstation 20s interconnected through the ATOMIC LAN. <p> Switch ports and host interface ports can be connected to one another in arbitrary topologies including trees, meshes, etc. Packet transmission over the ATOMIC LAN has a very low error rate. Packet transfer tests of over 1,000 Terabits (1 fi 10 15 bits) over the prototype ATOMIC LAN <ref> [4] </ref> at USC/ISI resulted in no bit errors and no lost packets. The ATOMIC LAN employs a source-routed, cut-through packet switching technique. In cut-through routing, the header bytes in the packet determine the route, and the remaining data bytes follow in a pipeline fashion.
Reference: [5] <author> P. Druschel, L. L. Peterson, and B. S. Dave, </author> <title> "Experiences with a high-speed network adaptor: A software perspective," </title> <booktitle> in Proceedings of SIG--COMM'94, </booktitle> <pages> pp. 2-13, </pages> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: The CP is also termed multi-interface CP since it provides an independent application device interface, also known as the application device channel in <ref> [5] </ref>, for each process on the host computer to directly communicate with the host interface. Figure 1 shows a dual-interface CP that is able to support a host simultaneously running packets over both IP and the Myrinet-API.
Reference: [6] <author> C. L. Seitz, N. Boden, J. Seizovic, and W. Su, </author> <title> "The design of the caltech mosaic c. multiprocessor," </title> <booktitle> in Proceedings of the Washington Symposium on Integrated Systems, </booktitle> <address> (Seattle), </address> <year> 1993. </year>
Reference-contexts: The original prototype for the ATOMIC LAN was developed by USC/ISI using Caltech's Mosaic chip <ref> [6] </ref>. The network switches, hosts interfaces, and system software are currently provided by Myricom [7] as commercial products. Although Myricom has made many improvements in both hardware and software based on the ATOMIC prototype, its commercial product uses the original ATOMIC networking architecture.
Reference: [7] <author> N. Boden, D. Cohen, R. Felderman, A. Kulawik, C. Seitz, J. Seizovic, and W.-K. Su, "Myrinet: </author> <title> A gigabit-per-second local area network," </title> <journal> IEEE Micro, </journal> <volume> vol. 15, </volume> <pages> pp. 29-35, </pages> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: The original prototype for the ATOMIC LAN was developed by USC/ISI using Caltech's Mosaic chip [6]. The network switches, hosts interfaces, and system software are currently provided by Myricom <ref> [7] </ref> as commercial products. Although Myricom has made many improvements in both hardware and software based on the ATOMIC prototype, its commercial product uses the original ATOMIC networking architecture.
Reference: [8] <author> J. Touch, A. DeSchon, H. Xu, T. Faber, T. Fisher, and A. Sachdev, "ATOMIC-2: </author> <title> Production use of a gigabit lan (abstract)," </title> <booktitle> in Proceedings of Gi-gabit Networking Workshop'95 at INFOCOM'95, </booktitle> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: Although Myricom has made many improvements in both hardware and software based on the ATOMIC prototype, its commercial product uses the original ATOMIC networking architecture. Researchers at USC/ISI are currently using the Myricom product and continuing ATOMIC research <ref> [8] </ref> in supporting the ATOMIC LAN outside the lab environment for daily computing and networking needs. 3 PVM Service Decomposition PVM allows a collection of host computers to appear as a single virtual distributed-memory parallel machine.
Reference: [9] <author> V. Jacobson and R. Braden, </author> <title> TCP Extensions for Long-Delay Paths. </title> <type> RFC 1072, </type> <month> Oct. </month> <year> 1988. </year>
Reference-contexts: This information is included in the ACK and is sent back to atp send (). This scheme is known as selective acknowledgement <ref> [9] </ref>. For example, in Figure 6 (b), five packets are sent from atp send () to the remote atp recv (). Packets 2 and 3 become lost.
Reference: [10] <author> Myricom, </author> <title> Myrinet Link and Routing Specification, </title> <month> Jan. </month> <year> 1995. </year>
Reference-contexts: As a result, the sender host interface will have room to store ACKs even if its regular recv packet queue is full. Congestion Control and Flow Control When congestion occurs in the ATOMIC LAN, Myrinet link-layer back-pressure hardware will prevent senders from injecting more packets into the network <ref> [10] </ref>. Therefore, our protocol does not provide extra software support to deal with congestion control over the ATOMIC LAN. Flow control in the ATOMIC LAN is handled by atp recv () in how it controls the bundle size it advertises to atp send (). <p> This protocol generates a very low RTT (round trip time) for device control command streams. The major reason driving us to implement reliable communication in the host computer is the limitation of the on-board processor speed and on-board memory capacity. With a 128K-byte RAM and 25 MHz LANai processor <ref> [10] </ref>, a Myrinet ATOMIC interface can only afford to support a very limited number of application device interfaces without significantly degrading its performance. Thus, an application device interface may have to be shared by multiple processes among many various user applications including distributed-computing and real-time applications.
Reference: [11] <author> C. A. Thekkath, T. D. Nguyen, E. Moy, and E. D. Lazowska, </author> <title> "Implementing network protocols at user level," </title> <booktitle> in Proceedings of SIG-COMM'93, </booktitle> <pages> pp. 64-73, </pages> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: The related issues of implementing user-level protocols on other network architecture can be found in <ref> [11, 12, 13] </ref>. The host interface is an alternative place to implement a reliable data protocol. An RPC-based (remote procedure call based) reliable protocol [14] has been implemented in the host interface CP in order to minimize the per-packet overhead and optimize communication latency for small messages.
Reference: [12] <author> A. Edwards, G. Watson, J. Lumley, D. Banks, C. Calamvokis, and C. Dalton, </author> <title> "User-space protocols deliver high performance to applications on a low-cost gb/s LAN," </title> <booktitle> in Proceedings of SIG-COMM'94, </booktitle> <pages> pp. 14-23, </pages> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: The related issues of implementing user-level protocols on other network architecture can be found in <ref> [11, 12, 13] </ref>. The host interface is an alternative place to implement a reliable data protocol. An RPC-based (remote procedure call based) reliable protocol [14] has been implemented in the host interface CP in order to minimize the per-packet overhead and optimize communication latency for small messages.
Reference: [13] <author> C. Maeda and B. Bershad, </author> <title> "Protocol service decomposition for high-performance networking," </title> <booktitle> in Proceedings of SIGOPS'93, </booktitle> <pages> pp. 244-255, </pages> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: The related issues of implementing user-level protocols on other network architecture can be found in <ref> [11, 12, 13] </ref>. The host interface is an alternative place to implement a reliable data protocol. An RPC-based (remote procedure call based) reliable protocol [14] has been implemented in the host interface CP in order to minimize the per-packet overhead and optimize communication latency for small messages.
Reference: [14] <author> G. G. Finn, </author> <title> "Device control via the network: Interface limitations and issues," </title> <booktitle> in Proceedings of 4th Annual Principal Investigators Meeting (Networking'94), p. 22, ARPA, </booktitle> <month> Sept. </month> <year> 1995. </year>
Reference-contexts: The related issues of implementing user-level protocols on other network architecture can be found in [11, 12, 13]. The host interface is an alternative place to implement a reliable data protocol. An RPC-based (remote procedure call based) reliable protocol <ref> [14] </ref> has been implemented in the host interface CP in order to minimize the per-packet overhead and optimize communication latency for small messages. This protocol was specificly designed for device control over the ATOMIC LAN in the USC/ISI Netstation Project [15].
Reference: [15] <author> G. G. Finn, </author> <title> "An integration of network communication with workstation architecture," in omputer Communication Review, </title> <address> Oct. </address> <year> 1991. </year>
Reference-contexts: An RPC-based (remote procedure call based) reliable protocol [14] has been implemented in the host interface CP in order to minimize the per-packet overhead and optimize communication latency for small messages. This protocol was specificly designed for device control over the ATOMIC LAN in the USC/ISI Netstation Project <ref> [15] </ref>. This protocol generates a very low RTT (round trip time) for device control command streams. The major reason driving us to implement reliable communication in the host computer is the limitation of the on-board processor speed and on-board memory capacity.
References-found: 15

