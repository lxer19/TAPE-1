URL: ftp://ftp.cs.wisc.edu/machine-learning/shavlik-group/opitz.isiknh94.ps
Refering-URL: http://www.cs.wisc.edu/~shavlik/abstracts/opitz.isiknh94.ps.abstract.html
Root-URL: 
Email: fopitz,shavlikg@cs.wisc.edu  
Title: Genetically Refining Topologies of Knowledge-Based Neural Networks  
Author: David W. Opitz and Jude W. Shavlik 
Address: Madison, WI 53706, U.S.A.  
Affiliation: Computer Sciences Department University of Wisconsin Madison  
Date: May 1994.  
Note: To appear in the Proceedings of the International Symposium on Integrating Knowledge and Neural Heuristics (ISIKNH'94); Pensacola, FL;  
Abstract: Traditional approaches to connectionist theory refinement map the dependencies of a domain-specific rulebase into a neural network, then refine these reformulated rules using neural learning. These approaches have proven to be effective at classifying previously unseen examples; however, most of these approaches suffer in that they are unable to refine the topology of the networks they produce. Thus, when given an impoverished domain theory, they generalize poorly. A recently published improvement to these approaches, the TopGen algorithm, addressed this limitation by heuristically searching expansions to the knowledge-based networks produced by these algorithms. We show, however, that TopGen's search is too restricted. In response, we present the Regent algorithm, which uses genetic algorithms to broaden the type of networks seen during its search. It does this by using (a) the domain theory to help create an initial population and (b) crossover and mutation operators specifically designed for knowledge-based networks. Experiments on three real-world domains indicate that our new algorithm is able to significantly increase generalization when compared to both TopGen and a standard approach that does not alter its knowledge-based network's topology.
Abstract-found: 1
Intro-found: 1
Reference: <author> Dodd, N. </author> <year> (1990). </year> <title> Optimization of network structure using genetic techniques. </title> <booktitle> In Proceedings of the IEEE International Joint Conference on Neural Networks (volume III), </booktitle> <pages> (pp. 965-970), </pages> <address> Paris. </address>
Reference: <author> Fu, L. M. </author> <year> (1989). </year> <title> Integration of neural heuristics into knowledge-based inference. </title> <journal> Connection Science, </journal> <volume> 1 </volume> <pages> 325-340. </pages>
Reference: <author> Goldberg, D. E. </author> <year> (1989). </year> <title> Genetic Algorithms in Search, Optimization, and Machine Learning. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA. </address> <note> 9 Hansen, </note> <author> L. K. & Salamon, P. </author> <year> (1990). </year> <title> Neural network ensembles. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 12 </volume> <pages> 993-1001. </pages>
Reference-contexts: Regent adds newly trained networks to the population only if their validation-set correctness is better than or equal to an existing member of the population. When Regent replaces a member, it chooses the oldest member having the lowest correctness. Other techniques <ref> (Goldberg, 1989) </ref>, such as replacing the member nearest the new candidate network, can promote diverse populations; however, we do not want to promote diversity at the expense of decreased generalization. <p> Hansen and Salamon (1990) showed that combining the output of several neural networks will improve generalization if the individual networks tend to be independent in their error. To help promote this independence, we plan to investigate incorporating techniques that help create subpopulations <ref> (Goldberg, 1989) </ref>, then select a network from each subpopulation. 6 Conclusion Connectionist theory-refinement systems have been shown to be effective at translating a domain theory into a neural network; however, most of these systems, such as the Kbann algorithm, suffer in that they do not alter their topology.
Reference: <author> Harp, S. A., Samad, T., & Guha, A. </author> <year> (1991). </year> <title> Designing application-specific neural networks using the genetic algorithm. </title> <editor> In Touretzky, D., editor, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 2), </booktitle> <pages> (pp. 447-454), </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Holland, J. H. </author> <year> (1975). </year> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> University of Michigan Press, </publisher> <address> Ann Arbor, MI. </address>
Reference: <author> Lacher, R. C., Hruska, S. I., & Kuncicky, D. C. </author> <year> (1992). </year> <title> Back-propagation learning in expert networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 3(1) </volume> <pages> 62-72. </pages>
Reference: <author> MacKay, D. J. </author> <year> (1992). </year> <title> A practical Bayesian framework for backpropagation networks. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 448-472. </pages>
Reference-contexts: Also, as we increase the number of networks searched, Regent may start selecting networks that overfit the validation set. Future work, then, is to investigate selection methods, such as Bayesian techniques <ref> (MacKay, 1992) </ref>, that do not use a validation set. This would also allow us to use all the training instances to train the networks.
Reference: <author> Miller, G. F., Todd, P. M., & Hegde, S. U. </author> <year> (1989). </year> <title> Designing neural networks using genetic algorithms. </title> <booktitle> In Proceedings of the Third International Conference on Genetic Algorithms, </booktitle> <pages> (pp. 379-384), </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Opitz, D. W. & Shavlik, J. W. </author> <year> (1993). </year> <title> Heuristically expanding knowledge-based neural networks. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> (pp. 1360-1365), </pages> <address> Chambery, France. </address>
Reference-contexts: It then refines these reformulated rules using backpropagation. However, Kbann, and other connectionist theory-refinement systems that do not alter their network topologies, suffer when given impoverished domain theories ones that are missing rules needed to adequately learn the true concept (Towell & Shavlik, 1992; Opitz & Shavlik, 1993). TopGen <ref> (Opitz & Shavlik, 1993) </ref> is an improvement to these systems; it heuristically searches through the space of possible network topologies by adding hidden nodes to the neural representation of the domain theory. TopGen showed statistically significant improvements over Kbann in several real-world domains (Opitz & Shavlik, 1993); however, in this paper <p> TopGen <ref> (Opitz & Shavlik, 1993) </ref> is an improvement to these systems; it heuristically searches through the space of possible network topologies by adding hidden nodes to the neural representation of the domain theory. TopGen showed statistically significant improvements over Kbann in several real-world domains (Opitz & Shavlik, 1993); however, in this paper we empirically show that as we increase the number of networks considered, TopGen suffers because it only considers simple expansions of the Kbann network. <p> TopGen showed statistically significant improvements over Kbann in several real-world domains and comparative experiments with a simple approach to adding nodes verified that new nodes must be added in an intelligent manner <ref> (Opitz & Shavlik, 1993) </ref>. Despite this success, TopGen suffers in that it only considers larger networks that contain 3 Table 1: The REGENT Algorithm. GOAL: Search for the best network topology describing the domain theory and data. 1. Set aside a validation set from the training instances. 2.
Reference: <author> Ourston, D. & Mooney, R. J. </author> <year> (1990). </year> <title> Changing the rules: A comprehensive approach to theory refinement. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 815-820), </pages> <address> Boston, MA. </address>
Reference: <author> Pazzani, M. & Kibler, D. </author> <year> (1992). </year> <title> The utility of knowledge in inductive learning. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 57-94. </pages>
Reference: <author> Romaniuk, S. G. </author> <year> (1993). </year> <title> Evolutionary growth perceptrons. </title> <booktitle> In Proceedings of the Fifth International Conference on Genetic Algorithms, </booktitle> <pages> (pp. 334-341), </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Towell, G. & Shavlik, J. </author> <year> (1992). </year> <title> Using symbolic learning to improve knowledge-based neural networks. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 177-182), </pages> <address> San Jose, CA. </address>
Reference: <author> Towell, G., Shavlik, J., & Noordewier, M. </author> <year> (1990). </year> <title> Refinement of approximate domain theories by knowledge-based neural networks. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 861-866), </pages> <address> Boston, MA. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: In this paper, we present such an approach, called Regent (REfining, with Genetic Evolution, Network Topologies), that uses genetic algorithms, along with the aid of a domain theory, to search for a good neural network topology. Kbann <ref> (Towell et al., 1990) </ref> is an example of a connectionist theory-refinement system that translates the provided domain theory into a neural network, thereby determining the network's topology. It then refines these reformulated rules using backpropagation.
Reference: <author> Towell, G. G. </author> <year> (1991). </year> <title> Symbolic Knowledge and Neural Networks: Insertion, Refinement, and Extraction. </title> <type> PhD thesis, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, WI. </institution>
Reference-contexts: Adding nodes in this fashion helps to correct the types of errors that Kbann is ineffective at correcting. For example, Kbann is effective at removing antecedents from existing rules <ref> (Towell, 1991) </ref>, so TopGen attempts to decrease false negatives by adding nodes in a fashion analogous to adding a new rule to the rule base.
Reference: <author> Tresp, V., Hollatz, J., & Ahmad, S. </author> <year> (1992). </year> <title> Network structuring and training using rule-based knowledge. </title> <editor> In Moody, J., Hanson, S., & Lippmann, R., editors, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 5), </booktitle> <pages> (pp. 871-878), </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 10 </pages>
References-found: 16

