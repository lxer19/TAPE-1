URL: http://ki.cs.tu-berlin.de/~scheffer/artikel/scheffer96.ps
Refering-URL: http://ki.cs.tu-berlin.de/~scheffer/publications.html
Root-URL: 
Email: email: scheffer@cs.tu-berlin.de  
Title: Algebraic Foundation and Improved Methods of Induction of Ripple Down Rules  
Author: Tobias Scheffer 
Date: December 5, 1996  
Address: Sekr. FR 5-8, Franklinstr 28-29, D-10587 Berlin, Germany  
Affiliation: Technische Universitat Berlin, Artificial Intelligence Research Group (KI),  
Abstract: Ripple down rules (RDR), that is rules with hierarchical exceptions, are used in knowledge acquisition because they provide a well intelligible and modifiable representation for even very large expert systems. In this paper a formal semantics for RDRs is proposed, that covers first order rules as well as attribute-value based rules. An algebraic foundation is proposed, including simplification of RDRs and transformation of RDRs into flat lists of rules and ripple down rule sets, hence these knowledge representation schemes are put into perspective. It is shown, that a RDR has a shorter description length than an equivalent decision list. Induction of rules with exceptions is characterized as bidirectional movement in the hypothesis space, while known algorithms for learning rules or decision trees either perform a top-down specialization of the most general or a bottom-up generalization of the most special hypothesis. Known algorithms for induction of RDRs are summarized and compared and a bidirectional algorithm is proposed, that handles continuous-valued attributes using an implication based generalization for attribute-value based representations, and shows good empirical results. 
Abstract-found: 1
Intro-found: 1
Reference: [BM90] <author> M. Bain and S. Muggleton. </author> <title> Non-monotonic learning. </title> <editor> In J. E. Hayes-Michie and E. Tyugu, editors, </editor> <booktitle> Machine Intelligence, </booktitle> <volume> volume 12, </volume> <pages> pages 105-119, </pages> <year> 1990. </year>
Reference-contexts: His learning algorithm does not need to delete rules with only a few counterexamples, as would do Shapiro's debugging system [Sha83] or most ILP algorithms. Bain <ref> [BM90] </ref> more formally introduces the most general correct specialization, a specialization operation that excludes a clause from the domain of another rule, Kijsirikul et al. [KNS92] propose a similar approach to predicate invention. <p> In [Wro88] Wrobel proposes an extension of RDT, that counts counterexamples and after exceeding a threshold recursively learns an exception predicate. In contrast, the most general correct specialization <ref> [BM90] </ref> is asymmetrical in a way that counterexamples are excluded from a rule but are not generalized to form an exception concept. 3.3 The Cut95 algorithm The Cut95 algorithm sets up ripple down rules, where rules are conjunctions of interval constraints.
Reference: [Cen87] <author> J. Cendrowska. </author> <title> An algorithm for inducing modular rules. </title> <journal> International Journal for Man-Machine Studies, </journal> <pages> pages 349-370, </pages> <year> 1987. </year>
Reference-contexts: A rule that achieves the desired exactness is no longer corrected, thus over-learning effects are avoided. The algorithm learns classifications in continuous domains. 3.3.2 Examples Fig. 3 shows a RDR that prescribes contact lenses (hard, soft or none) <ref> [Cen87] </ref>. The RDR can be considered quite compact and comprehensible, the corresponding rule set extracted from a decision tree constructed by ID3 [Qui86] needs 9 rules and 30 tests while the system can be modeled with a RDR of 4 rules and 7 tests.
Reference: [CJ88] <author> P. Compton and R. Jansen. </author> <title> Knowledge in context: a strategy for expert system main-tainance. </title> <booktitle> In Proceedings of the 2nd Australian Joint Artificial Intelligence Conference, volume 406 of LNAI, </booktitle> <pages> pages 292-306, </pages> <address> Adelaide, 1988. </address> <publisher> Springer. </publisher>
Reference-contexts: To improve the comprehensibility of large knowledge bases, a number of representational schemes was developed that provide modularity by supporting exceptions [Ver80, Riv87, KMU93b, HSW89, DK95], the most general for attribute-value based representations and most strongly structured scheme being ripple down rules <ref> [CJ88] </ref>. A ripple down rule (RDR) is a list of rules; each rule may be associated to another list of rules: its exceptions. <p> For various examples, it could be shown empirically that Ripple-Down-Rules take less rules to describe complex systems than are needed by a "flat" list of rules <ref> [CPKY94, GC92, PEC94, CJ88] </ref>. fl Proc. <p> Section 3 deals with the automatic induction of RDRs; known approaches are compared and a new algorithm for induction and revision of RDRs is proposed, that handles continuous attributes. 2 Ripple down rule algebra Ripple-Down-Rules (RDR), proposed informally in <ref> [CJ88] </ref>, are rules with hierarchically structured exceptions. Fig. 1 shows an example RDR. Lists of rules connected by "if not"-edges behave like decision lists: only if a rule is not applicable to an input object, the next rule is applied.
Reference: [CPKY94] <author> P. Compton, P. Preston, B. Kang, and T. Yip. </author> <title> Local patching produces compact knowledge bases. In A Future for Knowledge Acquisition / EKAW '94. </title> <publisher> Springer Verlag, </publisher> <year> 1994. </year>
Reference-contexts: For various examples, it could be shown empirically that Ripple-Down-Rules take less rules to describe complex systems than are needed by a "flat" list of rules <ref> [CPKY94, GC92, PEC94, CJ88] </ref>. fl Proc.
Reference: [DK95] <author> Yannis Dimopoulos and Antonis Kakas. </author> <title> Learning non-monotonic logic programs: Learning exceptions. </title> <editor> In Nada Lavrac and Stefan Wrobel, editors, </editor> <booktitle> Machine Learning: ECML-95 (Proc. European Conf. on Machine Learning, 1995), Lecture Notes in Artificial Intelligence 912, </booktitle> <pages> pages 122 - 137. </pages> <publisher> Springer Verlag, </publisher> <year> 1995. </year>
Reference-contexts: Two important aspects of this process addressed in this paper are representation and modularization of knowledge, and support of this process by machine learning algorithms. To improve the comprehensibility of large knowledge bases, a number of representational schemes was developed that provide modularity by supporting exceptions <ref> [Ver80, Riv87, KMU93b, HSW89, DK95] </ref>, the most general for attribute-value based representations and most strongly structured scheme being ripple down rules [CJ88]. A ripple down rule (RDR) is a list of rules; each rule may be associated to another list of rules: its exceptions. <p> In <ref> [DK95] </ref> an algorithm is presented, that uses a classical rule learning algorithm to find interesting rules and recursively learns their exceptions: 1.
Reference: [GC92] <author> B. R. Gaines and P. J. Compton. </author> <title> Induction of ripple down rules. </title> <booktitle> 5th Australian Conference on Artificial Intelligence, </booktitle> <pages> pages 349-355, </pages> <year> 1992. </year>
Reference-contexts: For various examples, it could be shown empirically that Ripple-Down-Rules take less rules to describe complex systems than are needed by a "flat" list of rules <ref> [CPKY94, GC92, PEC94, CJ88] </ref>. fl Proc. <p> The rule can then be re-specialized (top-down) by appending a literal "not (ExceptionConcept)" and solving the nested learning problem by generalizing the counterexamples and thus generating rules for the newly invented exception concept. This principle is reflected in Vere [Ver80] and the newly proposed approach but not in <ref> [GC92] </ref>, where learning is strictly top-down, or in [KMU93b], were learning is performed by searching a special region of the graph that is determined by a fixed exception depth. 3.2 Known approaches for induction of RDRs Vere [Ver80] proposed the following algorithm Counterfactuals that contains the basic principle of most learning <p> RDRs Vere [Ver80] proposed the following algorithm Counterfactuals that contains the basic principle of most learning algorithms dealing with exceptions: 1. find an interesting rule by generalizing samples, that possibly covers some negative examples 2. learn the exceptions recursively (except branch) 3. learn the remaining samples recursively (if-not branch) Gaines <ref> [GC92] </ref> proposed the similar algorithm Induct that is much more concrete in what is an interesting rule. It is obvious, that the quality of a RDR learning algorithm is strongly related to the strategy for finding interesting rules. 1. <p> The RDR can be considered quite compact and comprehensible, the corresponding rule set extracted from a decision tree constructed by ID3 [Qui86] needs 9 rules and 30 tests while the system can be modeled with a RDR of 4 rules and 7 tests. Induct with exceptions <ref> [GC92] </ref> yields a tree of 4 rules and 8 tests, the difference is a benefit of the proposed reduction rule. Both algorithms yield exact classifiers (hit rate 1).
Reference: [HSW89] <author> D. Helmbold, R. Sloan, and M. K. Warmuth. </author> <title> Learning nested differences of intersection-closed concept classes. </title> <booktitle> Proceedings of the Workshop on Computational Learning Theory, </booktitle> <pages> pages 41-56, </pages> <year> 1989. </year>
Reference-contexts: Two important aspects of this process addressed in this paper are representation and modularization of knowledge, and support of this process by machine learning algorithms. To improve the comprehensibility of large knowledge bases, a number of representational schemes was developed that provide modularity by supporting exceptions <ref> [Ver80, Riv87, KMU93b, HSW89, DK95] </ref>, the most general for attribute-value based representations and most strongly structured scheme being ripple down rules [CJ88]. A ripple down rule (RDR) is a list of rules; each rule may be associated to another list of rules: its exceptions. <p> Given a conclusion, find the rule that is least likely to predict that conclusion by chance 2. learn the exceptions recursively (except branch) 3. learn the remaining samples recursively (if-not branch) In <ref> [HSW89] </ref> an algorithm is proposed, that learns nested exceptions by repeatedly covering positive and negative examples, yet the algorithm does not deal with disjunctions of rules in the exception levels; this simplifies the learning problem. <p> The algorithm proposed in <ref> [HSW89] </ref> does not take that into account, because only one rule rather than a disjunction of rules is learned at each exception level, which is a major limitation.
Reference: [KCP95] <author> B. Kang, P. Compton, and P. Preston. </author> <title> Multiple classification ripple down rules: evaluation and possibilities. </title> <editor> In B. Gaines and M. Musen, editors, </editor> <booktitle> Proc. 9th Banff Knowledge Acquisition for Knowledge Based Systems Workshop, </booktitle> <pages> pages 17.1-17.20, </pages> <year> 1995. </year>
Reference-contexts: Ripple down rule sets (RDR-sets) [KMU93a], also called multiple classification ripple down rules (MC-RDR) <ref> [KCP95] </ref>, differ from RDRs in that rules in a list of successors may fire synchronously, while in a RDR, each rule suppresses its successors.
Reference: [KMU93a] <author> J. Kivinen, H. Mannila, and E. Ukkonen. </author> <title> Learning rules with local exceptions. </title> <booktitle> In Proceedings of the European Conference on Computational Learning Theory (COLT), </booktitle> <year> 1993. </year>
Reference-contexts: The implied concept of locality (an exception is applicable only if its next-general rule is applicable) makes RDRs comfortable for human needs: rough rules can be expressed first, the exceptions of which can be modeled as a refinement of the hypothesis later. Ripple down rule sets (RDR-sets) <ref> [KMU93a] </ref>, also called multiple classification ripple down rules (MC-RDR) [KCP95], differ from RDRs in that rules in a list of successors may fire synchronously, while in a RDR, each rule suppresses its successors. <p> A problem about RDRs is that the rules are implicitly augmented with the negations of all preceding rules, because the succeeding rules may only be applied if the current rule does not apply; this might spoil the good comprehensibility. In <ref> [KMU93a] </ref> ripple down rule sets (RDR-set) are proposed; here all rules in a list of successors may fire unless suppressed by their exceptions. Note that in contrast to a RDR, a RDR-set may yield more than one result even for propositional rules and observations.
Reference: [KMU93b] <author> Jyrki Kivinen, Heikki Mannila, and Esko Ukkonen. </author> <title> Learning hierarchical rule sets. </title> <booktitle> Proceedings of the ACM Workshop of Computational Learning Theory, </booktitle> <year> 1993. </year>
Reference-contexts: Two important aspects of this process addressed in this paper are representation and modularization of knowledge, and support of this process by machine learning algorithms. To improve the comprehensibility of large knowledge bases, a number of representational schemes was developed that provide modularity by supporting exceptions <ref> [Ver80, Riv87, KMU93b, HSW89, DK95] </ref>, the most general for attribute-value based representations and most strongly structured scheme being ripple down rules [CJ88]. A ripple down rule (RDR) is a list of rules; each rule may be associated to another list of rules: its exceptions. <p> This principle is reflected in Vere [Ver80] and the newly proposed approach but not in [GC92], where learning is strictly top-down, or in <ref> [KMU93b] </ref>, were learning is performed by searching a special region of the graph that is determined by a fixed exception depth. 3.2 Known approaches for induction of RDRs Vere [Ver80] proposed the following algorithm Counterfactuals that contains the basic principle of most learning algorithms dealing with exceptions: 1. find an interesting <p> The algorithm proposed in [HSW89] does not take that into account, because only one rule rather than a disjunction of rules is learned at each exception level, which is a major limitation. In <ref> [KMU93b] </ref> Mannila et al. present an Occam algorithm that learns sets of rules with exception to a fixed depth by repeatedly solving weighted set covering problems. In [Wro88] Wrobel proposes an extension of RDT, that counts counterexamples and after exceeding a threshold recursively learns an exception predicate.
Reference: [KNS92] <author> B. Kijsirikul, M. Numao, and M. Shimura. </author> <title> Discrimination-based constructive induction of logic programs. </title> <booktitle> In Proc. National Conf. on AI (AAAI), </booktitle> <pages> pages 44-49, </pages> <year> 1992. </year>
Reference-contexts: Bain [BM90] more formally introduces the most general correct specialization, a specialization operation that excludes a clause from the domain of another rule, Kijsirikul et al. <ref> [KNS92] </ref> propose a similar approach to predicate invention. Section 2 of this paper deals with the semantics and algebraic foundations of RDRs, including the simplification and transformation of RDRs into decision lists and ripple down rule sets.
Reference: [MB88] <author> S. H. Muggleton and W. Buntine. </author> <title> Machine invention of first-order predicates by inverting resolution. </title> <booktitle> In Proc. 5th International Conference on Machine Learning, </booktitle> <pages> pages 339-352, </pages> <year> 1988. </year>
Reference-contexts: Thus, for these algorithms, movement in the hypothesis graph is top-down. Learning by generalizing samples has mainly been performed in the area of ILP (e.g. <ref> [Plo70, MB88] </ref>).
Reference: [Mit82] <author> T. M. Mitchell. </author> <title> Generalization as search. </title> <journal> AI Journal, </journal> <volume> 18 </volume> <pages> 203-226, </pages> <year> 1982. </year>
Reference-contexts: Figure 5 shows an example transformation. 3 Learning rules with exceptions In this section, known approaches to induction of rules with exception are summarized and compared. A new algorithm with interesting properties is presented. The algorithms are described from a search in the hypothesis space point of view <ref> [Mit82] </ref>. The hypothesis space is a graph, the nodes of which are rules and the edges of which are set up by a more-specific-than relation.
Reference: [MMHL86] <author> R. S Michalski, I. Mozetic, J. Hong, and N Lavrac. </author> <title> The multi-purpose incremental learning system aq15 and its testing application to three medical domains. </title> <booktitle> Proceedings of the Fifth National Conference on Artificial Intelligence, </booktitle> <pages> pages 1041-1045, </pages> <year> 1986. </year>
Reference-contexts: covered by r 1 are a subset of the instances covered by r 2 . 3.1 Learning as movement in the hypothesis space Learning algorithms that generate a concept description by specializing an initially most general hypothesis, e.g. decision tree algorithms (e.g. [UW72, Qui86]), algorithms for induction of rules (e.g. <ref> [MMHL86] </ref>) or specialization based ILP techniques, e.g. MIS [Sha83], FOIL [Qui90], RDT [WK92], can be characterized by top-down movement starting from the topmost or a node close to the topmost node.
Reference: [MST94] <author> D. Michie, D. J. Spiegelhalter, and C. C. Taylor. </author> <title> Machine Learning, Neural and Statistical Classification. </title> <publisher> Ellis Horwood, </publisher> <year> 1994. </year>
Reference-contexts: Induct with exceptions [GC92] yields a tree of 4 rules and 8 tests, the difference is a benefit of the proposed reduction rule. Both algorithms yield exact classifiers (hit rate 1). On the Shuttle dataset <ref> [MST94] </ref>, containing 58,000 samples with discrete attributes and seven multimodally distributed classes, Cut95 outperforms all known machine learning, statistical and neural network learning algorithms. Learning time did not exceed 15 minutes. Good results were also achieved for the diabetes dataset (12-fold cross-validation) [MST94]; although the dataset is very noisy and good <p> On the Shuttle dataset <ref> [MST94] </ref>, containing 58,000 samples with discrete attributes and seven multimodally distributed classes, Cut95 outperforms all known machine learning, statistical and neural network learning algorithms. Learning time did not exceed 15 minutes. Good results were also achieved for the diabetes dataset (12-fold cross-validation) [MST94]; although the dataset is very noisy and good hypotheses are very simple (DIPOL [SW94] only uses one hyper-plane), Cut95 outperforms all symbolic algorithms. The experiment settings for both, the shuttle and the diabetes dataset were due to the specifications used in the ESPRIT project StatLog, described in [MST94]. <p> (12-fold cross-validation) <ref> [MST94] </ref>; although the dataset is very noisy and good hypotheses are very simple (DIPOL [SW94] only uses one hyper-plane), Cut95 outperforms all symbolic algorithms. The experiment settings for both, the shuttle and the diabetes dataset were due to the specifications used in the ESPRIT project StatLog, described in [MST94]. A single trial was used for the shuttle dataset, while 12-fold cross-validation was performed to obtain a confident estimation of the error for the diabetes dataset.
Reference: [PEC94] <author> P. Preston, G. Edwards, and P. Compton. </author> <title> A 2000 rule expert system without a knowledge engineer. </title> <booktitle> In Proc. AAAI Knowledge Acquisition for Knowledge Based Systems Workshop, </booktitle> <year> 1994. </year>
Reference-contexts: For various examples, it could be shown empirically that Ripple-Down-Rules take less rules to describe complex systems than are needed by a "flat" list of rules <ref> [CPKY94, GC92, PEC94, CJ88] </ref>. fl Proc.
Reference: [Plo70] <author> G. D. Plotkin. </author> <title> A note on inductive generalization. </title> <editor> In B. Meltzer and D. Michie, editors, </editor> <booktitle> Machine Intelligence, </booktitle> <volume> volume 5, </volume> <pages> pages 153-163, </pages> <year> 1970. </year>
Reference-contexts: Thus, for these algorithms, movement in the hypothesis graph is top-down. Learning by generalizing samples has mainly been performed in the area of ILP (e.g. <ref> [Plo70, MB88] </ref>).
Reference: [Qui86] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1(1), </volume> <year> 1986. </year>
Reference-contexts: a rule r 2 , if the instances covered by r 1 are a subset of the instances covered by r 2 . 3.1 Learning as movement in the hypothesis space Learning algorithms that generate a concept description by specializing an initially most general hypothesis, e.g. decision tree algorithms (e.g. <ref> [UW72, Qui86] </ref>), algorithms for induction of rules (e.g. [MMHL86]) or specialization based ILP techniques, e.g. MIS [Sha83], FOIL [Qui90], RDT [WK92], can be characterized by top-down movement starting from the topmost or a node close to the topmost node. <p> The algorithm learns classifications in continuous domains. 3.3.2 Examples Fig. 3 shows a RDR that prescribes contact lenses (hard, soft or none) [Cen87]. The RDR can be considered quite compact and comprehensible, the corresponding rule set extracted from a decision tree constructed by ID3 <ref> [Qui86] </ref> needs 9 rules and 30 tests while the system can be modeled with a RDR of 4 rules and 7 tests. Induct with exceptions [GC92] yields a tree of 4 rules and 8 tests, the difference is a benefit of the proposed reduction rule.
Reference: [Qui90] <author> J. R. Quinlan. </author> <title> Learning logical definitions from releations. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 239-266, </pages> <year> 1990. </year>
Reference-contexts: MIS [Sha83], FOIL <ref> [Qui90] </ref>, RDT [WK92], can be characterized by top-down movement starting from the topmost or a node close to the topmost node. In a decision tree, each node is associated to a node in the graph that is given by the attributes/propositions tested so far.
Reference: [Riv87] <author> R. L. Rivest. </author> <title> Learning decision lists. </title> <journal> Machine Learning, </journal> <volume> 2(2) </volume> <pages> 229-246, </pages> <year> 1987. </year>
Reference-contexts: Two important aspects of this process addressed in this paper are representation and modularization of knowledge, and support of this process by machine learning algorithms. To improve the comprehensibility of large knowledge bases, a number of representational schemes was developed that provide modularity by supporting exceptions <ref> [Ver80, Riv87, KMU93b, HSW89, DK95] </ref>, the most general for attribute-value based representations and most strongly structured scheme being ripple down rules [CJ88]. A ripple down rule (RDR) is a list of rules; each rule may be associated to another list of rules: its exceptions. <p> Ripple down rule sets (RDR-sets) [KMU93a], also called multiple classification ripple down rules (MC-RDR) [KCP95], differ from RDRs in that rules in a list of successors may fire synchronously, while in a RDR, each rule suppresses its successors. Decision lists <ref> [Riv87] </ref> contain rules that suppress their successors hence negation and exceptions can be expressed but there is no distinction between rules, the instances of which are proper subsets, and rules that partially overlap. <p> Thus, if the proposition holds for the subtrees X and N , it holds for hrule; X; N i. Fig. 3 shows an example reduction. Each node's context is augmented boldly. Tests that are always satisfied within their context, are removed. 2.9 RDRs and decision lists Decision lists <ref> [Riv87] </ref> are "flat" lists of rules which are applied only if none of their predecessors are applicable. That is, the first applicable rule classifies the input. Obviously, a RDR containing no exceptions is a decision list.
Reference: [Sha83] <author> E. Y. Shapiro. </author> <title> Algorithmic Program Debugging. </title> <publisher> MIT Press, </publisher> <year> 1983. </year>
Reference-contexts: His learning algorithm does not need to delete rules with only a few counterexamples, as would do Shapiro's debugging system <ref> [Sha83] </ref> or most ILP algorithms. Bain [BM90] more formally introduces the most general correct specialization, a specialization operation that excludes a clause from the domain of another rule, Kijsirikul et al. [KNS92] propose a similar approach to predicate invention. <p> MIS <ref> [Sha83] </ref>, FOIL [Qui90], RDT [WK92], can be characterized by top-down movement starting from the topmost or a node close to the topmost node. In a decision tree, each node is associated to a node in the graph that is given by the attributes/propositions tested so far.
Reference: [SW94] <author> B. Schulmeister and F. Wysotzki. </author> <title> The piecewise linear classifier dipol92. </title> <editor> In F. Bergadano and L. De Raedt, editors, </editor> <booktitle> Machine Learning: </booktitle> <address> ECML-94, </address> <publisher> LNAI 784. Springer Verlag, </publisher> <year> 1994. </year>
Reference-contexts: Learning time did not exceed 15 minutes. Good results were also achieved for the diabetes dataset (12-fold cross-validation) [MST94]; although the dataset is very noisy and good hypotheses are very simple (DIPOL <ref> [SW94] </ref> only uses one hyper-plane), Cut95 outperforms all symbolic algorithms. The experiment settings for both, the shuttle and the diabetes dataset were due to the specifications used in the ESPRIT project StatLog, described in [MST94].
Reference: [UW72] <author> P. Ulbrich and F. Wysotzki. </author> <title> Metaalgoritmen zur konstruktion von klassifizierungsal-gorithmen und ihre simulation auf rechenautomaten. </title> <journal> Kybernetik-Forschung, </journal> <volume> 1, </volume> <year> 1972. </year>
Reference-contexts: a rule r 2 , if the instances covered by r 1 are a subset of the instances covered by r 2 . 3.1 Learning as movement in the hypothesis space Learning algorithms that generate a concept description by specializing an initially most general hypothesis, e.g. decision tree algorithms (e.g. <ref> [UW72, Qui86] </ref>), algorithms for induction of rules (e.g. [MMHL86]) or specialization based ILP techniques, e.g. MIS [Sha83], FOIL [Qui90], RDT [WK92], can be characterized by top-down movement starting from the topmost or a node close to the topmost node.
Reference: [Ver80] <author> S. A. Vere. </author> <title> Multilevel counterfactuals for generalizations of relational concepts and productions. </title> <journal> Artificial Intelligence, </journal> <volume> 14 </volume> <pages> 139-164, </pages> <year> 1980. </year>
Reference-contexts: Two important aspects of this process addressed in this paper are representation and modularization of knowledge, and support of this process by machine learning algorithms. To improve the comprehensibility of large knowledge bases, a number of representational schemes was developed that provide modularity by supporting exceptions <ref> [Ver80, Riv87, KMU93b, HSW89, DK95] </ref>, the most general for attribute-value based representations and most strongly structured scheme being ripple down rules [CJ88]. A ripple down rule (RDR) is a list of rules; each rule may be associated to another list of rules: its exceptions. <p> In the area of machine learning, a number of algorithms were developed, that are based on hypothesis languages that include exceptions. For first order hypothesis languages Vere <ref> [Ver80] </ref> proposed a notation of "multilevel counterfactuals", that is quite similar to the RDR notation, as well as a learning algorithm, most later approaches are based on or are similar to; Wrobel [Wro88] introduces the invention of exception predicates. <p> The rule can then be re-specialized (top-down) by appending a literal "not (ExceptionConcept)" and solving the nested learning problem by generalizing the counterexamples and thus generating rules for the newly invented exception concept. This principle is reflected in Vere <ref> [Ver80] </ref> and the newly proposed approach but not in [GC92], where learning is strictly top-down, or in [KMU93b], were learning is performed by searching a special region of the graph that is determined by a fixed exception depth. 3.2 Known approaches for induction of RDRs Vere [Ver80] proposed the following algorithm <p> is reflected in Vere <ref> [Ver80] </ref> and the newly proposed approach but not in [GC92], where learning is strictly top-down, or in [KMU93b], were learning is performed by searching a special region of the graph that is determined by a fixed exception depth. 3.2 Known approaches for induction of RDRs Vere [Ver80] proposed the following algorithm Counterfactuals that contains the basic principle of most learning algorithms dealing with exceptions: 1. find an interesting rule by generalizing samples, that possibly covers some negative examples 2. learn the exceptions recursively (except branch) 3. learn the remaining samples recursively (if-not branch) Gaines [GC92] proposed the
Reference: [WK92] <author> S. Wrobel and J.-U. Kietz. </author> <title> Controlling the complexity of learning in logic through syntactic and task-oriented models. </title> <editor> In S. Muggleton, editor, </editor> <booktitle> Inductive Logic Programming, </booktitle> <address> London, 1992. </address> <publisher> Academic Press. </publisher>
Reference-contexts: MIS [Sha83], FOIL [Qui90], RDT <ref> [WK92] </ref>, can be characterized by top-down movement starting from the topmost or a node close to the topmost node. In a decision tree, each node is associated to a node in the graph that is given by the attributes/propositions tested so far.
Reference: [Wro88] <author> S. Wrobel. </author> <title> Automatic representation adjustment in an observational discovery system. </title> <booktitle> In Proc. European Working Session on Learning, </booktitle> <year> 1988. </year>
Reference-contexts: For first order hypothesis languages Vere [Ver80] proposed a notation of "multilevel counterfactuals", that is quite similar to the RDR notation, as well as a learning algorithm, most later approaches are based on or are similar to; Wrobel <ref> [Wro88] </ref> introduces the invention of exception predicates. His learning algorithm does not need to delete rules with only a few counterexamples, as would do Shapiro's debugging system [Sha83] or most ILP algorithms. <p> In [KMU93b] Mannila et al. present an Occam algorithm that learns sets of rules with exception to a fixed depth by repeatedly solving weighted set covering problems. In <ref> [Wro88] </ref> Wrobel proposes an extension of RDT, that counts counterexamples and after exceeding a threshold recursively learns an exception predicate.
References-found: 26

