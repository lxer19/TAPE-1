URL: http://www.cs.helsinki.fi/~mannila/postscripts/randomized-icdt.ps
Refering-URL: http://www.cs.helsinki.fi/research/pmdm/publications/
Root-URL: 
Email: Heikki.Mannila@cs.helsinki.fi.  
Phone: 2  
Title: Discovering all Most Specific Sentences by Randomized Algorithms Extended Abstract  
Author: Dimitrios Gunopulos and Heikki Mannila and Sanjeev Saluja 
Note: Work supported by Alexander von Humbold-Stiftung and the  
Web: gunopulo@mpi-sb.mpg.de  saluja@mpi-sb.mpg.de  
Address: Im Stadtwald, 66123 Saarbrucken, Germany.  FIN-00014 Helsinki, Finland.  Im Stadtwald, 66123 Saarbrucken, Germany.  
Affiliation: 1 Max-Planck-Insitut Informatik,  University of Helsinki, Dept. of Computer Science,  Academy of Finland. 3 Max-Planck-Institut Informatik,  
Abstract: Data mining can in many instances be viewed as the task of computing a representation of a theory of a model or a database. In this paper we present a randomized algorithm that can be used to compute the representation of a theory in terms of the most specific sentences of that theory. In addition to randomization, the algorithm uses a generalization of the concept of hypergraph transversal. We apply the general algorithm, for discovering maximal frequent sets in 0/1 data, and for computing minimal keys in relations. We present some empirical results on the performance of these methods on real data. We also show some complexity theoretic evidence of the hardness of these problems.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> R. Agrawal, T. Imielinski, and A. Swami. </author> <title> Mining association rules between sets of items in large databases. </title> <booktitle> In Proceedings of ACM SIGMOD Conference on Management of Data (SIGMOD'93), </booktitle> <pages> pages 207 - 216, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The computation of maximal frequent sets is a fundamental data mining problem which is required in discovering association rules <ref> [1, 2] </ref>. Minimal keys can be used for semantic query optimization, which leads to fast query processing in database systems [22, 18, 5, 26]. Here we refer to possible keys that exist in a specific instance of a relational database and are not designed as such. <p> This iterative procedure is performed until no more sentences in T h are found. This level-wise algorithm has been used in various forms in finding association rules, episodes, sequential rules, etc. <ref> [2, 3, 24, 23, 1, 13, 14, 25] </ref>. The drawback with this algorithm is that it always computes the whole set T h (L; r; q), even in the cases where a condensed representation of T h using most specific sentences would be useful. <p> The computational problem that we study in this section is the following. Problem 3 Given a 0/1 relation r over attributes R, and a support value 2 <ref> [0; 1] </ref>, find all maximal -frequent sets of r. We start by presenting two results which show the computational hardness of the above problem. Theorem 4 The problem of finding the number of -frequent sets of a given 0-1 relation r and a threshold 2 [0; 1] is #P-hard. <p> and a support value 2 <ref> [0; 1] </ref>, find all maximal -frequent sets of r. We start by presenting two results which show the computational hardness of the above problem. Theorem 4 The problem of finding the number of -frequent sets of a given 0-1 relation r and a threshold 2 [0; 1] is #P-hard. In addition, the problem of deciding if there is a maximal -frequent set with at least t attributes for a given 0/1 relation r, and a threshold 2 [0; 1], is NP-complete. <p> problem of finding the number of -frequent sets of a given 0-1 relation r and a threshold 2 <ref> [0; 1] </ref> is #P-hard. In addition, the problem of deciding if there is a maximal -frequent set with at least t attributes for a given 0/1 relation r, and a threshold 2 [0; 1], is NP-complete. Proof: Finding the number of -frequent sets is shown to be #P-hard by reducing the problem of computing the number of satisfying assignments of a monotone-2CNF (shown to be #P-hard by [28]) to it.
Reference: 2. <author> R. Agrawal, H. Mannila, R. Srikant, H. Toivonen, and A. I. Verkamo. </author> <title> Fast discovery of association rules. </title> <editor> In U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 307 - 328. </pages> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA, </address> <year> 1996. </year>
Reference-contexts: The computation of maximal frequent sets is a fundamental data mining problem which is required in discovering association rules <ref> [1, 2] </ref>. Minimal keys can be used for semantic query optimization, which leads to fast query processing in database systems [22, 18, 5, 26]. Here we refer to possible keys that exist in a specific instance of a relational database and are not designed as such. <p> The roots of this approach are in the use of diagrams of models in model theory (see, e.g., [7]). The approach has been used in various forms for example in <ref> [2, 8, 9, 16, 17, 21] </ref>. One should note that in contrast with, e.g., [8] our emphasis is on very simple representation languages. <p> This iterative procedure is performed until no more sentences in T h are found. This level-wise algorithm has been used in various forms in finding association rules, episodes, sequential rules, etc. <ref> [2, 3, 24, 23, 1, 13, 14, 25] </ref>. The drawback with this algorithm is that it always computes the whole set T h (L; r; q), even in the cases where a condensed representation of T h using most specific sentences would be useful. <p> Another interesting possibility we plan to explore is to use the randomized algorithm in combination with the level-wise algorithm. The randomized algorithm for maximal -frequent sets can be used to select the right range for , as a preprocessing step to the level-wise algorithm of Agrawal et. al. <ref> [2] </ref>.
Reference: 3. <author> R. Agrawal and R. Srikant. </author> <title> Mining sequential patterns. </title> <booktitle> In International Conference on Data Engineering, </booktitle> <month> Mar. </month> <year> 1995. </year>
Reference-contexts: This iterative procedure is performed until no more sentences in T h are found. This level-wise algorithm has been used in various forms in finding association rules, episodes, sequential rules, etc. <ref> [2, 3, 24, 23, 1, 13, 14, 25] </ref>. The drawback with this algorithm is that it always computes the whole set T h (L; r; q), even in the cases where a condensed representation of T h using most specific sentences would be useful.
Reference: 4. <author> S. Bell. </author> <title> Deciding distinctness of query results by discovered constraints. </title> <type> Manuscript. </type>
Reference-contexts: The knowledge of all minimal keys in the relation instance can help in semantic query optimization. In this process, a database manager substitutes a computationally expensive query by a semantically equivalent query which can be processed much faster ( <ref> [4] </ref>). However, the problem of finding the minimal keys turns out to be a difficult one. Theorem 9 The problem of finding the number of minimal keys of a relation r is #P-hard. Proof: We present two polynomial time reductions.
Reference: 5. <author> S. Bell and P. Brockhausen. </author> <title> Discovery of data dependencies in relational databases. </title> <type> Technical Report LS-8 14, </type> <institution> Universitat Dortmund, Fachbereich Informatik, Lehrstuhl VIII, Kunstliche Intelligenz, </institution> <year> 1995. </year>
Reference-contexts: The computation of maximal frequent sets is a fundamental data mining problem which is required in discovering association rules [1, 2]. Minimal keys can be used for semantic query optimization, which leads to fast query processing in database systems <ref> [22, 18, 5, 26] </ref>. Here we refer to possible keys that exist in a specific instance of a relational database and are not designed as such. The computation of sentences of a theory is an enumeration problem. The computation involves listing combinatorial substructures related with the input.
Reference: 6. <author> C. Berge. </author> <title> Hypergraphs. Combinatorics of Finite Sets. </title> <publisher> North-Holland Publishing Company, </publisher> <address> Amsterdam, </address> <year> 1989. </year>
Reference-contexts: A minimal orthogonal element with respect to S is a sentence in L, such that is unrelated to all the sentences in S under and for no sentence , this property holds. The definition of minimal orthogonal elements is a natural extension of the minimal hypergraph transversal <ref> [6, 10] </ref>. Then we can show the following lemma: Lemma 1 Let S be a set of most specific sentences in M T h, and let T be the set of all minimal orthogonal elements with respect to S.
Reference: 7. <author> C. C. Chang and H. J. Keisler. </author> <title> Model Theory. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1973. </year> <note> 3rd ed., </note> <year> 1990. </year>
Reference-contexts: The roots of this approach are in the use of diagrams of models in model theory (see, e.g., <ref> [7] </ref>). The approach has been used in various forms for example in [2, 8, 9, 16, 17, 21]. One should note that in contrast with, e.g., [8] our emphasis is on very simple representation languages.
Reference: 8. <author> L. De Raedt and M. Bruynooghe. </author> <title> A theory of clausal discovery. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence (IJCAI-93), </booktitle> <pages> pages 1058 - 1053, </pages> <address> Chambery, France, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The roots of this approach are in the use of diagrams of models in model theory (see, e.g., [7]). The approach has been used in various forms for example in <ref> [2, 8, 9, 16, 17, 21] </ref>. One should note that in contrast with, e.g., [8] our emphasis is on very simple representation languages. <p> The roots of this approach are in the use of diagrams of models in model theory (see, e.g., [7]). The approach has been used in various forms for example in [2, 8, 9, 16, 17, 21]. One should note that in contrast with, e.g., <ref> [8] </ref> our emphasis is on very simple representation languages. Obviously, if L is infinite and q (r; ') is satisfied for infinitely many sentences, (an explicit representation of) T h (L; r; q) cannot be computed feasibly.
Reference: 9. <author> L. De Raedt and S. Dzeroski. </author> <title> First-order jk-clausal theories are PAC-learnable. </title> <journal> Artificial Intelligence, </journal> <volume> 70:375 - 392, </volume> <year> 1994. </year>
Reference-contexts: The roots of this approach are in the use of diagrams of models in model theory (see, e.g., [7]). The approach has been used in various forms for example in <ref> [2, 8, 9, 16, 17, 21] </ref>. One should note that in contrast with, e.g., [8] our emphasis is on very simple representation languages.
Reference: 10. <author> T. Eiter and G. Gottlob. </author> <title> Identifying the minimal transversals of a hypergraph and related problems. </title> <journal> SIAM Journal on Computing, </journal> <volume> 24(6):1278 - 1304, </volume> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: A minimal orthogonal element with respect to S is a sentence in L, such that is unrelated to all the sentences in S under and for no sentence , this property holds. The definition of minimal orthogonal elements is a natural extension of the minimal hypergraph transversal <ref> [6, 10] </ref>. Then we can show the following lemma: Lemma 1 Let S be a set of most specific sentences in M T h, and let T be the set of all minimal orthogonal elements with respect to S.
Reference: 11. <editor> U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors. </editor> <booktitle> Advances in Knowledge Discovery and Data Mining. </booktitle> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA, </address> <year> 1996. </year>
Reference-contexts: 1 Introduction Data mining has recently emerged as an important application area <ref> [11] </ref>. The goal of data mining can briefly be stated as "finding useful high-level knowledge from large masses of data". The area combines methods and tools from databases, machine learning, and statistics.
Reference: 12. <author> M. Garey and D. Johnson. </author> <title> Computers and Intractability A Guide to the Theory of NP-Completeness. W.H. </title> <publisher> Freeman, </publisher> <address> New York, </address> <year> 1979. </year>
Reference-contexts: Deciding if there is a maximal -frequent set with at least t attributes can be shown to be NP-complete by reducing the Balanced Bipartite Clique problem (known to be NP-hard, <ref> [12] </ref>), to it. 2 This theorem rules out the possibility of an efficient algorithm which outputs the maximal frequent sets in the decreasing order of their size. We now discuss a refinement of the algorithm of Section 3 for computing all maximal frequent sets.
Reference: 13. <author> J. Han and Y. Fu. </author> <title> Discovery of multiple-level association rules from large data-bases. </title> <booktitle> In Proceedings of the 21st International Conference on Very Large Data Bases (VLDB'95), </booktitle> <pages> pages 420 - 431, </pages> <address> Zurich, Swizerland, </address> <year> 1995. </year>
Reference-contexts: This iterative procedure is performed until no more sentences in T h are found. This level-wise algorithm has been used in various forms in finding association rules, episodes, sequential rules, etc. <ref> [2, 3, 24, 23, 1, 13, 14, 25] </ref>. The drawback with this algorithm is that it always computes the whole set T h (L; r; q), even in the cases where a condensed representation of T h using most specific sentences would be useful.
Reference: 14. <author> M. Houtsma and A. Swami. </author> <title> Set-oriented mining of association rules. </title> <type> Research Report RJ 9567, </type> <institution> IBM Almaden Research Center, </institution> <address> San Jose, California, </address> <month> October </month> <year> 1993. </year>
Reference-contexts: This iterative procedure is performed until no more sentences in T h are found. This level-wise algorithm has been used in various forms in finding association rules, episodes, sequential rules, etc. <ref> [2, 3, 24, 23, 1, 13, 14, 25] </ref>. The drawback with this algorithm is that it always computes the whole set T h (L; r; q), even in the cases where a condensed representation of T h using most specific sentences would be useful.
Reference: 15. <author> D. S. Johnson, M. Yannakakis, and C. H. Papadimitriou. </author> <title> On generating all maximal independent sets. </title> <journal> Information Processing Letters, </journal> <volume> 27 </volume> <pages> 119-123, </pages> <year> 1988. </year>
Reference-contexts: Computing the min-ortho-elements is in general computationally non-trivial. In the problems we consider the problem reduces to the problem of computing the minimal traversals of a hypergraph, a problem for which no output polynomial algorithm is known <ref> [15] </ref>. We use a simple enumeration heuristic that attempts to cut down the number of sets that cannot be transversals. This algorithm is exponential in the worst case.
Reference: 16. <author> J.-U. Kietz and S. Wrobel. </author> <title> Controlling the complexity of learning in logic through syntactic and task-oriented models. </title> <editor> In S. Muggleton, editor, </editor> <booktitle> Inductive Logic Programming, </booktitle> <pages> pages 335 - 359. </pages> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1992. </year>
Reference-contexts: The roots of this approach are in the use of diagrams of models in model theory (see, e.g., [7]). The approach has been used in various forms for example in <ref> [2, 8, 9, 16, 17, 21] </ref>. One should note that in contrast with, e.g., [8] our emphasis is on very simple representation languages.
Reference: 17. <author> W. Kloesgen. </author> <title> Efficient discovery of interesting statements in databases. </title> <journal> Journal of Intelligent Information Systems, </journal> <volume> 4(1):53 - 69, </volume> <year> 1995. </year>
Reference-contexts: The roots of this approach are in the use of diagrams of models in model theory (see, e.g., [7]). The approach has been used in various forms for example in <ref> [2, 8, 9, 16, 17, 21] </ref>. One should note that in contrast with, e.g., [8] our emphasis is on very simple representation languages.
Reference: 18. <author> A. J. Knobbe and P. W. Adriaans. </author> <title> Discovering foreign key relations in relational databases. </title> <booktitle> In Workshop Notes of the ECML-95 Workshop on Statistics, Machine Learning, and Knowledge Discovery in Databases, </booktitle> <pages> pages 94 - 99, </pages> <address> Heraklion, Crete, Greece, </address> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: The computation of maximal frequent sets is a fundamental data mining problem which is required in discovering association rules [1, 2]. Minimal keys can be used for semantic query optimization, which leads to fast query processing in database systems <ref> [22, 18, 5, 26] </ref>. Here we refer to possible keys that exist in a specific instance of a relational database and are not designed as such. The computation of sentences of a theory is an enumeration problem. The computation involves listing combinatorial substructures related with the input.
Reference: 19. <author> H. Mannila. </author> <title> Aspects of data mining. </title> <booktitle> In Workshop Notes of the ECML-95 Workshop on Statistics, Machine Learning, and Knowledge Discovery in Databases, </booktitle> <pages> pages 1-6, </pages> <address> Heraklion, Crete, Greece, </address> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: Typically, the frequency criterion states that there are sufficiently many instances in the database satisfying the sentence. Examples where this formulation works include the discovery of association rules, strong rules, episodes, and keys. Using this theory extraction formulation <ref> [19, 20, 23] </ref> one can formulate general results about the complexity of algorithms for various data mining tasks. <p> Finally, in Section 6 we give a short conclusion. 2 Data mining as theory extraction The model of knowledge discovery that we consider is the following <ref> [19, 23, 20] </ref>. Given a database r, a language L for expressing properties or defining subgroups of the data, and a frequency criterion q for evaluating whether a sentence ' 2 L defines a sufficiently large subclass of r.
Reference: 20. <author> H. Mannila. </author> <title> Data mining: machine learning, statistics, and databases. </title> <booktitle> In Proceedings of the 8th International Conference on Scientific and Statistical Database Management, </booktitle> <address> Stockholm, </address> <year> 1996. </year> <note> To appear. </note>
Reference-contexts: Typically, the frequency criterion states that there are sufficiently many instances in the database satisfying the sentence. Examples where this formulation works include the discovery of association rules, strong rules, episodes, and keys. Using this theory extraction formulation <ref> [19, 20, 23] </ref> one can formulate general results about the complexity of algorithms for various data mining tasks. <p> Finally, in Section 6 we give a short conclusion. 2 Data mining as theory extraction The model of knowledge discovery that we consider is the following <ref> [19, 23, 20] </ref>. Given a database r, a language L for expressing properties or defining subgroups of the data, and a frequency criterion q for evaluating whether a sentence ' 2 L defines a sufficiently large subclass of r.
Reference: 21. <author> H. Mannila and K.-J. Raiha. </author> <title> Design by example: An application of Armstrong relations. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 33(2):126 - 141, </volume> <year> 1986. </year>
Reference-contexts: The roots of this approach are in the use of diagrams of models in model theory (see, e.g., [7]). The approach has been used in various forms for example in <ref> [2, 8, 9, 16, 17, 21] </ref>. One should note that in contrast with, e.g., [8] our emphasis is on very simple representation languages.
Reference: 22. <author> H. Mannila and K.-J. Raiha. </author> <title> Algorithms for inferring functional dependencies. </title> <journal> Data & Knowledge Engineering, </journal> <volume> 12(1):83 - 99, </volume> <month> Feb. </month> <year> 1994. </year>
Reference-contexts: The computation of maximal frequent sets is a fundamental data mining problem which is required in discovering association rules [1, 2]. Minimal keys can be used for semantic query optimization, which leads to fast query processing in database systems <ref> [22, 18, 5, 26] </ref>. Here we refer to possible keys that exist in a specific instance of a relational database and are not designed as such. The computation of sentences of a theory is an enumeration problem. The computation involves listing combinatorial substructures related with the input.
Reference: 23. <author> H. Mannila and H. Toivonen. </author> <title> On an algorithm for finding all interesting sentences. </title> <booktitle> In Cybernetics and Systems Research '96, </booktitle> <address> Vienna, Austria, </address> <month> Apr. </month> <year> 1996. </year> <note> To appear. </note>
Reference-contexts: Typically, the frequency criterion states that there are sufficiently many instances in the database satisfying the sentence. Examples where this formulation works include the discovery of association rules, strong rules, episodes, and keys. Using this theory extraction formulation <ref> [19, 20, 23] </ref> one can formulate general results about the complexity of algorithms for various data mining tasks. <p> Finally, in Section 6 we give a short conclusion. 2 Data mining as theory extraction The model of knowledge discovery that we consider is the following <ref> [19, 23, 20] </ref>. Given a database r, a language L for expressing properties or defining subgroups of the data, and a frequency criterion q for evaluating whether a sentence ' 2 L defines a sufficiently large subclass of r. <p> For T L, let T i denote the set of the sentences of L with rank i. The level-wise algorithm <ref> [23] </ref> for computing T h = T h (L; r; q) proceeds by first computing the set T h 0 consisting of the sentences of rank 0 that are in T h. <p> This iterative procedure is performed until no more sentences in T h are found. This level-wise algorithm has been used in various forms in finding association rules, episodes, sequential rules, etc. <ref> [2, 3, 24, 23, 1, 13, 14, 25] </ref>. The drawback with this algorithm is that it always computes the whole set T h (L; r; q), even in the cases where a condensed representation of T h using most specific sentences would be useful.
Reference: 24. <author> H. Mannila, H. Toivonen, and A. I. Verkamo. </author> <title> Discovering frequent episodes in sequences. </title> <booktitle> In Proceedings of the First International Conference on Knowledge Discovery and Data Mining (KDD'95), </booktitle> <pages> pages 210 - 215, </pages> <address> Montreal, Canada, </address> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: This iterative procedure is performed until no more sentences in T h are found. This level-wise algorithm has been used in various forms in finding association rules, episodes, sequential rules, etc. <ref> [2, 3, 24, 23, 1, 13, 14, 25] </ref>. The drawback with this algorithm is that it always computes the whole set T h (L; r; q), even in the cases where a condensed representation of T h using most specific sentences would be useful.
Reference: 25. <author> A. Savasere, E. Omiecinski, and S. Navathe. </author> <title> An efficient algorithm for mining association rules in large databases. </title> <booktitle> In Proceedings of the 21st International Conference on Very Large Data Bases (VLDB'95), </booktitle> <pages> pages 432 - 444, </pages> <address> Zurich, Swizerland, </address> <year> 1995. </year>
Reference-contexts: This iterative procedure is performed until no more sentences in T h are found. This level-wise algorithm has been used in various forms in finding association rules, episodes, sequential rules, etc. <ref> [2, 3, 24, 23, 1, 13, 14, 25] </ref>. The drawback with this algorithm is that it always computes the whole set T h (L; r; q), even in the cases where a condensed representation of T h using most specific sentences would be useful.
Reference: 26. <author> J. Schlimmer. </author> <title> Using learned dependencies to automatically construct sufficient and sensible editing views. In Knowledge Discovery in Databases, </title> <booktitle> Papers from the 1993 AAAI Workshop (KDD'93), </booktitle> <pages> pages 186 - 196, </pages> <address> Washington, D.C., </address> <year> 1993. </year>
Reference-contexts: The computation of maximal frequent sets is a fundamental data mining problem which is required in discovering association rules [1, 2]. Minimal keys can be used for semantic query optimization, which leads to fast query processing in database systems <ref> [22, 18, 5, 26] </ref>. Here we refer to possible keys that exist in a specific instance of a relational database and are not designed as such. The computation of sentences of a theory is an enumeration problem. The computation involves listing combinatorial substructures related with the input.
Reference: 27. <author> J. D. Ullman. </author> <title> Principles of Database and Knowledge-Base Systems, volume I. </title> <publisher> Computer Science Press, </publisher> <address> Rockville, MD, </address> <year> 1988. </year>
Reference: 28. <author> L. G. Valiant. </author> <title> The complexity of enumeration and reliability problems. </title> <journal> SIAM Journal on Computing, </journal> <volume> 8(3) </volume> <pages> 410-421, </pages> <year> 1979. </year> <title> This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: Proof: Finding the number of -frequent sets is shown to be #P-hard by reducing the problem of computing the number of satisfying assignments of a monotone-2CNF (shown to be #P-hard by <ref> [28] </ref>) to it. <p> Since the problem of computing the number of minimal vertex covers of a graph is known to be #P hard <ref> [28] </ref>, the theorem follows. Recall that a vertex cover of a graph G is a set of vertices of G such that every edge of G is incident on at least one vertex in the set.
References-found: 28

