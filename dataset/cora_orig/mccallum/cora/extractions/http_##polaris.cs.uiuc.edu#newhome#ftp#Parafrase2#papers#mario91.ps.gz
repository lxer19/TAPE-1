URL: http://polaris.cs.uiuc.edu/newhome/ftp/Parafrase2/papers/mario91.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/newhome/ftp/Parafrase2/papers/
Root-URL: http://www.cs.uiuc.edu
Title: Run-Time Management of Lisp Parallelism and the Hierarchical Task Graph Program Representation  
Author: Mario Furnari Constantine D. Polychronopoulos 
Note: [20].  
Date: May 29, 1991 revised July 24, 1991  
Address: Via Toiano, 6 80072 Arco Felice (Na) Italy  305 Talbot -104 South Wright Street Urbana, IL 61801-2932  
Affiliation: Istituto di Cibernetica  Center for Supercomputing Research and Development University of Illinois  
Abstract: This paper suggests how to extend the Hierarchical Task Graph program representation, and its execution model to address the Lisp code parallelization problems. The advantages of this approach lies in the fact of on avoiding to annotate Lisp programs, and in accounting for run-time scheduling policies all into a unified environment. We start by reviewing the problem of run-time parallelism management, first in the imperative languages setting, and next in Lisp setting. Before describing the Hierarchical Task Graph (HTG) program representation [7], we review the basic notions of control and data dependence analysis. Finally we describe how to modify the HTG and its execution model to take into account for the Lazy Task Creation model [19], and the Sponsor Model for exploiting speculative computations in Lisp 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> V. Aho, R. Sethi, J.D. </author> <title> Ullman Compilers: </title> <booktitle> Principles, Techniques, and Tools Addi-son Wesley, </booktitle> <address> Reading, Mass. U.S.A., </address> <year> (1986) </year>
Reference-contexts: Data flow, takes into account instruction dependence arising from different memory usage. Many interesting techniques has been developed to solve in an efficient way the flow analysis problems for most imperative programming languages, such as Fortran, C, Pascal, etc., see for example <ref> [1] </ref>. Some of them have been developed to cope with parallelization problems, both vectorization and concurrentization, see for example [7, 8, 2].
Reference: [2] <author> J. Ferrante, K.J. Ottenstein, </author> <title> J.D.Warren The Program Dependence Graph and Its Use in Optimization ACM-TOPLAS, </title> <type> 9, </type> <year> (1987), </year> <pages> pp. 319-343 </pages>
Reference-contexts: Many interesting techniques has been developed to solve in an efficient way the flow analysis problems for most imperative programming languages, such as Fortran, C, Pascal, etc., see for example [1]. Some of them have been developed to cope with parallelization problems, both vectorization and concurrentization, see for example <ref> [7, 8, 2] </ref>. <p> A node y post-dominates node x, denoted by y p x, iff every path from x to STOP contains y. The notation x 6 p y will denote x does not post-dominate y. In <ref> [2, 7] </ref> has been proved that the set of post-dominators of a node x form a chain. Therefore, there exists a least element called the immediate post-dominator of x.
Reference: [3] <author> Gabriel R. P., and McCarthy J., </author> <booktitle> Queue-based MultiProcessing LISP Conference Record of the 1984 ACM Symposium on LISP and Functional Programming, </booktitle> <pages> pages 25 - 43. </pages>
Reference-contexts: In other words, the parallelism is annotated by the added language constructs (annotating systems) <ref> [9, 10, 3, 4, 5] </ref>. * the second one relies on using the existing code and smart compiling methods to parallelize the source code [12, 14, 6] (restructuring systems). The former approach places a significant burden on the programmer (which depend directly on the level and complexity of constructs). <p> Unfortunately, Lisp restructuring is a difficult problem, and many related problems has been proved to be NP-complete [17]. 2.1 Annotation systems Widespreaded multiprocessing Lisp dialects include the QLisp <ref> [3, 4, 5] </ref>, and the Mul-tiLisp [10]. <p> This is a very limiting factor in spreading their use in practical applications of symbolic computation applications, such as expert systems, natural language recognizers, and so on. QLisp <ref> [3, 4] </ref> provides the propositional parameters as tools for building tasks of acceptable granularity at run-time. Such fine-tuning may be necessary in some cases to maximize performance, but it involves programming effort and may compromise program clarity.
Reference: [4] <author> Goldman R. and Gabriel R. P.,QLISP: </author> <title> Experience and New Directions Symposium on Parallel Programming: Experience with Applications, </title> <booktitle> Language and Systems. ACM, </booktitle> <pages> pages. 111-123, </pages> <month> July </month> <year> 1988 </year>
Reference-contexts: In other words, the parallelism is annotated by the added language constructs (annotating systems) <ref> [9, 10, 3, 4, 5] </ref>. * the second one relies on using the existing code and smart compiling methods to parallelize the source code [12, 14, 6] (restructuring systems). The former approach places a significant burden on the programmer (which depend directly on the level and complexity of constructs). <p> Unfortunately, Lisp restructuring is a difficult problem, and many related problems has been proved to be NP-complete [17]. 2.1 Annotation systems Widespreaded multiprocessing Lisp dialects include the QLisp <ref> [3, 4, 5] </ref>, and the Mul-tiLisp [10]. <p> This is a very limiting factor in spreading their use in practical applications of symbolic computation applications, such as expert systems, natural language recognizers, and so on. QLisp <ref> [3, 4] </ref> provides the propositional parameters as tools for building tasks of acceptable granularity at run-time. Such fine-tuning may be necessary in some cases to maximize performance, but it involves programming effort and may compromise program clarity.
Reference: [5] <author> Goldman R. and Gabriel R. P., </author> <title> QLISP: </title> <booktitle> Parallel Processing in Lisp IEEE Software 6, </booktitle> <pages> pages. 5159, </pages> <year> (1989) </year>
Reference-contexts: In other words, the parallelism is annotated by the added language constructs (annotating systems) <ref> [9, 10, 3, 4, 5] </ref>. * the second one relies on using the existing code and smart compiling methods to parallelize the source code [12, 14, 6] (restructuring systems). The former approach places a significant burden on the programmer (which depend directly on the level and complexity of constructs). <p> Unfortunately, Lisp restructuring is a difficult problem, and many related problems has been proved to be NP-complete [17]. 2.1 Annotation systems Widespreaded multiprocessing Lisp dialects include the QLisp <ref> [3, 4, 5] </ref>, and the Mul-tiLisp [10].
Reference: [6] <author> Gray S. L. </author> <title> Using Future to Exploit Parallelism in Lisp M.S. </title> <type> Thesis, </type> <institution> Dept. Electrical Engineering and Computer Science, M.I.T. </institution> <year> 1986 </year>
Reference-contexts: In other words, the parallelism is annotated by the added language constructs (annotating systems) [9, 10, 3, 4, 5]. * the second one relies on using the existing code and smart compiling methods to parallelize the source code <ref> [12, 14, 6] </ref> (restructuring systems). The former approach places a significant burden on the programmer (which depend directly on the level and complexity of constructs). Furthermore, the definition of a complete and orthogonal parallel constructs-set, satisfying simplicity and expressiveness, have yet to be solved. <p> MultiLisp provides the construct delay required. In other words, a process is associated with the form (delay exp), but it does not start until an explicit or implicit touch to exp is carried out. 2.2 Restructuring systems Gray at MIT investigated inserting futures into side-effect-free Lisp programs <ref> [6] </ref>. He sought concurrency in two places. The first, was concurrent execution of syntactically parallel tasks, such as the evaluation of actual arguments in a function call. The second, involved overlapped execution of a subexpression and its containing expression.
Reference: [7] <author> M. Girkar, </author> <title> C.D. Polychronopoulos The HTG: An Intermediate Representation for Programs Based on Control and Data Dependences CSRD TR1046, </title> <institution> University of Illinois at Urbana-Champaign, Illinois, U.S.A., </institution> <year> 1990 </year>
Reference-contexts: For the Lisp-like languages Mohr proposed in [19] the Lazy Task Creation strategy, and Osborne in [20] proposed the sponsor model to cope with symbolic speculative computations. In this paper we describe the use of an extension of the autoscheduling technique, the Hierarchical Task Graph program representation <ref> [7] </ref>, and its corresponding computational model, in the Lisp realm. Starting from the algorithm described in [22], we show step by step how to modify it to accommodate the lazy tasks creation computation model [19] and the touching sponsor for the speculative computations [20]. <p> Many interesting techniques has been developed to solve in an efficient way the flow analysis problems for most imperative programming languages, such as Fortran, C, Pascal, etc., see for example [1]. Some of them have been developed to cope with parallelization problems, both vectorization and concurrentization, see for example <ref> [7, 8, 2] </ref>. <p> A node y post-dominates node x, denoted by y p x, iff every path from x to STOP contains y. The notation x 6 p y will denote x does not post-dominate y. In <ref> [2, 7] </ref> has been proved that the set of post-dominators of a node x form a chain. Therefore, there exists a least element called the immediate post-dominator of x. <p> The control dependence graph CDG, of a control flow graph CFG, is defined as the directed graph with labeled arcs such that: 1. V C = V F 2. (x, y) 2 E C with label x-a iff x ffi c y with label x-a. In <ref> [7] </ref> a procedure to build and optimize the CDG, based on the CFG and the loop hierarchy, is described. <p> y implies a path from x to y in CFG, the graph containing the arcs of both DDG and CDG is also acyclic if the CDG is acyclic. 5 The Hierarchical Task Graph Both Control Dependence and Data Dependence relations can be framed together in the Hierarchical Task Graph (HTG) <ref> [7] </ref>. The hierarchical task graph is a directed acyclic graph HTG = (V H E H ) with unique nodes START and STOP belonging to V H such that there exists a path from START to every node in V H and a path from every node to STOP. <p> Task 5 represents the summation of the partial results and start and stop function as defined in <ref> [22, 7] </ref>. The solid edges represent data and control dependences, and the shaded edge represents data parallel dependence.
Reference: [8] <author> M. Girkar, </author> <title> C.D. Polychronopoulos Automatic Detection and Generation of Unstructured Parallelism in Ordinary Programs to appear in IEEE Transaction on Parallel & Distributed Processing, </title> <institution> and CSRD TR, University of Illinois at Urbana-Champaign, Illinois, U.S.A., </institution> <year> 1990 </year>
Reference-contexts: Many interesting techniques has been developed to solve in an efficient way the flow analysis problems for most imperative programming languages, such as Fortran, C, Pascal, etc., see for example [1]. Some of them have been developed to cope with parallelization problems, both vectorization and concurrentization, see for example <ref> [7, 8, 2] </ref>.
Reference: [9] <author> Halstead R.H. Jr., </author> <title> Implementation of Multilisp: </title> <booktitle> LISP on a multiprocessor Conference Record of the 1984 ACM Symposium on LISP and Functional Programming, </booktitle> <pages> pages. 9 - 17. </pages>
Reference-contexts: In other words, the parallelism is annotated by the added language constructs (annotating systems) <ref> [9, 10, 3, 4, 5] </ref>. * the second one relies on using the existing code and smart compiling methods to parallelize the source code [12, 14, 6] (restructuring systems). The former approach places a significant burden on the programmer (which depend directly on the level and complexity of constructs).
Reference: [10] <author> R.H. </author> <title> Halstead Multilisp: </title> <booktitle> a language for concurrent symbolic computations ACM TOPLAS, </booktitle> <volume> vol 7, </volume> <pages> (1985) pp 501-538 18 </pages>
Reference-contexts: In other words, the parallelism is annotated by the added language constructs (annotating systems) <ref> [9, 10, 3, 4, 5] </ref>. * the second one relies on using the existing code and smart compiling methods to parallelize the source code [12, 14, 6] (restructuring systems). The former approach places a significant burden on the programmer (which depend directly on the level and complexity of constructs). <p> Unfortunately, Lisp restructuring is a difficult problem, and many related problems has been proved to be NP-complete [17]. 2.1 Annotation systems Widespreaded multiprocessing Lisp dialects include the QLisp [3, 4, 5], and the Mul-tiLisp <ref> [10] </ref>. The QLisp design goals include the following: 1) targeting multiprocessors with shared memory organization; 2) the ability to limit the degree of multiprocessing at run-time; 3) minimal extensions with respect to ordinary Lisp to cope with parallelism; and 4) all constructs should work also in a uni-processing setting. <p> A process closure may be activated as a new process when it is applied. Applying a process closure causes the associated process to be started, and the arguments are evaluated by the spawning processes. 3 MultiLisp is an extension of Scheme, developed by Halstead <ref> [10] </ref>, which retains the Scheme possibility of side effects. It may be characterized as a Lisp dialect with the following features: 1) lexical scoped binding discipline; 2) shared memory computer organization; 3) sequential default computation mechanism; 4) a small set of constructs to cope with parallelism.
Reference: [11] <author> C.T. Haynes, D.P. Friedman, M. </author> <title> Wand Obtaining Coroutines with continuation Computer Languages, </title> <type> 11, </type> <year> (1986), </year> <pages> pp. 143-153 </pages>
Reference: [12] <author> W.L. </author> <title> Harrison Compiling Lisp for Evaluation on a Tightly Coupled Multiprocessor CSRD TR-565 University of Illinois at Urbana-Champaign, </title> <publisher> Illinois, </publisher> <address> U.S.A., </address> <year> 1986 </year>
Reference-contexts: In other words, the parallelism is annotated by the added language constructs (annotating systems) [9, 10, 3, 4, 5]. * the second one relies on using the existing code and smart compiling methods to parallelize the source code <ref> [12, 14, 6] </ref> (restructuring systems). The former approach places a significant burden on the programmer (which depend directly on the level and complexity of constructs). Furthermore, the definition of a complete and orthogonal parallel constructs-set, satisfying simplicity and expressiveness, have yet to be solved. <p> Transactions execute without regarding possible conflicts. A monitor examines the read and write traces for each transaction to detect conflicts and restart transactions that violate the serializability constraints. A transaction commits its results when it can no longer conflict with any other transactions. Harrison <ref> [12] </ref> proposed techniques for concurrently executing non-pure Lisp. His 4 transformation system, Parcel, contains a new representation for lists that allows the use of parallel algorithms normally associated with numeric programs. This list representation stores the car pointers of a list segment contiguously, without the cdr pointers.
Reference: [13] <author> W.L. </author> <title> Harrison The interprocedural Analysis and Automatic Parallelization of Scheme Programs Int. </title> <journal> Journal of Lisp and Symbolic Computation, </journal> <volume> 2, </volume> <year> (1989), </year> <pages> pp. 179-396 </pages>
Reference-contexts: In the rest of the paper we assume that we have gathered sufficient control and data flow information, using one of the tools described in <ref> [13, 23] </ref>, and that have also been strictness and quickness analysis carried out to take into account for sources of not useful parallelism. Let us briefly recall the future insertion strategy.
Reference: [14] <author> Harrison W.L., Padua D. A. </author> <title> PARCEL: Project for the Automatic Restructuring and Concurrent Evaluation of LISP TR. </title> <type> 653, </type> <institution> Dept. Computer Science, University of Illinois, </institution> <year> 1987 </year>
Reference-contexts: In other words, the parallelism is annotated by the added language constructs (annotating systems) [9, 10, 3, 4, 5]. * the second one relies on using the existing code and smart compiling methods to parallelize the source code <ref> [12, 14, 6] </ref> (restructuring systems). The former approach places a significant burden on the programmer (which depend directly on the level and complexity of constructs). Furthermore, the definition of a complete and orthogonal parallel constructs-set, satisfying simplicity and expressiveness, have yet to be solved.
Reference: [15] <author> D. </author> <title> Kuck The Structure of Computers & Computations John Wiley & Sons, </title> <address> Boston, Ma, </address> <year> 1978 </year>
Reference-contexts: For example we can associate the sets of the input variables, and the set of the output variables, within a basic block. There are three types of Data Dependences <ref> [15] </ref>: * a flow dependence, in which one statement writes a value read by a later statement, and will be denoted by ffi; * an anti-dependence, in which one statement reads a location subsequently written by another statement, and will be denoted by ffi; * an output dependence, in which two
Reference: [16] <author> M. Katz ParaTran: </author> <title> A Transparent, Transaction-Based Runtime Mechanism for Parallel Execution of Scheme Master Thesis, </title> <institution> Dept. Electrical Engineering and Computer Science, M.I.T., 198B </institution>
Reference-contexts: Since side-effects-free programs contain only simple flow dependence, data dependence analysis was unnecessary and futures provided all the required synchronization. In this context the major difficulty is avoiding spawning too many processes. Katz proposed a general approach for concurrently executing Lisp programs with side-effects <ref> [16] </ref>. His system, ParaTran, relies on a combination of program transformations and run-time error checking to implement optimistic concurrency. Under this scheme (borrowed from databases) a transaction is assumed to have few conflicts with other transactions. Transactions execute without regarding possible conflicts.
Reference: [17] <author> Larus J.R, </author> <title> Hilfinger P.N., Restructuring LISP Programs for concurrent executions Symposium on Parallel Programming: Experience with Applications, </title> <booktitle> Language and Systems. ACM, </booktitle> <pages> pages. 100 - 110, </pages> <year> 1988 </year>
Reference-contexts: By contrast, the latter approach is attractive for it allows the use of already existing code without modifications, and it relieves the programmer from significant conceptual effort needed for manual parallelization. Unfortunately, Lisp restructuring is a difficult problem, and many related problems has been proved to be NP-complete <ref> [17] </ref>. 2.1 Annotation systems Widespreaded multiprocessing Lisp dialects include the QLisp [3, 4, 5], and the Mul-tiLisp [10]. <p> The representations also contain the number of elements remaining in the list. Its main advantage is that the i th element of a list can be obtained faster by treating blocks of pointers as a vector. Curare is another restructuring system, developed by Larus <ref> [17, 18] </ref> which automatically transforms sequential Lisp programs into semantically equivalent concurrent programs. Curare treats recursive functions, because they offer several advantages over loops and provide a convenient framework for inserting locks and handling the dynamic behaviour of symbolic programs.
Reference: [18] <author> J.L. </author> <title> Larus Compiling Lisp Programs for Parallel Execution Int. </title> <journal> Journal of Lisp and Symbolic Computation, </journal> <volume> 4, </volume> <year> (1991), </year> <pages> pp. 29-99 </pages>
Reference-contexts: The representations also contain the number of elements remaining in the list. Its main advantage is that the i th element of a list can be obtained faster by treating blocks of pointers as a vector. Curare is another restructuring system, developed by Larus <ref> [17, 18] </ref> which automatically transforms sequential Lisp programs into semantically equivalent concurrent programs. Curare treats recursive functions, because they offer several advantages over loops and provide a convenient framework for inserting locks and handling the dynamic behaviour of symbolic programs.
Reference: [19] <author> E. Mohr, D.A. Kranz, </author> <title> R.H. Halstead Lazy Task Creation: A Technique for Increasing the Granularity of Parallel Programs ACM Lisp Conf. </title> <booktitle> On Lisp and Functional Languages, Nice (1990), </booktitle> <pages> pp. 185-197 </pages>
Reference-contexts: For the imperative languages there are many efforts addressing this problem, for example, Polychronopoulos [22] suggests moving the scheduling policy inside the program, by introducing the notion of autoscheduling. For the Lisp-like languages Mohr proposed in <ref> [19] </ref> the Lazy Task Creation strategy, and Osborne in [20] proposed the sponsor model to cope with symbolic speculative computations. In this paper we describe the use of an extension of the autoscheduling technique, the Hierarchical Task Graph program representation [7], and its corresponding computational model, in the Lisp realm. <p> Starting from the algorithm described in [22], we show step by step how to modify it to accommodate the lazy tasks creation computation model <ref> [19] </ref> and the touching sponsor for the speculative computations [20]. The main advantage of this approach lies in avoiding to annotate the Lisp programs. In Section 2 we report on the current status of the parallel Lisp extensions, and the strategies used to manage task granularity at run-time. <p> The main advantage of this approach lies in avoiding to annotate the Lisp programs. In Section 2 we report on the current status of the parallel Lisp extensions, and the strategies used to manage task granularity at run-time. Attention is focussed on the lazy task creation proposed in <ref> [19] </ref>, and the sponsor model for speculative computations proposed in [20]. In Section 3 we review and summarize the most important notions of control and data dependence analysis. Next, in Section 4 we describe the efforts in using this framework to parallelize imperative languages. <p> Finally, in Section 5, we extend the previous proposal to Lisp, showing how the HTG is a good candidate for synthesizing both the lazy task creation, and the sponsor model avoiding the use of ad hoc techniques such as described in <ref> [19, 20] </ref>. 2 Parallelism and Lisp-Like languages In order to cope with parallelism in Lisp two approaches have been pursued: * the first one, extends Lisp with parallel constructs to help the compiler to parallelize a Lisp program. <p> Also, certain program parameters need to be calibrated through experimentations, and this work may need to be repeated for a different target machine or data set. Or, worst yet for each job submission, when the program runs in a parallel multiprogramming. A system is described in <ref> [19] </ref> in which the programmer decides on what can be computed safely in parallel, leaving the decision on how task-splitting will done to the run-time system. This system relies on the freedom to interpret the future operational semantics. <p> This process is called inlining. The run-time task spawning strategy adopted in <ref> [19] </ref> consists of inlining every task at compile-time, saving sufficient information in such a way as to be selectively un-inlined as processing resources become available at run-time. <p> This scheduling strategy is called oldest-first stealing policy <ref> [19] </ref>, i.e., when an idle processor steals a task, the oldest fork point is chosen 1 to un-inlining the futures. Only a small change in the execution run-time behaviour is sufficient to take into account the oldest-first stealing policy in the HTG framework. <p> The dynamic processes partition allows to delay the decision to split a task at fixed points (the process partition points) according to emptiness of the processor private queue. This techniques was extended in defining the lazy task creation <ref> [19] </ref>, by means of the inlining method. The main difference among the dynamic processes partition and the lazy task creation resides into the inlining process, for it allows to selectively un-inline the task as processing resources become available.
Reference: [20] <author> R.B. </author> <title> Osborne Speculative Computation in Multilisp: </title> <booktitle> An Overview ACM Lisp Conf. On Lisp and Functional Languages, Nice (1990), </booktitle> <pages> pp. 198-208 </pages>
Reference-contexts: For the imperative languages there are many efforts addressing this problem, for example, Polychronopoulos [22] suggests moving the scheduling policy inside the program, by introducing the notion of autoscheduling. For the Lisp-like languages Mohr proposed in [19] the Lazy Task Creation strategy, and Osborne in <ref> [20] </ref> proposed the sponsor model to cope with symbolic speculative computations. In this paper we describe the use of an extension of the autoscheduling technique, the Hierarchical Task Graph program representation [7], and its corresponding computational model, in the Lisp realm. <p> Starting from the algorithm described in [22], we show step by step how to modify it to accommodate the lazy tasks creation computation model [19] and the touching sponsor for the speculative computations <ref> [20] </ref>. The main advantage of this approach lies in avoiding to annotate the Lisp programs. In Section 2 we report on the current status of the parallel Lisp extensions, and the strategies used to manage task granularity at run-time. <p> In Section 2 we report on the current status of the parallel Lisp extensions, and the strategies used to manage task granularity at run-time. Attention is focussed on the lazy task creation proposed in [19], and the sponsor model for speculative computations proposed in <ref> [20] </ref>. In Section 3 we review and summarize the most important notions of control and data dependence analysis. Next, in Section 4 we describe the efforts in using this framework to parallelize imperative languages. <p> Finally, in Section 5, we extend the previous proposal to Lisp, showing how the HTG is a good candidate for synthesizing both the lazy task creation, and the sponsor model avoiding the use of ad hoc techniques such as described in <ref> [19, 20] </ref>. 2 Parallelism and Lisp-Like languages In order to cope with parallelism in Lisp two approaches have been pursued: * the first one, extends Lisp with parallel constructs to help the compiler to parallelize a Lisp program. <p> Controller sponsors the three previous types of sponsors are all passive; they merely act as fixed attribute sources or pass on attributes from other sources. Controller sponsors receive sponsorship and actively distribute it among the tasks in their control domain according to some built-in control strategy. In <ref> [20] </ref> only the touching model implementation is described, together with some applications.
Reference: [21] <author> J.D. Pehoushek, J.S. </author> <title> Weening Low-cost process Creation and Dynamic Partiotion in QLISP, I Parallel Lisp: Languages and Systems Y.Ito, R.H. </title> <editor> Halstead ed, </editor> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1990, </year> <pages> pp 182-199 </pages>
Reference-contexts: The balancing between this increased complexity and the benefit of the parallelism has been investigated both for the annotating and restructuring parallelizing systems. Since the time of its definition QLisp offered to the users the propositional parameters to the fine-tuning of the run-time parallelism. Pehoushek in <ref> [21] </ref> describes the use of dynamic processes partition and scheduling methods to improve the run-time performance of parallel Lisp code. The dynamic processes partition allows to delay the decision to split a task at fixed points (the process partition points) according to emptiness of the processor private queue.
Reference: [22] <institution> C.D. Polychronopoulos Autoscheduling: Control and Data Flow Come Together CSRD TR-1058 University of Illinois at Urbana-Champaign, Illinois, U.S.A., </institution> <year> 1990 </year>
Reference-contexts: In fact, to be really usable the parallelization must enable the user to run simultaneously several parallel programs on the same machine without a significant drop on parallel program performance. For the imperative languages there are many efforts addressing this problem, for example, Polychronopoulos <ref> [22] </ref> suggests moving the scheduling policy inside the program, by introducing the notion of autoscheduling. For the Lisp-like languages Mohr proposed in [19] the Lazy Task Creation strategy, and Osborne in [20] proposed the sponsor model to cope with symbolic speculative computations. <p> In this paper we describe the use of an extension of the autoscheduling technique, the Hierarchical Task Graph program representation [7], and its corresponding computational model, in the Lisp realm. Starting from the algorithm described in <ref> [22] </ref>, we show step by step how to modify it to accommodate the lazy tasks creation computation model [19] and the touching sponsor for the speculative computations [20]. The main advantage of this approach lies in avoiding to annotate the Lisp programs. <p> Special attention will be payed to the problem of automatically discovering and managing at run-time different parallel constructs, with a dynamic and varying granularity. In this context we review the HTG, and the task 2 allocation algorithm proposed in <ref> [22] </ref>. <p> ) is the disjunction of control flow paths that result in x i , not being executed, and then the dependence need not be enforced in that execution of the program. 5.2 The Auto-Scheduling environment and the HTG Here we review how the HTG abstract program representation is used in <ref> [22] </ref> together the auto-scheduling environment. Remember that at each level the HTG is derived merging the CDG and the DDG, and annotating the nodes with the control conditions. The idea behind the HTG may be easily captured looking at it as a graph clusterization. <p> It may be interesting to point out here that this run time behaviour does not have any provision to dynamically expand the HTG hierarchy in depth, past its static depth. This capability is useful in accommodating recursion. 6 The HTG and parallel task management in Lisp To extend HTG <ref> [22] </ref>, we need first of all to accommodate for recursive call. Recursion provides the most important control structuring Lisp construct. Next, we are faced with the large number of function calls present in a Lisp program. Also iterative constructs are written using tail-recursion in Lisp. <p> Task 5 represents the summation of the partial results and start and stop function as defined in <ref> [22, 7] </ref>. The solid edges represent data and control dependences, and the shaded edge represents data parallel dependence.
Reference: [23] <author> O. </author> <title> Shivers Control Flow Analysis in Scheme SIGPLAN Conf on Language Design and Implementation, </title> <address> Atlanta, Georgia, </address> <year> (1988), </year> <pages> pp 164-174 19 </pages>
Reference-contexts: In the rest of the paper we assume that we have gathered sufficient control and data flow information, using one of the tools described in <ref> [13, 23] </ref>, and that have also been strictness and quickness analysis carried out to take into account for sources of not useful parallelism. Let us briefly recall the future insertion strategy.
References-found: 23

