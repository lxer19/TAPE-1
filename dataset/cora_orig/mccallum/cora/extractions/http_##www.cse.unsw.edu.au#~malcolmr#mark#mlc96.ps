URL: http://www.cse.unsw.edu.au/~malcolmr/mark/mlc96.ps
Refering-URL: http://www.cse.unsw.edu.au/~malcolmr/mark/mlc96-abs.html
Root-URL: 
Title: Actual return reinforcement learning versus Temporal Differences:  
Abstract: Some theoretical and experimental results Abstract This paper argues that for many domains, we can expect credit-assignment methods that use actual returns to be more effective for reinforcement learning than the more commonly used temporal difference methods. We present analysis and empirical evidence from three sets of experiments in different domains to support this claim. A new algorithm we call C-Trace, a variant of the P-Trace RL algorithm is introduced, and some possible advantages of using algorithms of this type are discussed.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A.G. Barto and M. Duff. </author> <title> Monte Carlo matrix inversion and reinforcement learning. </title> <editor> In D.S.Touretsky, ed., </editor> <booktitle> Advances in Neural Information Processing Systems 6. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: This is discussed further in section 3. 3 Related work It is instructive to compare the P-Trace algorithm and its variants to both Q ()-learning [12], and to Monte Carlo methods (see, for example, <ref> [1] </ref>). As we have already mentioned, the P-Trace mechanism is conceptually related to Monte Carlo methods, and indeed can be viewed a Monte Carlo method spe-cialised for sampling Q (s; a) values, where represents the current policy.
Reference: [2] <author> A.G. Barto, R.S. Sutton, and C.W. Anderson. </author> <title> Neu-ronlike adaptive elements that can solve difficult learning control problems. </title> <journal> IEEE Trans. on Systems, Man, and Cybernetics, </journal> <volume> SMC-13(5):834-846, </volume> <year> 1983. </year>
Reference-contexts: It uses actual returns rather than a TD approach to learning expected payoffs from state/action pairs. The Adaptive Heuristic Critic (AHC) algorithm <ref> [2] </ref> was also originally studied in the context of the pole-and-cart problem.
Reference: [3] <author> D. P. Bertsekas. </author> <title> A counterexample to temporal differences learning. </title> <journal> Neural Computation, </journal> <volume> 7 </volume> <pages> 270-279, </pages> <year> 1995. </year>
Reference-contexts: The evidence to date has been mixed. In [17], Sutton points out the former enjoy a number of theoretical advantages when certain classes of function approximators are used <ref> [7, 3] </ref>; Boyan and Moore [4] have demonstrated such effects particularly convincingly in simulated domains. On the other hand, in empirical studies the latter have been shown to enjoy better learning rates when T D ()-based implementations have been used [16, 17].
Reference: [4] <author> J.A. Boyan and A.W. Moore. </author> <title> Generalization in reinforcement learning: Safely approximating the value function. </title> <booktitle> In Advances in Neural Information Processing Systems 7. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: The evidence to date has been mixed. In [17], Sutton points out the former enjoy a number of theoretical advantages when certain classes of function approximators are used [7, 3]; Boyan and Moore <ref> [4] </ref> have demonstrated such effects particularly convincingly in simulated domains. On the other hand, in empirical studies the latter have been shown to enjoy better learning rates when T D ()-based implementations have been used [16, 17]. <p> Comparing a slightly newer version of C-Trace to a T D ()-based algorithm (Sutton's variant of Rummery & Niranjan's sarsa, see [17]) in the domains puddle-world, mountain-car, pole-and-cart and the 21-state Markov chain prediction problem <ref> [4, 17, 15] </ref>, we found that the C-Trace variant performed very nearly equally with respect to learning rate to sarsa optimised for , with generally much better parameter insensitivity characteristics for the choice of the step-size fi.
Reference: [5] <author> Rodney Brooks. </author> <title> Intelligence without reason. </title> <booktitle> In Proc. of the 12 th Int. Joint Conf. on Art. Intell., </booktitle> <pages> pages 569-595, </pages> <year> 1991. </year>
Reference-contexts: The robot was given a set of primitive "reflexes" in the spirit of Rodney Brooks' "subsumption" architecture <ref> [5] </ref>. A leg that is triggered will incrementally move to lift up if on the ground and forward if already lifted. A leg that does not receive activation will tend to drop down if lifted and backwards if already on the ground.
Reference: [6] <author> P. Cichosz. </author> <title> Truncating temporal differences: On the efficient implementation of TD() for reinforcement learning. </title> <journal> JAIR, </journal> <volume> 2 </volume> <pages> 287-318, </pages> <year> 1995. </year>
Reference-contexts: As previously mentioned, the P-Trace mechanism for calculating actual returns is experimentation insensitive because the trace is "zeroed" after a non-policy action is selected. Further, P-Trace also avoids a difficulty with the Q ()-learning approach described by both Peng & Williams [12] and Cichosz <ref> [6] </ref> to do with an inaccuracy proportional to the square of the learning factor fi in the calculated return. 3 3 In some preliminary experiments with Q ()-learning algorithm on the pole-and-cart problem with close to 1, Q ()-learning would not learn the task even in the absence Additionally, both P-Trace
Reference: [7] <author> P. Dayan. </author> <title> The convergence of TD() for general . Machine Learning, </title> <booktitle> 8 </booktitle> <pages> 341-362. </pages>
Reference-contexts: The evidence to date has been mixed. In [17], Sutton points out the former enjoy a number of theoretical advantages when certain classes of function approximators are used <ref> [7, 3] </ref>; Boyan and Moore [4] have demonstrated such effects particularly convincingly in simulated domains. On the other hand, in empirical studies the latter have been shown to enjoy better learning rates when T D ()-based implementations have been used [16, 17].
Reference: [8] <author> T. Jaakkola, S.P. Singh, and M.I. Jordan. </author> <title> Reinforcement learning algorithm for partially observable Markov decision problems. </title> <booktitle> In Advances in Neural Information Processing Systems 7. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: On the other hand, in empirical studies the latter have been shown to enjoy better learning rates when T D ()-based implementations have been used [16, 17]. Jaakkola et al. <ref> [8] </ref> have pointed to the advantages of actual return learners over TD methods when learning in a non-Markovian domain where the optimal policy is nonstationary. <p> This simple experiment effectively highlights the advantages an actual return learner has over a pure TD learner in a non-Markovian environment even when the optimal policy is a stationary one 9 ; an actual return learner does not suffer what might be termed the 9 In complementary work <ref> [8] </ref>, the advantage of actual return learning for non-Markovian decision problems (NMDPs) when the optimal policy is non-stationary has been discussed; in this experiment we consider the case of learning for NMDPs with stationary optimal policies. "history aliasing" effect a TD method introduces. 6 Experiment 3: Walking robot The final study
Reference: [9] <author> L-J Lin. </author> <title> Self-improving reactive agents based on reinforcement learning, planning and teaching. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 293-321, </pages> <year> 1992. </year>
Reference-contexts: payoff per trial for five algorithms 8 is shown Table 1 (each row represents a separate run of 10,000 trials for each algorithm using a different seed for the pseudo-random number generator). 7 For QL, Q-Trace and P-Trace a simple "Boltzmann distribution" stochastic action selector (SAS) as described in Lin <ref> [9] </ref> was used with temperature T set to 1.0. 8 BOXES was not included in the experiment because in its present form it does not learn from mixed sign reinforcement signals.
Reference: [10] <author> D. Michie and R.A. Chambers. </author> <title> BOXES: An experiment in adaptive control. </title> <editor> In E.Dale and D.Michie, eds., </editor> <booktitle> Machine Intelligence 2, </booktitle> <pages> pages 137-152. </pages> <address> Edin-burgh: </address> <publisher> Edinburgh Univ. Press, </publisher> <year> 1968. </year>
Reference-contexts: 1-step Q-learning) to a variant of the actual return learner P-Trace algorithm we call C-Trace. 2 Algorithms 2.1 BOXES, AHC and Q-learning The BOXES algorithm is Sammut and Law's [13] modern variant of Michie and Chambers' original reinforcement learning algorithm, which was first applied to the benchmark pole-and-cart control problem <ref> [10] </ref>. It uses actual returns rather than a TD approach to learning expected payoffs from state/action pairs. The Adaptive Heuristic Critic (AHC) algorithm [2] was also originally studied in the context of the pole-and-cart problem.
Reference: [11] <author> M.D. Pendrith. </author> <title> On reinforcement learning of control actions in noisy and non-Markovian domains. </title> <type> Tech. report, </type> <institution> UNSW-CSE-TR-9410, School of Comp. Sci. and Eng., Uni. of NSW, Australia, </institution> <year> 1994. </year>
Reference-contexts: Jaakkola et al. [8] have pointed to the advantages of actual return learners over TD methods when learning in a non-Markovian domain where the optimal policy is nonstationary. In this paper, we extend previous work <ref> [11] </ref> which looked at the possible advantages of using actual return-based methods for credit-assignment in noisy and non-Markovian domains. We present results from three different sets of experiments. <p> Five reinforcement learning algorithms have been studied, three of which (BOXES, AHC and 1-step Q-learning) have been written about extensively; the others, P-Trace and Q-Trace, have been described in <ref> [11] </ref>. Motivated directly by issues arising from the results of the first experiments, the second experiment compares the performance of four of these algorithms in an artificial non-Markovian decision problem. In the third experiment, we scale up from simulations to test our hypotheses on a real-world control application. <p> More recently, variants which incorporate both multiple step TD and state/action pair Q-value representation have been proposed; these include the Q ()- learning algorithm [12]. This is discussed briefly in the section 3. 2.2 P-Trace This subsection briefly describes the P-Trace algorithm. More detail can be found in <ref> [11] </ref>. P-Trace is an algorithm which, like Q-learning, learns a mapping of hstate,actioni to expected total future rewards (or payoff). It learns these state/action pair Q-values using direct rather than TD methods. <p> It was designed primarily to empirically investigate possible advantages of a mixed mode credit-assignment method. It is described more fully in <ref> [11] </ref>. 1 Refer to [11] for more details, if required. while not terminal state do get current state s select action a ( stochastic action selection ) if non-policy action selected then for all (i; j) such that V isitCount ij &gt; 0 do #if C-Trace c max m Q im <p> It was designed primarily to empirically investigate possible advantages of a mixed mode credit-assignment method. It is described more fully in <ref> [11] </ref>. 1 Refer to [11] for more details, if required. while not terminal state do get current state s select action a ( stochastic action selection ) if non-policy action selected then for all (i; j) such that V isitCount ij &gt; 0 do #if C-Trace c max m Q im k GlobalClock StepCount ij
Reference: [12] <author> J. Peng and R.J. Williams. </author> <title> Incremental multi-step Q learning. </title> <editor> In W.Cohen and H.Hirsh, eds., </editor> <booktitle> Proc. 11 th Int. Conf. on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: The key advance here was to represent explicitly the learnt value function as state/action pair payoffs rather than state payoffs, which meant the algorithm gained the key property of experimentation insensitivity <ref> [18, 12] </ref> in Markov domains, and led to proofs of convergence and optimality in these domains. More recently, variants which incorporate both multiple step TD and state/action pair Q-value representation have been proposed; these include the Q ()- learning algorithm [12]. <p> More recently, variants which incorporate both multiple step TD and state/action pair Q-value representation have been proposed; these include the Q ()- learning algorithm <ref> [12] </ref>. This is discussed briefly in the section 3. 2.2 P-Trace This subsection briefly describes the P-Trace algorithm. More detail can be found in [11]. P-Trace is an algorithm which, like Q-learning, learns a mapping of hstate,actioni to expected total future rewards (or payoff). <p> This is accomplished by delaying the application of the truncation correction to the return for as long as possible, only applying it at the point when a non-policy action is selected. The property of experimentation insensitivity <ref> [12] </ref> means that, like 1-step Q-learning, it can learn the optimal policy even if, for the sake of active exploration, it does not always following the optimal policy. <p> This is discussed further in section 3. 3 Related work It is instructive to compare the P-Trace algorithm and its variants to both Q ()-learning <ref> [12] </ref>, and to Monte Carlo methods (see, for example, [1]). As we have already mentioned, the P-Trace mechanism is conceptually related to Monte Carlo methods, and indeed can be viewed a Monte Carlo method spe-cialised for sampling Q (s; a) values, where represents the current policy. <p> state will not cancel each other out properly to provide an accurate return with respect to the greedy policy if a non-policy action is chosen somewhere along the way; Q ()-learning is experimentation sensitive for = 1 (and, in general, for all &gt; 0) if active exploration is to occur <ref> [12] </ref>. As previously mentioned, the P-Trace mechanism for calculating actual returns is experimentation insensitive because the trace is "zeroed" after a non-policy action is selected. Further, P-Trace also avoids a difficulty with the Q ()-learning approach described by both Peng & Williams [12] and Cichosz [6] to do with an inaccuracy <p> &gt; 0) if active exploration is to occur <ref> [12] </ref>. As previously mentioned, the P-Trace mechanism for calculating actual returns is experimentation insensitive because the trace is "zeroed" after a non-policy action is selected. Further, P-Trace also avoids a difficulty with the Q ()-learning approach described by both Peng & Williams [12] and Cichosz [6] to do with an inaccuracy proportional to the square of the learning factor fi in the calculated return. 3 3 In some preliminary experiments with Q ()-learning algorithm on the pole-and-cart problem with close to 1, Q ()-learning would not learn the task even in the absence <p> However, it would learn if this factor was reduced to 0.2, the suggested setting in <ref> [12] </ref>. This seems to suggest that this systemic inaccuracy should not be treated as insignificant. 4 It is well documented that T D () performs relatively poorly with = 1 [16, 17]; it has been suggested this is empirical evidence for the superiority of TD over actual return methods. <p> an action made in state C or D may depend not only upon the current state, but also upon what the previous states were (that is, we cannot assume the Markov property.) The reinforcement schedule is set as follows: 6 Also, while QL has been shown to be "experimentation insensitive" <ref> [12] </ref> in Markovian domains, it may not be widely appreciated that this property does not hold for QL in non-Markovian domains.
Reference: [13] <author> C.A. Sammut. </author> <title> Recent progress with BOXES. </title> <editor> In K.Furakawa, S.Muggleton, and D.Michie, eds., </editor> <booktitle> Machine Intelligence 13. </booktitle> <publisher> The Clarendon Press, </publisher> <address> OUP, </address> <year> 1994. </year>
Reference-contexts: In this experiment, we directly compare a TD learner (Watkins' 1-step Q-learning) to a variant of the actual return learner P-Trace algorithm we call C-Trace. 2 Algorithms 2.1 BOXES, AHC and Q-learning The BOXES algorithm is Sammut and Law's <ref> [13] </ref> modern variant of Michie and Chambers' original reinforcement learning algorithm, which was first applied to the benchmark pole-and-cart control problem [10]. It uses actual returns rather than a TD approach to learning expected payoffs from state/action pairs.
Reference: [14] <author> S.P. Singh, T. Jaakkola, and M.I. Jordan. </author> <title> Learning without state-estimation in partially observable Markovian decision processes. </title> <editor> In W.Cohen and H.Hirsh, eds., </editor> <booktitle> Proc. 11 th Int. Conf. on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: Thus, the important general advantage the actual return learner has in non Markovian domains should be clear from this example: 5 A similar construction is used for didactic purposes in Singh, Jaakkola and Jordan <ref> [14] </ref>.
Reference: [15] <author> S.P. Singh and R.S. Sutton. </author> <title> Reinforcement learning with replacing eligibility traces. </title> <note> To Appear In: Machine Learning, </note> <year> 1996. </year>
Reference-contexts: D ()-based methods, where a non-zero reward is received on average every n time-steps. (This is a consequence of the eligibility list having to be processed only after a nonzero reward is received, rather than after every time-step, as in the case of TD methods.) When using standard "accumulating traces" <ref> [17, 15] </ref>, the worst-case complexity (space and time) is O (S fi A) for T D () based implementations (where S is the number of discrete states in the system and A is the number of allowable actions from within each state), as opposed to O (S) for P-Trace. <p> Comparing a slightly newer version of C-Trace to a T D ()-based algorithm (Sutton's variant of Rummery & Niranjan's sarsa, see [17]) in the domains puddle-world, mountain-car, pole-and-cart and the 21-state Markov chain prediction problem <ref> [4, 17, 15] </ref>, we found that the C-Trace variant performed very nearly equally with respect to learning rate to sarsa optimised for , with generally much better parameter insensitivity characteristics for the choice of the step-size fi.
Reference: [16] <author> R.S. Sutton. </author> <title> Learning to predict by the methods of temporal difference. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: On the other hand, in empirical studies the latter have been shown to enjoy better learning rates when T D ()-based implementations have been used <ref> [16, 17] </ref>. Jaakkola et al. [8] have pointed to the advantages of actual return learners over TD methods when learning in a non-Markovian domain where the optimal policy is nonstationary. <p> In the case of C-Trace, applying the CTR before trace-zeroing does not affect experimentation insensitivity. Secondly, since the T D () return is equal to the actual return when = 1 <ref> [16, 18] </ref>, it is reasonable to ask whether P-Trace is equivalent to Q ()-learning with = 1. Perhaps surprisingly, the answer is no. <p> However, it would learn if this factor was reduced to 0.2, the suggested setting in [12]. This seems to suggest that this systemic inaccuracy should not be treated as insignificant. 4 It is well documented that T D () performs relatively poorly with = 1 <ref> [16, 17] </ref>; it has been suggested this is empirical evidence for the superiority of TD over actual return methods. This interpretation does not fit well with our results, however.
Reference: [17] <author> R.S. Sutton. </author> <title> Generalization in reinforcement learning: Successful examples using sparse coarse coding. To Appear in: </title> <booktitle> Advances in Neural Information Processing Systems 8. </booktitle> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: Whether it is better to learn on the basis of actual outcomes (also known as rollouts or actual returns), as in Monte Carlo methods, or to learn on the basis of interim estimates, as in temporal difference (TD) methods, has been identified as a key question in reinforcement learning (RL) <ref> [17] </ref>. The evidence to date has been mixed. In [17], Sutton points out the former enjoy a number of theoretical advantages when certain classes of function approximators are used [7, 3]; Boyan and Moore [4] have demonstrated such effects particularly convincingly in simulated domains. <p> of actual outcomes (also known as rollouts or actual returns), as in Monte Carlo methods, or to learn on the basis of interim estimates, as in temporal difference (TD) methods, has been identified as a key question in reinforcement learning (RL) <ref> [17] </ref>. The evidence to date has been mixed. In [17], Sutton points out the former enjoy a number of theoretical advantages when certain classes of function approximators are used [7, 3]; Boyan and Moore [4] have demonstrated such effects particularly convincingly in simulated domains. <p> On the other hand, in empirical studies the latter have been shown to enjoy better learning rates when T D ()-based implementations have been used <ref> [16, 17] </ref>. Jaakkola et al. [8] have pointed to the advantages of actual return learners over TD methods when learning in a non-Markovian domain where the optimal policy is nonstationary. <p> D ()-based methods, where a non-zero reward is received on average every n time-steps. (This is a consequence of the eligibility list having to be processed only after a nonzero reward is received, rather than after every time-step, as in the case of TD methods.) When using standard "accumulating traces" <ref> [17, 15] </ref>, the worst-case complexity (space and time) is O (S fi A) for T D () based implementations (where S is the number of discrete states in the system and A is the number of allowable actions from within each state), as opposed to O (S) for P-Trace. <p> However, it would learn if this factor was reduced to 0.2, the suggested setting in [12]. This seems to suggest that this systemic inaccuracy should not be treated as insignificant. 4 It is well documented that T D () performs relatively poorly with = 1 <ref> [16, 17] </ref>; it has been suggested this is empirical evidence for the superiority of TD over actual return methods. This interpretation does not fit well with our results, however. <p> Indeed, we can report briefly on preliminary results from such a study currently in progress. Comparing a slightly newer version of C-Trace to a T D ()-based algorithm (Sutton's variant of Rummery & Niranjan's sarsa, see <ref> [17] </ref>) in the domains puddle-world, mountain-car, pole-and-cart and the 21-state Markov chain prediction problem [4, 17, 15], we found that the C-Trace variant performed very nearly equally with respect to learning rate to sarsa optimised for , with generally much better parameter insensitivity characteristics for the choice of the step-size fi. <p> Comparing a slightly newer version of C-Trace to a T D ()-based algorithm (Sutton's variant of Rummery & Niranjan's sarsa, see [17]) in the domains puddle-world, mountain-car, pole-and-cart and the 21-state Markov chain prediction problem <ref> [4, 17, 15] </ref>, we found that the C-Trace variant performed very nearly equally with respect to learning rate to sarsa optimised for , with generally much better parameter insensitivity characteristics for the choice of the step-size fi.
Reference: [18] <author> C.J.C.H. Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> Ph.D. Thesis, </type> <institution> King's College, </institution> <address> Cambridge, </address> <year> 1989. </year>
Reference-contexts: This algorithm was developed very much in the spirit of a 1-step TD design proposed by Witten [19]; the main conceptual advance is from the 1-step to a multiple step TD mechanism, using an "eligibility trace" with an exponentially decayed recency parameter . 1-step Q-learning was introduced by Watkins <ref> [18] </ref>. The key advance here was to represent explicitly the learnt value function as state/action pair payoffs rather than state payoffs, which meant the algorithm gained the key property of experimentation insensitivity [18, 12] in Markov domains, and led to proofs of convergence and optimality in these domains. <p> The key advance here was to represent explicitly the learnt value function as state/action pair payoffs rather than state payoffs, which meant the algorithm gained the key property of experimentation insensitivity <ref> [18, 12] </ref> in Markov domains, and led to proofs of convergence and optimality in these domains. More recently, variants which incorporate both multiple step TD and state/action pair Q-value representation have been proposed; these include the Q ()- learning algorithm [12]. <p> C-Trace performs updates more frequently than P-Trace. In addition to updating when a terminal state is reached according to the P-Trace rule, it will perform an update using a corrected truncated return (CTR) <ref> [18] </ref> whenever a non-policy action has been selected for execution. <p> C-Trace uses of a mixture of n-step CTRs, where the number of steps n in the return horizon may vary from one return to the next. This distinguishes C-Trace from other RL algorithms. Varying Watkins' <ref> [18] </ref> notation slightly, we use V (x) for the expected value of the actual return from state x following policy from that point on (assuming a Markovian domain). <p> In the case of C-Trace, applying the CTR before trace-zeroing does not affect experimentation insensitivity. Secondly, since the T D () return is equal to the actual return when = 1 <ref> [16, 18] </ref>, it is reasonable to ask whether P-Trace is equivalent to Q ()-learning with = 1. Perhaps surprisingly, the answer is no.
Reference: [19] <author> I.H. Witten. </author> <title> An adaptive optimal controller for discrete-time Markov environments. </title> <journal> Information and Control, </journal> <volume> 34 </volume> <pages> 286-295, </pages> <year> 1977. </year>
Reference-contexts: The Adaptive Heuristic Critic (AHC) algorithm [2] was also originally studied in the context of the pole-and-cart problem. This algorithm was developed very much in the spirit of a 1-step TD design proposed by Witten <ref> [19] </ref>; the main conceptual advance is from the 1-step to a multiple step TD mechanism, using an "eligibility trace" with an exponentially decayed recency parameter . 1-step Q-learning was introduced by Watkins [18].
References-found: 19

