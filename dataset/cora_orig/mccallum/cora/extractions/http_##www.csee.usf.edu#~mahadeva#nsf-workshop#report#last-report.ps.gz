URL: http://www.csee.usf.edu/~mahadeva/nsf-workshop/report/last-report.ps.gz
Refering-URL: http://www.csee.usf.edu/~mahadeva/nsf-workshop/homepage.html
Root-URL: 
Author: Sridhar Mahadevan Leslie Pack Kaelbling 
Keyword: Learning  
Affiliation: Department of Computer Science and Engineering University of South Florida  Department of Computer Science Brown University  
Note: Sponsored by the National Science Foundation  
Abstract: Report of the 1996 Workshop on Reinforcement 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Baird and H. Klopf. </author> <title> Reinforcement learning with high-dimensionsal continuous actions. </title> <type> Technical Report WL-TR-93-1147, </type> <institution> Wright Laboratory, Wright-Patterson AirForce Base, </institution> <year> 1993. </year>
Reference-contexts: promising initial results in applying this algorithm to the domain of backgammon. 2.3 Scaling up RL Continuous State and Action Spaces Leemon Baird (US Air Force Academy) discussed scaling issues for reinforcement learning, including dealing with continuous actions, the use of function approximators, and continuous time (or small time steps) <ref> [1] </ref>. He described a variety of different methods for finding optimal continuous actions. He also discussed issues in choosing an appropriate function approximator, ranging from simple linear approximators to more complex non-linear approximators. He described some issues in extending Q-learning to continuous-time problems.
Reference: [2] <author> C. Bandera, V. Francisco, B. Jose, M. Harmon, and L. Baird. </author> <title> Residual q-learning aplied to visual attention. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <pages> pages 20-27. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1996. </year>
Reference-contexts: Bandera et al. <ref> [2] </ref> used Q-learning to aquire an action perception strategy to decide what parts of an object to foveate in an attempt to recognize the object's type.
Reference: [3] <author> A. Barto, S. Bradkte, and S. Singh. </author> <title> Learning to act using real-time dynamic programming. </title> <journal> Artificial Intelligence, </journal> <volume> 72 </volume> <pages> 81-138, </pages> <year> 1995. </year>
Reference-contexts: RL can be viewed as an adaptive control paradigm that enables autonomous agents, embedded in some stochastic environment, to learn optimal policies <ref> [3, 17, 31] </ref>. Here, an agent is placed in an initially unknown task environment, and learns by trial and error to choose actions that maximize, over the long run, rewards that it receives. <p> Pointers to RL and DP Literature A good general introduction to RL may be found in the survey paper by Kaelbling, Littman, and Moore [18]. Another good overview of RL is found in Barto, Bradtke, and Singh <ref> [3] </ref>, which describes the intimate connection between RL, real-time DP, and search techniques in AI. A more in-depth presentation of various research projects can be found in two special issues on RL of the Machine Learning journal, edited by Kaelbling [16] and an earlier issue edited by Sutton [31].
Reference: [4] <author> A. Barto, R. Sutton, and C. Anderson. </author> <title> Neuronlike adaptive elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 13(5) </volume> <pages> 834-846, </pages> <year> 1983. </year>
Reference-contexts: One of the most influential models proposed for RL is the actor-critic system proposed by Barto, Sutton, and Anderson <ref> [4] </ref>. Here, the actor is responsible for executing a policy, and the critic learns to solve the credit-assignment problem of evaluating policies.
Reference: [5] <author> D. Bertsekas. </author> <title> Dynamic Programming and Optimal Control. </title> <publisher> Athena Scientific, </publisher> <year> 1995. </year>
Reference-contexts: Puterman's book [25] provides an extensive discussion of MDP's, which form the theoretical basis of RL. A detailed overview of DP, including a discussion of simulation-based approaches such as RL is given in Bertsekas' book <ref> [5] </ref>. 2 Workshop Summary In this section we provide a brief description of the topics discussed at the workshop.
Reference: [6] <author> D. Bertsekas and J. Tsitsiklis. </author> <title> Neuro-Dynamic Programming. </title> <publisher> Athena Scientific, </publisher> <year> 1996. </year>
Reference-contexts: A more in-depth presentation of various research projects can be found in two special issues on RL of the Machine Learning journal, edited by Kaelbling [16] and an earlier issue edited by Sutton [31]. At least two books on RL are forthcoming, including one by Bertsekas and Tsitsiklis <ref> [6] </ref> and one by Sutton and Barto [32]. Puterman's book [25] provides an extensive discussion of MDP's, which form the theoretical basis of RL. <p> Reinforcement-learning methods have been successfully applied to elevator scheduling [11], job-shop scheduling for NASA missions [42], backgammon [36], and cellular-phone channel assignment <ref> [6] </ref>. There are a number of other ongoing applications, and case studies of systems of this kind will appear in Bertsekas and Tsitsiklis' new book [6]. <p> Reinforcement-learning methods have been successfully applied to elevator scheduling [11], job-shop scheduling for NASA missions [42], backgammon [36], and cellular-phone channel assignment <ref> [6] </ref>. There are a number of other ongoing applications, and case studies of systems of this kind will appear in Bertsekas and Tsitsiklis' new book [6]. There is considerable enthusiasm among members of the operations research community about these techniques, which enable the approximate solution of problems that were heretofore unad-dressable.
Reference: [7] <author> D. Blackwell. </author> <title> Discrete dynamic programming. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 33 </volume> <pages> 719-726, </pages> <year> 1962. </year>
Reference-contexts: He discussed a Bayesian network approach to represent action models, where the topology of the network is part of the prior knowledge. Sridhar Mahadevan (Univ. of South Florida) presented a framework called sensitive discount optimality, due to Blackwell <ref> [7] </ref> and Veinott [40], which offers an elegant way of linking the discounted and average-reward optimality critera. This framework is based on studying the properties of the expected cumulative discounted reward, as discounting tends to 1.
Reference: [8] <author> C. Boutilier. </author> <title> Exploiting structure in policy construction. </title> <booktitle> In Proceedings of the Fourteenth IJCAI, </booktitle> <year> 1995. </year>
Reference-contexts: Craig Boutilier (Univ. of British Columbia) illustrated how Bayesian networks could alleviate the problem of specifying and solving MDP's <ref> [8] </ref>. He showed how networks reveal regularities and structure in the system dynamics and reward function that can be exploited computationally. He examined three different abstraction methods, and described some ways of performing region-based DP in large, finite state and action problems. <p> Although compact models are elegant, they will not really be useful until we can use them to solve MDP or RL problems more efficiently. Recent work by Boutilier and others <ref> [8] </ref> seeks to exploit structure in the representation of an MDP to solve it more efficiently. 10 Although we know a good deal about the applications of reinforcement learning to control problems, it has recently been successfully used in a perception application.
Reference: [9] <author> Craig Boutilier, Tom Dean, and Steve Hanks. </author> <title> Planning under uncertainty: Structural assumptions and computational leverage. </title> <booktitle> In Proceedings, 2nd European Planning Workshop, </booktitle> <year> 1995. </year>
Reference-contexts: He examined three different abstraction methods, and described some ways of performing region-based DP in large, finite state and action problems. Planning using MDP Framework Steve Hanks (Univ. of Washington) contrasted the view of work in decision-theoretic planning <ref> [14, 9, 15] </ref>, where agents continually build suboptimal plans for achieving dynamic goals, with the work in RL on computing optimal plans for a fixed goal.
Reference: [10] <author> J. Boyan and A. Moore. </author> <title> Learning evaluation functions for large acyclic domains. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <pages> pages 63-70. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1996. </year>
Reference-contexts: He discussed a mixture-model based architecture for automatically decomposing sequential tasks. Function Approximation Justin Boyan (CMU) described an algorithm for approximating the value function, based on using an efficient shortest-path algorithms from graph theory <ref> [10] </ref>. He focused on the important subclass of acyclic tasks. His algorithm, called ROUT, can be used in large stochastic state spaces requiring function approximation. He showed significant improvements over TD () in both efficiency and value function approximation accuracy on several medium-sized domains.
Reference: [11] <author> B. Crites and A. Barto. </author> <title> Improving elevator performance using reinforcement learning. </title> <booktitle> In Neural Information Processing Systems (NIPS) 8, </booktitle> <year> 1996. </year>
Reference-contexts: Examples include Tesauro's TD-Gammon system [36], the elevator dispatching system developed in his group by Robert Crites <ref> [11] </ref>, and Zhang and Dietterich's NASA job shop scheduling system [42]. <p> He also enumerated a number of other opportunities for using RL to improve the efficiency of industrial processes. Elevator Scheduling Robert Crites (Univ. of Mass., Amherst) described an application of Q-learning to control a multi-agent team of elevators in a multi-story building <ref> [11] </ref>. <p> Also, the learning algorithms use function-approximation techniques to store the value function; if the bias in the approximator is well-suited to the form of the value function, then powerful generalization can occur between "similar" states. Reinforcement-learning methods have been successfully applied to elevator scheduling <ref> [11] </ref>, job-shop scheduling for NASA missions [42], backgammon [36], and cellular-phone channel assignment [6]. There are a number of other ongoing applications, and case studies of systems of this kind will appear in Bertsekas and Tsitsiklis' new book [6].
Reference: [12] <author> M. Dorigo and M. Colombetti. </author> <title> Robot shaping: Developing autonomous agents through learning. </title> <journal> Artificial Intelligence, </journal> <volume> 71 </volume> <pages> 321-370, </pages> <year> 1994. </year>
Reference-contexts: decision spaces are factored using variables and the dependencies involving these variables were made explicit. 2.5 Biasing RL systems Classifier Systems and RL Marco Dorigo (Universite Libre de Bruxelles) discussed the role that an external trainer, who provides immediate reinforcements, has in speeding up the learning in real autonomous agents <ref> [12] </ref>. He focused on a learning technique called the Learning Classifier System.
Reference: [13] <author> M. Dorigo and M. Colombetti. </author> <title> Robot Shaping: An Experiment in Behavior Engineering. </title> <publisher> MIT Press, </publisher> <year> 1997. </year>
Reference-contexts: He focused on a learning technique called the Learning Classifier System. He described "robot shaping" as a way of using RL as a means to translate suggestions coming from an external trainer into an effective control strategy that allows a robot to achieve a goal <ref> [13] </ref>. 8 Learning from Advice Jude Shavlik (Univ. of Wisconsin) discussed the need to provide additional training information to reinforcement learners [21]. His research group has been investigating techniques for augmenting the standard feedback from scalar reinforcements. Their RATLE system accepts broad "advice" expressed in a simple programming language.
Reference: [14] <author> D. Draper, S. Hanks, and D. Weld. </author> <title> A probabilistic model of action for least-commitment planning with information gathering. </title> <booktitle> In Proceedings, Uncertainty in Artificial Intelligence, </booktitle> <year> 1994. </year> <month> 14 </month>
Reference-contexts: He examined three different abstraction methods, and described some ways of performing region-based DP in large, finite state and action problems. Planning using MDP Framework Steve Hanks (Univ. of Washington) contrasted the view of work in decision-theoretic planning <ref> [14, 9, 15] </ref>, where agents continually build suboptimal plans for achieving dynamic goals, with the work in RL on computing optimal plans for a fixed goal.
Reference: [15] <author> D. Draper, S. Hanks, and D. Weld. </author> <title> Probabilistic planning with information gathering and contingent execution. </title> <booktitle> In Proceedings, 2nd Conference on AI Planning Systems, </booktitle> <month> June </month> <year> 1994. </year>
Reference-contexts: He examined three different abstraction methods, and described some ways of performing region-based DP in large, finite state and action problems. Planning using MDP Framework Steve Hanks (Univ. of Washington) contrasted the view of work in decision-theoretic planning <ref> [14, 9, 15] </ref>, where agents continually build suboptimal plans for achieving dynamic goals, with the work in RL on computing optimal plans for a fixed goal.
Reference: [16] <author> L. Kaelbing, </author> <title> editor. Reinforcement Learning. </title> <publisher> Kluwer Academic Press, </publisher> <year> 1996. </year> <journal> Special Issue of Machine Learning Journal, </journal> <volume> Vol 22. </volume>
Reference-contexts: A more in-depth presentation of various research projects can be found in two special issues on RL of the Machine Learning journal, edited by Kaelbling <ref> [16] </ref> and an earlier issue edited by Sutton [31]. At least two books on RL are forthcoming, including one by Bertsekas and Tsitsiklis [6] and one by Sutton and Barto [32]. Puterman's book [25] provides an extensive discussion of MDP's, which form the theoretical basis of RL.
Reference: [17] <author> L. Kaelbling. </author> <title> Learning in Embedded Systems. </title> <type> PhD thesis, </type> <institution> Stanford University., </institution> <year> 1990. </year>
Reference-contexts: RL can be viewed as an adaptive control paradigm that enables autonomous agents, embedded in some stochastic environment, to learn optimal policies <ref> [3, 17, 31] </ref>. Here, an agent is placed in an initially unknown task environment, and learns by trial and error to choose actions that maximize, over the long run, rewards that it receives.
Reference: [18] <author> L. Kaelbling, M. Littman, and A. Moore. </author> <title> Reinforcement learning: A survey. </title> <journal> Journal of AI Research, </journal> <volume> 4 </volume> <pages> 237-285, </pages> <year> 1996. </year>
Reference-contexts: Pointers to RL and DP Literature A good general introduction to RL may be found in the survey paper by Kaelbling, Littman, and Moore <ref> [18] </ref>. Another good overview of RL is found in Barto, Bradtke, and Singh [3], which describes the intimate connection between RL, real-time DP, and search techniques in AI.
Reference: [19] <author> Nicholas Kushmerick, Steve Hanks, and Daniel Weld. </author> <title> An algorithm for probabilistic planning. </title> <booktitle> Artificial Intelligence, </booktitle> <pages> 76(1-2), </pages> <year> 1995. </year>
Reference-contexts: Since most work in reinforcement learning addresses the problem of learning how to behave in sequential environments, there is a deep connection with work in AI planning. As AI planning has begun to adopt models with a decision-theoretic orientation <ref> [19] </ref> and to be interested in partial policies rather than straight-line plans, there has been a convergence on MDPs as the basic model underlying all our work. One of the biggest contributions that mainstream AI can make to work in reinforcement-learning is in understanding how to use richer representations.
Reference: [20] <author> M. Littman and C. Szepesvari. </author> <title> A generalized reinforcement-learning model: Convergence and applications. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <pages> pages 310-318. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1996. </year>
Reference-contexts: He presented two new model-free and model-based algorithms [22], derived from this framework, that optimize not only the expected average reward (gain-optimality), but also maximize total reward among all gain-optimal policies (bias-optimality). Generalizing MDPs Michael Littman (Brown Univ.) <ref> [20] </ref> described a generalized MDP model that unifies standard MDP models with alternating Markov games and information-state MDP's. The generalized MDP model applies to several different optimality criteria, including finite horizon, expected discounted sum, and risk-sensitive discounted reward.
Reference: [21] <author> R. Maclin and J. Shavlik. </author> <title> Creating advice-taking reinforcement learners. </title> <journal> Machine Learning, </journal> <volume> 22 </volume> <pages> 251-281, </pages> <year> 1996. </year>
Reference-contexts: a way of using RL as a means to translate suggestions coming from an external trainer into an effective control strategy that allows a robot to achieve a goal [13]. 8 Learning from Advice Jude Shavlik (Univ. of Wisconsin) discussed the need to provide additional training information to reinforcement learners <ref> [21] </ref>. His research group has been investigating techniques for augmenting the standard feedback from scalar reinforcements. Their RATLE system accepts broad "advice" expressed in a simple programming language.
Reference: [22] <author> S. Mahadevan. </author> <title> An average-reward reinforcement learning algorithm for computing bias-optimal policies. </title> <booktitle> In Proceedings of the Thirteenth AAAI, </booktitle> <pages> pages 875-880. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: This framework is based on studying the properties of the expected cumulative discounted reward, as discounting tends to 1. He presented two new model-free and model-based algorithms <ref> [22] </ref>, derived from this framework, that optimize not only the expected average reward (gain-optimality), but also maximize total reward among all gain-optimal policies (bias-optimality). Generalizing MDPs Michael Littman (Brown Univ.) [20] described a generalized MDP model that unifies standard MDP models with alternating Markov games and information-state MDP's.
Reference: [23] <author> P. Montague, Dayan P., C. Person, and T. Sejnowski. </author> <title> Bee foraging in uncertain environments using predictive hebbian learning. </title> <journal> Nature, </journal> <volume> 377 </volume> <pages> 725-728, </pages> <year> 1995. </year>
Reference-contexts: the different algorithms. 2.6 RL in Neuroscience Peter Dayan (MIT) presented joint work with Read Montague (Baylor College of Medicine) and Terry Sejnowski (Salk Institute) that addresses the question of how animals learn to predict which stimuli and actions are associated with rewards and how they change their policies accordingly <ref> [23, 24] </ref>. He showed recent neurophysiological evidence on the activities of cells in nuclei in the monkey brain that are connected with reward learning. These cells project the neuromodulator dopamine to various targets, including the limbic system and pre-frontal cortex.
Reference: [24] <author> P. Montague, Dayan P., and T. Sejnowski. </author> <title> A framework for mesencephalic dopamine systems based on predictive hebbian learning. </title> <journal> Journal of Neuroscience, </journal> <volume> 16 </volume> <pages> 1936-1947, </pages> <year> 1996. </year>
Reference-contexts: the different algorithms. 2.6 RL in Neuroscience Peter Dayan (MIT) presented joint work with Read Montague (Baylor College of Medicine) and Terry Sejnowski (Salk Institute) that addresses the question of how animals learn to predict which stimuli and actions are associated with rewards and how they change their policies accordingly <ref> [23, 24] </ref>. He showed recent neurophysiological evidence on the activities of cells in nuclei in the monkey brain that are connected with reward learning. These cells project the neuromodulator dopamine to various targets, including the limbic system and pre-frontal cortex.
Reference: [25] <author> M. Puterman. </author> <title> Markov Decision Processes: Discrete Dynamic Stochastic Programming. </title> <publisher> John Wiley, </publisher> <year> 1994. </year>
Reference-contexts: At least two books on RL are forthcoming, including one by Bertsekas and Tsitsiklis [6] and one by Sutton and Barto [32]. Puterman's book <ref> [25] </ref> provides an extensive discussion of MDP's, which form the theoretical basis of RL. <p> He introduced the general Markov decision process (MDP) model <ref> [25] </ref> that underlies much of the work in DP as well as RL. He also described the standard DP algorithms such as policy iteration and value iteration for computing optimal policies, variations on these algorithms such as modified policy iteration, and finally the relationship between discounted optimality and average-case optimality.
Reference: [26] <author> G. Rummery and M. Niranjan. </author> <title> Online q-learning using connectionist systems. </title> <type> Technical Report CUED/F-INFENG/TR166, </type> <institution> Cambridge University, </institution> <year> 1994. </year>
Reference-contexts: Finally, he also noted that it does not work well with eligibility traces, such as used in TD () [30], and that because of the max operation, it could introduce systematic overestimation error (as noted by Thrun and Schwartz [37]). He argued in favor of SARSA <ref> [26] </ref>, a modified Q-learning algorithm, that overcomes these problems. Analysis of TD () Satinder Singh (Harlequin) described his joint work with Peter Dayan (MIT) on how the bias and variance of the TD () family of algorithms behaves with increasing experience.
Reference: [27] <author> S. Russell, J. Binder, K. Daphne, and K. </author> <title> Kanazawa. Local learning in probabilistic networks with hidden variables. </title> <booktitle> In Proceedings of the Fourteenth IJCAI, </booktitle> <year> 1995. </year>
Reference-contexts: Ronald Parr (Berkeley) described some of the work he did in collaboration with Stuart Russell, which is based on a particular type of Bayesian belief network called a dynamic probabilistic network (DPN) <ref> [27] </ref>. This network decomposes the representation of the state transition model and the sensor model according to conditional independence relationships among the state and sensor variables. DPN models can be learned from observations, even in the partially observable case, using local gradient descent techniques.
Reference: [28] <author> S. Russell and R. Parr. </author> <title> Approximating optimal policies for partially observable stochastic domains. </title> <booktitle> In Proceedings of the Fourteenth IJCAI, </booktitle> <year> 1995. </year>
Reference-contexts: Their aim is to demonstrate that the combination of these methods with techniques for approximate solution of partially observable MDPs should allow RL to scale up to large, uncertain, partially observable decision problems <ref> [28] </ref>. 7 2.4 Integrating RL into AI Structured Policies using Bayesian Nets Bayesian networks have been widely adopted in AI as a powerful tool for dealing with uncertainty. Craig Boutilier (Univ. of British Columbia) illustrated how Bayesian networks could alleviate the problem of specifying and solving MDP's [8].
Reference: [29] <author> S. Singh. </author> <title> Learning to Solve Markovian Decision Processes. </title> <type> PhD thesis, </type> <institution> Univ of Mas-sachusetts, Amherst, </institution> <year> 1994. </year> <month> 15 </month>
Reference-contexts: He described some issues in extending Q-learning to continuous-time problems. Finally, he summarized the known convergence results for various function approximators and learning algorithms (both for prediction and control). Hierarchical Models and Task Decomposition Satinder Singh (Harlequin) summarized the work in RL on hierarchical models and task decomposition <ref> [29] </ref>. The basic idea is that faster learning can be achieved by decomposing the overall task into a collection of simpler subtasks. He discussed a mixture-model based architecture for automatically decomposing sequential tasks.
Reference: [30] <author> R. Sutton. </author> <title> Learning to predict by the method of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: Also, it can learn a policy that performs badly if the agent continues to explore, since it is only optimal with no exploration. Finally, he also noted that it does not work well with eligibility traces, such as used in TD () <ref> [30] </ref>, and that because of the max operation, it could introduce systematic overestimation error (as noted by Thrun and Schwartz [37]). He argued in favor of SARSA [26], a modified Q-learning algorithm, that overcomes these problems.
Reference: [31] <author> R. Sutton, </author> <title> editor. Reinforcement Learning. </title> <publisher> Kluwer Academic Press, </publisher> <year> 1992. </year> <note> Special Issue of Machine Learning Journal Vol 8, Nos 3-4, </note> <month> May </month> <year> 1992. </year>
Reference-contexts: RL can be viewed as an adaptive control paradigm that enables autonomous agents, embedded in some stochastic environment, to learn optimal policies <ref> [3, 17, 31] </ref>. Here, an agent is placed in an initially unknown task environment, and learns by trial and error to choose actions that maximize, over the long run, rewards that it receives. <p> A more in-depth presentation of various research projects can be found in two special issues on RL of the Machine Learning journal, edited by Kaelbling [16] and an earlier issue edited by Sutton <ref> [31] </ref>. At least two books on RL are forthcoming, including one by Bertsekas and Tsitsiklis [6] and one by Sutton and Barto [32]. Puterman's book [25] provides an extensive discussion of MDP's, which form the theoretical basis of RL.
Reference: [32] <author> R. Sutton and A. Barto. </author> <title> Learning values: An introduction to reinforcement learning. </title> <publisher> Forthcoming, </publisher> <year> 1997. </year>
Reference-contexts: At least two books on RL are forthcoming, including one by Bertsekas and Tsitsiklis [6] and one by Sutton and Barto <ref> [32] </ref>. Puterman's book [25] provides an extensive discussion of MDP's, which form the theoretical basis of RL.
Reference: [33] <author> P. Tadepalli and D. </author> <title> Ok. Auto-exploratory average-reward reinforcement learning. </title> <booktitle> In Proceedings of the Thirteenth AAAI, </booktitle> <pages> pages 881-887. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: Undiscounted RL Prasad Tadepalli (Oregon State Univ.) argued that, in contrast to the standard practice in RL of maximizing the discounted total reward, in most real-world domains the average reward received per time step is a more natural metric. He introduced an average-reward RL method called H-learning <ref> [33] </ref>, and presented empirical results in several simple automated-guided vehicle (AGV) scheduling domains. He also described a local linear regression algorithm for approximating the value function. He discussed a Bayesian network approach to represent action models, where the topology of the network is part of the prior knowledge.
Reference: [34] <author> P. Tadepalli and D. </author> <title> Ok. Scaling up average reward reinforcement learning by approximating the domain models and the value function. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <pages> pages 471-479. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1996. </year>
Reference-contexts: One of the biggest contributions that mainstream AI can make to work in reinforcement-learning is in understanding how to use richer representations. Bayesian networks provide an ideal representation for stochastic state transition and reward functions in complex domains; this representation has been used by Tadepalli <ref> [34] </ref>, among others, for reinforcement-learning problems. In addition, there are now a number of good techniques for learning Bayesian networks from data, making this a plausible strategy for acquiring world models.
Reference: [35] <author> Y. Takahashi, M. Asada, and K. Hosoda. </author> <title> Reasonable performance in less learning time by real robot based on incremental state space segmentation. </title> <booktitle> In Proc. of IEEE/RSJ International Conference on Intelligent Robots and Systems 1996 (IROS96), </booktitle> <year> 1996. </year>
Reference-contexts: He showed how these techniques can be used to define regions where greedy local optimization is sufficient to produce globally optimal trajectories. Minoru Asada (Osaka Univ.) described an algorithm for coordinating multiple behaviors obtained independently by Q-learning <ref> [39, 35] </ref>. His method coordinates learning multiple behaviors taking account of a trade-off between learning time and performance. He applied the method to one-to-one soccer playing robots. He also nicely demonstrated, through a videotape, computer simulation and real robot experiments to show the validity of the proposed method.
Reference: [36] <author> G. Tesauro. </author> <title> Practical issues in temporal difference learning. </title> <editor> In R. Sutton, editor, </editor> <title> Reinforcement Learning. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1992. </year>
Reference-contexts: Reinforcement Learning Andrew Barto (Univ. of Mass., Amherst) observed that the most dramatic RL successes to date have been achieved in completely off-line applications in which experience was generated entirely using simulation models of the systems of interest. Examples include Tesauro's TD-Gammon system <ref> [36] </ref>, the elevator dispatching system developed in his group by Robert Crites [11], and Zhang and Dietterich's NASA job shop scheduling system [42]. <p> The incremental segmentation is performed by constructing local models in the state space. Game-Playing Gerrald Tesauro (IBM) presented some of his recent work on improving the performance of his well-known TD-gammon backgammon player <ref> [36] </ref>. He described a Monte-Carlo simulation algorithm for real-time policy improvement of an adaptive controller. 6 In the Monte-Carlo simulation, the long-term expected reward of each possible action is statistically measured, using the initial policy to make decisions in each step of the simulation. <p> Reinforcement-learning methods have been successfully applied to elevator scheduling [11], job-shop scheduling for NASA missions [42], backgammon <ref> [36] </ref>, and cellular-phone channel assignment [6]. There are a number of other ongoing applications, and case studies of systems of this kind will appear in Bertsekas and Tsitsiklis' new book [6].
Reference: [37] <author> S. Thrun and A. Schwartz. </author> <title> Issues in using function approximation for reinforcement learning. </title> <booktitle> In Proceedings of the Fourth Connectionist Models Summer School. </booktitle> <publisher> Lawrence Erlbaum, </publisher> <year> 1993. </year>
Reference-contexts: Finally, he also noted that it does not work well with eligibility traces, such as used in TD () [30], and that because of the max operation, it could introduce systematic overestimation error (as noted by Thrun and Schwartz <ref> [37] </ref>). He argued in favor of SARSA [26], a modified Q-learning algorithm, that overcomes these problems. Analysis of TD () Satinder Singh (Harlequin) described his joint work with Peter Dayan (MIT) on how the bias and variance of the TD () family of algorithms behaves with increasing experience.
Reference: [38] <author> S. Thrun and A. Schwartz. </author> <title> Finding structure in reinforcement learning. </title> <booktitle> In Neural Information Processing Systems (NIPS) 7, </booktitle> <year> 1995. </year>
Reference-contexts: Transfer across Tasks Sebastian Thrun (CMU) discussed methods for incorporating background knowledge into RL <ref> [38] </ref>. He investigated the feasibility of transferring learned knowledge across multiple, related RL tasks. He described several different algorithms (SKILLS, EBNN, and TC) that boost the performance of a reinforcement learner using previously learned domain knowledge.
Reference: [39] <author> E. Uchibe, M. Asada, and K. Hosoda. </author> <title> Behavior coordination for a mobile robot using modular reinforcement learning. </title> <booktitle> In Proceedings of the 1996 International Conference on Intelligent Robots and Systems (IROS). </booktitle> <publisher> IEEE Press, </publisher> <year> 1996. </year>
Reference-contexts: He showed how these techniques can be used to define regions where greedy local optimization is sufficient to produce globally optimal trajectories. Minoru Asada (Osaka Univ.) described an algorithm for coordinating multiple behaviors obtained independently by Q-learning <ref> [39, 35] </ref>. His method coordinates learning multiple behaviors taking account of a trade-off between learning time and performance. He applied the method to one-to-one soccer playing robots. He also nicely demonstrated, through a videotape, computer simulation and real robot experiments to show the validity of the proposed method.
Reference: [40] <author> A. Veinott. </author> <title> Discrete dynamic programming with sensitive discount optimality criteria. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 40(5) </volume> <pages> 1635-1660, </pages> <year> 1969. </year>
Reference-contexts: He discussed a Bayesian network approach to represent action models, where the topology of the network is part of the prior knowledge. Sridhar Mahadevan (Univ. of South Florida) presented a framework called sensitive discount optimality, due to Blackwell [7] and Veinott <ref> [40] </ref>, which offers an elegant way of linking the discounted and average-reward optimality critera. This framework is based on studying the properties of the expected cumulative discounted reward, as discounting tends to 1.
Reference: [41] <author> C. Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College, </institution> <year> 1989. </year>
Reference-contexts: Richard Sutton (Univ. of Mass., Amherst) discussed some problems with Q-learning, one of the most popular model-free RL algorithms originally developed by Watkins <ref> [41] </ref>. Among the drawbacks noted in his talk were that Q-learning, as an off-policy method, can be unstable even with linear function approximators. Also, it can learn a policy that performs badly if the agent continues to explore, since it is only optimal with no exploration.
Reference: [42] <author> W. Zhang and T. Dietterich. </author> <title> A reinforcement learning approach to job-shop scheduling. </title> <booktitle> In Proceedings of the Fourteenth IJCAI, </booktitle> <year> 1995. </year> <month> 16 </month>
Reference-contexts: Examples include Tesauro's TD-Gammon system [36], the elevator dispatching system developed in his group by Robert Crites [11], and Zhang and Dietterich's NASA job shop scheduling system <ref> [42] </ref>. He pointed out that a great advantage of these methods is that although they require models, these models do not need to be explicit probability models of an MDP; simulation models suffice, which are often very much easier to obtain. <p> This work also nicely demonstrates the advantage of using industrial simulation models to train an RL system. Job-Shop Scheduling Thomas Dietterich (Oregon State Univ.) described an application of TD () to a job-shop scheduling problem for the NASA space-shuttle program <ref> [42] </ref>. The problem involves scheduling a set of tasks to satisfy a set of temporal and resource constraints while also seeking to minimize the total length of the schedule. <p> Reinforcement-learning methods have been successfully applied to elevator scheduling [11], job-shop scheduling for NASA missions <ref> [42] </ref>, backgammon [36], and cellular-phone channel assignment [6]. There are a number of other ongoing applications, and case studies of systems of this kind will appear in Bertsekas and Tsitsiklis' new book [6].
References-found: 42

