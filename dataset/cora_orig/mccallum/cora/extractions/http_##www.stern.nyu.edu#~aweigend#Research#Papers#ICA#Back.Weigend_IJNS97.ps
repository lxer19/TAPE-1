URL: http://www.stern.nyu.edu/~aweigend/Research/Papers/ICA/Back.Weigend_IJNS97.ps
Refering-URL: http://www.stern.nyu.edu/~aweigend/Research/Papers/ICA/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: back@brain.riken.go.jp  aweigend@stern.nyu.edu  
Title: Working Paper IS-97-22 (Information Systems) A First Application of Independent Component Analysis to Extracting Structure
Author: Leonard N. Stern Andrew D. Back Andreas S. Weigend Leonard N. Stern 
Web: http://www.stern.nyu.edu/~aweigend/Research/Papers/ICA  www.bip.riken.go.jp/absl/back  www.stern.nyu.edu/~aweigend  
Note: Forthcoming in: International Journal of Neural Systems, Vol. 8 (1997) (Special Issue on Data Mining in Finance)  
Address: New York University  2-1 Hirosawa, Wako-shi, Saitama 351-0198, Japan  New York University 44 West Fourth Street, MEC 9-74 New York, NY 10012, USA  
Affiliation: School of Business,  Brain Science Institute The Institute of Physical and Chemical Research (RIKEN)  Department of Information Systems  School of Business  
Abstract: This paper discusses the application of a modern signal processing technique known as independent component analysis (ICA) or blind source separation to multivariate financial time series such as a portfolio of stocks. The key idea of ICA is to linearly map the observed multivariate time series into a new space of statistically independent components (ICs). This can be viewed as a factorization of the portfolio since joint probabilities become simple products in the coordinate system of the ICs. We apply ICA to three years of daily returns of the 28 largest Japanese stocks and compare the results with those obtained using principal component analysis. The results indicate that the estimated ICs fall into two categories, (i) infrequent but large shocks (responsible for the major changes in the stock prices), and (ii) frequent smaller fluctuations (contributing little to the overall level of the stocks). We show that the overall stock price can be reconstructed surprisingly well by using a small number of thresholded weighted ICs. In contrast, when using shocks derived from principal components instead of independent components, the reconstructed price is less similar to the original one. Independent component analysis is a potentially powerful method of analyzing and understanding driving mechanisms in financial markets. There are further promising applications to risk management since ICA focuses on higher order statistics. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Amari, S. </author> <year> (1998). </year> <title> Natural gradient works efficiently in learning, </title> <booktitle> Neural Computation 10(2): </booktitle> <pages> 251-276. </pages>
Reference: <author> Amari, S. and Cichocki, A. </author> <year> (1998). </year> <title> Blind signal processing- neural network approaches, </title> <journal> Proc. IEEE. </journal> <note> to appear. 14 Amari, </note> <author> S., Cichocki, A. and Yang, H. </author> <year> (1996). </year> <title> A new learning algorithm for blind signal separation, </title> <editor> in G. Tesauro, D. Touretzky and T. Leen (eds), </editor> <booktitle> Advances in Neural Information Processing Systems 8 (NIPS*95), </booktitle> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <pages> pp. 757-763. </pages>
Reference: <author> Baldi, P. and Hornik, K. </author> <year> (1989). </year> <title> Neural networks and principal component analysis: Learning from examples without local minima, </title> <booktitle> Neural Networks 2: </booktitle> <pages> 53-58. </pages>
Reference: <author> Baram, Y. and Roth, Z. </author> <year> (1995). </year> <title> Forecasting by density shaping using neural networks, </title> <booktitle> Proceedings of the 1995 Conference on Computational Intelligence for Financial Engineering (CIFEr), </booktitle> <publisher> IEEE Service Center, </publisher> <address> Piscataway, NJ, </address> <pages> pp. 57-71. </pages>
Reference: <author> Bell, A. and Sejnowski, T. </author> <year> (1995). </year> <title> An information maximization approach to blind separation and blind deconvolution, </title> <booktitle> Neural Computation 7: </booktitle> <pages> 1129-1159. </pages>
Reference-contexts: ICA seeks to extract out these independent components (ICs) as well as the mixing process. ICA can be expressed in terms of the related concepts of entropy <ref> (Bell and Sejnowski 1995) </ref>, mutual information (Amari, Cichocki and Yang 1996), contrast functions (Comon 1994) and other measures of the statistical independence of signals. For independent signals, the joint probability can be factorized into the product of the marginal probabilities. <p> Since then, various approaches have been proposed in the literature to implement ICA. These include: minimizing higher order moments (Cardoso 1989) or higher order cumulants 3 (Cardoso and Souloumiac 1993), maximization of mutual information of the outputs or maximization of the output entropy <ref> (Bell and Sejnowski 1995) </ref>, minimization of the Kullback-Leibler divergence between the joint and the product of the marginal distributions of the outputs (Amari et al. 1996). ICA algorithms are typically implemented in either off-line (batch) form or using an on-line approach.
Reference: <author> Belouchrani, A., Abed Meraim, K., Cardoso, J. and Moulines, E. </author> <year> (1997). </year> <title> A blind source separation technique based on second order statistics, </title> <journal> IEEE Trans. on S.P. </journal> <volume> 45(2): </volume> <pages> 434-44. </pages>
Reference: <author> Bogner, R. E. </author> <year> (1992). </year> <title> Blind separation of sources, </title> <type> Working Paper 4559, </type> <institution> Defence Research Agency, Malvern. </institution>
Reference-contexts: ICA algorithms are typically implemented in either off-line (batch) form or using an on-line approach. A standard approach for batch ICA algorithms is the following two-stage procedure <ref> (Bogner 1992, Cardoso and Souloumiac 1993) </ref>. 1. Decorrelation or whitening. Here we seek to diagonalize the covariance matrix of the input signals. 2. Rotation. The second stage minimizes a measure of the higher order statistics which will ensure the non-Gaussian output signals are as statistically independent as possible.
Reference: <author> Bourlard, H. and Kamp, Y. </author> <year> (1988). </year> <title> Auto-association by multilayer perceptrons and singular value decomposition, </title> <booktitle> Biological Cybernetics 59: </booktitle> <pages> 291-294. </pages>
Reference: <author> Burel, G. </author> <year> (1992). </year> <title> Blind separation of sources: a nonlinear neural algorithm, </title> <booktitle> Neural Networks 5: </booktitle> <pages> 937-947. </pages>
Reference: <author> Cardoso, J. </author> <year> (1989). </year> <title> Source separation using higher order moments, </title> <booktitle> International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pp. 2109-2112. </pages>
Reference-contexts: On the other hand, ICA algorithms may use higher order 2 statistical information for separating the signals <ref> (see for example Cardoso 1989, Comon 1994) </ref>. For this reason non-Gaussian signals (or at most, one Gaussian signal) are normally required for ICA algorithms based on higher order statistics. <p> Since then, various approaches have been proposed in the literature to implement ICA. These include: minimizing higher order moments <ref> (Cardoso 1989) </ref> or higher order cumulants 3 (Cardoso and Souloumiac 1993), maximization of mutual information of the outputs or maximization of the output entropy (Bell and Sejnowski 1995), minimization of the Kullback-Leibler divergence between the joint and the product of the marginal distributions of the outputs (Amari et al. 1996).
Reference: <author> Cardoso, J. </author> <year> (1998). </year> <title> Blind signal separation: a review, </title> <journal> Proc. IEEE. </journal> <note> To appear. </note>
Reference: <author> Cardoso, J. and Laheld, B. </author> <year> (1996). </year> <title> Equivariant adaptive source separation, </title> <journal> IEEE Trans. Signal Processing 44(12): </journal> <pages> 3017-3030. </pages>
Reference: <author> Cardoso, J. and Souloumiac, A. </author> <year> (1993). </year> <title> Blind beamforming for non-Gaussian signals, </title> <journal> IEE Proc. F. </journal> <volume> 140(6): </volume> <pages> 771-774. </pages>
Reference-contexts: Since then, various approaches have been proposed in the literature to implement ICA. These include: minimizing higher order moments (Cardoso 1989) or higher order cumulants 3 <ref> (Cardoso and Souloumiac 1993) </ref>, maximization of mutual information of the outputs or maximization of the output entropy (Bell and Sejnowski 1995), minimization of the Kullback-Leibler divergence between the joint and the product of the marginal distributions of the outputs (Amari et al. 1996). <p> Rotation. The second stage minimizes a measure of the higher order statistics which will ensure the non-Gaussian output signals are as statistically independent as possible. It can be shown that this can be carried out by a unitary rotation matrix <ref> (Cardoso and Souloumiac 1993) </ref>. <p> For non-Gaussian random signals the implication is that not only should the signals be uncorrelated, but that the higher order cross-statistics (eg. moments or cumulants) are zeroed. The empirical study carried out in this paper uses the JADE (Joint Approximate Diagonalization of Eigenmatrices) algorithm <ref> (Cardoso and Souloumiac 1993) </ref> which is a batch algorithm and is an efficient version of the above two step procedure. The first stage is performed by computing the sample covariance matrix, giving the second order statistics of the observed outputs. <p> The second stage consists of finding a rotation matrix which jointly diagonalizes eigenmatrices formed from the fourth order cumulants of the whitened data. The outputs from this stage are the independent components. For specific details of the algorithm, the reader is referred to <ref> (Cardoso and Souloumiac 1993) </ref>. The JADE algorithm has been extended by Pope and Bogner (1994). Other examples of two-step methods were proposed by Cardoso (1989), Comon (1989) and Bogner (1992). <p> Figure 4 shows these normalized stock returns. 4.2 Structure of the Independent Components We performed ICA on the stock returns using the JADE algorithm <ref> (Cardoso and Souloumiac 1993) </ref> described in Section 2.2.
Reference: <author> Chin, E. I. A. and Weigend, A. S. </author> <year> (1998). </year> <title> Independent component analysis and stock returns, </title> <type> Working paper, </type> <institution> Information Systems Department, Leonard N. Stern School of Business, New York University. </institution>
Reference: <author> Choi, S., Liu, R. and Cichocki, A. </author> <year> (1998). </year> <title> A spurious equilibria-free learning algorithm for the blind separation of non-zero skewness signals, </title> <booktitle> Neural Processing Letters 7: </booktitle> <pages> 1-8. </pages>
Reference: <author> Cichocki, A., Amari, S. and Thawonmas, R. </author> <year> (1996). </year> <title> Blind signal extraction using self-adaptive non-linear hebbian learning rule, </title> <booktitle> Proc. of Int. Symposium on Nonlinear Theory and its Applications, </booktitle> <address> NOLTA-96, Kochi, Japan, </address> <pages> pp. 377-380. </pages>
Reference-contexts: ICA seeks to extract out these independent components (ICs) as well as the mixing process. ICA can be expressed in terms of the related concepts of entropy (Bell and Sejnowski 1995), mutual information <ref> (Amari, Cichocki and Yang 1996) </ref>, contrast functions (Comon 1994) and other measures of the statistical independence of signals. For independent signals, the joint probability can be factorized into the product of the marginal probabilities.
Reference: <author> Cichocki, A. and Moszczynski, L. </author> <year> (1992). </year> <title> New learning algorithm for blind separation of sources, </title> <journal> Electronics Letters 28(21): </journal> <pages> 1986-1987. </pages>
Reference: <author> Cichocki, A. and Unbehauen, R. </author> <year> (1993). </year> <title> Neural Networks for Optimization and Signal Processing, </title> <publisher> Wiley. </publisher>
Reference: <author> Cichocki, A., Thawonmas, R. and Amari, S. </author> <year> (1997). </year> <title> Sequential blind signal extraction in order specified by stochastic properties, </title> <journal> Electronics Letters 33(1): </journal> <pages> 64-65. </pages> <note> 15 Cichocki, </note> <author> A., Unbehauen, R. and Rummert, E. </author> <year> (1994). </year> <title> Robust learning algorithm for blind separation of signals, </title> <journal> Electronics Letters 30(17): </journal> <pages> 1386-1387. </pages>
Reference: <author> Comon, P. </author> <year> (1989). </year> <title> Separation of sources using high-order cumulants, </title> <booktitle> SPIE Conference on Advanced Algorithms and Architectures for Signal Processing, </booktitle> <volume> Vol. </volume> <pages> XII, </pages> <address> San Diego, CA, </address> <pages> pp. 170-181. </pages>
Reference-contexts: On the other hand, ICA algorithms may use higher order 2 statistical information for separating the signals <ref> (see for example Cardoso 1989, Comon 1994) </ref>. For this reason non-Gaussian signals (or at most, one Gaussian signal) are normally required for ICA algorithms based on higher order statistics.
Reference: <author> Comon, P. </author> <year> (1994). </year> <title> Independent component analysis anew concept?, </title> <booktitle> Signal Processing 36(3): </booktitle> <pages> 287-314. </pages>
Reference-contexts: ICA seeks to extract out these independent components (ICs) as well as the mixing process. ICA can be expressed in terms of the related concepts of entropy (Bell and Sejnowski 1995), mutual information (Amari, Cichocki and Yang 1996), contrast functions <ref> (Comon 1994) </ref> and other measures of the statistical independence of signals. For independent signals, the joint probability can be factorized into the product of the marginal probabilities.
Reference: <author> Delfosse, N. and Loubaton, P. </author> <year> (1995). </year> <title> Adaptive blind separation of independent sources: a deflation approach, </title> <booktitle> Signal Processing 45: </booktitle> <pages> 59-83. </pages>
Reference: <author> Douglas, S. and Cichocki, A. </author> <year> (1997). </year> <title> Neural networks for blind decorrelation of signals, </title> <journal> IEEE Trans. Signal Processing 45(11): </journal> <pages> 2829-2842. </pages>
Reference: <author> Friedman, J. and Tukey, J. </author> <year> (1974). </year> <title> A projection pursuit algorithm for exploratory data analysis, </title> <journal> IEEE Trans. </journal> <volume> Computers 23: </volume> <pages> 881-889. </pages>
Reference-contexts: When the mutual information between variables vanish, they are statistically independent. * Find a set of "interesting" directions. The goal of finding interesting is similar to projection pursuit <ref> (Friedman and Tukey 1974, Huber 1985, Friedman 1987) </ref>. In the knowledge discovery and data mining community the term "interestingness" (Ripley 1996) is also used to denote unexpectedness (Silberschatz and Tuzhilin 1996). From this basis, algorithms can be derived to extract the desired independent components.
Reference: <author> Friedman, J. H. </author> <year> (1987). </year> <title> Exploratory projection pursuit, </title> <journal> Journal of the American Statistical Association 82: </journal> <pages> 249-266. </pages>
Reference: <author> Girolami, M. and Fyfe, C. </author> <year> (1997a). </year> <title> An extended exploratory projection pursuit network with linear and nonlinear anti-hebbian connections applied to the cocktail party problem, </title> <booktitle> Neural Networks 10(9): </booktitle> <pages> 1607-1618. </pages>
Reference: <author> Girolami, M. and Fyfe, C. </author> <year> (1997b). </year> <title> Extraction of independent signal sources using a deflationary exploratory projection pursuit network with lateral inhibition, </title> <booktitle> IEE Proceedings on Vision, Image and Signal Processing 14(5): </booktitle> <pages> 299-306. </pages>
Reference: <author> Herault, J. and Jutten, C. </author> <year> (1986). </year> <title> Space or time adaptive signal processing by neural network models, </title> <editor> in J. S. Denker (ed.), </editor> <booktitle> Neural Networks for Computing. Proceedings of AIP Conference, American Institute of Physics, </booktitle> <address> New York, </address> <pages> pp. 206-211. </pages>
Reference-contexts: The method is known as independent component analysis (ICA) and is also referred to as blind source separation <ref> (Herault and Jutten 1986, Jutten and Herault 1991, Comon 1994) </ref>. The central assumption is that an observed multivariate time series (such as daily stock returns) reflect the reaction of a system (such as the stock market) to a few statistically independent time series.
Reference: <author> Huber, P. J. </author> <year> (1985). </year> <title> Projection pursuit, </title> <journal> The Annals of Statistics 13: </journal> <pages> 435-475. </pages>
Reference: <author> Hyvarinen, A. </author> <year> (1996). </year> <title> Simple one-unit algorithms for blind source separation and blind deconvolution, </title> <booktitle> Progress in Neural Information Processing ICONIP'96, </booktitle> <volume> Vol. 2, </volume> <publisher> Springer, </publisher> <pages> pp. 1201-1206. </pages>
Reference: <author> Jutten, C. and Herault, J. </author> <year> (1991). </year> <title> Blind separation of sources, Part I: An adaptive algorithm based on neuromimetic architecture, </title> <booktitle> Signal Processing 24: </booktitle> <pages> 1-10. </pages>
Reference: <author> Jutten, C., Nguyen Thi, H., Dijkstra, E., Vittoz, E. and Caelen, J. </author> <year> (1991). </year> <title> Blind separation of sources, an algorithm for separation of convolutive mixtures, </title> <booktitle> Proceedings of Int. Workshop on High Order Statistics, Chamrousse (France), </booktitle> <pages> pp. 273-276. </pages>
Reference: <author> Karhunen, J. </author> <year> (1996). </year> <title> Neural approaches to independent component analysis and source separation, </title> <booktitle> Proceedings of 4th European Symp. on Artificial Neural Networks (ESANN'96), </booktitle> <address> Bruges, Belgium, </address> <pages> pp. 249-266. </pages>
Reference: <author> Lacoume, J. and Ruiz, P. </author> <year> (1992). </year> <title> Separation of independent sources from correlated inputs, </title> <journal> IEEE Trans. </journal> <note> Signal Processing 40: 3074-3078. 16 Lee, </note> <author> T., Girolami, M., Bell, A. and Sejnowski, T. </author> <year> (1998). </year> <title> A unifying information theoretic framework for independent component analysis, </title> <journal> International Journal on Mathematical and Computer Modelling. </journal>
Reference: <author> Li, S. and Sejnowski, T. </author> <year> (1995). </year> <title> Adaptive separation of mixed broad-band sound sources with delays by a beamforming Herault-Jutten network, </title> <journal> IEEE Journal of Oceanic Engineering 20(1): </journal> <pages> 73-79. </pages>
Reference: <author> Lin, J. K., Grier, D. G. and Cowan, J. D. </author> <year> (1997). </year> <title> Faithful representation of separable distributions, </title> <booktitle> Neural Computation 9: </booktitle> <pages> 1305-1320. </pages>
Reference: <author> Ling, X.-T., Huang, Y.-F. and Liu, R. </author> <year> (1994). </year> <title> A neural network for blind signal separation, </title> <booktitle> Proc. of IEEE Int. Symposium on Circuits and Systems (ISCAS-94), </booktitle> <publisher> IEEE Press, </publisher> <address> New York, NY, </address> <pages> pp. 69-72. </pages>
Reference: <author> Moody, J. E. and Wu, L. </author> <year> (1996). </year> <title> What is the "true price"? State space models for high frequency financial data, </title> <booktitle> Progress in Neural Information Processing (ICONIP'96), </booktitle> <publisher> Springer, Berlin, </publisher> <pages> pp. 697-704. </pages>
Reference: <author> Moody, J. E. and Wu, L. </author> <year> (1997a). </year> <title> What is the "true price"? State space models for high frequency FX data, </title> <editor> in A. S. Weigend, Y. S. Abu-Mostafa and A.-P. N. Refenes (eds), </editor> <booktitle> Decision Technologies for Financial Engineering (Proceedings of the Fourth International Conference on Neural Networks in the Capital Markets, NNCM-96), World Scientific, Singapore, </booktitle> <pages> pp. 346-358. </pages>
Reference: <author> Moody, J. E. and Wu, L. </author> <year> (1997b). </year> <title> What is the "true price"? State space models for high frequency FX data, </title> <booktitle> Proceedings of the IEEE/IAFE 1997 Conference on Computational Intelligence for Financial Engineering (CIFEr), </booktitle> <publisher> IEEE Service Center, </publisher> <address> Piscataway, NJ, </address> <pages> pp. 150-156. </pages>
Reference: <author> Moreau, E. and Macchi, O. </author> <year> (1994). </year> <title> Complex self-adaptive algorithms for source separation based on high order contrasts, </title> <booktitle> Signal Processing VII Proceedings of EUSIPCO-94, EURASIP, Lausanne, Switzer-land, </booktitle> <pages> pp. 1157-1160. </pages>
Reference: <author> Nguyen Thi, H.-L. and Jutten, C. </author> <year> (1995). </year> <title> Blind source separation for convolutive mixtures, </title> <booktitle> Signal Processing 45(2): </booktitle> <pages> 209-229. </pages>
Reference: <author> Oja, E. </author> <year> (1989). </year> <title> Neural networks, principal components and subspaces, </title> <journal> International Journal of Neural Systems 1: </journal> <pages> 61-68. </pages>
Reference: <author> Oja, E. and Karhunen, J. </author> <year> (1995). </year> <title> Signal separation by nonlinear hebbian learning, </title> <editor> in M. Palaniswami, Y. Attikiouzel, R. Marks II, D. Fogel and T. Fukuda (eds), </editor> <booktitle> Computational Intelligence ADynamic System Perspective, </booktitle> <publisher> IEEE Press, </publisher> <address> New York, NY, </address> <pages> pp. 83-97. </pages>
Reference: <author> Parra, L., Spence, C. and de Vries, B. </author> <year> (1997). </year> <title> Convolutive source separation and signal modeling with ML, </title> <booktitle> International Symposium on Intelligent Systems (ISIS'97), </booktitle> <institution> University of Reggio Calabria, Italy. </institution>
Reference: <author> Pearlmutter, B. A. and Parra, L. C. </author> <year> (1997). </year> <title> Maximum likelihood blind source separation: A context-sensitive generalization of ICA, </title> <editor> in M. C. Mozer, M. I. Jordan and T. Petsche (eds), </editor> <booktitle> Advances in Neural Information Processing Systems 9 (NIPS*96), </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <pages> pp. 613-619. </pages>
Reference: <author> Pham, D., Garat, P. and Jutten, C. </author> <year> (1992). </year> <title> Separation of a mixture of independent sources through a maximum likelihood approach, </title> <editor> in J. Vandevalle, R. Boite, M. Moonen and A. Oosterlink (eds), </editor> <booktitle> Signal Processing VI: Theories and Applications, </booktitle> <publisher> Elsevier, </publisher> <pages> pp. 771-774. </pages>
Reference: <author> Pope, K. and Bogner, R. </author> <year> (1994). </year> <title> Blind separation of speech signals, </title> <booktitle> Proc. of the Fifth Australian Int. Conf. on Speech Science and Technology, </booktitle> <address> Perth, Western Australia, </address> <pages> pp. 46-50. </pages> <note> 17 Pope, </note> <author> K. and Bogner, R. </author> <year> (1996a). </year> <title> Blind signal separation. I: Linear, instantaneous combinations, </title> <booktitle> Digital Signal Processing 6: </booktitle> <pages> 5-16. </pages>
Reference: <author> Pope, K. and Bogner, R. </author> <year> (1996b). </year> <title> Blind signal separation. II: Linear, convolutive combinations, </title> <booktitle> Digital Signal Processing 6: </booktitle> <pages> 17-28. </pages>
Reference: <author> Ripley, B. </author> <year> (1996). </year> <title> Pattern Recognition and Neural Networks, </title> <publisher> Cambridge University Press. </publisher>
Reference-contexts: When the mutual information between variables vanish, they are statistically independent. * Find a set of "interesting" directions. The goal of finding interesting is similar to projection pursuit (Friedman and Tukey 1974, Huber 1985, Friedman 1987). In the knowledge discovery and data mining community the term "interestingness" <ref> (Ripley 1996) </ref> is also used to denote unexpectedness (Silberschatz and Tuzhilin 1996). From this basis, algorithms can be derived to extract the desired independent components. In general, these algorithms can be considered as unsupervised learning procedures.
Reference: <author> Silberschatz, A. and Tuzhilin, A. </author> <year> (1996). </year> <title> What makes patterns interesting in knowledge discovery systems, </title> <journal> IEEE Transactions on Knowledge and Data Engineering 8(6): </journal> <pages> 970-974. </pages>
Reference-contexts: The goal of finding interesting is similar to projection pursuit (Friedman and Tukey 1974, Huber 1985, Friedman 1987). In the knowledge discovery and data mining community the term "interestingness" (Ripley 1996) is also used to denote unexpectedness <ref> (Silberschatz and Tuzhilin 1996) </ref>. From this basis, algorithms can be derived to extract the desired independent components. In general, these algorithms can be considered as unsupervised learning procedures. Recent reviews are given in Amari and Cichocki (1998), Cardoso (1998),Pope and Bogner (1996a) and Pope and Bogner (1996b).
Reference: <author> Tong, L., Liu, R., Soon, V. and Huang, Y. </author> <year> (1991). </year> <title> Indeterminacy and identifiability of blind identification, </title> <journal> IEEE Trans. Circuits, Syst. </journal> <volume> 38(5): </volume> <pages> 499-509. </pages>
Reference-contexts: If W = A 1 ; then y (t) = s (t); and perfect separation occurs. In general, it is only possible to find W such that WA = PD where P is a permutation matrix and D is a diagonal scaling matrix <ref> (Tong, Liu, Soon and Huang 1991) </ref>. To find such a matrix W, the following assumptions are made: * The sources fs j (t)g are statistically independent.
Reference: <author> Tong, L., Soon, V. C., Huang, Y. F. and Liu, R. </author> <year> (1990). </year> <title> AMUSE: A new blind identification algorithm, </title> <booktitle> International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pp. 1784-1787. </pages>
Reference: <author> Torkkola, K. </author> <year> (1996). </year> <title> Blind separation of convolved sources based on information maximization, </title> <editor> in S. Usui, Y. Tohkura, S. Katagiri and E. Wilson (eds), </editor> <booktitle> Proc. of the 1996 IEEE Workshop Neural Networks for Signal Processing 6 (NNSP96), </booktitle> <publisher> IEEE Press, </publisher> <address> New York, NY, </address> <pages> pp. 423-432. </pages>
Reference: <author> Utans, J., Holt, W. T. and Refenes, A. N. </author> <year> (1997). </year> <title> Principal component analysis for modeling multi-currency portfolios, </title> <editor> in A. S. Weigend, Y. S. Abu-Mostafa and A.-P. N. Refenes (eds), </editor> <booktitle> Decision Technologies for Financial Engineering (Proceedings of the Fourth International Conference on Neural Networks in the Capital Markets, NNCM-96), World Scientific, Singapore, </booktitle> <pages> pp. 359-368. </pages>
Reference-contexts: The figures indicate that the thresholded ICs provide useful morphological information and can extract the turning points of original time series. 4.4 Comparison with PCA PCA is a well established tool in finance. Applications range from Arbitrage Pricing Theory and factor models to input selection for multi-currency portfolios <ref> (Utans, Holt and Refenes 1997) </ref>. Here we seek to compare the performance of PCA with ICA. 10 previous figure. Note that the price for 1,000 points plotted is characterized well by only a few innovations. Singular value decomposition (SVD) is used to obtain the principal components as follows.
Reference: <author> Weinstein, E., Feder, M. and Oppenheim, A. </author> <year> (1993). </year> <title> Multi-channel signal separation by de-correlation, </title> <journal> IEEE Trans. Speech and Audio Processing 1(10): </journal> <pages> 405-413. </pages>
Reference: <author> Wu, L. and Moody, J. </author> <year> (1997). </year> <title> Multi-effect decompositions for financial data modelling, </title> <editor> in M. C. Mozer, M. I. Jordan and T. Petsche (eds), </editor> <booktitle> Advances in Neural Information Processing Systems 9 (NIPS*96), </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <pages> pp. 995-1001. </pages>
Reference: <author> Yang, H., Amari, S. and Cichocki, A. </author> <year> (1998). </year> <title> Information-theoretic approach to blind separation of sources in non-linear mixture, </title> <booktitle> Signal Processing 64(3): </booktitle> <pages> 291-300. </pages>
Reference: <author> Yang, H. H., Amari, S. and Cichocki, A. </author> <year> (1997). </year> <title> Information back-propagation for blind separation of sources in non-linear mixtures, </title> <booktitle> IEEE International Conference on Neural Networks, </booktitle> <address> Houston TX (ICNN'97), </address> <publisher> IEEE-Press, </publisher> <pages> pp. 2141-2146. </pages>
Reference: <author> Yang, H. H. and Amari, S. </author> <year> (1997). </year> <title> Adaptive on-line learning algorithms for blind separation: Maximum entropy and minimum mutual information, </title> <booktitle> Neural Computation 9: </booktitle> <pages> 1457-1482. </pages>
Reference: <author> Yellin, D. and Weinstein, E. </author> <year> (1996). </year> <title> Multichannel signal separation: Methods and analysis, </title> <journal> IEEE Transactions on Signal Processing 44: </journal> <pages> 106-118. 18 </pages>
References-found: 61

