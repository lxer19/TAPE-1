URL: ftp://ftp.eecs.umich.edu/people/durfee/gdn93-gd.ps.Z
Refering-URL: http://ai.eecs.umich.edu/diag/RMM.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: piotr@caen.engin.umich.edu, durfee@caen.engin.umich.edu  
Phone: (313) 763-1549  
Title: Toward a Theory of Honesty and Trust Among Communicating Autonomous Agents 0  
Author: Piotr J. Gmytrasiewicz and Edmund H. Durfee 
Keyword: Key Words: Honesty, Trust, Belief, Disbelief, Rational Communication, Multiagent Systems, Distributed Artificial Intelligence  
Address: Ann Arbor, Michigan 48109  
Affiliation: Department of Electrical Engineering and Computer Science University of Michigan  
Abstract: This paper outlines, through a number of examples, a method that can be used by autonomous agents to decide among potential messages to send to other agents, without having to assume that a message must be truthful and that it must be believed by the hearer. The main idea is that communicative behavior of autonomous agents is guided by the principle of economic rationality, whereby agents transmit messages to increase the effectiveness of interaction measured by their expected utilities. We are using a recursive, decision-theoretic formalism that allows agents to model each other and to infer the impact of a message on its recipient. The recursion can be continued into deeper levels and agents can model the recipient modeling the sender in an effort to assess the truthfulness of the received message. We show how our method often allows the agents to decide to communicate in spite of the possibility that the messages will not be believed. In certain situations, on the other hand, our method shows that the possibility of the hearer not believing what it hears makes communication useless. Our method thus provides the rudiments of a theory of how honesty and trust could emerge through rational, selfish behavior. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Robert Axelrod. </author> <title> The Evolution of Cooperation. </title> <publisher> Basic Books, </publisher> <year> 1984. </year>
Reference-contexts: Prisoner's Dilemma is an abstraction of many different real-life situations, ranging from trench warfare to evolutionary genetics <ref> [1] </ref>. Traditionally, game theorists do not analyze the PD game in the context of the players communicating because it is considered that communication in this game is either useless or dishonest. Our aim in this section is to show that these assumptions can be derived using our approach.
Reference: [2] <author> E. Ephrati and J. S. Rosenschein. </author> <title> The clarke tax as a consensus mechanism among automated agents. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 173-178, </pages> <address> San Jose, California, </address> <month> July </month> <year> 1991. </year> <month> 20 </month>
Reference-contexts: If lying is rational, so be it. What we find interesting is to understand why rational, selfish agents would ever choose to tell the truth, believe in what they hear, or even communicate at all, without the external imposition of a protocol [9] or incentives <ref> [2] </ref> that force them to be honest. Clearly, among people, honesty and trust can emerge among friends, while deception and disbelief often 1 predominate among adversaries. Our motivation is to understand how rational decisions among these options are made, and possibly their social implications (Section ??).
Reference: [3] <author> Piotr J. Gmytrasiewicz and Edmund H. Durfee. </author> <title> Truth, lies, belief and disbelief in communication between autonomous agnets. </title> <booktitle> In Proceedings of the Eleventh International Workshop on Distributed Artificlal Intelligence, </booktitle> <month> February </month> <year> 1992. </year>
Reference-contexts: C R1 R2 R1 send M2 23 13 not-send M2 13 13 C R1 is: B not-B R1 send M2 10 10 not-send M2 5 5 3.2 A Lie that does not Pay In this subsection, we summarize an example (see <ref> [3] </ref> for details) very similar to the one considered in the previous subsection, with a slightly different assignment of values to the goals and their costs, as depicted in Figure 9.
Reference: [4] <author> Piotr J. Gmytrasiewicz, Edmund H. Durfee, and David K. Wehe. </author> <title> A decision-theoretic approach to coordinating multiagent interactions. </title> <booktitle> In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 62-68, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Our approach is based on the Recursive Modeling Method (RMM) <ref> [4] </ref>, and on our analysis of how communication transforms the RMM hierarchy [5] (outlined also in the next section), which is intended to be a complete representation of an agent's knowledge relevant to the decision making process in a multiagent environment. <p> We get here a recursive pattern on the communicative level, and the resulting recursive hierarchy, which we call a communication hierarchy, can be solved by methods similar to those used in the case of recursive hierarchies containing physical actions, called action hierarchies and analyzed in <ref> [4, 5] </ref>. However, as it turns out, each of the recursive levels of communicative options of the agents requires solution of at least one action hierarchy, as opposed to the solution of a single payoff matrix in action hierarchies. <p> were the case that G1' were not behind the wall: R2 R1 G1 1 1 Finally, let us denote by P R2 G1 0 the payoff matrix describing R2 knowing about G1': R1 G1 1 3 1 S 2 2 0 With these matrices, R1 can build the recursive hierarchy <ref> [4] </ref> before communication takes place, shown in Figure 2. R1 knows that R2 does not see G1' and models it on the second level using the matrix P R2 . <p> The rest of the levels are constructed in a similar way. Thus, the goal G1' is not present at all on the lower levels of the hierarchy in Figure 2. Agent R1 can easily solve this hierarchy <ref> [4] </ref> by propagating the information in the hierarchy bottom-up (starting from any level be low the second in this case). Let us say that we use the hierarchy ending with the matrix 5 P R1 . <p> Something that should be stressed about this matrix is that it represents R2's options of believing M1 or not in rows, but the columns do not represent R1's options as in the cases considered in <ref> [4, 5] </ref>. The columns represent the state of Nature instead. This is because it is not in R1's power to make M1 true or false. <p> the following matrix C R1 : B not-B R1 send M1 1 1 not-send M1 1 1 From the communicative matrices C, a recursive communication hierarchy, depicted in here This hierarchy fully illustrates the recursive nesting of beliefs of agent R1 and is similar to the action hierarchies considered in <ref> [4, 5] </ref>. The important difference lies in its analysis and method of solution. Let us note that, as mentioned before, the rows of matrices C R1 G1 0 and C R1 on the third level cannot be directly translated to the columns of the matrix C R2 . <p> The fourth level reverses this conclusion again, and a pattern becomes apparent according to which the analysis of the recursive communicative hierarchy flip-flops from level to level. As we mentioned before, our suggestion of a way out of this impasse coincides with one described in <ref> [4] </ref> for the case of action hierarchies. The recursive analysis simply does not provide a conclusive answer in this case, beyond telling R1 that R2 will either believe M3 or not. <p> We think that the insights gained in this paper can be used to predict that some agents will turn out to converse a great deal, while not communicating with the others. The fact that communication is likely to be repetitive brings about questions, already explored in <ref> [4] </ref> for decision making about physical actions, about how repetition will influence the whole idea of lies and belief. We think that it is fairly safe to qualitatively extend the results obtained in [4] to these issues. <p> The fact that communication is likely to be repetitive brings about questions, already explored in <ref> [4] </ref> for decision making about physical actions, about how repetition will influence the whole idea of lies and belief. We think that it is fairly safe to qualitatively extend the results obtained in [4] to these issues. Thus, just as repetitive interactions facilitates cooperation of the agents in their choice of physical action, so too will it promote cooperative traits in communication. <p> Of course, acting as if it were true based on the expected utility of doing so can be rational, as we will see. 3 As we remark in <ref> [4] </ref>, the inability of the analysis to converge on a unique solution simply means that the information contained in the hierarchy is inconclusive. While a conclusive answer is obviously desirable, we must avoid the temptation to have the system imply that it can conclude more than it should.
Reference: [5] <author> Piotr J. Gmytrasiewicz, Edmund H. Durfee, and David K. Wehe. </author> <title> The utility of communication in coordinating intelligent agents. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 166-172, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: The challenge in designing such systems, though, is to develop the theories and methods needed by the autonomous systems to rationally use their capabilities. In this paper we extend the method presented in <ref> [5] </ref> to allow an agent to rationally choose what to communicate to another agent, without the assumption that the transmitted information has to be truthful, and is guaranteed to be believed. <p> Our approach is based on the Recursive Modeling Method (RMM) [4], and on our analysis of how communication transforms the RMM hierarchy <ref> [5] </ref> (outlined also in the next section), which is intended to be a complete representation of an agent's knowledge relevant to the decision making process in a multiagent environment. <p> We get here a recursive pattern on the communicative level, and the resulting recursive hierarchy, which we call a communication hierarchy, can be solved by methods similar to those used in the case of recursive hierarchies containing physical actions, called action hierarchies and analyzed in <ref> [4, 5] </ref>. However, as it turns out, each of the recursive levels of communicative options of the agents requires solution of at least one action hierarchy, as opposed to the solution of a single payoff matrix in action hierarchies. <p> Something that should be stressed about this matrix is that it represents R2's options of believing M1 or not in rows, but the columns do not represent R1's options as in the cases considered in <ref> [4, 5] </ref>. The columns represent the state of Nature instead. This is because it is not in R1's power to make M1 true or false. <p> the following matrix C R1 : B not-B R1 send M1 1 1 not-send M1 1 1 From the communicative matrices C, a recursive communication hierarchy, depicted in here This hierarchy fully illustrates the recursive nesting of beliefs of agent R1 and is similar to the action hierarchies considered in <ref> [4, 5] </ref>. The important difference lies in its analysis and method of solution. Let us note that, as mentioned before, the rows of matrices C R1 G1 0 and C R1 on the third level cannot be directly translated to the columns of the matrix C R2 .
Reference: [6] <author> Roger B. Mayerson. </author> <title> Incentive constraints and optimal communication systems. </title> <booktitle> In Proceedings of the Second Conference on Theoretical Aspects of Reasoning about Knowledge, </booktitle> <pages> pages 179-193, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: Unlike our work, however, they do not consider the reciprocal issue of whether a message recipient should believe what it hears. The complications created by the possibility of dishonest communication have also been analyzed in the economics literature <ref> [6] </ref>. There, issues of designing a communication system that optimizes the information exchanged by rational agents are discussed.
Reference: [7] <author> Gilad Zlotkin and Jeffrey S. Rosenschein. </author> <title> Negotiation and task sharing among autonomous agents in cooperative domains. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 912-917, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: Our study also has practical importance for designing autonomous agents that can function in adversarial situations, such as competitive markets and battlefields. The issue of lies in communication has recently been addressed in the Distributed AI literature by Zlotkin and Rosenschein <ref> [8, 7, 9] </ref>. In their work, Zlotkin and Rosenschein analyze the use of lies in negotiation and conflict resolution in various domains. They study how pre-established negotiation protocols, for particular domains, can ensure that agents will not lie. We, on the other hand, do not assume a pre-arranged protocol.
Reference: [8] <author> Gilad Zlotkin and Jeffrey S. Rosenschein. </author> <title> Blocks, lies, and postal freight: Nature of deception in negotiation. </title> <booktitle> In Proceedings of the 1990 Distributed AI Workshop, </booktitle> <month> October </month> <year> 1990. </year>
Reference-contexts: Our study also has practical importance for designing autonomous agents that can function in adversarial situations, such as competitive markets and battlefields. The issue of lies in communication has recently been addressed in the Distributed AI literature by Zlotkin and Rosenschein <ref> [8, 7, 9] </ref>. In their work, Zlotkin and Rosenschein analyze the use of lies in negotiation and conflict resolution in various domains. They study how pre-established negotiation protocols, for particular domains, can ensure that agents will not lie. We, on the other hand, do not assume a pre-arranged protocol. <p> sense for R1 to lie. 3 Lying in Modeling Messages In this section, we build on the intuitions developed in the previous section to analyze two examples of lies in modeling messages. 3.1 A Lie that Pays Let us consider a scenario similar to the "phantom letters" scenario considered in <ref> [8] </ref>, and depicted in Figure 6. Let us say that R1 contemplates sending a message, M2, to R2 stating "There is a G2, worth 15, at Location1". Let us define the following payoff matrices.

References-found: 8

