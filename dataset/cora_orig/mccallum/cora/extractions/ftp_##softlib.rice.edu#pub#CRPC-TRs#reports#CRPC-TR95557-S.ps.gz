URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR95557-S.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: Compiler Blockability of Dense Matrix Factorizations  
Author: Steve Carr R. B. Lehoucq 
Date: August 9, 1995  
Abstract: Recent architectural advances have made memory accesses a significant bottleneck for computational problems. Even though cache memory helps in some cases, it fails to alleviate the bottleneck for problems with large working sets. As a result, scientists are forced to restructure their codes by hand to reduce the working-set size to fit a particular machine. Unfortunately, these hand optimizations create machine-specific code that is not portable across multiple architectures without a significant loss in performance or a significant effort to re-optimize the code. It is the thesis of this paper that most of the hand optimizations performed on matrix factorization codes are unnecessary because they can and should be performed by the compiler. It is better for the programmer to express algorithms in a machine-independent form and allow the compiler to handle the machine-dependent details. This gives the algorithms portability across architectures and removes the error-prone, expensive and tedious process of hand optimization. In this paper, we show that Cholesky and LU factorizations may be optimized automatically by the compiler to be as efficient as the same hand-optimized version found in LAPACK. We also show that the QR factorization may be optimized by the compiler to perform comparably with the hand-optimized LAPACK version on matrix sizes that are typically run on nodes of massively parallel systems. Our approach allows us to conclude that matrix factorizations can be expressed in a machine-independent form with the expectation of good memory performance across a variety of architectures.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.R. Allen and K. Kennedy. </author> <title> Automatic translation of Fortran programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: A dependence is carried by a loop if the references at the source and sink (beginning and end) of the dependence are on different iterations of the loop and the dependence is not carried by an outer loop <ref> [1] </ref>. In the loop below, there is a true dependence from A (I,J) to A (I-1,J) carried by the I-loop, a true dependence from A (I,J) to A (I,J-1) carried by the J-loop and an input dependence from A (I,J-1) to A (I-1,J) carried by the I-loop. <p> The techniques described in Section 3 for handling triangular and trapezoidal loops are necessary to optimize the above matrix-matrix multiply For vector architectures, a different loop optimization strategy may be more beneficial <ref> [1] </ref>. 10 4.1.2 Adding Partial Pivoting Although the compiler can discover the potential for blocking in LU decomposition without pivoting using index-set splitting, the same cannot be said when partial pivoting is added (see Figure 5 for LU decomposition with partial pivoting). <p> These matrix sizes are typical on workstations and nodes of massively parallel systems. We remark that this strategy does not necessarily translate to good performance on vector processors. Although compiler technology exists to generate efficient code on vector processors <ref> [1] </ref>, we did not use this technology. Given that future machine designs are certain to have increasingly complex memory hierarchies, compilers will need to adopt increasingly sophisticated memory-management strategies so that programmers can remain free to concentrate on program logic.
Reference: [2] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongara, J. Du Croz, A. Geenbaum, S. Ham-marling, A. McKenney, S. Ostrouchov, and D. Sorensen. </author> <title> LAPACK User's Guide. </title> <publisher> SIAM, </publisher> <address> Philadelphia, Pennsylvania, </address> <year> 1992. </year>
Reference-contexts: The second result, which we discuss in this paper, reveals an algorithmic approach that can be used to analyze and block matrix factorization algorithms automatically in the compiler. Our results with this algorithmic approach show that the block algorithms derived by the compiler are competitive with those of LAPACK <ref> [2] </ref>. For modest sized matrices (on the order of 200 or less), the compiler-derived variants are often superior. We feel this is important since the architectural trend is toward the amalgamation of many smaller processors. We begin our presentation with a review of background material related to memory optimization. <p> Because of the memory hierarchy and shared-memory multiprocessors, the LAPACK and the level 2 and 3 BLAS and BLAS projects became necessary. The projects restructured basic linear algebra algorithms in order to provide portable software that runs efficiently in the presence of a memory hierarchy <ref> [14, 2, 13] </ref>. 2.2 Dependence The fundamental tool available to the compiler is that of dependence|the same tool used in vec-torization and parallelization. <p> The single most important factor governing the efficiency of a software implementation in computing a factorization is : Managing the memory hierarchy. 8 Part of the motivation of the LAPACK <ref> [2] </ref> project was to recast the matrix factorization algo-rithms in LINPACK [11] with block ones. A block form of a factorization restructures the algorithm in terms of matrix operations that minimize the amount of data moved within the memory hierarchy while keeping the arithmetic units of the machine occupied.
Reference: [3] <author> D. Callahan, S. Carr, and K. Kennedy. </author> <title> Improving register allocation for subscripted variables. </title> <booktitle> In Proceedings of the SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <address> White Plains, NY, </address> <month> June </month> <year> 1990. </year> <month> 20 </month>
Reference-contexts: In addition, the temporal reuse of JS values of B out of cache occurs for every iteration of the J-loop if JS is less than the size of the cache and there is no interference [23]. A transformation analogous to strip-mine-and-interchange is unroll-and-jam <ref> [3] </ref>. Unroll-and-jam is used for register blocking instead of cache blocking and can be seen as an application of strip mining, loop interchange and loop unrolling. Essentially, the inner loop is completely unrolled after strip-mine-and-interchange to effect unroll-and-jam.
Reference: [4] <author> D. Callahan, K. Cooper, R. Hood, K. Kennedy, and L. Torczon. </author> <title> ParaScope: A parallel programming environment. </title> <booktitle> In Proceedings of the First International Conference on Supercomputing, </booktitle> <address> Athens, Greece, </address> <month> June </month> <year> 1987. </year>
Reference-contexts: Our compiler optimized versions were obtained by hand using the algorithms in the literature. The reason that this process could not be fully automated is because of a current deficiency in the dependence analyzer of our tool <ref> [4, 6] </ref>. All software was compiled with full optimization on all the machines. On the RS/6000, version 2.2 of the xlf compiler was used. On the HP, version 9.16 of the f77 compiler was used, and on the SGI, version 5.3 of the f77 compiler was used.
Reference: [5] <author> D. Callahan and K. Kennedy. </author> <title> Analysis of interprocedural side effects in a parallel programming environment. </title> <booktitle> In Proceedings of the First International Conference on Supercomputing. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Athens, Greece, </address> <year> 1987. </year>
Reference-contexts: DO 10 I = 1,N A (I,J) = A (I-1,J) + A (I,J-1)10 To enhance the dependence information, section analysis can be used to describe the portion of an array that is accessed by a particular reference or set of references <ref> [5, 19] </ref>. Sections describe common substructures of arrays such as elements, rows, columns and diagonals. As an example of section analysis consider the following loop. DO 10 I = 1,N 10 A (J,I) = ...
Reference: [6] <author> S. Carr. </author> <title> Memory-Hierarchy Management. </title> <type> PhD thesis, </type> <institution> Rice University, Department of Computer Science, </institution> <month> September </month> <year> 1992. </year>
Reference-contexts: DO 10 I = 1,N,IS DO 10 II = I,MIN ((J-fi)/ff,I+IS-1) 10 loop body This formula can be trivially extended to handle the cases where ff &lt; 0 and where a linear function of I appears in the upper bound instead of the lower bound <ref> [6] </ref>. Triangular strip-mine-and-interchange can be extended to triangular unroll-and-jam as follows. Since the iteration space defined by the two inner loops is a trapezoidal region, the number of iterations of the innermost loop vary with J, making unrolling more difficult. <p> Additionally, triangular unroll-and-jam can be extended to handle other common triangles <ref> [6] </ref>. 3.3 Trapezoidal Iteration Spaces While the previous method applies to many of the common non-rectangular-shaped iteration spaces, there are still some important loops that it will not handle. In linear algebra, seismic and partial differential equation codes, loops with trapezoidal-shaped iteration spaces occur. <p> Our compiler optimized versions were obtained by hand using the algorithms in the literature. The reason that this process could not be fully automated is because of a current deficiency in the dependence analyzer of our tool <ref> [4, 6] </ref>. All software was compiled with full optimization on all the machines. On the RS/6000, version 2.2 of the xlf compiler was used. On the HP, version 9.16 of the f77 compiler was used, and on the SGI, version 5.3 of the f77 compiler was used.
Reference: [7] <author> S. Carr and K. Kennedy. </author> <title> Blocking linear algebra codes for memory hierarchies. </title> <booktitle> In Proceedings of the Fourth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> Chicago, IL, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: To investigate the viability of memory-hierarchy management by the compiler, experiments were undertaken to determine if a compiler could automatically generate the block algorithms in LAPACK from the corresponding point algorithms expressed in Fortran 77 <ref> [7, 8, 14, 25] </ref>. This study addressed the following question: what information does a compiler need in order to derive block versions of matrix factorization codes that are competitive with the best hand-blocked versions? This study has yielded two major results.
Reference: [8] <author> S. Carr and K. Kennedy. </author> <title> Compiler blockability of numerical algorithms. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <pages> pages 114-124, </pages> <address> Minneapolis, MN, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: To investigate the viability of memory-hierarchy management by the compiler, experiments were undertaken to determine if a compiler could automatically generate the block algorithms in LAPACK from the corresponding point algorithms expressed in Fortran 77 <ref> [7, 8, 14, 25] </ref>. This study addressed the following question: what information does a compiler need in order to derive block versions of matrix factorization codes that are competitive with the best hand-blocked versions? This study has yielded two major results. <p> The derived algorithm depends upon the compiler for efficiency in contrast to the LAPACK algorithm that depends on hand optimization of the BLAS. Cd-QR can be obtained from the point algorithm for QR decomposition using array section analysis <ref> [8] </ref>. For reference, segments of the code for the point algorithm after strip mining of the outer loop are shown in Figure 6.
Reference: [9] <author> Steve Carr and Ken Kennedy. </author> <title> Improving the ratio of memory operations to floating-point operations in loops. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 16(6) </volume> <pages> 1768-1810, </pages> <year> 1994. </year>
Reference-contexts: This study addressed the following question: what information does a compiler need in order to derive block versions of matrix factorization codes that are competitive with the best hand-blocked versions? This study has yielded two major results. The first, which is detailed in another paper <ref> [9] </ref>, reveals that the hand loop unrolling performed when optimizing the level 3 BLAS [12] subroutines is rarely necessary. While the BLAS are useful, the hand optimization that is required to obtain good performance on a particular architecture should be left to the compiler. <p> For superscalar architectures whose performance is bound by cache, loop interchange can be used to put the KK-loop in the innermost position and unroll-and-jam can be applied to the J- and I-loops to further improve performance <ref> [26, 9] </ref>.
Reference: [10] <author> Stephanie Coleman and Kathryn S. McKinley. </author> <title> Tile size selection using cache organization. </title> <journal> SIGPLAN Notices, </journal> <volume> 30(6) </volume> <pages> 279-280, </pages> <month> June </month> <year> 1995. </year> <booktitle> Proceedings of the ACM SIGPLAN '95 Conference on Programming Language Design and Implementation. </booktitle>
Reference-contexts: The performance using the best block factor of this set for each matrix size is displayed in the figures. Although the compiler can effectively choose blocking factors automatically, we did not use the available algorithms <ref> [22, 10] </ref>. 5.1 LU Factorization Figures 8 and 9 show the performance of the compiler-derived version of LU factorization versus the LAPACK version. The results for the RS/6000 show that our compiler-derived version outperforms the hand-optimized LAPACK code for matrices of size less than 150x150.
Reference: [11] <author> J.J. Dongarra, J.R. Bunch, C.B. Moler, and G.W. Stewart. </author> <title> LINPACK Users' Guide. </title> <publisher> SIAM, </publisher> <address> Philadelphia, Pennsylvania, </address> <year> 1979. </year>
Reference-contexts: The single most important factor governing the efficiency of a software implementation in computing a factorization is : Managing the memory hierarchy. 8 Part of the motivation of the LAPACK [2] project was to recast the matrix factorization algo-rithms in LINPACK <ref> [11] </ref> with block ones. A block form of a factorization restructures the algorithm in terms of matrix operations that minimize the amount of data moved within the memory hierarchy while keeping the arithmetic units of the machine occupied.
Reference: [12] <author> J.J. Dongarra, J. DuCroz, I. Duff, and S. Hammerling. </author> <title> A set of level 3 basic linear algebra subprograms. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 16 </volume> <pages> 1-17, </pages> <year> 1990. </year>
Reference-contexts: The first, which is detailed in another paper [9], reveals that the hand loop unrolling performed when optimizing the level 3 BLAS <ref> [12] </ref> subroutines is rarely necessary. While the BLAS are useful, the hand optimization that is required to obtain good performance on a particular architecture should be left to the compiler. Experiments show that the compiler can automatically unroll loops as effectively as hand optimization in most cases. <p> LAPACK blocks matrix factorizations by restructuring the algorithms to use the level 2 and 3 BLAS <ref> [13, 12] </ref>. The motivation for the BLAS [24](Basic Linear Algebra Subprograms) was to provide a set of commonly used vector operations such as vector addition and dot product so that the programmer could invoke the subprograms instead of writing the code directly.
Reference: [13] <author> J.J. Dongarra, J. DuCroz, S. Hammerling, and R. Hanson. </author> <title> An extendend set of fortran basic linear algebra subprograms. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 14 </volume> <pages> 1-17, </pages> <year> 1988. </year>
Reference-contexts: Because of the memory hierarchy and shared-memory multiprocessors, the LAPACK and the level 2 and 3 BLAS and BLAS projects became necessary. The projects restructured basic linear algebra algorithms in order to provide portable software that runs efficiently in the presence of a memory hierarchy <ref> [14, 2, 13] </ref>. 2.2 Dependence The fundamental tool available to the compiler is that of dependence|the same tool used in vec-torization and parallelization. <p> LAPACK blocks matrix factorizations by restructuring the algorithms to use the level 2 and 3 BLAS <ref> [13, 12] </ref>. The motivation for the BLAS [24](Basic Linear Algebra Subprograms) was to provide a set of commonly used vector operations such as vector addition and dot product so that the programmer could invoke the subprograms instead of writing the code directly.
Reference: [14] <author> J.J. Dongarra, I.S. Duff, D.C. Sorensen, and H.A. Van der Vorst. </author> <title> Solving Linear Systems on Vector and Shared-Memory Computers. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1991. </year>
Reference-contexts: To investigate the viability of memory-hierarchy management by the compiler, experiments were undertaken to determine if a compiler could automatically generate the block algorithms in LAPACK from the corresponding point algorithms expressed in Fortran 77 <ref> [7, 8, 14, 25] </ref>. This study addressed the following question: what information does a compiler need in order to derive block versions of matrix factorization codes that are competitive with the best hand-blocked versions? This study has yielded two major results. <p> Because of the memory hierarchy and shared-memory multiprocessors, the LAPACK and the level 2 and 3 BLAS and BLAS projects became necessary. The projects restructured basic linear algebra algorithms in order to provide portable software that runs efficiently in the presence of a memory hierarchy <ref> [14, 2, 13] </ref>. 2.2 Dependence The fundamental tool available to the compiler is that of dependence|the same tool used in vec-torization and parallelization. <p> The higher level BLAS better utilize the underlying memory hierarchy. The reader is referred to the work of Dongarra, Duff, Sorensen and Van der Vorst <ref> [14] </ref> for further information regarding the BLAS and their use in deriving block matrix factorizations. As with the level 1 BLAS, responsibility for optimizing the higher level BLAS was left to others. Unfortunately, the cost of developing optimized versions in programming effort is significant. <p> handle pivoting. 4.1.1 No Pivoting Since the point algorithm for LU factorization exhibits poor cache performance on large matrices, scientists have developed a block algorithm that essentially groups a number of updates to the matrix A together and applies them all at once to a block portion of the array <ref> [14] </ref>. Consider the strip-mined version of the point algorithm shown below. <p> The rules for the preservation of data dependence prohibit the reversing of a dependence direction. This would seem to preclude the existence of a block analogue similar to the non-pivoting case. However, a block algorithm that ignores the preventing recurrence and distributes the KK-loop can still be mathematically derived <ref> [14] </ref>. Consider the following. <p> P i only depends upon the first i columns of A, allowing the computation of k P i 's and ^ M i 's, where k is the blocking factor, and then the block application of the ^ M i 's <ref> [14] </ref>. To install the above result into the compiler, we examine its implications from a data dependence viewpoint. In the point version, each row interchange is followed by a whole-column update in which each row element is updated independently. <p> For a more detailed discussion of the QR factorization see Golub and Van Loan [18]. The LAPACK block QR factorization is an attempt to recast the algorithm in terms of calls to level 3 BLAS <ref> [14] </ref>. If the level 3 BLAS are hand-tuned for a particular architecture, the block QR algorithm may perform significantly better than the point version on large matrix sizes (those that cause the working set to be much larger than the cache size). <p> Unfortunately, the block QR algorithm in LAPACK is not automatically derivable by a compiler. The block application of a number of elementary reflectors involves both computation and storage that does not exist in the original point algorithm <ref> [14] </ref>.
Reference: [15] <author> J.J. Dongarra, F.G. Gustavson, and A. Karp. </author> <title> Implementing linear algebra algorithms for dense matrices on a vector pipeline machine. </title> <journal> SIAM Review, </journal> <volume> 26(1) </volume> <pages> 91-112, </pages> <month> January </month> <year> 1984. </year>
Reference-contexts: With the advent of vector supercomputers, the efficiency of the factorizations were seen to depend dramatically upon the algorithmic form chosen for the implementation. Dongarra, Gustavson and Karp <ref> [15] </ref> gave a detailed study the algorithmic issues involved in constructing an efficient LU factorization on the early CRAY supercomputers. The work of Ortega [27], and Gallivan, Plemmons and Sameh [17] considered both algorithmic and computational issues involved in the efficient implementation of matrix factorizations on vector and parallel computers.
Reference: [16] <author> J.J. Dongarra, P. Mayes, and G. Radicati. </author> <title> The IBM RISC system/6000 and linear algebra operations. </title> <type> Technical Report CS-90-122, </type> <institution> Department of Computer Science, University of Tennessee, </institution> <year> 1990. </year> <note> LAPACK Working Note 28. </note>
Reference-contexts: These architectures were chosen because they are representative of the typical high-performance workstation and because the chips are used as building blocks for massively parallel architectures. On the RS/6000, we used the hand optimized level 3 BLAS subroutine dgemm <ref> [16] </ref> obtained from Netlib. For the HP and SGI we used the optimized BLAS distributed with the machine. Our compiler optimized versions were obtained by hand using the algorithms in the literature.
Reference: [17] <author> K.A. Gallivan, R.J. Plemmons, and A.H. Sameh. </author> <title> Parllel algorithms for dense linear algebra computations. </title> <journal> SIAM Review, </journal> <volume> 32 </volume> <pages> 54-135, </pages> <year> 1990. </year>
Reference-contexts: Dongarra, Gustavson and Karp [15] gave a detailed study the algorithmic issues involved in constructing an efficient LU factorization on the early CRAY supercomputers. The work of Ortega [27], and Gallivan, Plemmons and Sameh <ref> [17] </ref> considered both algorithmic and computational issues involved in the efficient implementation of matrix factorizations on vector and parallel computers.
Reference: [18] <author> G.H. Golub and C.F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins University Press, </publisher> <address> Baltimore, </address> <year> 1989. </year>
Reference-contexts: Efficiency of the implementation of the level 2 BLAS subroutines determines the rate at which the factorization is computed. For a more detailed discussion of the QR factorization see Golub and Van Loan <ref> [18] </ref>. The LAPACK block QR factorization is an attempt to recast the algorithm in terms of calls to level 3 BLAS [14].
Reference: [19] <author> P. Havlak and K. Kennedy. </author> <title> An implementation of interprocedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-360, </pages> <month> July </month> <year> 1991. </year> <month> 21 </month>
Reference-contexts: DO 10 I = 1,N A (I,J) = A (I-1,J) + A (I,J-1)10 To enhance the dependence information, section analysis can be used to describe the portion of an array that is accessed by a particular reference or set of references <ref> [5, 19] </ref>. Sections describe common substructures of arrays such as elements, rows, columns and diagonals. As an example of section analysis consider the following loop. DO 10 I = 1,N 10 A (J,I) = ... <p> The effectiveness of index-set splitting depends upon the representation of sections. The precision must be enough to relate the locations in the array to index variable values. The representation that we have chosen is equivalent to Fortran 90 array notation <ref> [19] </ref>. 4 Automatic Blocking of Dense Matrix Factorizations The three factorizations considered in this paper, the LU, Cholesky, and QR, are among the most frequently used by numerical linear algebra and its applications.
Reference: [20] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kauffman, </publisher> <address> San Mateo, California, </address> <year> 1990. </year>
Reference-contexts: Unfortunately, memory speeds have not kept pace. Therefore, with these gains in computational power has come an increase in the number of cycles for a memory access|a latency of 10 to 20 machine cycles is now quite common <ref> [20, 30] </ref>. Because the latency and bandwidth of memory systems have not kept pace with processor speed, computations are often delayed waiting for data from memory. As a result, processors see idle computational cycles more frequently.
Reference: [21] <author> D. Kuck. </author> <title> The Structure of Computers and Computations Volume 1. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: A dependence exists between two statements if there exists a control flow path from the first statement to the second, and both statements reference the same memory location <ref> [21] </ref>. * If the first statement writes to the location and the second reads from it, there is a true dependence, also called a flow dependence. * If the first statement reads from the location and the second writes to it, there is an antide pendence. * If both statements write
Reference: [22] <author> Monica S. Lam, Edward E. Rothberg, and Michael E. Wolf. </author> <title> The cache performance and optimizations of blocked algorithms. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 63-74, </pages> <address> Santa Clara, California, </address> <year> 1991. </year>
Reference-contexts: The performance using the best block factor of this set for each matrix size is displayed in the figures. Although the compiler can effectively choose blocking factors automatically, we did not use the available algorithms <ref> [22, 10] </ref>. 5.1 LU Factorization Figures 8 and 9 show the performance of the compiler-derived version of LU factorization versus the LAPACK version. The results for the RS/6000 show that our compiler-derived version outperforms the hand-optimized LAPACK code for matrices of size less than 150x150.
Reference: [23] <author> M.S. Lam, E.E. Rothberg, and M.E. Wolf. </author> <title> The cache performance and optimizations of blocked algorithms. </title> <booktitle> In Proceedings of the Fourth International Conference on Architecural Support for Programming Languages and Operating Systems, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: In addition, the temporal reuse of JS values of B out of cache occurs for every iteration of the J-loop if JS is less than the size of the cache and there is no interference <ref> [23] </ref>. A transformation analogous to strip-mine-and-interchange is unroll-and-jam [3]. Unroll-and-jam is used for register blocking instead of cache blocking and can be seen as an application of strip mining, loop interchange and loop unrolling. Essentially, the inner loop is completely unrolled after strip-mine-and-interchange to effect unroll-and-jam.
Reference: [24] <author> C. Lawson, R. Hanson, D. Kincaid, and F. Krogh. </author> <title> Basic linear algebra subprograms for fortran usage. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 5 </volume> <pages> 308-329, </pages> <year> 1979. </year>
Reference: [25] <author> Richard Lehoucq. </author> <title> Implementing efficient and portable dense matrix factorizations. </title> <booktitle> In Proceedings of the Fifth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <year> 1992. </year>
Reference-contexts: To investigate the viability of memory-hierarchy management by the compiler, experiments were undertaken to determine if a compiler could automatically generate the block algorithms in LAPACK from the corresponding point algorithms expressed in Fortran 77 <ref> [7, 8, 14, 25] </ref>. This study addressed the following question: what information does a compiler need in order to derive block versions of matrix factorization codes that are competitive with the best hand-blocked versions? This study has yielded two major results.
Reference: [26] <author> Kathryn McKinley, Steve Carr, and Chau-Wen Tseng. </author> <title> Compiler optimizations for improving data locality. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Santa Clara, California, </address> <year> 1994. </year>
Reference-contexts: For superscalar architectures whose performance is bound by cache, loop interchange can be used to put the KK-loop in the innermost position and unroll-and-jam can be applied to the J- and I-loops to further improve performance <ref> [26, 9] </ref>.
Reference: [27] <author> James M. Ortega. </author> <title> Introduction to Parallel and Vector Solutions of Linear Systems. </title> <publisher> Plenum Press, </publisher> <address> New York, New York, </address> <year> 1988. </year>
Reference-contexts: Dongarra, Gustavson and Karp [15] gave a detailed study the algorithmic issues involved in constructing an efficient LU factorization on the early CRAY supercomputers. The work of Ortega <ref> [27] </ref>, and Gallivan, Plemmons and Sameh [17] considered both algorithmic and computational issues involved in the efficient implementation of matrix factorizations on vector and parallel computers.
Reference: [28] <author> A.K. Porterfield. </author> <title> Software Methods for Improvement of Cache Performance on Supercomputer Applications. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: Strip-mine-and-interchange is a transformation that achieves this result <ref> [34, 28, 31] </ref>. It shortens the distance between the source and sink of a dependence so that it is more likely for the datum to reside in cache when the reuse occurs. Consider the following loop nest.
Reference: [29] <author> G.W. Stewart. </author> <title> Introduction to Matrix Computations. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: The block algorithm variant performs well on matrix sizes typically run on workstations and nodes of massively parallel systems. 4.1 LU Factorization The LU decomposition factors a non-singular matrix A into the product of two matrices, L and U , such that A = LU <ref> [29] </ref>. L is a unit lower triangular matrix and U is an upper triangular matrix.
Reference: [30] <author> Harold S. Stone. </author> <title> High-Performance Computer Architecture. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mas-sachusetts, 2nd edition, </address> <year> 1990. </year>
Reference-contexts: Unfortunately, memory speeds have not kept pace. Therefore, with these gains in computational power has come an increase in the number of cycles for a memory access|a latency of 10 to 20 machine cycles is now quite common <ref> [20, 30] </ref>. Because the latency and bandwidth of memory systems have not kept pace with processor speed, computations are often delayed waiting for data from memory. As a result, processors see idle computational cycles more frequently.
Reference: [31] <author> M.E. Wolf and M.S. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Proceedings of the SIG-PLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1991. </year>
Reference-contexts: Strip-mine-and-interchange is a transformation that achieves this result <ref> [34, 28, 31] </ref>. It shortens the distance between the source and sink of a dependence so that it is more likely for the datum to reside in cache when the reuse occurs. Consider the following loop nest.
Reference: [32] <author> M. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <type> PhD thesis, </type> <institution> University of Illinois, </institution> <month> October </month> <year> 1982. </year>
Reference-contexts: However, there is a recurrence between the definition of A (K) and the use of A (II) carried by the II-loop, preventing interchange with distribution. Standard dependence abstractions, such as distance or direction vectors, report that the recurrence exists for every value defined by A (K) <ref> [32] </ref>. This means blocking is prevented. However, analyzing the sections of the arrays that are accessed at the source and sink of the backward true dependence reveals that there is potential to apply blocking. Consider Figure 3.
Reference: [33] <author> M. Wolfe. </author> <title> Advanced loop interchange. </title> <booktitle> In Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1986. </year>
Reference-contexts: Interchanging loops that iterate over a triangular regions requires the modification of the loop bounds to preserve the semantics of the loop <ref> [33, 34] </ref>. Therefore, blocking triangular 5 - J M 1 J=ffII+fi regions also requires loop bound modification. Below, we derive the formula for determining loop bounds when blocking is performed on triangular iteration spaces. We begin with the derivation for strip-mine-and-interchange and then extend it to unroll-and-jam.
Reference: [34] <author> M. Wolfe. </author> <title> Iteration space tiling for memory hierarchies. </title> <booktitle> In Proceedings of the Third SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <month> December </month> <year> 1987. </year> <month> 22 </month>
Reference-contexts: Strip-mine-and-interchange is a transformation that achieves this result <ref> [34, 28, 31] </ref>. It shortens the distance between the source and sink of a dependence so that it is more likely for the datum to reside in cache when the reuse occurs. Consider the following loop nest. <p> Interchanging loops that iterate over a triangular regions requires the modification of the loop bounds to preserve the semantics of the loop <ref> [33, 34] </ref>. Therefore, blocking triangular 5 - J M 1 J=ffII+fi regions also requires loop bound modification. Below, we derive the formula for determining loop bounds when blocking is performed on triangular iteration spaces. We begin with the derivation for strip-mine-and-interchange and then extend it to unroll-and-jam.
References-found: 34

