URL: http://www.research.att.com/~lewis/papers/lewis96d.ps
Refering-URL: http://www.research.att.com/~lewis/chronobib.html
Root-URL: 
Email: flewis, schapireg@research.att.com  fcallan, papkag@cs.umass.edu  
Title: Training Algorithms for Linear Text Classifiers  
Author: David D. Lewis Robert E. Schapire James P. Callan Ron Papka 
Address: Murray Hill, NJ 07974 USA  Amherst, MA 01003 USA  
Affiliation: AT&T Laboratories  Center for Intelligent Information Retrieval Department of Computer Science University of Massachusetts  
Note: Appeared (w/ same pagination) in H. P. Frei, et al., eds., SIGIR 96: Proceedings of the 19th Annual International ACM-SIGIR Conference on  
Abstract: Systems for text retrieval, routing, categorization and other IR tasks rely heavily on linear classifiers. We propose that two machine learning algorithms, the Widrow-Hoff and EG algorithms, be used in training linear text classifiers. In contrast to most IR methods, theoretical analysis provides performance guarantees and guidance on parameter settings for these algorithms. Experimental data is presented showing Widrow-Hoff and EG to be more effective than the widely used Rocchio algorithm on several categorization and routing tasks. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Allan, J., Ballesteros, L., Callan, J. P., Croft, W. B., & Lu., Z. </author> <year> (1996). </year> <title> Recent experiments with INQUERY. </title> <booktitle> In Proceedings of TREC-4. </booktitle>
Reference-contexts: features used were the content words occurring in the textual description of the topic (on average 7:92 words/topic for topics 51-100 and 8:76 words/topic for topics 101-150), and either 50 or 1000 additional words chosen by a query expansion process similar to that used in the U Mass TREC-4 experiments <ref> (Allan et al., 1996) </ref>. Text Segment. All textual material was used. Starting Vector. Rocchio was used with a starting vector of (0; : : : ; 0). WH and EG were started with the output of the Rocchio algorithm.
Reference: <author> Blum, A. </author> <year> (1995). </year> <title> Emprical support for Winnow and Weighted-Majority based algorithms: results on a calendar scheduling domain. </title> <booktitle> In Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <pages> pp. 124-132. </pages>
Reference-contexts: In sum, Kivinen and Warmuth's results suggest that EG is likely to work well on high dimensional problems. Their results also give insight into how to deal with different document representations. Blum's recent success <ref> (Blum, 1995) </ref> with a related multiplicative update algorithm on a learning problem with some textual features also encouraged us to try EG. 5 Evaluation Techniques We tested the algorithms described above on two IR tasks where supervised learning is particularly applicable: categorization and routing. <p> Maintaining and updating very large weight vectors may take too much space or time, so methods for pruning weight vectors while maintaining theoretical guarantees <ref> (Blum, 1995) </ref> are also worth examining. 11 Summary IR methods are being applied to an increasingly broad range of problems, and by implementers who are less experienced with IR systems. Predictability and effectiveness of techniques under a wide range of conditions are important.
Reference: <author> Buckley, C., & Salton, G. </author> <year> (1995). </year> <title> Optimization of relevance feedback weights. </title> <booktitle> In SIGIR '95, </booktitle> <pages> pp. 351-357. </pages>
Reference-contexts: on a sequence of 100,000 examples drawn randomly with replacement from Topics Method 51-100 101-150 INQUERY Q+50w / Rocchio .326 .341 Q+50w / WH .361 .288 Q+1000w / Rocchio .203 .190 Q+1000w / WH .216 .192 (Buckley et al., 1994) Q+50w / Rocchio .3829 | Q+500w / Rocchio .4068 | <ref> (Buckley & Salton, 1995) </ref> Q+300w/30p : Rocchio .4045 | Q+200w/10p : DFO .4542 | Q+50w : Rocchio | .3471 Q+50w : Best DFO | .4078 Table 3: Mean R-precision across routing topics for various training procedures. <p> Table 3 compares our results for ranking (rather than binary classifying) the routing data with those of other researchers using the same topics, training data, and test data. For Q+50 features and Rocchio starting weights on topics 101-150, EG does as well as Buckley and Salton's <ref> (Buckley & Salton, 1995) </ref> computationally intensive Dynamic Feedback Optimization. On topics 51-100, Buckley and Salton's only results used a phrasal representation, and so are not directly comparable, but EG is at least competitive. <p> Cohen and Singer report preliminary results along these lines (Co-hen & Singer, 1996). It is not clear that minimizing squared error on the training set is the best approach to optimizing, for instance, F fi on the test set. The use of general optimization procedures <ref> (Buckley & Salton, 1995) </ref> is one answer to this problem, but one that sacrifices efficiency and theoretical guarantees. One alternative would be to apply EG to sigmoidal units (Helmbold et al., 1996), which produce probabilities usable for optimization (Lewis, 1995a).
Reference: <author> Buckley, C., Salton, G., & Allan, J. </author> <year> (1994). </year> <title> The effect of adding relevance information in a relevance feedback environment. </title> <booktitle> In SIGIR '94, </booktitle> <pages> pp. 292-300. </pages> <note> 305 Callan, </note> <author> J. P., Croft, W. B., & Broglio, J. </author> <year> (1995). </year> <title> TREC and TIPSTER experiments with INQUERY. </title> <booktitle> Information Processing and Management, </booktitle> <address> 31(3),327-343. </address>
Reference-contexts: WH used a starting vector of (0; : : : ; 0)and EG a starting vector of (1=d; : : : ; 1=d). Learning Rate. Rocchio used fi = 16 and fl = 4, as suggested by Buckley, et al. <ref> (Buckley et al., 1994) </ref>, but with ff = 0 since no query was used. WH used a learning rate of = 1=(4X 2 ), where X is the maximum value of kxk in the training set for that run. <p> Rocchio was trained in the usual batch mode fashion. WH was trained on a sequence of 100,000 examples drawn randomly with replacement from Topics Method 51-100 101-150 INQUERY Q+50w / Rocchio .326 .341 Q+50w / WH .361 .288 Q+1000w / Rocchio .203 .190 Q+1000w / WH .216 .192 <ref> (Buckley et al., 1994) </ref> Q+50w / Rocchio .3829 | Q+500w / Rocchio .4068 | (Buckley & Salton, 1995) Q+300w/30p : Rocchio .4045 | Q+200w/10p : DFO .4542 | Q+50w : Rocchio | .3471 Q+50w : Best DFO | .4078 Table 3: Mean R-precision across routing topics for various training procedures. <p> On topics 51-100, Buckley and Salton's only results used a phrasal representation, and so are not directly comparable, but EG is at least competitive. Buck-ley, Salton, and Allan <ref> (Buckley et al., 1994) </ref> found Rocchio better suited to large feature sets on topics 51-100 than we did, probably due to differences in document length normalization. 10 Future Work There are many improvements possible in our techniques for learning linear classifiers for IR.
Reference: <author> Cesa-Bianchi, N., Long, P. M., & Warmuth, M. K. </author> <year> (1993). </year> <title> Worst-case quadratic loss bounds for a generalization of the Widrow-Hoff rule. </title> <booktitle> In Proceedings of COLT-93, </booktitle> <pages> pp. 429-438. </pages>
Reference: <author> Cohen, W. W. </author> <year> (1995). </year> <title> Text categorization and relational learning. </title> <booktitle> In Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <pages> pp. 124-132. </pages>
Reference: <author> Cohen, W. W., & Singer, Y. </author> <year> (1996). </year> <title> Context-sensitive learning methods for text categorization. </title> <booktitle> In SIGIR '96. </booktitle>
Reference: <author> Croft, W. B., & Harper, D. J. </author> <year> (1979). </year> <title> Using probabilistic models of document retrieval without relevance feedback. </title> <journal> Journal of Documentation, 35(4),285-295. </journal>
Reference-contexts: the Robertson/Sparck Jones probabilistic retrieval model documents are ranked by this linear function: d X x j log (1 p j )q j where p i and q i are probabilities to be estimated based on training data (Robertson & Sparck Jones, 1976) or the text of a user request <ref> (Croft & Harper, 1979) </ref>, and the x j 's are binary (1 if a word is present in a document, 0 otherwise).
Reference: <author> Duda, R. O., & Hart, P. E. </author> <year> (1973). </year> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley-Interscience, </publisher> <address> New York. </address>
Reference-contexts: We distinguish between parametric and nonparametric training algorithms <ref> (Duda & Hart, 1973, p. 130) </ref>. Parametric algorithms use training data to estimate parameters of a probability distribution, and a classifier is produced under the assumption that the estimated distribution is correct. Many probabilistic IR algorithms, for instance the Robertson/Sparck Jones relevance feedback algorithm, are parametric algorithms. <p> The hope is that the weight vector will generalize well, i.e., that it will also optimize the criterion function, or some other effectiveness measure, on new data. Different training algorithms can be produced by varying the criterion function and search procedure used <ref> (Duda & Hart, 1973, pp. 130-131) </ref>. Search procedures can operate in either online or batch fashion. Online algorithms are presented with one training example at a time. They update their current weight vector based on that example and then discard the example, retaining only the new weight vector. <p> algorithm are restricted to having nonnegative weights, so that instead of using the raw w from Equation (1), one uses w 0 where w j = w j if w j &gt; 0 0 otherwise: 3.2 The Widrow-Hoff Algorithm The LMS or Widrow-Hoff algorithm (Widrow & Stearns, 1985, Ch. 6) <ref> (Duda & Hart, 1973, p. 156) </ref> (here abbreviated WH) is an online algorithm. It runs through the training examples one at a time updating a weight vector at each step. We denote the value of this weight vector before processing the ith training example by w i .
Reference: <author> Gallant, S. I. </author> <year> (1986). </year> <title> Optimal linear discriminants. </title> <booktitle> In International Conference on Pattern Recognition, </booktitle> <pages> pp. 849-852. </pages>
Reference-contexts: DFO is Buckley and Salton's Dynamic Feedback Optimization. the full training set. EG was trained on 100,000 examples drawn randomly with replacement from either the positive (probability 1/2) or negative (probability 1/2) training examples. Final Classifier. The final classifier was selected by a pocketing strategy <ref> (Gallant, 1986) </ref>. We pocket (record) the weight vector after 100 training examples. After every 100 subsequent training examples the current weight vector is used to rank the training data and the value of SAP is measured.
Reference: <author> Harman, D. </author> <year> (1992a). </year> <title> Ranking algorithms. </title> <editor> In Frakes, W. B., & Baeza-Yates, R., editors, </editor> <booktitle> Information Retrieval: Data Structures and Algorithms, </booktitle> <pages> pp. 363-392. </pages> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ. </address>
Reference-contexts: The classical vector space model (Salton & McGill, 1983, pp. 120-123) <ref> (Harman, 1992a) </ref>, ranks documents using a nonlinear similarity measure called the cosine correlation: SIM (q; x) = kqk kxk Research and Development in Information Retrieval,, (August 18-22, 1996, Zurich). <p> might be 1 if a word appeared in a textual user request and 0 otherwise, while x j is the number of times the word occurs in the document of interest, times its inverse document frequency, i.e., a form of tf fi idf weighting (Salton & McGill, 1983, p. 63), <ref> (Harman, 1992a) </ref>.
Reference: <author> Harman, D. </author> <year> (1992b). </year> <title> Relevance feedback and other query modification techniques. </title> <editor> In Frakes, W. B., & Baeza-Yates, R., editors, </editor> <booktitle> Information Retrieval: Data Structures and Algorithms, </booktitle> <pages> pp. 241-263. </pages> <publisher> Prentice Hall, </publisher> <address> En-glewood Cliffs, NJ. </address>
Reference: <author> Harman, D. </author> <year> (1995a). </year> <booktitle> Overview of the third Text REtrieval Conference (TREC-3). In (Harman, </booktitle> <year> 1995b). </year>
Reference-contexts: As such, routing systems share characteristics of both retrieval and categorization systems. 7.1 A TREC Routing Data Set Our routing experiments used data developed in the TREC evaluations <ref> (Harman, 1995a) </ref>. The 741,856 documents from TIPSTER Volumes 1 & 2 were used for training, and the 336,310 documents from Volume 3 were used for testing. (See Section 6.2 for availability.) The TIPSTER distribution includes several sets of "topics" describing the needs of hypothetical users for information. <p> For topics 51-100, a mean of 1,784 training documents (328 relevant and 1,456 nonrelevant) and 2,340 test documents (220 relevant and 2,121 nonrelevant), selected by a pooling strategy <ref> (Harman, 1995a) </ref>, have been judged for relevance. Similarly, for topics 101-150, a mean of 1,252 training documents (233 relevant and 1,019 nonrelevant) and 1,333 test documents (187 relevant and 1,146 nonrelevant) were judged. In our experiments, we train only on the judged training documents.
Reference: <author> Harman, D. K., </author> <title> editor (1995b). </title> <booktitle> Overview of the Third Text REtrieval Conference (TREC-3), </booktitle> <address> Gaithersburg, MD 20899-0001. </address> <institution> National Institute of Standards and Technology. </institution> <note> Special Publication 500-225. </note>
Reference-contexts: A classifier is applied to each test document, and the documents are sorted by the resulting scores. We measure how close to perfect ranking the classifier came using simple average precision (SAP), which is the mean of precision measured at each class member in the ranking <ref> (Harman, 1995b, p. A-9) </ref>. 6 Text Categorization Task Text categorization systems classify units of natural language text into pre-defined categories. <p> R-precision is precision at a number of documents equal to the number of relevant documents <ref> (Harman, 1995b, p. A-10) </ref>. w indicates that expansion terms are words, p indicates phrases. DFO is Buckley and Salton's Dynamic Feedback Optimization. the full training set. EG was trained on 100,000 examples drawn randomly with replacement from either the positive (probability 1/2) or negative (probability 1/2) training examples. Final Classifier.
Reference: <author> Helmbold, D. P., Kivinen, J., & Warmuth, M. K. </author> <year> (1996). </year> <title> Worst-case loss bounds for single neurons. In Advances in Neural Information Processing Systems 8. </title> <note> To appear. </note>
Reference-contexts: The use of general optimization procedures (Buckley & Salton, 1995) is one answer to this problem, but one that sacrifices efficiency and theoretical guarantees. One alternative would be to apply EG to sigmoidal units <ref> (Helmbold et al., 1996) </ref>, which produce probabilities usable for optimization (Lewis, 1995a). Another would be to define error measures for learning which are more tightly coupled with the ultimate effectiveness measure.
Reference: <author> Hersh, W., Buckley, C., Leone, T. J., & Hickman, D. </author> <year> (1994). </year> <title> OHSUMED: an interactive retrieval evaluation and new large test collection for research. </title> <booktitle> In SIGIR '94, </booktitle> <pages> pp. 192-201. </pages>
Reference-contexts: We describe two new text categorization data sets and how they were used in our experiments. 6.1 The OHSUMED Text Categorization Test Col lection The first collection consists of Medline records from the years 1987 to 1991, distributed as part of the OHSUMED text retrieval test collection <ref> (Hersh et al., 1994) </ref>. For text categorization experiments, we ignore the queries and relevance judgments in the collection, and make use of the MeSH (Lowe & Barnett, 1994) controlled vocabulary terms assigned to the records by National Library of Medicine indexers.
Reference: <author> Kivinen, J., & Warmuth, M. K. </author> <year> (1994). </year> <title> Exponentiated gradient versus gradient descent for linear predictors. </title> <type> Technical Report UCSC-CRL-94-16, </type> <institution> Basking Center for Computer Engineering & Information Sciences; University of California, </institution> <address> Santa Cruz, CA. </address>
Reference-contexts: Thus, WH tries to move in a direction in which this loss is (locally) decreasing the fastest. For classifying new instances, it may seem natural to use the final weight vector w n+1 . However, there are theoretical arguments (e.g. <ref> (Kivinen & Warmuth, 1994) </ref>) which suggest that a better choice is the average of the weight vectors computed along the way: w = n + 1 i=1 3.3 Kivinen & Warmuth's EG Algorithm The exponentiated-gradient or EG algorithm was introduced by Kivinen and Warmuth (Kivinen & Warmuth, 1994). <p> However, there are theoretical arguments (e.g. <ref> (Kivinen & Warmuth, 1994) </ref>) which suggest that a better choice is the average of the weight vectors computed along the way: w = n + 1 i=1 3.3 Kivinen & Warmuth's EG Algorithm The exponentiated-gradient or EG algorithm was introduced by Kivinen and Warmuth (Kivinen & Warmuth, 1994). This algorithm is similar to WH in that it maintains a weight vector w i and runs through training examples one at a time. With EG, however, the components of the weight vector w i are restricted to be nonnegative and to sum to one. <p> The threshold is chosen so as to optimize the desired effectiveness measure on the training set, with the hope that effectiveness on the test set will also be optimized, though this approach has weaknesses (Lewis, 1995a). 4 Error Bounds for WH and EG Kivinen and Warmuth <ref> (Kivinen & Warmuth, 1994) </ref> study in detail the theoretical behavior of EG and WH, building on previous work (Cesa-Bianchi et al., 1993; Widrow & Stearns, 1985). Kivinen and Warmuth focus on deriving upper bounds on the error of WH and EG for various settings of the learning rate .
Reference: <author> Lewis, D. D. </author> <year> (1995a). </year> <title> Evaluating and optimizing autonomous text classification systems. </title> <booktitle> In SIGIR '95, </booktitle> <pages> pp. 246-254. </pages>
Reference-contexts: The threshold is chosen so as to optimize the desired effectiveness measure on the training set, with the hope that effectiveness on the test set will also be optimized, though this approach has weaknesses <ref> (Lewis, 1995a) </ref>. 4 Error Bounds for WH and EG Kivinen and Warmuth (Kivinen & Warmuth, 1994) study in detail the theoretical behavior of EG and WH, building on previous work (Cesa-Bianchi et al., 1993; Widrow & Stearns, 1985). <p> The use of general optimization procedures (Buckley & Salton, 1995) is one answer to this problem, but one that sacrifices efficiency and theoretical guarantees. One alternative would be to apply EG to sigmoidal units (Helmbold et al., 1996), which produce probabilities usable for optimization <ref> (Lewis, 1995a) </ref>. Another would be to define error measures for learning which are more tightly coupled with the ultimate effectiveness measure. This may require using a batch mode version of EG, which we in any case wish to compare with other batch mode error minimization procedures (Yang & Chute, 1994).
Reference: <author> Lewis, D. D. </author> <year> (1995b). </year> <title> A sequential algorithm for training text classifiers: Corrigendum and additional data. </title> <booktitle> SIGIR Forum, </booktitle> <address> 29(2),13-19. </address>
Reference-contexts: Several previous text categorization studies with a proprietary AP collection have used two sets of 10 categories: Set 1 (Lewis & Gale, 1994; Cohen, 1995; Cohen & Singer, 1996) and Set 2 <ref> (Lewis, 1995b) </ref>. We have defined these categories on the TREC-AP data set as well (see Table 1). For the experiments reported here, we use the years 1988 and 1989 (142,791 documents) as a training set, and the year 1990 (66,992 documents) as the testing set.
Reference: <author> Lewis, D. D., & Gale, W. A. </author> <year> (1994). </year> <title> A sequential algorithm for training text classifiers. </title> <booktitle> In SIGIR '94, </booktitle> <pages> pp. 3-12. </pages>
Reference-contexts: Several effectiveness measures can be defined in terms of these values, for instance: * recall (R) = a=(a + c) * precision (P ) = a=(a + b). We used the F-measure <ref> (Lewis & Gale, 1994) </ref> (see also (van Rijsbergen, 1979, pp. 173-176)), a weighted combination of recall and precision that can be defined in terms of the contingency table values: F fi = fi 2 P + R (fi 2 + 1)a We use F fi with fi = 1, i.e., F
Reference: <author> Lowe, H. J., & Barnett, G. O. </author> <year> (1994). </year> <title> Understanding and using the medical subject headings (MeSH) vocabulary to perform literature searches. </title> <journal> Journal of the American Medical Association, 271(14),1103-1108. </journal>
Reference-contexts: For text categorization experiments, we ignore the queries and relevance judgments in the collection, and make use of the MeSH <ref> (Lowe & Barnett, 1994) </ref> controlled vocabulary terms assigned to the records by National Library of Medicine indexers. Of the 348,566 OHSUMED records, all but 23 have MeSH categories assigned. These 348,543 records all have titles, but only 233,445 of them have abstracts. <p> A total of 14,626 distinct main headings occur in the OHSUMED records. In text categorization research with OHSUMED we have focused on the set of 119 MeSH categories in the Heart Disease subtree of the Cardiovascular Diseases tree structure <ref> (Lowe & Barnett, 1994) </ref>. The frequencies of these 119 heart disease categories vary widely, and some in fact do not actually appear in the OHSUMED data.
Reference: <author> Robertson, S. E., & Sparck Jones, K. </author> <year> (1976). </year> <title> Relevance weighting of search terms. </title> <journal> Journal of the American Society for Information Science, </journal> <pages> pp. 129-146. </pages>
Reference-contexts: For instance, in the Robertson/Sparck Jones probabilistic retrieval model documents are ranked by this linear function: d X x j log (1 p j )q j where p i and q i are probabilities to be estimated based on training data <ref> (Robertson & Sparck Jones, 1976) </ref> or the text of a user request (Croft & Harper, 1979), and the x j 's are binary (1 if a word is present in a document, 0 otherwise).
Reference: <author> Rocchio, Jr., J. J. </author> <year> (1971). </year> <title> Relevance feedback in information retrieval. </title> <editor> In Salton, G., editor, </editor> <booktitle> The SMART Retrieval System: Experiments in Automatic Document Processing, </booktitle> <pages> pp. 313-323. </pages> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, New Jersey. </address>
Reference: <author> Salton, G., & Buckley, C. </author> <year> (1988). </year> <title> Term-weighting approaches in automatic text retrieval. </title> <booktitle> Information Processing and Management, </booktitle> <address> 24(5),513-523. </address>
Reference-contexts: Feature Extraction. The set of features for each problem was defined by a crude tokenizer that replaced everything but alphabetic characters with a blank, and down-cased alphabetic characters. Both binary feature values and cosine-normalized tf fi idf feature values (SMART tfc weights <ref> (Salton & Buckley, 1988) </ref>) were used for Rocchio and WH, with idf estimated on the training set for that run. (This is a deviation from the strict online learning framework.) EG was only run on a binary representation, due to limitations of our current software. Feature Selection.
Reference: <author> Salton, G., & Buckley, C. </author> <year> (1990). </year> <title> Improving retrieval performance by relevance feedback. </title> <journal> Journal of the American Society for Information Science, 41(4),288-297. </journal>
Reference-contexts: Document representation had a clear impact on results. Both WH and Rocchio were improved by moving to the more informative tf fi idf representation. Rocchio performed particularly poorly on a binary representation, as has previously been observed <ref> (Salton & Buckley, 1990) </ref>. On the text categorization data EG was run only on a binary representation, as mentioned earlier, but had higher effectiveness than Rocchio running on a tf fi idf representation in many cases.
Reference: <author> Salton, G., & McGill, M. J. </author> <year> (1983). </year> <title> Introduction to Modern Information Retrieval. </title> <publisher> McGraw-Hill Book Company, </publisher> <address> New York. </address>
Reference-contexts: The classical vector space model <ref> (Salton & McGill, 1983, pp. 120-123) </ref> (Harman, 1992a), ranks documents using a nonlinear similarity measure called the cosine correlation: SIM (q; x) = kqk kxk Research and Development in Information Retrieval,, (August 18-22, 1996, Zurich). <p> For instance, q j might be 1 if a word appeared in a textual user request and 0 otherwise, while x j is the number of times the word occurs in the document of interest, times its inverse document frequency, i.e., a form of tf fi idf weighting <ref> (Salton & McGill, 1983, p. 63) </ref>, (Harman, 1992a).
Reference: <author> Siegel, S. </author> <year> (1956). </year> <title> Nonparametric Statistics for the Behavioral Sciences. </title> <publisher> McGraw-Hill, </publisher> <address> New York. </address>
Reference-contexts: First, we count the number of classes on which WH (or EG) has a higher F 1 value than Rocchio, and vice versa, as shown in the Wins columns. WH and EG counts are significantly higher (p &lt; 0:05) than the corresponding Rocchio counts by a one-tailed sign test <ref> (Siegel, 1956, Ch. 5) </ref> unless a "?" is shown. Second, we compute the mean F 1 value across classes for each algorithm and compare this in the Mean F 1 columns. The general pattern of results is as expected.
Reference: <author> Singhal, A., Buckley, C., & Mitra, M. </author> <year> (1996). </year> <title> Pivoted document length normalization. </title> <booktitle> In SIGIR '96. </booktitle> <editor> van Rijsbergen, C. J. </editor> <booktitle> (1979). Information Retrieval. </booktitle> <address> But-terworths, London, </address> <note> second edition. </note>
Reference-contexts: elements of its weight vector: w = kqk and similarly incorporating the document length normaliza tion into the document vector feature values: x 0 = kxk Indeed, recent work on the vector space model replaces the cosine normalization with other length normalizations, but maintains the linear form of the classifier <ref> (Singhal et al., 1996) </ref>. Many commercial ranked retrieval systems also are based on linear functions, the evaluation of which can be made very efficient via inverted files and other techniques.
Reference: <author> Widrow, B., & Stearns, S. D. </author> <year> (1985). </year> <title> Adaptive Signal Processing. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ. </address>
Reference-contexts: Typically, classifiers produced with the Rocchio algorithm are restricted to having nonnegative weights, so that instead of using the raw w from Equation (1), one uses w 0 where w j = w j if w j &gt; 0 0 otherwise: 3.2 The Widrow-Hoff Algorithm The LMS or Widrow-Hoff algorithm <ref> (Widrow & Stearns, 1985, Ch. 6) </ref> (Duda & Hart, 1973, p. 156) (here abbreviated WH) is an online algorithm. It runs through the training examples one at a time updating a weight vector at each step.
Reference: <author> Yang, Y., & Chute, C. G. </author> <year> (1994). </year> <title> An example-based mapping method for text categorization and retrieval. </title> <journal> ACM Transactions on Information Systems, 12(3),252-277. </journal> <volume> 306 </volume>
Reference-contexts: Another would be to define error measures for learning which are more tightly coupled with the ultimate effectiveness measure. This may require using a batch mode version of EG, which we in any case wish to compare with other batch mode error minimization procedures <ref> (Yang & Chute, 1994) </ref>.
References-found: 30

