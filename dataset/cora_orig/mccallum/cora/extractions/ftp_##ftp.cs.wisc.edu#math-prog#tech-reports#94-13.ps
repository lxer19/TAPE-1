URL: ftp://ftp.cs.wisc.edu/math-prog/tech-reports/94-13.ps
Refering-URL: http://www.cs.wisc.edu/math-prog/tech-reports/
Root-URL: 
Title: Alternating Directions Methods for the Parallel Solution of Large-Scale Block-Structured Optimization Problems  
Author: By Spyridon A. Kontogiorgis 
Degree: A thesis submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy (Computer Sciences) at the  
Date: 1994  
Address: WISCONSIN MADISON  
Affiliation: UNIVERSITY OF  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A.V. Aho, J.E. Hopcroft, and J.D. Ullman. </author> <title> The design and analysis of computer algorithms. </title> <publisher> Addison-Wesley, </publisher> <year> 1974. </year>
Reference-contexts: We will reserve superscript t to denote vectors and matrices generated at step t = 0; 1; 2; : : : of an iterative process. We will use the notation x <ref> [1] </ref> ; x [2] ; : : : ; x [K] to denote K disjoint sub-vectors that a vector x can be partitioned to. In case it is clear from the context, we sometimes let x represent the concatenation of the vectors x [1] ; x [2] ; : : : <p> We will use the notation x <ref> [1] </ref> ; x [2] ; : : : ; x [K] to denote K disjoint sub-vectors that a vector x can be partitioned to. In case it is clear from the context, we sometimes let x represent the concatenation of the vectors x [1] ; x [2] ; : : : ; x [K] . For scalars ff and fi, the operators minfff; fig, maxfff; fig have the standard meaning. We define ff + := maxfff; 0g and ff := maxfff; 0g. <p> some of the basics of parallel complexity models, following the survey of Mayr [67], which also has a sizeable bibliography. 1.3.3.3 Complexity-Theoretic Parallel Computation The basic computing model is the Parallel Random Access Machine, or PRAM, which consists of an unbounded number of computing cells, which are Random Access Machines <ref> [1, chapter 1] </ref>, and 9 an unbounded number of global, shared memory cells. A RAM consists of a read-only input tape, a write-only output tape, a stored program and registers. Any RAM can access any global memory cell in a single step. <p> The convex block-angular problem, denoted as (CBA), is min f <ref> [1] </ref> (x [1] ) + f [2] (x [2] ) + : : : + f [K] (x [K] ) subject to A [1] x [1] = b [1] A [2] x [2] = b [2] A [K] x [K] = b [K] 0 x [i] u [i] ; i = <p> The convex block-angular problem, denoted as (CBA), is min f <ref> [1] </ref> (x [1] ) + f [2] (x [2] ) + : : : + f [K] (x [K] ) subject to A [1] x [1] = b [1] A [2] x [2] = b [2] A [K] x [K] = b [K] 0 x [i] u [i] ; i = 1; : <p> The convex block-angular problem, denoted as (CBA), is min f <ref> [1] </ref> (x [1] ) + f [2] (x [2] ) + : : : + f [K] (x [K] ) subject to A [1] x [1] = b [1] A [2] x [2] = b [2] A [K] x [K] = b [K] 0 x [i] u [i] ; i = 1; : : : ; K: 10 In the above x [i] are the vectors of variables and K P f [i] () <p> The convex block-angular problem, denoted as (CBA), is min f <ref> [1] </ref> (x [1] ) + f [2] (x [2] ) + : : : + f [K] (x [K] ) subject to A [1] x [1] = b [1] A [2] x [2] = b [2] A [K] x [K] = b [K] 0 x [i] u [i] ; i = 1; : : : ; K: 10 In the above x [i] are the vectors of variables and K P f [i] () is the <p> The convex block-angular problem, denoted as (CBA), is min f <ref> [1] </ref> (x [1] ) + f [2] (x [2] ) + : : : + f [K] (x [K] ) subject to A [1] x [1] = b [1] A [2] x [2] = b [2] A [K] x [K] = b [K] 0 x [i] u [i] ; i = 1; : : : ; K: 10 In the above x [i] are the vectors of variables and K P f [i] () is the objective function. <p> We would take f [K+1] 0, u [K+1] = +1, and the matrix D [K+1] to be the identity on IR m 0 fim 0 . Then D <ref> [1] </ref> x [1] + D [2] x [2] + : : : + D [K] x [K] + x [K+1] = d; 0 x [i] u [i] ; i = 1; : : : ; K + 1 11 We now describe two real-world optimization problems that give rise to (CBA) <p> We would take f [K+1] 0, u [K+1] = +1, and the matrix D [K+1] to be the identity on IR m 0 fim 0 . Then D <ref> [1] </ref> x [1] + D [2] x [2] + : : : + D [K] x [K] + x [K+1] = d; 0 x [i] u [i] ; i = 1; : : : ; K + 1 11 We now describe two real-world optimization problems that give rise to (CBA) problems. <p> problem (55) as min G 1 (x 1 ; x 2 ; x 3 ) + G 2 (x 4 ) subject to x 3 + x 4 = 1 which is in the form of the general problem (9), with the correspondences A [ 0 0 1 ]; B <ref> [ 1 ] </ref>; b 1 Observe that the splitting matrix A has not full column rank. In the ADI algorithm we take, for simplicity, H t = I; 8t. <p> Since the functions f [i] , i = 1; : : : ; K are continuous, by assumption, the value inf K P f [i] (x [i] ) subject to (x <ref> [1] </ref> ; : : :; x [K] ) 2 F (72) is attained, by the Bolzano-Weierstrass theorem 1.6.15. <p> These sets are non-empty, by assumption 3.2.1. Then h [i] = f [i] + ( j B [i] ) and the extended-real-valued function h [i] is proper, closed and convex. Let now J = bK=2c. We induce a split by defining G 1 x <ref> [1] </ref> ; : : : ; x [J] := i=1 and K X h [i] (x [i] ) (75) 44 Both G 1 and G 2 are proper, by assumption 3.2.1. Problem (CBA) is equivalent to min G 1 x [1] ; : : :; x [J] + G 2 x <p> We induce a split by defining G 1 x <ref> [1] </ref> ; : : : ; x [J] := i=1 and K X h [i] (x [i] ) (75) 44 Both G 1 and G 2 are proper, by assumption 3.2.1. Problem (CBA) is equivalent to min G 1 x [1] ; : : :; x [J] + G 2 x [J+1] ; : : : ; x [K] subject to J X D [i] x [i] d = i=J+1 which is in the form of problem (9), with the correspondences A [D [1] : : : D [J] ]; B <p> is equivalent to min G 1 x <ref> [1] </ref> ; : : :; x [J] + G 2 x [J+1] ; : : : ; x [K] subject to J X D [i] x [i] d = i=J+1 which is in the form of problem (9), with the correspondences A [D [1] : : : D [J] ]; B [D [J+1] : : : D [K] ]; b d We let the penalty matrix be of the form H t = diag (H t 1 ; H t 2 ), with H t 1 and H t 2 both symmetric positive definite <p> The iterative step of the ADI algorithm consists of solving the two subproblems <ref> [1] </ref> ; : : : ; x t+1 2 argmin x [1] ; : : : ; x [J] i=1 J X D [i] x [i] 1 fl fl fl i=1 K X D [i] x t fl fl fl H t subject to x [i] 2 B [i] ; i <p> The iterative step of the ADI algorithm consists of solving the two subproblems <ref> [1] </ref> ; : : : ; x t+1 2 argmin x [1] ; : : : ; x [J] i=1 J X D [i] x [i] 1 fl fl fl i=1 K X D [i] x t fl fl fl H t subject to x [i] 2 B [i] ; i = 1; : : : ; J [J+1] ; : : <p> In this scheme the constraint terms in the objective correspond to proximal terms for both the activities x [i] and the resource allocation vectors ~ d [i] . 4.3 Derivation We define G 1 x <ref> [1] </ref> ; : : : ; x [K] ; ~ d [1] ; : : :; ~ d [K] := &gt; &gt; &lt; K X h [i] (x [i] ) if D [i] x [i] ~ d [i] ; 8i = 1; : : : ; K +1 otherwise (77) in <p> In this scheme the constraint terms in the objective correspond to proximal terms for both the activities x [i] and the resource allocation vectors ~ d [i] . 4.3 Derivation We define G 1 x <ref> [1] </ref> ; : : : ; x [K] ; ~ d [1] ; : : :; ~ d [K] := &gt; &gt; &lt; K X h [i] (x [i] ) if D [i] x [i] ~ d [i] ; 8i = 1; : : : ; K +1 otherwise (77) in which the extended block-objective function h [i] (x [i] ) is <p> We also define G 2 y <ref> [1] </ref> ; : : : ; y [K] ; d [1] ; : : : ; d [K] := &gt; &gt; &lt; 0 if i=1 +1 otherwise (78) in which each y [i] 2 IR n i and each d [i] 2 IR m 0 . <p> We also define G 2 y <ref> [1] </ref> ; : : : ; y [K] ; d [1] ; : : : ; d [K] := &gt; &gt; &lt; 0 if i=1 +1 otherwise (78) in which each y [i] 2 IR n i and each d [i] 2 IR m 0 . <p> Both functions G 1 and G 2 are closed, convex and also proper, because of our assumption that (CBA) is solvable. Problem (CBA) is equivalent to min G 1 x <ref> [1] </ref> ; : : : ; x [K] ; ~ d [1] ; : : : ; ~ d [K] + G 2 y [1] ; : : : ; y [K] ; d [1] ; : : : ; d [K] subject to x [i] = y [i] 9 ; <p> Both functions G 1 and G 2 are closed, convex and also proper, because of our assumption that (CBA) is solvable. Problem (CBA) is equivalent to min G 1 x <ref> [1] </ref> ; : : : ; x [K] ; ~ d [1] ; : : : ; ~ d [K] + G 2 y [1] ; : : : ; y [K] ; d [1] ; : : : ; d [K] subject to x [i] = y [i] 9 ; which is in the form of the general problem (9), with <p> Problem (CBA) is equivalent to min G 1 x <ref> [1] </ref> ; : : : ; x [K] ; ~ d [1] ; : : : ; ~ d [K] + G 2 y [1] ; : : : ; y [K] ; d [1] ; : : : ; d [K] subject to x [i] = y [i] 9 ; which is in the form of the general problem (9), with the correspondences A I; B I; b 0 (80) The introduction of ~ <p> Problem (CBA) is equivalent to min G 1 x <ref> [1] </ref> ; : : : ; x [K] ; ~ d [1] ; : : : ; ~ d [K] + G 2 y [1] ; : : : ; y [K] ; d [1] ; : : : ; d [K] subject to x [i] = y [i] 9 ; which is in the form of the general problem (9), with the correspondences A I; B I; b 0 (80) The introduction of ~ d [i] thus allows us to split implicitly the coupling <p> We now present the extended ADI algorithm for this splitting. We will use a shorthand notation, and write x t+1 for the concatenation of the vectors x t+1 <ref> [1] </ref> ; : : : ; x t+1 [K] , and similarly for y t+1 etc. We will also write f (x) for K P f [i] (x [i] ). We define H t x := diag x [1] ; : : :; H t and let the symmetric positive definite <p> and write x t+1 for the concatenation of the vectors x t+1 <ref> [1] </ref> ; : : : ; x t+1 [K] , and similarly for y t+1 etc. We will also write f (x) for K P f [i] (x [i] ). We define H t x := diag x [1] ; : : :; H t and let the symmetric positive definite matrix H t K consist of K copies of H t d placed along the diagonal. <p> Then, the iterates are such that: (i) q t+1 = 0, t 0. (iii) i=1 [i] = d, t 0 <ref> [1] </ref> = p t+1 [K] , t 0 Proof: (i) Part (i) follows from substituting the value of y t+1 from (86) in the q update (83). Part (ii) then follows from taking t + 1 for t in (86) and then using part (i). <p> also get the convergence of the sequence n [i] , i = 1; : : :; K to an optimal multiplier p fl [i] for the d [i] = ~ d [i] constraints of the extended problem (79), and from part (iv) of lemma 4.4.1 we get that p fl <ref> [1] </ref> = : : : = p fl [K] . Since (CBA) and the two subproblems have polyhedral constraints, they satisfy a Constraint Qualification, therefore each minimizer is a Karush Kuhn-Tucker point and associated optimal duals exist for each. <p> Then, the iterates are such that: (i) q t+1 = 0, t 0. (iii) i=1 [i] = d, t 0 <ref> [1] </ref> = p t+1 [K] , t 0 Proof: Part (i) follows after substitution of (125) in (127). Then, part (ii) follows from taking t + 1 for t in (129) and then using part (i). <p> case of an (ARP) algorithm. 70 71 5.2 Derivation We introduce again the extended objective h [i] (x [i] ) := &lt; f [i] (x [i] ) if A [i] x [i] = b [i] and 0 x [i] u [i] +1 otherwise (136) Then we define G 1 x <ref> [1] </ref> ; : : : ; x [K] := i=1 and 8 &gt; &gt; : K X d [i] d +1 otherwise (138) with d [1] ; : : : ; d [K] 2 IR m 0 . <p> [i] ) if A [i] x [i] = b [i] and 0 x [i] u [i] +1 otherwise (136) Then we define G 1 x <ref> [1] </ref> ; : : : ; x [K] := i=1 and 8 &gt; &gt; : K X d [i] d +1 otherwise (138) with d [1] ; : : : ; d [K] 2 IR m 0 . Functions G 1 and G 2 are both closed, convex and also proper, because (CBA) is solvable, by assumption 3.2.1. <p> We take the splitting matrix D to be block-diagonal, of size (Km 0 ) fi K P n i . D := diag Problem (CBA) is equivalent to min G 1 x <ref> [1] </ref> ; : : : ; x [K] + G 2 d [1] ; : : : ; d [K] subject to D [i] x [i] = d [i] ; i = 1; : : : ; K (140) which is in the form of the general problem (9), with the <p> We take the splitting matrix D to be block-diagonal, of size (Km 0 ) fi K P n i . D := diag Problem (CBA) is equivalent to min G 1 x <ref> [1] </ref> ; : : : ; x [K] + G 2 d [1] ; : : : ; d [K] subject to D [i] x [i] = d [i] ; i = 1; : : : ; K (140) which is in the form of the general problem (9), with the correspondences A D; B I; b 0 (141) The coupling constraints of <p> Then the D [i] x [i] terms for (RP) behave like d t+ 1 [i] terms for (ARP). In more detail: we derive an (ARP) splitting by defining a function G 1 as G 1 x <ref> [1] </ref> ; : : : ; x [K] ; ~ d [1] ; : : : ; ~ d [K] := &gt; &gt; &lt; K X f [i] (x [i] ) + (x [i] j B [i] ) if D [i] x [i] = ~ d [i] ; 8i +1 otherwise <p> Then the D [i] x [i] terms for (RP) behave like d t+ 1 [i] terms for (ARP). In more detail: we derive an (ARP) splitting by defining a function G 1 as G 1 x <ref> [1] </ref> ; : : : ; x [K] ; ~ d [1] ; : : : ; ~ d [K] := &gt; &gt; &lt; K X f [i] (x [i] ) + (x [i] j B [i] ) if D [i] x [i] = ~ d [i] ; 8i +1 otherwise and by defining a function G 2 of the d [i] <p> G 2 d <ref> [1] </ref> ; : : :; d [K] := &gt; &gt; &lt; 0 if i=1 +1 otherwise (157) 77 Then we can write the (CBA) problem as min G 1 x [1] ; : : : ; x [K] ; ~ d [1] ; : : : ; ~ d [K] + <p> G 2 d <ref> [1] </ref> ; : : :; d [K] := &gt; &gt; &lt; 0 if i=1 +1 otherwise (157) 77 Then we can write the (CBA) problem as min G 1 x [1] ; : : : ; x [K] ; ~ d [1] ; : : : ; ~ d [K] + G 2 d [1] ; : : : ; d [K] subject to d [i] = ~ d [i] ; i = 1; : : :; K (158) in which <p> G 2 d <ref> [1] </ref> ; : : :; d [K] := &gt; &gt; &lt; 0 if i=1 +1 otherwise (157) 77 Then we can write the (CBA) problem as min G 1 x [1] ; : : : ; x [K] ; ~ d [1] ; : : : ; ~ d [K] + G 2 d [1] ; : : : ; d [K] subject to d [i] = ~ d [i] ; i = 1; : : :; K (158) in which the x [i] variables are not included in the explicit constraints. <p> :; d [K] := &gt; &gt; &lt; 0 if i=1 +1 otherwise (157) 77 Then we can write the (CBA) problem as min G 1 x <ref> [1] </ref> ; : : : ; x [K] ; ~ d [1] ; : : : ; ~ d [K] + G 2 d [1] ; : : : ; d [K] subject to d [i] = ~ d [i] ; i = 1; : : :; K (158) in which the x [i] variables are not included in the explicit constraints. <p> the (ARP) splitting. 6.2 Derivation We introduce again the extended objective function h [i] (x [i] ) := &lt; f [i] (x [i] ) if A [i] x [i] = b [i] and 0 x [i] u [i] +1 otherwise (163) and we define, as for (RP), G 1 x <ref> [1] </ref> ; : : : ; x [K] := i=1 The difference with the (RP) splitting lies in the definition of G 2 . We take here G 2 y [1] ; : : : ; y [K] := &gt; &gt; &lt; 0 if i=1 +1 otherwise (165) with (y [1] <p> b [i] and 0 x [i] u [i] +1 otherwise (163) and we define, as for (RP), G 1 x <ref> [1] </ref> ; : : : ; x [K] := i=1 The difference with the (RP) splitting lies in the definition of G 2 . We take here G 2 y [1] ; : : : ; y [K] := &gt; &gt; &lt; 0 if i=1 +1 otherwise (165) with (y [1] ; : : : ; y [K] ) 2 K Q IR n i . Thus the coupling constraints of (CBA) are incorporated in G 2 . <p> <ref> [1] </ref> ; : : : ; x [K] := i=1 The difference with the (RP) splitting lies in the definition of G 2 . We take here G 2 y [1] ; : : : ; y [K] := &gt; &gt; &lt; 0 if i=1 +1 otherwise (165) with (y [1] ; : : : ; y [K] ) 2 K Q IR n i . Thus the coupling constraints of (CBA) are incorporated in G 2 . Functions G 1 and G 2 are both closed, convex and also proper, because (CBA) is solvable, by assumption 3.2.1. <p> Functions G 1 and G 2 are both closed, convex and also proper, because (CBA) is solvable, by assumption 3.2.1. We take the splitting matrix to be the identity in K Q IR n i . We write (CBA) as min G 1 x <ref> [1] </ref> ; : : : ; x [K] + G 2 y [1] ; : : : ; y [K] subject to x [i] = y [i] ; i = 1; : : : ; K (166) which is in the form of the general problem (9), with the correspondences A <p> We take the splitting matrix to be the identity in K Q IR n i . We write (CBA) as min G 1 x <ref> [1] </ref> ; : : : ; x [K] + G 2 y [1] ; : : : ; y [K] subject to x [i] = y [i] ; i = 1; : : : ; K (166) which is in the form of the general problem (9), with the correspondences A I; B I; b 0 (167) A tentative Lagrange multiplier vector p <p> For added flexibility we let each block i maintain its own symmetric positive definite penalty matrix H t [i] . We define H t := diag <ref> [1] </ref> ; : : : ; H t 80 We now present the resulting ADI method using shorthand notation. <p> subproblem (185) is given by y t+1 [i];! + p t T t+1 = !x t+1 [i] + p t T t+1 for t+1 the solution to the problem LCP d ! i=1 [i] (1 !) i=1 [i] DD T t ; DD T (188) in which D := [D <ref> [1] </ref> : : : D [K] ]. Then the p multiplier update yields p t+1 T t+1 (189) so that, for canonical initialization, we have that y t+1 [i] + (1 !)y t 1 D [i] t t+1 One has a strong convergence result for this variant.
Reference: [2] <author> R. Ahuja, T.L Magnanti, and J.B. Orlin. </author> <title> Network flows: theory, algorithms, and applications. </title> <publisher> Prentice-Hall Inc., </publisher> <year> 1993. </year>
Reference-contexts: We will reserve superscript t to denote vectors and matrices generated at step t = 0; 1; 2; : : : of an iterative process. We will use the notation x [1] ; x <ref> [2] </ref> ; : : : ; x [K] to denote K disjoint sub-vectors that a vector x can be partitioned to. In case it is clear from the context, we sometimes let x represent the concatenation of the vectors x [1] ; x [2] ; : : : ; x [K] <p> use the notation x [1] ; x <ref> [2] </ref> ; : : : ; x [K] to denote K disjoint sub-vectors that a vector x can be partitioned to. In case it is clear from the context, we sometimes let x represent the concatenation of the vectors x [1] ; x [2] ; : : : ; x [K] . For scalars ff and fi, the operators minfff; fig, maxfff; fig have the standard meaning. We define ff + := maxfff; 0g and ff := maxfff; 0g. <p> The convex block-angular problem, denoted as (CBA), is min f [1] (x [1] ) + f <ref> [2] </ref> (x [2] ) + : : : + f [K] (x [K] ) subject to A [1] x [1] = b [1] A [2] x [2] = b [2] A [K] x [K] = b [K] 0 x [i] u [i] ; i = 1; : : : ; K: <p> The convex block-angular problem, denoted as (CBA), is min f [1] (x [1] ) + f <ref> [2] </ref> (x [2] ) + : : : + f [K] (x [K] ) subject to A [1] x [1] = b [1] A [2] x [2] = b [2] A [K] x [K] = b [K] 0 x [i] u [i] ; i = 1; : : : ; K: 10 In <p> The convex block-angular problem, denoted as (CBA), is min f [1] (x [1] ) + f <ref> [2] </ref> (x [2] ) + : : : + f [K] (x [K] ) subject to A [1] x [1] = b [1] A [2] x [2] = b [2] A [K] x [K] = b [K] 0 x [i] u [i] ; i = 1; : : : ; K: 10 In the above x [i] are the vectors of variables and K P f [i] () is the objective function. <p> The convex block-angular problem, denoted as (CBA), is min f [1] (x [1] ) + f <ref> [2] </ref> (x [2] ) + : : : + f [K] (x [K] ) subject to A [1] x [1] = b [1] A [2] x [2] = b [2] A [K] x [K] = b [K] 0 x [i] u [i] ; i = 1; : : : ; K: 10 In the above x [i] are the vectors of variables and K P f [i] () is the objective function. <p> The convex block-angular problem, denoted as (CBA), is min f [1] (x [1] ) + f <ref> [2] </ref> (x [2] ) + : : : + f [K] (x [K] ) subject to A [1] x [1] = b [1] A [2] x [2] = b [2] A [K] x [K] = b [K] 0 x [i] u [i] ; i = 1; : : : ; K: 10 In the above x [i] are the vectors of variables and K P f [i] () is the objective function. <p> We would take f [K+1] 0, u [K+1] = +1, and the matrix D [K+1] to be the identity on IR m 0 fim 0 . Then D [1] x [1] + D <ref> [2] </ref> x [2] + : : : + D [K] x [K] + x [K+1] = d; 0 x [i] u [i] ; i = 1; : : : ; K + 1 11 We now describe two real-world optimization problems that give rise to (CBA) problems. <p> We would take f [K+1] 0, u [K+1] = +1, and the matrix D [K+1] to be the identity on IR m 0 fim 0 . Then D [1] x [1] + D <ref> [2] </ref> x [2] + : : : + D [K] x [K] + x [K+1] = d; 0 x [i] u [i] ; i = 1; : : : ; K + 1 11 We now describe two real-world optimization problems that give rise to (CBA) problems. <p> A good coverage of these is provided in the survey articles by Kennington [55] and Assad [6], and in the books by Kennigton and Helgason [56] and by 12 Ahuja, Magnanti and Orlin <ref> [2] </ref>. 1.4.3 Scenario Analysis In some cases of modeling there is uncertainty about the value of some of the parameters of the system. In certain situations one can assign "reasonable" values to such parameters; otherwise, one has to include the uncertainty into the model, and solve a stochastic optimization problem. <p> We call such a choice a canonical initialization. Definition 4.4.7 An initialization d 0 ; q 0 ; p 0 for the (ARP) scheme is canonical if q 0 = 0; p 0 <ref> [2] </ref> = : : : = p 0 K X d 0 4.5 The iterative step simplified We now use the results of the previous section and canonical initialization in order to present a simpler version of the iterative step: we pass only three vectors from an iteration to the next, <p> Lemma 4.8.1 (Invariants) Let the ADI scheme given by (109)-(114) be started from any (q 0 ; p 0 ; y 0 ; d 0 ). Then, the iterates are such that: (i) q t+1 = 0, t 0. 61 K X d t+1 (iv) p t+1 <ref> [2] </ref> = : : : = p t+1 Proof: Part (i) follows after substitution of (115) in (113). Then, part (ii) follows from taking t + 1 for t in (115) and then using part (i). <p> Lemma 5.3.1 (Invariants of the (RP) splitting) Let the ADI scheme described by (142)-(144) be started from any (p 0 ; d 0 ) and any diagonal positive penalty matrix fl 0 . Then, (i) p t+1 <ref> [2] </ref> = : : : = p t+1 (ii) i=1 [i] d, t 0. Proof: Part (i) follows from (151) and (150). Part (ii) follows from the fact that the vectors d t+1 [i] , for t 0, solve (143) and thus satisfy the inequality constraints therein. <p> A canonical initialization establishes these properties also for the starting vectors. Definition 5.3.2 An initialization d 0 ; p 0 for the ADI scheme for (RP) is canonical if p 0 <ref> [2] </ref> = : : : = p 0 K X d 0 5.4 The iterative step simplified We now present a simpler version of the iterative step, by making use of the reverse-order update formulas and of canonical initialization.
Reference: [3] <author> S.G. Akl. </author> <title> The design and analysis of parallel algorithms. </title> <publisher> Prentice-Hall Inc., </publisher> <year> 1989. </year>
Reference-contexts: There is extensive experimentation going on in the design and configuration of both the processors and the interconnection network. For a general introduction to parallel computation one may consult, among others, the books by Hwang [51] and Lawson [60], on the design of hardware, and by Akl <ref> [3] </ref> and Bertsekas and Tsitsiklis [11], on the design of parallel scientific algorithms. 3 Although parallel programming is still far from being standardized, the basic desirable parallel computing modes have been characterized.
Reference: [4] <author> A.I. Ali and J.L. Kennington. </author> <title> MNETGEN program documentation. </title> <type> Technical Report IEOR 77003, </type> <institution> Dept. of Industrial Engineering and Operations Research, Southern Methodist University, Dallas, TX, </institution> <year> 1977. </year>
Reference-contexts: For details the interested reader can also refer to the CM-5 technical summary [96] and the CMMD library guide [97]. 90 7.3 The suite of test problems To assess the relative performance of the three splittings we used MNETGEN <ref> [4] </ref>, a derivative of NETGEN [57], to generate three hundred random multicommodity network problems (MC), one hundred with linear objectives and two hundred with quadratic objective functions.
Reference: [5] <author> G. </author> <title> Amdahl. The validity of the single processor approach to achieving large-scale computing capabilities. </title> <booktitle> In Proceedings of the AFIPS Spring Joint Computer Conference, Atlantic City, NJ, </booktitle> <volume> volume 30, </volume> <pages> pages 483-485. </pages> <publisher> AFIPS Press, </publisher> <address> Reston, VA, </address> <month> April </month> <year> 1967. </year>
Reference-contexts: However, this is difficult to achieve, because of the limit on the amount of parallelism that can be extracted; this is described quantitatively by Amdahl's Law <ref> [5] </ref>, which states that: "parallel speedup cannot exceed the inverse of the fraction of the residual sequential code." The relevant formula, stated in terms of efficiency, is p = ([p 1]s + 1) (4) where s is the fraction of the residual sequential (:= non-parallelizable) code.
Reference: [6] <author> A.A. Assad. </author> <title> Multicommodity network flows: a survey. </title> <journal> Networks, </journal> <volume> 8 </volume> <pages> 37-91, </pages> <year> 1978. </year>
Reference-contexts: The multicommodity network flow problem has been studied extensively, and specialized algorithms have been developed for it over the years. A good coverage of these is provided in the survey articles by Kennington [55] and Assad <ref> [6] </ref>, and in the books by Kennigton and Helgason [56] and by 12 Ahuja, Magnanti and Orlin [2]. 1.4.3 Scenario Analysis In some cases of modeling there is uncertainty about the value of some of the parameters of the system.
Reference: [7] <author> J.F. Benders. </author> <title> Partitioning procedures for solving mixed-variables programming problems. </title> <journal> Numerische Mathematik, </journal> <volume> 4 </volume> <pages> 238-252, </pages> <year> 1962. </year>
Reference-contexts: Indirect methods essentially follow the fork-join protocol, by decomposing the original problem into decoupled first-level subproblems and then solving a master coordination problem. Eckstein [27] surveys parallel implementations to date. The Dantzig-Wolfe [19] and Benders <ref> [7] </ref> decomposition are two classic indirect methods. When applied to (CBA), both can be implemented in parallel, since each block can be assigned to a PE.
Reference: [8] <author> R.E. Benner, G.R. Montry, and J.L. Gustafson. </author> <title> A structural analysis algorithm for massively parallel computers. In G.F. </title> <editor> Carey, editor, </editor> <booktitle> Parallel Supercomputing: Methods, Algorithms and Applications, </booktitle> <pages> pages 207-222. </pages> <publisher> John Wiley & Sons, </publisher> <year> 1989. </year>
Reference-contexts: Over the years there have been re-evaluations [46] and extensions [103] of Amdahl's Law, in an effort to incorporate parallel environment factors, such as the process granularity, the synchronization overhead etc. Gustafson <ref> [8] </ref> also showed that for certain numerical problems the fraction of residual sequential code can be reduced as the problem size increases, and thus efficiency may improve with problem size.
Reference: [9] <author> D.P. Bertsekas. </author> <title> Multiplier methods: a survey. </title> <journal> Automatica, </journal> <volume> 12 </volume> <pages> 133-145, </pages> <year> 1976. </year>
Reference-contexts: We make the following basic assumption. Assumption 2.2.1 Problem (9) admits a lagrangean saddlepoint. Thus, a way of solving (9) is by finding a saddle-point of the Lagrangian [101], or, for reasons of computational stability <ref> [9] </ref>, of the augmented Lagrangian L (x; z; p) := G 1 (x) + G 2 (z) + p T (Ax + b Bz) + 2 2 in which , the penalty parameter, is any positive scalar. Both Lagrangians have the same set of saddle-points.
Reference: [10] <author> D.P. Bertsekas. </author> <title> Constrained optimization and Lagrange multiplier methods. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1982. </year> <pages> 119 120 </pages>
Reference-contexts: The best results for this class of problems were obtained with an updating scheme similar to the ones used in the method of multipliers for the Augmented Lagrangian algorithm: a penalty parameter is increased when the violation in the corresponding coupling constraint is not sufficiently reduced [69], <ref> [10, chapter 2] </ref>, [78]. In our implementation all penalty parameters remain within positive bounds, as required by the convergence theory. In the initial iterations we allow for reduced accuracy in solving the subproblems, and also employ over-relaxation. (ARP) : The penalty factors are updated every T = 15 iterations.
Reference: [11] <author> D.P. Bertsekas and J.N. Tsitsiklis. </author> <title> Parallel and distributed computation: numerical methods. </title> <publisher> Prentice-Hall Inc., </publisher> <year> 1989. </year>
Reference-contexts: For a general introduction to parallel computation one may consult, among others, the books by Hwang [51] and Lawson [60], on the design of hardware, and by Akl [3] and Bertsekas and Tsitsiklis <ref> [11] </ref>, on the design of parallel scientific algorithms. 3 Although parallel programming is still far from being standardized, the basic desirable parallel computing modes have been characterized. One can attempt a broad classification of parallel systems on the basis of the implementation of the overall control of the computing process. <p> We define the (absolute) speedup [79], [104], <ref> [11, chapter 1] </ref> as the ratio s fl T fl (1) 7 the relative speedup s p as the ratio s p := T p and the efficiency p as p := p (3) The value of p is usually less that one, indicating efficiency loss due to a number of <p> For the finite-dimensional case, A is assumed to have full column rank [25], [28] or G 1 is assumed to have compact level sets <ref> [11, chapter 3] </ref>; for the dual algorithm in [36], both primal and dual problems are assumed to be feasible and the solution set 21 of the primal is assumed to be bounded. <p> Our proof is modeled after <ref> [11, chapter 3] </ref> and [41, chapter 3]. Theorem 2.3.1 (Convergence of ADI) Let assumptions 2.2.1 and 2.2.2 hold, and let f (x t ; z t ; p t )g be a sequence of iterates produced by the ADI algorithm (12)-(14).
Reference: [12] <author> M.L. Best, A. Greenberg, C. Stanfill, and L.W. Tucker. </author> <title> CMMD I/O: a parallel unix I/O. </title> <booktitle> In Proceedings of the 7th International Parallel Processing Symposium, </booktitle> <address> Newport Beach, CA, </address> <pages> pages 489-495. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> April </month> <year> 1993. </year>
Reference-contexts: Our description of the CM-5 architecture is based on the article of Leiserson et al. [61], which also discusses the philosophy behind the design of the interconnection networks. The article by Best et al. <ref> [12] </ref> presents the parallel I/O modes of the machine, while the article by Hillis and Tucker [49] provides a general overview and presents examples of current applications of the CM-5.
Reference: [13] <author> Aake Bjorck. </author> <title> Least squares methods. </title> <type> Technical report, </type> <institution> Dept. of Mathematics, Linkoping University, Linkoping, Sweden, </institution> <month> May 23, </month> <year> 1991. </year>
Reference-contexts: For linear-quadratic (CBA), the block subproblem for each iteration is a least-squares linear-quadratic problem with linear constraints, usually sparse. For these problems, an efficient sparse solver may be developed by employing algorithms such as those described in Bjorck <ref> [13, sections 25-27] </ref>. (Fast sparse least-squares solvers are needed as building blocks in other mathematical programming algorithms, as well; for instance, for calculating a Newton direction in logarithmic barrier methods for linear programs, see Murray [75, section 7].) For the multicom-modity network problem, in particular, specialized solvers that also take into
Reference: [14] <editor> H. Brezis. Operateurs maximaux monotones et semi-groupes de contractions dans les espaces de Hilbert. </editor> <publisher> North-Holland, </publisher> <year> 1973. </year>
Reference-contexts: When used to solve partial differential equations, the ADI method is usually called the Douglas-Rachford method, because it corresponds to the method in [24] by these two authors. This viewpoint is discussed in detail in [37], [41], [62], [25]. Brezis <ref> [14] </ref> provides a good textbook level introduction to the general theory of maximal monotone operators.
Reference: [15] <author> J. </author> <title> Cea. Lectures on optimization theory and algorithms. </title> <publisher> Tata Institute, </publisher> <address> Bombay, </address> <year> 1978. </year>
Reference-contexts: Also, since the constraints for problem (9) are linear, each optimal point corresponds to a saddle-point. In the proof of theorem 2.3.1 we will also use the following linearization lemma, which specializes <ref> [15, theorem 2.3] </ref>. 25 Lemma 2.3.3 Let X 0 be a nonempty, convex subset of IR n .
Reference: [16] <author> Y. Censor and A. Lent. </author> <title> An iterative row-action method for interval convex programming. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 34 </volume> <pages> 321-353, </pages> <year> 1981. </year>
Reference-contexts: Several other decomposition methods take advantage of the matrix structure of the problem. Zenios [106] specializes the row-action method of Censor and Lent <ref> [16] </ref> to quadratic (MC) ; the algorithm iterates on each constraint in an almost cycling fashion, adjusting primal and dual variables to preserve complementarity and constraint satisfaction.
Reference: [17] <author> G. Cheng and M. Teboulle. </author> <title> A proximal-based decomposition method for convex minimization problems. </title> <journal> Mathematical Programming, Series A, </journal> <volume> 64(1) </volume> <pages> 81-110, </pages> <year> 1994. </year>
Reference-contexts: He then proceeds to derive ADI methods for monotropic programming; in [26] he discusses their fine-grain implementation on the Connection Machine CM-2 parallel computer. Fukushima [36] constructs an ADI method for the dual, rather than the primal problem. Cheng and Teboulle <ref> [17] </ref> present a modified ADI method, in which quadratic proximal terms replace the augmented Lagrangian penalty terms.
Reference: [18] <author> R.W. Cottle, J.-S. Pang, and R.E. Stone. </author> <title> The linear complementarity problem. </title> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference-contexts: The convergence theory for ADI, presented in the following sections, requires that both quadratic terms in the objective be convex, i.e. that both B and C matrices be positive semidefinite. In comparison, the "regular" splittings for the LCP in (8) (see [65], <ref> [18, chapter 5] </ref>) require that the matrix B C be positive definite. 2.2 Description of the algorithm We want to solve the following linearly-constrained convex problem min G 1 (x) + G 2 (z) subject to Ax + b = Bz (9) where G 1 : IR n ! IR [ <p> One approach is to formulate the problem as a Linear Complementarity Problem (LCP) and then solve it by an algorithm amenable to parallelization, such as an iterative matrix-splitting scheme <ref> [18, chapter 5] </ref>, [21].
Reference: [19] <author> G.B. Dantzig and P. Wolfe. </author> <title> Decomposition principle for linear programs. </title> <journal> Operations Research, </journal> <volume> 8 </volume> <pages> 101-111, </pages> <year> 1960. </year>
Reference-contexts: Fork-Join coordination is quite popular in parallel optimization; it is employed by many decomposition algorithms such as: the Dantzig-Wolfe decomposition <ref> [19] </ref>, the Schultz-Meyer shifted barrier 6 method [92], the Parallel Variable Distribution [33], and Parallel Constraint Distribution [32] algorithms of Ferris and Mangasarian, and the Diagonal Quadratic Approximation method of Mulvey and Ruszczynski [71]. Methods that employ fork-join coordination are popular in other areas of scientific computing, as well. <p> Indirect methods essentially follow the fork-join protocol, by decomposing the original problem into decoupled first-level subproblems and then solving a master coordination problem. Eckstein [27] surveys parallel implementations to date. The Dantzig-Wolfe <ref> [19] </ref> and Benders [7] decomposition are two classic indirect methods. When applied to (CBA), both can be implemented in parallel, since each block can be assigned to a PE.
Reference: [20] <author> R. De Leone, M. Gaudioso, and M.-F. Monaco. </author> <title> Nonsmooth optimization methods for parallel decomposition of multicommodity flow problems. </title> <type> Technical Report 1080, </type> <institution> Computer Sciences Department, University of Wisconsin, </institution> <year> 1992. </year>
Reference-contexts: The master problem is non-smooth, and can be solved by a subgradient method (Geof-frion [39], Kennington and Helgason [56, chapter 6]), or a bundle method (Medhi [68], Ferris and Horn [31], De Leone et al. <ref> [20] </ref>.) In coarse-grain implementations of subproblem/master algorithms, the solution of the master 16 problem on a single processor is a potential bottleneck that can reduce parallel efficiency.
Reference: [21] <author> R. De Leone and O.L. Mangasarian. </author> <title> Asynchronous parallel successive overrelaxation for the symmetric linear complementarity problem. </title> <journal> Mathematical Programming, Series B, </journal> <volume> 42 </volume> <pages> 347-362, </pages> <year> 1988. </year>
Reference-contexts: Thus, in the recent mathematical programming literature, one also finds measurements of parallel efficiency based on the relative speedup (Mangasarian with Deleone <ref> [21] </ref> and Ferris [33].) In our efficiency estimates for the ADI algorithm in section 7.5.1.3, we will also adopt this approach. <p> One approach is to formulate the problem as a Linear Complementarity Problem (LCP) and then solve it by an algorithm amenable to parallelization, such as an iterative matrix-splitting scheme [18, chapter 5], <ref> [21] </ref>. <p> Another area of future research is the design of parallel ADI methods for other classes of important optimization problems such as the Linear Complementarity Problem; such methods may be similar to the successive over-relaxation schemes that have been successfully implemented in a parallel environment (De Leone and Mangasarian <ref> [21] </ref>).
Reference: [22] <author> R. De Leone, R.R. Meyer, S. Kontogiorgis, A. Zakarian, and G. Zakeri. </author> <title> Coordination methods in coarse-grained decomposition. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 4(4) </volume> <pages> 777-793, </pages> <year> 1994. </year>
Reference-contexts: The trade-off between simple and complex coordination is that an algorithm with a complex coordination phase may be able to utilize better the subproblem information, and thus converge in fewer iterations than a scheme with simple coordination. (For further discussion see De Leone et al. <ref> [22] </ref>. See also Ho et al. [50], [43] for an improvement of the parallel performance of Dantzig-Wolfe decomposition via load-balancing schemes). 1.6 Facts from convex analysis In this section we group some basic definitions and properties of convex functions and sets. We follow Robinson [83] in our presentation. <p> Also, for a fixed number of blocks, the parallel efficiency decreases, as the size of the block increases: as subproblems become larger, the solution time variability again increases. 7.5.1.4 Comparison to other decomposition methods Deleone presents in <ref> [22] </ref> an implementation of the Dantzig-Wolfe decomposition algorithm on the CM-5. In each iteration, a designated node solves a linear master subproblem to determine new prices, and then the remaining slave processors solve in parallel linear block-subproblems to determine new columns with negative reduced costs.
Reference: [23] <editor> J.J. Dongarra and W. Gentzsch, editors. </editor> <booktitle> Computer benchmarks: advances in parallel computing vol. </booktitle> <volume> 8. </volume> <publisher> Elsevier, </publisher> <year> 1993. </year> <month> 121 </month>
Reference-contexts: The collection <ref> [23] </ref> presents an overview of parallel benchmarking, including a discussion of performance metrics and descriptions of benchmark suites in use today. 1.3.3.2 Speedup and Amdahl's Law Let now T p stand for the execution time of a program on a system with p processors.
Reference: [24] <author> J. Douglas and H.H. Rachford Jr. </author> <title> On the numerical solution of heat conduction problems in two- and three-space variables. </title> <journal> Transactions of the American Mathematical Society, </journal> <volume> 82 </volume> <pages> 421-439, </pages> <year> 1956. </year>
Reference-contexts: The Progressive Hedging algorithm of Rockafellar and Wets [88, 102] is a version of the ADI algorithm applied to scenario analysis problems. When used to solve partial differential equations, the ADI method is usually called the Douglas-Rachford method, because it corresponds to the method in <ref> [24] </ref> by these two authors. This viewpoint is discussed in detail in [37], [41], [62], [25]. Brezis [14] provides a good textbook level introduction to the general theory of maximal monotone operators.
Reference: [25] <author> J. Eckstein. </author> <title> Splitting methods for monotone operators with applications to parallel optimization. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, Department of Civil Engineering, </institution> <year> 1989. </year>
Reference-contexts: The ADI method for optimization has been studied extensively and sufficient conditions for convergence have been obtained by using either the theory of maximal monotone operators (Lions and Mercier [62], Gabay [37], Eckstein and Bertsekas <ref> [25] </ref>, [28]) or the theory of saddle-points of Lagrangian functions (Glowinski with Marrocco, Fortin and Le Tallec [42], [34], [41]). The Progressive Hedging algorithm of Rockafellar and Wets [88, 102] is a version of the ADI algorithm applied to scenario analysis problems. <p> When used to solve partial differential equations, the ADI method is usually called the Douglas-Rachford method, because it corresponds to the method in [24] by these two authors. This viewpoint is discussed in detail in [37], [41], [62], <ref> [25] </ref>. Brezis [14] provides a good textbook level introduction to the general theory of maximal monotone operators. <p> In [29] Eckstein and Ferris construct Douglas-Rachford and Peaceman-Rachford operator splitting methods for the monotone linear complementarity problem. In his Ph.D. thesis <ref> [25] </ref> Eckstein demonstrates that, for the minimization problem shown above, some other decomposition algorithms, among them the algorithm of Han and Lou [47], Spingarn's Method of Partial Inverses [93], [94] and Golshtein's Block Method of Convex Programming [44], [45], are instances of the Douglas-Rachford algorithm. <p> For the finite-dimensional case, A is assumed to have full column rank <ref> [25] </ref>, [28] or G 1 is assumed to have compact level sets [11, chapter 3]; for the dual algorithm in [36], both primal and dual problems are assumed to be feasible and the solution set 21 of the primal is assumed to be bounded. <p> The method is named after the authors of [80], in which a related ADI method for solving partial differential equations is introduced. The correspondence is brought forth by Gabay [37] and by Glowinski and Le Tallec [41]. The following theorem is from Eckstein <ref> [25, pp. 123] </ref>. Theorem 2.6.2 (Peaceman-Rachford method for convex optimization) Assume that the problem (67) has a Kuhn-Tucker pair. <p> We found that the case reported in Figure 4 is typical for (MC) problems. Similar experience for a variety of applications is reported by Fukushima [36], Glowinski and Marrocco [34, chapter 5], 94 and Eckstein <ref> [25, chapter 7] </ref>. <p> it is not, in general feasible, for any given optimization problem, to determine a good value of the penalty a priori, one has to resort to ad hoc techniques or to heuristics which can allow initial large steps in both the primal and dual space, such as Eckstein's "twin lambda" <ref> [25, section 7.2.4] </ref>: in this heuristic the initial primal penalty is small, 0:1 , and is slowly increased to the final value , while the penalty used in the initial dual step is large, 10 , and is slowly decreased to the final value . <p> Eckstein <ref> [25] </ref>, [26] has also observed oscillations when applying the Douglas-Rachford method to monotropic network optimization. 7.5.1.3 Estimates of parallel efficiency and relative speedup Another topic of interest is processor utilization, i.e. the fraction of total CPU time that the processors spend on "useful" work, as opposed to busy-waiting for synchronization.
Reference: [26] <author> J. Eckstein. </author> <title> The alternating step method for monotropic programming on the Connection Machine CM-2. </title> <journal> ORSA Journal on Computing, </journal> <volume> 5(1) </volume> <pages> 293-318, </pages> <year> 1993. </year>
Reference-contexts: He then proceeds to derive ADI methods for monotropic programming; in <ref> [26] </ref> he discusses their fine-grain implementation on the Connection Machine CM-2 parallel computer. Fukushima [36] constructs an ADI method for the dual, rather than the primal problem. Cheng and Teboulle [17] present a modified ADI method, in which quadratic proximal terms replace the augmented Lagrangian penalty terms. <p> Eckstein [25], <ref> [26] </ref> has also observed oscillations when applying the Douglas-Rachford method to monotropic network optimization. 7.5.1.3 Estimates of parallel efficiency and relative speedup Another topic of interest is processor utilization, i.e. the fraction of total CPU time that the processors spend on "useful" work, as opposed to busy-waiting for synchronization.
Reference: [27] <author> J. Eckstein. </author> <title> Large-scale parallel computing, optimization, and operations research: a survey. </title> <journal> ORSA Computer Science Technical Section Newsletter, </journal> <volume> 14(2) </volume> <pages> 1-27, </pages> <year> 1993. </year>
Reference-contexts: Indirect methods essentially follow the fork-join protocol, by decomposing the original problem into decoupled first-level subproblems and then solving a master coordination problem. Eckstein <ref> [27] </ref> surveys parallel implementations to date. The Dantzig-Wolfe [19] and Benders [7] decomposition are two classic indirect methods. When applied to (CBA), both can be implemented in parallel, since each block can be assigned to a PE.
Reference: [28] <author> J. Eckstein and D.P. Bertsekas. </author> <title> On the Douglas-Rachford splitting method and the proximal point method for maximal monotone operators. </title> <journal> Mathematical Programming, Series A, </journal> <volume> 55(3) </volume> <pages> 293-318, </pages> <year> 1992. </year>
Reference-contexts: The ADI method for optimization has been studied extensively and sufficient conditions for convergence have been obtained by using either the theory of maximal monotone operators (Lions and Mercier [62], Gabay [37], Eckstein and Bertsekas [25], <ref> [28] </ref>) or the theory of saddle-points of Lagrangian functions (Glowinski with Marrocco, Fortin and Le Tallec [42], [34], [41]). The Progressive Hedging algorithm of Rockafellar and Wets [88, 102] is a version of the ADI algorithm applied to scenario analysis problems. <p> For the finite-dimensional case, A is assumed to have full column rank [25], <ref> [28] </ref> or G 1 is assumed to have compact level sets [11, chapter 3]; for the dual algorithm in [36], both primal and dual problems are assumed to be feasible and the solution set 21 of the primal is assumed to be bounded. <p> In the first variant the subproblems are solved inexactly and a relaxation parameter ! is added. The following theorem, describing the variant, is from Eckstein and Bertsekas <ref> [28, theorem 8] </ref>.
Reference: [29] <author> J. Eckstein and M. Ferris. </author> <title> Operator splitting methods for monotone linear complementarity problems. </title> <type> Technical Report TMC-239, </type> <institution> Thinking Machines Corporation, </institution> <year> 1992. </year>
Reference-contexts: In <ref> [29] </ref> Eckstein and Ferris construct Douglas-Rachford and Peaceman-Rachford operator splitting methods for the monotone linear complementarity problem. <p> We also observe that the Peaceman-Rachford algorithm requires, in general, slightly fewer iterations to converge than the Douglas-Rachford algorithm, yet the aggregate solution 114 corresponds to solid lines.) time is similar for both schemes. Eckstein and Ferris in <ref> [29] </ref> report that the Douglas-Rachford and Peaceman-Rachford schemes require roughly the same time to converge when applied to solving monotone linear complementarity problems.
Reference: [30] <author> M.C. Ferris. </author> <title> Parallel constraint distribution for convex quadratic programs. </title> <type> Technical Report 1009, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin, </institution> <year> 1991. </year> <note> To appear in Mathematics of Operations Research. </note>
Reference-contexts: We are thus interested in methods that generate master problems which are of small size and/or have an explicit solution, that can be computed in parallel. The Parallel Constraint Distribution method of Ferris and Mangasarian <ref> [30] </ref>, [32] displays such attractive features; it distributes the constraints among the PEs and modifies each subproblem objective function with augmented Lagrangian terms from other PEs. New Lagrangian information, arising from the solution of the subproblems, is aggregated on a master PE, and the cycle is repeated.
Reference: [31] <author> M.C. Ferris and J.D. Horn. </author> <title> Partitioning mathematical problems for parallel solution. </title> <type> Technical Report 1232, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin, </institution> <year> 1994. </year>
Reference-contexts: The master problem is non-smooth, and can be solved by a subgradient method (Geof-frion [39], Kennington and Helgason [56, chapter 6]), or a bundle method (Medhi [68], Ferris and Horn <ref> [31] </ref>, De Leone et al. [20].) In coarse-grain implementations of subproblem/master algorithms, the solution of the master 16 problem on a single processor is a potential bottleneck that can reduce parallel efficiency.
Reference: [32] <author> M.C. Ferris and O.L. Mangasarian. </author> <title> Parallel constraint distribution. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 1(4) </volume> <pages> 487-500, </pages> <year> 1991. </year>
Reference-contexts: Fork-Join coordination is quite popular in parallel optimization; it is employed by many decomposition algorithms such as: the Dantzig-Wolfe decomposition [19], the Schultz-Meyer shifted barrier 6 method [92], the Parallel Variable Distribution [33], and Parallel Constraint Distribution <ref> [32] </ref> algorithms of Ferris and Mangasarian, and the Diagonal Quadratic Approximation method of Mulvey and Ruszczynski [71]. Methods that employ fork-join coordination are popular in other areas of scientific computing, as well. <p> We are thus interested in methods that generate master problems which are of small size and/or have an explicit solution, that can be computed in parallel. The Parallel Constraint Distribution method of Ferris and Mangasarian [30], <ref> [32] </ref> displays such attractive features; it distributes the constraints among the PEs and modifies each subproblem objective function with augmented Lagrangian terms from other PEs. New Lagrangian information, arising from the solution of the subproblems, is aggregated on a master PE, and the cycle is repeated.
Reference: [33] <author> M.C. Ferris and O.L. Mangasarian. </author> <title> Parallel variable distribution. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 4(4) </volume> <pages> 815-832, </pages> <year> 1994. </year>
Reference-contexts: Fork-Join coordination is quite popular in parallel optimization; it is employed by many decomposition algorithms such as: the Dantzig-Wolfe decomposition [19], the Schultz-Meyer shifted barrier 6 method [92], the Parallel Variable Distribution <ref> [33] </ref>, and Parallel Constraint Distribution [32] algorithms of Ferris and Mangasarian, and the Diagonal Quadratic Approximation method of Mulvey and Ruszczynski [71]. Methods that employ fork-join coordination are popular in other areas of scientific computing, as well. <p> Thus, in the recent mathematical programming literature, one also finds measurements of parallel efficiency based on the relative speedup (Mangasarian with Deleone [21] and Ferris <ref> [33] </ref>.) In our efficiency estimates for the ADI algorithm in section 7.5.1.3, we will also adopt this approach. <p> New Lagrangian information, arising from the solution of the subproblems, is aggregated on a master PE, and the cycle is repeated. The Parallel Variable Distribution method by the same authors <ref> [33] </ref>, distributes the variables among the PEs, so that each PE has primary responsibility for updating its own block of variables while allowing the remaining ones to change in a restricted fashion.
Reference: [34] <author> M. Fortin and R. Glowinski, </author> <title> editors. Augmented Lagrangian methods: applications to the numerical solution of boundary-valued problems. </title> <publisher> North-Holland, </publisher> <year> 1983. </year>
Reference-contexts: for optimization has been studied extensively and sufficient conditions for convergence have been obtained by using either the theory of maximal monotone operators (Lions and Mercier [62], Gabay [37], Eckstein and Bertsekas [25], [28]) or the theory of saddle-points of Lagrangian functions (Glowinski with Marrocco, Fortin and Le Tallec [42], <ref> [34] </ref>, [41]). The Progressive Hedging algorithm of Rockafellar and Wets [88, 102] is a version of the ADI algorithm applied to scenario analysis problems. <p> In the existing literature convergence of the ADI algorithm for optimization is proved under rather strong assumptions on the problem: strict convexity and differentiability of G 1 and growth of G 1 faster than linear at infinity <ref> [34] </ref>, [41]. <p> The intention is to incorporate in the objective function of the second minimization the most recent information on the violation of the constraints. However, the resulting algorithm requires more stringent assumptions for convergence. It is also less robust in general, as Fortin and Glowinski point out <ref> [34] </ref>, citing experience in a variety of numerical analysis applications. The method is named after the authors of [80], in which a related ADI method for solving partial differential equations is introduced. The correspondence is brought forth by Gabay [37] and by Glowinski and Le Tallec [41]. <p> We have not been able to derive a variable penalty ADI method for the relaxation and the Peaceman-Rachford variants, because the proof technique employed in theorem 2.3.1 does not generalize to these variants. However, it might be possible to extend a laborious proof of Glowinski and Fortin <ref> [34, chapter3] </ref> to cover the case; this is a topic of future research. Finally, in section 4.10 we demonstrate the connection between the two variants and the Douglas-Rachford method. <p> We found that the case reported in Figure 4 is typical for (MC) problems. Similar experience for a variety of applications is reported by Fukushima [36], Glowinski and Marrocco <ref> [34, chapter 5] </ref>, 94 and Eckstein [25, chapter 7].
Reference: [35] <author> M. Frank and F. Wolfe. </author> <title> An algorithm for quadratic programming. </title> <journal> Naval Research Logistics Quarterly, </journal> <volume> 3 </volume> <pages> 95-110, </pages> <year> 1956. </year> <month> 122 </month>
Reference-contexts: In the case of problem (LQ), if we assume that the feasible set F is nonempty, we may guarantee the existence of a minimizer (and therefore, the existence of a saddlepoint) even when (some of) the upper bounds are +1, by the following theorem. Theorem 3.2.2 (Frank-Wolfe <ref> [35] </ref>) A quadratic function that is bounded below (resp. above) on a polyhedral set achieves its minimum (resp. maximum) on that set. For (LQ), each f [i] is bounded below on B [i] if, for instance, c [i] is non-negative, or if Q [i] is positive definite.
Reference: [36] <author> M. Fukushima. </author> <title> Application of the alternating direction method of multipliers to separable convex programming problems. </title> <journal> Computational Optimization and Applications, </journal> <volume> 1 </volume> <pages> 93-111, </pages> <year> 1992. </year>
Reference-contexts: He then proceeds to derive ADI methods for monotropic programming; in [26] he discusses their fine-grain implementation on the Connection Machine CM-2 parallel computer. Fukushima <ref> [36] </ref> constructs an ADI method for the dual, rather than the primal problem. Cheng and Teboulle [17] present a modified ADI method, in which quadratic proximal terms replace the augmented Lagrangian penalty terms. <p> For the finite-dimensional case, A is assumed to have full column rank [25], [28] or G 1 is assumed to have compact level sets [11, chapter 3]; for the dual algorithm in <ref> [36] </ref>, both primal and dual problems are assumed to be feasible and the solution set 21 of the primal is assumed to be bounded. <p> We found that the case reported in Figure 4 is typical for (MC) problems. Similar experience for a variety of applications is reported by Fukushima <ref> [36] </ref>, Glowinski and Marrocco [34, chapter 5], 94 and Eckstein [25, chapter 7].
Reference: [37] <author> D. Gabay. </author> <title> Applications of the method of multipliers to variational inequalities. </title> <editor> In M. Fortin and R. Glowinski, editors, </editor> <title> Augmented Lagrangian methods: applications to the numerical solution of boundary-valued problems. </title> <publisher> North-Holland, </publisher> <year> 1983. </year>
Reference-contexts: The ADI method for optimization has been studied extensively and sufficient conditions for convergence have been obtained by using either the theory of maximal monotone operators (Lions and Mercier [62], Gabay <ref> [37] </ref>, Eckstein and Bertsekas [25], [28]) or the theory of saddle-points of Lagrangian functions (Glowinski with Marrocco, Fortin and Le Tallec [42], [34], [41]). The Progressive Hedging algorithm of Rockafellar and Wets [88, 102] is a version of the ADI algorithm applied to scenario analysis problems. <p> When used to solve partial differential equations, the ADI method is usually called the Douglas-Rachford method, because it corresponds to the method in [24] by these two authors. This viewpoint is discussed in detail in <ref> [37] </ref>, [41], [62], [25]. Brezis [14] provides a good textbook level introduction to the general theory of maximal monotone operators. <p> The method is named after the authors of [80], in which a related ADI method for solving partial differential equations is introduced. The correspondence is brought forth by Gabay <ref> [37] </ref> and by Glowinski and Le Tallec [41]. The following theorem is from Eckstein [25, pp. 123]. Theorem 2.6.2 (Peaceman-Rachford method for convex optimization) Assume that the problem (67) has a Kuhn-Tucker pair.
Reference: [38] <author> M.R. Garey and D.S. Johnson. </author> <title> Computers and intractability: a guide to the theory of NP-Completeness. W.H. </title> <publisher> Freeman, </publisher> <year> 1979. </year>
Reference-contexts: In case there are more blocks than processors, we would make the assignment via a load-balancing scheme, such as a "bin packing" heuristic <ref> [38] </ref>. The feasible set in the first subproblem is just J Y B [i] and thus we would like to decompose the subproblem into J decoupled ones, each having B [i] as its feasible set, and an objective function involving only the block variables x [i] .
Reference: [39] <author> A.M. Geoffrion. </author> <title> Primal resource-directive approaches for optimizing nonlinear decomposable systems. </title> <journal> Operations Research, </journal> <pages> pages 375-403, </pages> <month> May-June </month> <year> 1970. </year>
Reference-contexts: The master problem is non-smooth, and can be solved by a subgradient method (Geof-frion <ref> [39] </ref>, Kennington and Helgason [56, chapter 6]), or a bundle method (Medhi [68], Ferris and Horn [31], De Leone et al. [20].) In coarse-grain implementations of subproblem/master algorithms, the solution of the master 16 problem on a single processor is a potential bottleneck that can reduce parallel efficiency.
Reference: [40] <author> P.E. Gill, S.J. Hammarling, W. Murray, M.A. Saunders, and M.H Wright. </author> <title> User's guide for LSSOL: a FORTRAN package for constrained linear least-squares and convex quadratic programming. </title> <type> Technical Report SOL 86-1R, </type> <institution> Stanford University, </institution> <year> 1987. </year>
Reference-contexts: MINOS 5.4 can treat matrices as sparse. Since there is considerable sparsity in the (MC) problems, we found this to be a significant advantage: computational experience with the LSSOL package <ref> [40] </ref>, which employs a specialized two-phase active-set method to solve quadratic problems but treats matrices as dense, has shown sparse MINOS 5.4 to be much faster than LSSOL for larger problems.
Reference: [41] <author> R. Glowinski and P. Le Tallec. </author> <title> Augmented Lagrangian and operator-splitting methods in nonlinear mechanics. </title> <institution> Society for Industrial and Applied Mathematics, </institution> <year> 1989. </year>
Reference-contexts: optimization has been studied extensively and sufficient conditions for convergence have been obtained by using either the theory of maximal monotone operators (Lions and Mercier [62], Gabay [37], Eckstein and Bertsekas [25], [28]) or the theory of saddle-points of Lagrangian functions (Glowinski with Marrocco, Fortin and Le Tallec [42], [34], <ref> [41] </ref>). The Progressive Hedging algorithm of Rockafellar and Wets [88, 102] is a version of the ADI algorithm applied to scenario analysis problems. <p> When used to solve partial differential equations, the ADI method is usually called the Douglas-Rachford method, because it corresponds to the method in [24] by these two authors. This viewpoint is discussed in detail in [37], <ref> [41] </ref>, [62], [25]. Brezis [14] provides a good textbook level introduction to the general theory of maximal monotone operators. <p> In the existing literature convergence of the ADI algorithm for optimization is proved under rather strong assumptions on the problem: strict convexity and differentiability of G 1 and growth of G 1 faster than linear at infinity [34], <ref> [41] </ref>. <p> Our proof is modeled after [11, chapter 3] and <ref> [41, chapter 3] </ref>. Theorem 2.3.1 (Convergence of ADI) Let assumptions 2.2.1 and 2.2.2 hold, and let f (x t ; z t ; p t )g be a sequence of iterates produced by the ADI algorithm (12)-(14). Let the symmetric positive definite matrices fH t g be ultimately fixed. <p> The method is named after the authors of [80], in which a related ADI method for solving partial differential equations is introduced. The correspondence is brought forth by Gabay [37] and by Glowinski and Le Tallec <ref> [41] </ref>. The following theorem is from Eckstein [25, pp. 123]. Theorem 2.6.2 (Peaceman-Rachford method for convex optimization) Assume that the problem (67) has a Kuhn-Tucker pair.
Reference: [42] <author> R. Glowinski and A. Marrocco. </author> <title> Sur l' approximation par elements finis d' ordre un, et la resolution par penalisation-dualite d' une classe de problemes de Dirichlet nonlineaires. </title> <journal> Revue Fran~caise d' Automatique, Informatique et Recherche Operationelle, </journal> <volume> 2 </volume> <pages> 41-76, </pages> <year> 1975. </year>
Reference-contexts: method for optimization has been studied extensively and sufficient conditions for convergence have been obtained by using either the theory of maximal monotone operators (Lions and Mercier [62], Gabay [37], Eckstein and Bertsekas [25], [28]) or the theory of saddle-points of Lagrangian functions (Glowinski with Marrocco, Fortin and Le Tallec <ref> [42] </ref>, [34], [41]). The Progressive Hedging algorithm of Rockafellar and Wets [88, 102] is a version of the ADI algorithm applied to scenario analysis problems.
Reference: [43] <author> S.K. Gnanendran and J.K. Ho. </author> <title> Load balancing in the parallel optimization of block-angular linear programs. </title> <journal> Mathematical Programming, </journal> <volume> 62 </volume> <pages> 41-67, </pages> <year> 1993. </year>
Reference-contexts: See also Ho et al. [50], <ref> [43] </ref> for an improvement of the parallel performance of Dantzig-Wolfe decomposition via load-balancing schemes). 1.6 Facts from convex analysis In this section we group some basic definitions and properties of convex functions and sets. We follow Robinson [83] in our presentation. The reader may also consult Rockafellar [85].
Reference: [44] <author> Ye. G. Golshtein. </author> <title> The block method of convex programming. </title> <journal> Soviet Mathematics Doklady, </journal> <volume> 33 </volume> <pages> 584-587, </pages> <year> 1986. </year>
Reference-contexts: In his Ph.D. thesis [25] Eckstein demonstrates that, for the minimization problem shown above, some other decomposition algorithms, among them the algorithm of Han and Lou [47], Spingarn's Method of Partial Inverses [93], [94] and Golshtein's Block Method of Convex Programming <ref> [44] </ref>, [45], are instances of the Douglas-Rachford algorithm. He then proceeds to derive ADI methods for monotropic programming; in [26] he discusses their fine-grain implementation on the Connection Machine CM-2 parallel computer. Fukushima [36] constructs an ADI method for the dual, rather than the primal problem.
Reference: [45] <author> Ye. G. Golshtein. </author> <title> A general approach to decomposition of optimization systems. </title> <journal> Soviet Journal of Computer and Systems Sciences, </journal> <volume> 25(3) </volume> <pages> 105-114, </pages> <year> 1987. </year>
Reference-contexts: In his Ph.D. thesis [25] Eckstein demonstrates that, for the minimization problem shown above, some other decomposition algorithms, among them the algorithm of Han and Lou [47], Spingarn's Method of Partial Inverses [93], [94] and Golshtein's Block Method of Convex Programming [44], <ref> [45] </ref>, are instances of the Douglas-Rachford algorithm. He then proceeds to derive ADI methods for monotropic programming; in [26] he discusses their fine-grain implementation on the Connection Machine CM-2 parallel computer. Fukushima [36] constructs an ADI method for the dual, rather than the primal problem.
Reference: [46] <author> J.L. Gustafson. </author> <title> Reevaluating Amdahl's law. </title> <journal> Communications of the ACM, </journal> <volume> 31(5) </volume> <pages> 532-533, </pages> <year> 1988. </year>
Reference-contexts: Thus, if s = 0, the speedup is almost linear, if we allow a small margin for communications overhead. Otherwise, if, say, 10% of the operations must be performed sequentially, then the maximum 8 speedup is 10. Over the years there have been re-evaluations <ref> [46] </ref> and extensions [103] of Amdahl's Law, in an effort to incorporate parallel environment factors, such as the process granularity, the synchronization overhead etc.
Reference: [47] <author> S.P Han and G. Lou. </author> <title> A parallel algorithms for a class of convex problems. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 26 </volume> <pages> 345-355, </pages> <year> 1988. </year> <month> 123 </month>
Reference-contexts: In [29] Eckstein and Ferris construct Douglas-Rachford and Peaceman-Rachford operator splitting methods for the monotone linear complementarity problem. In his Ph.D. thesis [25] Eckstein demonstrates that, for the minimization problem shown above, some other decomposition algorithms, among them the algorithm of Han and Lou <ref> [47] </ref>, Spingarn's Method of Partial Inverses [93], [94] and Golshtein's Block Method of Convex Programming [44], [45], are instances of the Douglas-Rachford algorithm. He then proceeds to derive ADI methods for monotropic programming; in [26] he discusses their fine-grain implementation on the Connection Machine CM-2 parallel computer.
Reference: [48] <author> J.L. Hennessy and D.A. Patterson. </author> <title> Computer architecture: a quantitative approach. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: systems; it consists of a careful organization of forward and backward steps of Gaussian elimination, the majority of which are carried out in parallel, in fork phases, and the rest are carried out in join phases. 1.3.3 Metrics for Parallel Performance 1.3.3.1 Execution time As argued by Hennessy and Patterson <ref> [48] </ref>, the most reliable performance metric for any computer system is program execution time; it incorporates several other measures of goodness, that, when taken independently, do not portray accurately the overall performance.
Reference: [49] <author> W.D. Hillis and L.W. Tucker. </author> <title> The CM-5 Connection Machine: a scalable supercomputer. </title> <journal> Communications of the ACM, </journal> <volume> 36(11) </volume> <pages> 31-40, </pages> <year> 1993. </year>
Reference-contexts: The article by Best et al. [12] presents the parallel I/O modes of the machine, while the article by Hillis and Tucker <ref> [49] </ref> provides a general overview and presents examples of current applications of the CM-5.
Reference: [50] <author> J.K. Ho, T.C. Lee, and R.P. Sundarraj. </author> <title> Decomposition of linear programs using parallel computation. </title> <journal> Mathematical Programming, Series B, </journal> <volume> 42 </volume> <pages> 391-406, </pages> <year> 1988. </year>
Reference-contexts: See also Ho et al. <ref> [50] </ref>, [43] for an improvement of the parallel performance of Dantzig-Wolfe decomposition via load-balancing schemes). 1.6 Facts from convex analysis In this section we group some basic definitions and properties of convex functions and sets. We follow Robinson [83] in our presentation. The reader may also consult Rockafellar [85].
Reference: [51] <author> K. Hwang. </author> <title> Advanced computer architecture with parallel programming. </title> <publisher> McGraw-Hill, </publisher> <year> 1993. </year>
Reference-contexts: There is extensive experimentation going on in the design and configuration of both the processors and the interconnection network. For a general introduction to parallel computation one may consult, among others, the books by Hwang <ref> [51] </ref> and Lawson [60], on the design of hardware, and by Akl [3] and Bertsekas and Tsitsiklis [11], on the design of parallel scientific algorithms. 3 Although parallel programming is still far from being standardized, the basic desirable parallel computing modes have been characterized.
Reference: [52] <author> S.L. Johnsson, Y. Saad, and M. Schultz. </author> <title> Alternating direction methods on multiprocessors. </title> <journal> SIAM journal of Scientific and Statistical Computing, </journal> <volume> 8(5) </volume> <pages> 686-700, </pages> <year> 1987. </year>
Reference-contexts: connection is that fixed-point iteration on the resolvent of the subdifferential operator is equivalent to the proximal point algorithm. (See also Rockafellar [86], [87] on this.) For a general theoretical discussion of ADI and other splitting methods for the solution of partial differential equations, see Marchuk [66]; Johnsson et al. <ref> [52] </ref> discuss parallel implementations. In [29] Eckstein and Ferris construct Douglas-Rachford and Peaceman-Rachford operator splitting methods for the monotone linear complementarity problem.
Reference: [53] <author> P. </author> <title> Kall. Stochastic linear programming. </title> <publisher> Springer Verlag, </publisher> <year> 1976. </year>
Reference-contexts: Our presentation of the scenario analysis problem is based on material in Wets [102] and Ruszczynski [89]. For a theoretical treatment of Stochastic Linear Programming one may consult the book by Kall <ref> [53] </ref>. 14 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 x x x x x x x x x x x x x x x x 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 five rows are
Reference: [54] <author> P.V. Kamesam and R.R. Meyer. </author> <title> Multipoint methods for separable nonlinear networks. </title> <journal> Mathematical Programming Study, </journal> <volume> 22 </volume> <pages> 185-205, </pages> <year> 1984. </year>
Reference-contexts: This enables the code to handle much more general convex block-angular problems. In the case of (MC) one might be able to solve the quadratic network subproblems faster by employing specialized algorithms, such as the two-segment linearization simplex method of Meyer and Kamesam <ref> [54] </ref> or the quasi-Newton method of Toint and Tuyttens [100], [99]. Table 1 displays the characteristics of the test problems. Five instances were randomly generated for each case; the table reports average characteristics.
Reference: [55] <author> J.L. Kennington. </author> <title> A survey of linear cost multicommodity network flows. </title> <journal> Operations Research, </journal> <volume> 26 </volume> <pages> 209-236, </pages> <year> 1978. </year>
Reference-contexts: The multicommodity network flow problem has been studied extensively, and specialized algorithms have been developed for it over the years. A good coverage of these is provided in the survey articles by Kennington <ref> [55] </ref> and Assad [6], and in the books by Kennigton and Helgason [56] and by 12 Ahuja, Magnanti and Orlin [2]. 1.4.3 Scenario Analysis In some cases of modeling there is uncertainty about the value of some of the parameters of the system.
Reference: [56] <author> J.L. Kennington and R.V. Helgason. </author> <title> Algorithms for network programming. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1980. </year>
Reference-contexts: The multicommodity network flow problem has been studied extensively, and specialized algorithms have been developed for it over the years. A good coverage of these is provided in the survey articles by Kennington [55] and Assad [6], and in the books by Kennigton and Helgason <ref> [56] </ref> and by 12 Ahuja, Magnanti and Orlin [2]. 1.4.3 Scenario Analysis In some cases of modeling there is uncertainty about the value of some of the parameters of the system. <p> The master problem is non-smooth, and can be solved by a subgradient method (Geof-frion [39], Kennington and Helgason <ref> [56, chapter 6] </ref>), or a bundle method (Medhi [68], Ferris and Horn [31], De Leone et al. [20].) In coarse-grain implementations of subproblem/master algorithms, the solution of the master 16 problem on a single processor is a potential bottleneck that can reduce parallel efficiency.
Reference: [57] <author> D. Klingman, A Napier, and J. Stutz. NETGEN: </author> <title> a program for generating large scale capaci-tated assignment, transportation and minimum cost network problems. </title> <journal> Management Science, </journal> <volume> 20(8) </volume> <pages> 814-821, </pages> <year> 1974. </year>
Reference-contexts: For details the interested reader can also refer to the CM-5 technical summary [96] and the CMMD library guide [97]. 90 7.3 The suite of test problems To assess the relative performance of the three splittings we used MNETGEN [4], a derivative of NETGEN <ref> [57] </ref>, to generate three hundred random multicommodity network problems (MC), one hundred with linear objectives and two hundred with quadratic objective functions.
Reference: [58] <author> J.S. Kowalik and S.P. Kumar. </author> <title> Parallel algorithms for recurrence and tridiagonal equations. </title> <editor> In J.S. Kowalik, editor, </editor> <title> Parallel MIMD computation: </title> <booktitle> the HEP supercomputer and its applications, </booktitle> <pages> pages 295-307. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1985. </year>
Reference-contexts: Methods that employ fork-join coordination are popular in other areas of scientific computing, as well. For instance, Kowalik and Kumar present in <ref> [58] </ref> such a method for solving tridiagonal linear systems; it consists of a careful organization of forward and backward steps of Gaussian elimination, the majority of which are carried out in parallel, in fork phases, and the rest are carried out in join phases. 1.3.3 Metrics for Parallel Performance 1.3.3.1 Execution
Reference: [59] <author> L.S Lasdon. </author> <title> Optimization theory for large systems. </title> <publisher> MacMillan, </publisher> <year> 1970. </year>
Reference-contexts: 6 6 6 6 6 6 6 x x x x x x x x x x x x x x x x 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 five rows are the non-anticipativity constraints. 15 1.5 Previous work Following Lasdon <ref> [59] </ref>, we classify methods for solving large, structured mathematical problems, such as (CBA), into direct and indirect. Direct methods specialize an existing algorithm to take advantage of problem structure.
Reference: [60] <editor> H.W. Lawson. </editor> <booktitle> Parallel processing in industrial real-time applications. </booktitle> <publisher> Prentice-Hall Inc., </publisher> <year> 1992. </year>
Reference-contexts: There is extensive experimentation going on in the design and configuration of both the processors and the interconnection network. For a general introduction to parallel computation one may consult, among others, the books by Hwang [51] and Lawson <ref> [60] </ref>, on the design of hardware, and by Akl [3] and Bertsekas and Tsitsiklis [11], on the design of parallel scientific algorithms. 3 Although parallel programming is still far from being standardized, the basic desirable parallel computing modes have been characterized.
Reference: [61] <author> C.E. Leiserson et al. </author> <title> The network architecture of the Connection Machine CM-5. </title> <booktitle> In Proceedings of the 4th Annual ACM Symposium on Parallel Algorithms and Architectures. ACM, </booktitle> <year> 1992. </year> <month> 124 </month>
Reference-contexts: The machine is currently running the CMOST Version 7.3 Beta.2.7 operating system. For in-terprocessor communication we have employed version 3.1 of the CMMD message-passing library, which includes the vector reduce functions. Our description of the CM-5 architecture is based on the article of Leiserson et al. <ref> [61] </ref>, which also discusses the philosophy behind the design of the interconnection networks. The article by Best et al. [12] presents the parallel I/O modes of the machine, while the article by Hillis and Tucker [49] provides a general overview and presents examples of current applications of the CM-5.
Reference: [62] <author> P.L. Lions and B. Mercier. </author> <title> Splitting algorithms for the sum of two nonlinear operators. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 16(6) </volume> <pages> 964-979, </pages> <year> 1979. </year>
Reference-contexts: The ADI method for optimization has been studied extensively and sufficient conditions for convergence have been obtained by using either the theory of maximal monotone operators (Lions and Mercier <ref> [62] </ref>, Gabay [37], Eckstein and Bertsekas [25], [28]) or the theory of saddle-points of Lagrangian functions (Glowinski with Marrocco, Fortin and Le Tallec [42], [34], [41]). The Progressive Hedging algorithm of Rockafellar and Wets [88, 102] is a version of the ADI algorithm applied to scenario analysis problems. <p> When used to solve partial differential equations, the ADI method is usually called the Douglas-Rachford method, because it corresponds to the method in [24] by these two authors. This viewpoint is discussed in detail in [37], [41], <ref> [62] </ref>, [25]. Brezis [14] provides a good textbook level introduction to the general theory of maximal monotone operators. <p> The method works towards achieving optimality in both the primal and the dual space, by taking alternating steps in each. For strongly convex problems satisfying a Lipschitz condition it can be proven that the rate of convergence is linear <ref> [62] </ref>. In order to speed convergence, we vary the penalty per iteration, by a heuristic procedure, that may be tuned to the application at hand. An even more general approach is to use a separate value of for each linear constraint, also varying per iteration.
Reference: [63] <author> O.L. Mangasarian. </author> <title> Nonlinear programming. </title> <publisher> McGraw-Hill, </publisher> <year> 1969. </year>
Reference-contexts: Proof: By arguments similar to those in <ref> [63, section 5.3.1] </ref>, both x 1 and x 2 solve min (x) subject to g (x) = 0 and therefore (x 1 ) = (x 2 ) and g (x 1 ) = g (x 2 ) = 0.
Reference: [64] <author> O.L. Mangasarian. </author> <title> Solution of symmetric linear complementarity problems by iterative methods. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 22(4) </volume> <pages> 465-485, </pages> <year> 1977. </year>
Reference-contexts: ADI methods can be constructed also for the Linear Complementarity Problem LCP (M , q) where the matrix M is symmetric positive semidefinite. Then the LCP is equivalent to the quadratic problem <ref> [64] </ref> min 1 x T M x + q T x (8) Let M be split as M = B + C.
Reference: [65] <author> O.L. Mangasarian. </author> <title> Convergence of iterates of an inexact matrix splitting algorithm for the symmetric monotone linear complementarity problem. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 1(1) </volume> <pages> 114-122, </pages> <year> 1991. </year>
Reference-contexts: The convergence theory for ADI, presented in the following sections, requires that both quadratic terms in the objective be convex, i.e. that both B and C matrices be positive semidefinite. In comparison, the "regular" splittings for the LCP in (8) (see <ref> [65] </ref>, [18, chapter 5]) require that the matrix B C be positive definite. 2.2 Description of the algorithm We want to solve the following linearly-constrained convex problem min G 1 (x) + G 2 (z) subject to Ax + b = Bz (9) where G 1 : IR n ! IR
Reference: [66] <author> G.I. </author> <title> Marchuk. Splitting and alternating direction methods. </title> <editor> In P.G. Ciarlet and J.L. Lions, editors, </editor> <title> Handbook of numerical analysis. </title> <publisher> North-Holland, </publisher> <address> Berlin, </address> <year> 1990. </year>
Reference-contexts: an optimization viewpoint; the connection is that fixed-point iteration on the resolvent of the subdifferential operator is equivalent to the proximal point algorithm. (See also Rockafellar [86], [87] on this.) For a general theoretical discussion of ADI and other splitting methods for the solution of partial differential equations, see Marchuk <ref> [66] </ref>; Johnsson et al. [52] discuss parallel implementations. In [29] Eckstein and Ferris construct Douglas-Rachford and Peaceman-Rachford operator splitting methods for the monotone linear complementarity problem.
Reference: [67] <author> E.W. Mayr. </author> <title> Basic parallel algorithms in graph theory. </title> <editor> In G. Tinhofer, E.W. Mayr, H. Nolten-meier, and M.M Syslo, editors, </editor> <booktitle> Computational graph theory, </booktitle> <pages> pages 69-91. </pages> <publisher> Springer Verlag, </publisher> <address> Vienna, </address> <year> 1990. </year>
Reference-contexts: There is ongoing theoretical work on deriving models for parallel computation, and on deriving efficient algorithms on these models. Success has been partial so far. We present here some of the basics of parallel complexity models, following the survey of Mayr <ref> [67] </ref>, which also has a sizeable bibliography. 1.3.3.3 Complexity-Theoretic Parallel Computation The basic computing model is the Parallel Random Access Machine, or PRAM, which consists of an unbounded number of computing cells, which are Random Access Machines [1, chapter 1], and 9 an unbounded number of global, shared memory cells. <p> However, for depth-first search in a general graph, no N C algorithms are currently known, and it might be that the problem is inherently sequential. (For a detailed discussion see Mayr <ref> [67] </ref>). 1.4 The block-angular problem 1.4.1 Description Our object of study is a convex minimization problem with linear constraints having a specific block structure in both the objective function and the constraint matrix.
Reference: [68] <author> D. Medhi. </author> <title> Parallel bundle-based decomposition for large-scale structured mathematical programming problems. </title> <journal> Annals of Operation Research, </journal> <volume> 22 </volume> <pages> 101-127, </pages> <year> 1990. </year>
Reference-contexts: The master problem is non-smooth, and can be solved by a subgradient method (Geof-frion [39], Kennington and Helgason [56, chapter 6]), or a bundle method (Medhi <ref> [68] </ref>, Ferris and Horn [31], De Leone et al. [20].) In coarse-grain implementations of subproblem/master algorithms, the solution of the master 16 problem on a single processor is a potential bottleneck that can reduce parallel efficiency. <p> Table 6 presents timing results on a subset of our linear (MC) test problems. Comparison with Table 2 shows that, for the three largest of these test problems, both the (AP) and (RP) splittings are faster than Dantzig-Wolfe by a factor of 2.6 to 4.5. Medhi in <ref> [68] </ref> uses a complex coordinator based on the bundle method to solve (CBA) problems. It is difficult to compare our results to his, since Medhi's test problems are randomly generated and have few (10 - 50) coupling constraints.
Reference: [69] <author> A. Miele, P.E. Moseley, A.V. Levy, and G.M. Coggins. </author> <title> On the method of multipliers for mathematical programming problems. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 10(1) </volume> <pages> 1-33, </pages> <year> 1972. </year>
Reference-contexts: The best results for this class of problems were obtained with an updating scheme similar to the ones used in the method of multipliers for the Augmented Lagrangian algorithm: a penalty parameter is increased when the violation in the corresponding coupling constraint is not sufficiently reduced <ref> [69] </ref>, [10, chapter 2], [78]. In our implementation all penalty parameters remain within positive bounds, as required by the convergence theory. In the initial iterations we allow for reduced accuracy in solving the subproblems, and also employ over-relaxation. (ARP) : The penalty factors are updated every T = 15 iterations.
Reference: [70] <author> J.M. Mulvey and A. Ruszczynski. </author> <title> A new scenario decomposition method for large-scale stochastic optimization. </title> <type> Technical Report SOR 91-19, </type> <institution> Princeton University, </institution> <year> 1991. </year>
Reference-contexts: ) subject to x l 2 C l ; l = 1; : : :; L; x t j for all j 2 A (i; t); j = 1; : : :; L; t = 1; : : : ; T The non-anticipativity constraints can also be modeled as follows <ref> [70] </ref>: suppose that at stage t scenarios s 1 ; s 2 ; : : : ; s k have common past and present data.
Reference: [71] <author> J.M. Mulvey and A. Ruszczynski. </author> <title> A diagonal quadratic approximation method for large scale linear programs. </title> <journal> Operations Research Letters, </journal> <volume> 12 </volume> <pages> 205-215, </pages> <year> 1992. </year>
Reference-contexts: quite popular in parallel optimization; it is employed by many decomposition algorithms such as: the Dantzig-Wolfe decomposition [19], the Schultz-Meyer shifted barrier 6 method [92], the Parallel Variable Distribution [33], and Parallel Constraint Distribution [32] algorithms of Ferris and Mangasarian, and the Diagonal Quadratic Approximation method of Mulvey and Ruszczynski <ref> [71] </ref>. Methods that employ fork-join coordination are popular in other areas of scientific computing, as well. <p> In this general framework one can place the logarithmic shifted barrier algorithm of Schultz and Meyer [91], [92], the smooth linear-quadratic penalty method of Zenios, Pinar and Dembo [107], [81], and the Diagonal Quadratic Approximation method of Mulvey and Ruszczynski <ref> [71] </ref>, [89]. Several other decomposition methods take advantage of the matrix structure of the problem. <p> would include the Jacobi-like term (D [i] x [i] ) D [j] x t The application of linearization to the Augmented Lagrangian is one of the key ideas in the approximate method of multipliers of Stephanopoulos and Westerberg [95] and in the Diagonal Quadratic Approximation method of Mulvey and Ruszczynski <ref> [71] </ref>. Chapter 4 The Activity-and-Resource Proximization splitting (ARP) 4.1 Overview In this chapter we study the Activity-and-Resource Proximization splitting (ARP), the first of the three block-decomposable splittings for the convex block-angular problem (CBA). <p> Then, the relative performance of the three splittings on scenario problems has to be investigated, as well as their comparison with other specialized algorithms, such as the Diagonal Quadratic Approximation (Mulvey and Ruszczynski <ref> [71] </ref>) and Progressive Hedging (Rockafellar and Wets [88, 102], Mulvey and Vladimirou [72, 73, 74]).
Reference: [72] <author> J.M. Mulvey and H. Vladimirou. </author> <title> Evaluation of a parallel hedging algorithm for stochastic network programming. </title> <editor> In R. Sharda et al., editors, </editor> <booktitle> Impacts of recent computer advances on operations research. </booktitle> <publisher> North-Holland, </publisher> <year> 1989. </year>
Reference-contexts: Then, the relative performance of the three splittings on scenario problems has to be investigated, as well as their comparison with other specialized algorithms, such as the Diagonal Quadratic Approximation (Mulvey and Ruszczynski [71]) and Progressive Hedging (Rockafellar and Wets [88, 102], Mulvey and Vladimirou <ref> [72, 73, 74] </ref>).
Reference: [73] <author> J.M. Mulvey and H. Vladimirou. </author> <title> Solving multistage stochastic networks: an application of scenario aggregation. </title> <journal> Networks, </journal> <volume> 21(6) </volume> <pages> 619-643, </pages> <year> 1990. </year> <month> 125 </month>
Reference-contexts: Then, the relative performance of the three splittings on scenario problems has to be investigated, as well as their comparison with other specialized algorithms, such as the Diagonal Quadratic Approximation (Mulvey and Ruszczynski [71]) and Progressive Hedging (Rockafellar and Wets [88, 102], Mulvey and Vladimirou <ref> [72, 73, 74] </ref>).
Reference: [74] <author> J.M. Mulvey and H. Vladimirou. </author> <title> Stochastic network programming for financial planning problems. </title> <journal> Management Science, </journal> <volume> 38(11) </volume> <pages> 1642-1664, </pages> <year> 1992. </year>
Reference-contexts: Then, the relative performance of the three splittings on scenario problems has to be investigated, as well as their comparison with other specialized algorithms, such as the Diagonal Quadratic Approximation (Mulvey and Ruszczynski [71]) and Progressive Hedging (Rockafellar and Wets [88, 102], Mulvey and Vladimirou <ref> [72, 73, 74] </ref>).
Reference: [75] <author> W. Murray. </author> <title> Methods for large-scale linear programming. In Algorithms and model formulations in mathematical programming, </title> <booktitle> NATO ASI Series F vol. </booktitle> <volume> 51, </volume> <pages> pages 115-137. </pages> <publisher> Springer Verlag, </publisher> <year> 1989. </year>
Reference-contexts: efficient sparse solver may be developed by employing algorithms such as those described in Bjorck [13, sections 25-27]. (Fast sparse least-squares solvers are needed as building blocks in other mathematical programming algorithms, as well; for instance, for calculating a Newton direction in logarithmic barrier methods for linear programs, see Murray <ref> [75, section 7] </ref>.) For the multicom-modity network problem, in particular, specialized solvers that also take into account the network structure can be developed; flow push heuristics can be used to determine a good, feasible initial point.
Reference: [76] <author> B.A. Murtagh and M.A. Saunders. </author> <title> MINOS 5.1 user's guide. </title> <type> Technical Report SOL 83.20R, </type> <institution> Stanford University, </institution> <year> 1987. </year>
Reference-contexts: As we showed in lemma 4.4.2, the v t+1 [i] vectors are optimal duals for subproblem (85). Standard optimization packages, such as MINOS <ref> [76, 77] </ref>, which can be employed to solve this subproblem, return both primal and dual vectors at optimality, and thus we can have the v t+1 [i] vectors available at no extra computational cost.
Reference: [77] <author> B.A. Murtagh and M.A. Saunders. </author> <title> MINOS 5.4 release notes, appendix to MINOS 5.1 user's guide. </title> <type> Technical report, </type> <institution> Stanford University, </institution> <year> 1992. </year>
Reference-contexts: As we showed in lemma 4.4.2, the v t+1 [i] vectors are optimal duals for subproblem (85). Standard optimization packages, such as MINOS <ref> [76, 77] </ref>, which can be employed to solve this subproblem, return both primal and dual vectors at optimality, and thus we can have the v t+1 [i] vectors available at no extra computational cost. <p> The solution of the quadratic block subproblems was obtained using the standard optimization package MINOS 5.4 ([76], <ref> [77] </ref>). MINOS 5.4 uses a reduced gradient method to solve problems with nonlinear objective and linear constraints.
Reference: [78] <author> R.J. O' Doherty and B.L. Pierson. </author> <title> A numerical study of augmented penalty function algorithms for terminally constrained optimal control problems. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 14(4) </volume> <pages> 393-403, </pages> <year> 1974. </year>
Reference-contexts: The best results for this class of problems were obtained with an updating scheme similar to the ones used in the method of multipliers for the Augmented Lagrangian algorithm: a penalty parameter is increased when the violation in the corresponding coupling constraint is not sufficiently reduced [69], [10, chapter 2], <ref> [78] </ref>. In our implementation all penalty parameters remain within positive bounds, as required by the convergence theory. In the initial iterations we allow for reduced accuracy in solving the subproblems, and also employ over-relaxation. (ARP) : The penalty factors are updated every T = 15 iterations.
Reference: [79] <author> P.C. Patton. </author> <title> Performance limits for parallel processors. In G.F. </title> <editor> Carey, editor, </editor> <booktitle> Parallel supercomputing: methods, algorithms and applications, </booktitle> <pages> pages 1-16. </pages> <publisher> John Wiley & Sons, </publisher> <year> 1989. </year>
Reference-contexts: We define the (absolute) speedup <ref> [79] </ref>, [104], [11, chapter 1] as the ratio s fl T fl (1) 7 the relative speedup s p as the ratio s p := T p and the efficiency p as p := p (3) The value of p is usually less that one, indicating efficiency loss due to a <p> By Amdahl's Law, the speedup in this case cannot exceed 3, and this was corroborated by observations in practice. Amdahl himself (as quoted in <ref> [79] </ref>) has recently remarked that the great majority of problems in technical computing exhibit 50 70% concurrency available for exploitation by either a vector or a parallel architecture; only few exhibit concurrency of 85 94%, and this results from an intrinsic physical or logical decomposability of their structure.
Reference: [80] <author> D.W. Peaceman and H.H. Rachford Jr. </author> <title> The numerical solution of parabolic and elliptic differential equations. </title> <journal> SIAM Journal, </journal> <volume> 3 </volume> <pages> 28-42, </pages> <year> 1955. </year>
Reference-contexts: However, the resulting algorithm requires more stringent assumptions for convergence. It is also less robust in general, as Fortin and Glowinski point out [34], citing experience in a variety of numerical analysis applications. The method is named after the authors of <ref> [80] </ref>, in which a related ADI method for solving partial differential equations is introduced. The correspondence is brought forth by Gabay [37] and by Glowinski and Le Tallec [41]. The following theorem is from Eckstein [25, pp. 123].
Reference: [81] <author> M.C~ . Pinar and S.A. Zenios. </author> <title> Parallel decomposition of multicommodity network flows using a linear-quadratic penalty algorithm. </title> <journal> ORSA Journal on Computing, </journal> <volume> 4(3) </volume> <pages> 235-248, </pages> <year> 1992. </year>
Reference-contexts: In this general framework one can place the logarithmic shifted barrier algorithm of Schultz and Meyer [91], [92], the smooth linear-quadratic penalty method of Zenios, Pinar and Dembo [107], <ref> [81] </ref>, and the Diagonal Quadratic Approximation method of Mulvey and Ruszczynski [71], [89]. Several other decomposition methods take advantage of the matrix structure of the problem.
Reference: [82] <author> B.T. Polyak. </author> <title> Introduction to optimization. Optimization Software, </title> <publisher> Inc., Publications Division, </publisher> <year> 1987. </year>
Reference-contexts: A characteristic of the method is convergence to the optimal value of the objective function even if the primal iterates do not converge. (This is a common occurrence in optimization algorithms; e.g. the subgradient method for unconstrained nondifferentiable optimization in Polyak <ref> [82, section 5.3.2] </ref>.) We present an example illustrating this mode of convergence for the ADI method in section 2.4. Then we provide, in section 2.5, a sequence of corollaries that deal with special cases of interest 19 20 and with an implementable stopping criterion.
Reference: [83] <author> S.M. Robinson. </author> <title> Lectures on convex analysis. </title> <address> Madison, Wisconsin, </address> <year> 1978. </year>
Reference-contexts: See also Ho et al. [50], [43] for an improvement of the parallel performance of Dantzig-Wolfe decomposition via load-balancing schemes). 1.6 Facts from convex analysis In this section we group some basic definitions and properties of convex functions and sets. We follow Robinson <ref> [83] </ref> in our presentation. The reader may also consult Rockafellar [85]. We are concerned exclusively with sets and functions defined on real, finite-dimensional Hilbert spaces.
Reference: [84] <author> S.M. Robinson. </author> <title> Lecture notes on maximal monotone operators, </title> <type> IE 823. </type> <institution> Madison, Wisconsin, </institution> <year> 1990. </year>
Reference-contexts: This viewpoint is discussed in detail in [37], [41], [62], [25]. Brezis [14] provides a good textbook level introduction to the general theory of maximal monotone operators. Robinson <ref> [84] </ref> presents the theory from an optimization viewpoint; the connection is that fixed-point iteration on the resolvent of the subdifferential operator is equivalent to the proximal point algorithm. (See also Rockafellar [86], [87] on this.) For a general theoretical discussion of ADI and other splitting methods for the solution of partial
Reference: [85] <author> R.T. Rockafellar. </author> <title> Convex analysis. </title> <publisher> Princeton University Press, </publisher> <year> 1970. </year>
Reference-contexts: We follow Robinson [83] in our presentation. The reader may also consult Rockafellar <ref> [85] </ref>. We are concerned exclusively with sets and functions defined on real, finite-dimensional Hilbert spaces. Definition 1.6.1 A set C IR n is convex if for each x and y in C and each 2 (0; 1), one has that (1 )x + y 2 C.
Reference: [86] <author> R.T. Rockafellar. </author> <title> Augmented lagrangians and applications of the proximal point algorithm in convex programming. </title> <journal> Mathematics of Operations Research, </journal> <volume> 1 </volume> <pages> 97-116, </pages> <year> 1976. </year> <month> 126 </month>
Reference-contexts: Brezis [14] provides a good textbook level introduction to the general theory of maximal monotone operators. Robinson [84] presents the theory from an optimization viewpoint; the connection is that fixed-point iteration on the resolvent of the subdifferential operator is equivalent to the proximal point algorithm. (See also Rockafellar <ref> [86] </ref>, [87] on this.) For a general theoretical discussion of ADI and other splitting methods for the solution of partial differential equations, see Marchuk [66]; Johnsson et al. [52] discuss parallel implementations. In [29] Eckstein and Ferris construct Douglas-Rachford and Peaceman-Rachford operator splitting methods for the monotone linear complementarity problem.
Reference: [87] <author> R.T. Rockafellar. </author> <title> Monotone operators and the proximal point algorithm. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 14 </volume> <pages> 887-898, </pages> <year> 1976. </year>
Reference-contexts: Brezis [14] provides a good textbook level introduction to the general theory of maximal monotone operators. Robinson [84] presents the theory from an optimization viewpoint; the connection is that fixed-point iteration on the resolvent of the subdifferential operator is equivalent to the proximal point algorithm. (See also Rockafellar [86], <ref> [87] </ref> on this.) For a general theoretical discussion of ADI and other splitting methods for the solution of partial differential equations, see Marchuk [66]; Johnsson et al. [52] discuss parallel implementations. In [29] Eckstein and Ferris construct Douglas-Rachford and Peaceman-Rachford operator splitting methods for the monotone linear complementarity problem.
Reference: [88] <author> R.T. Rockafellar and R. J.-B. Wets. </author> <title> Scenarios and policy aggregation in optimization under uncertainty. </title> <journal> Mathematics of Operations Research, </journal> <volume> 16 </volume> <pages> 119-147, </pages> <year> 1991. </year>
Reference-contexts: The Progressive Hedging algorithm of Rockafellar and Wets <ref> [88, 102] </ref> is a version of the ADI algorithm applied to scenario analysis problems. When used to solve partial differential equations, the ADI method is usually called the Douglas-Rachford method, because it corresponds to the method in [24] by these two authors. <p> Then, the relative performance of the three splittings on scenario problems has to be investigated, as well as their comparison with other specialized algorithms, such as the Diagonal Quadratic Approximation (Mulvey and Ruszczynski [71]) and Progressive Hedging (Rockafellar and Wets <ref> [88, 102] </ref>, Mulvey and Vladimirou [72, 73, 74]).
Reference: [89] <author> A. Ruszczynski. </author> <title> Augmented lagrangian decomposition for sparse convex optimization. </title> <note> Working Paper WP 92-75, IIASA, </note> <month> April </month> <year> 1993. </year>
Reference-contexts: This formulation results in coupling matrices that are quite sparse: for a three-stage problem, Figure 2 shows the decision tree, and Figure 3 shows the structure of the constraint matrix. Our presentation of the scenario analysis problem is based on material in Wets [102] and Ruszczynski <ref> [89] </ref>. <p> In this general framework one can place the logarithmic shifted barrier algorithm of Schultz and Meyer [91], [92], the smooth linear-quadratic penalty method of Zenios, Pinar and Dembo [107], [81], and the Diagonal Quadratic Approximation method of Mulvey and Ruszczynski [71], <ref> [89] </ref>. Several other decomposition methods take advantage of the matrix structure of the problem.
Reference: [90] <editor> R.A. Saleh et al. </editor> <title> Parallel circuit simulation on supercomputers. </title> <booktitle> In Proceedings of the IEEE, special issue on supercomputer technology, </booktitle> <pages> pages 1983-1991. </pages> <publisher> IEEE Press, </publisher> <month> December </month> <year> 1989. </year>
Reference-contexts: In coarse-grain parallel systems, PEs operate as autonomous uniprocessors executing local programs, with limited coordination. Granularity can also be assigned to a parallel algorithm, based on the size of the data unit it operates upon. As an example, in an algorithm for LU matrix factorization <ref> [90] </ref> it is possible to have: * Fine-grain parallelism associated with element-level update operations. * Medium-grain parallelism associated with row-level update operations. * Coarse-grain parallelism associated with independent pivots.
Reference: [91] <author> G.L. Schultz. </author> <title> Barrier decomposition for the parallel optimization of block-angular programs. </title> <type> PhD thesis, </type> <institution> University of Wisconsin-Madison, </institution> <year> 1991. </year>
Reference-contexts: Then the problem decomposes into independent block subproblems, the solution of which is used to redefine the penalty in the next iteration. In this general framework one can place the logarithmic shifted barrier algorithm of Schultz and Meyer <ref> [91] </ref>, [92], the smooth linear-quadratic penalty method of Zenios, Pinar and Dembo [107], [81], and the Diagonal Quadratic Approximation method of Mulvey and Ruszczynski [71], [89]. Several other decomposition methods take advantage of the matrix structure of the problem.
Reference: [92] <author> G.L. Schultz and R.R. Meyer. </author> <title> An interior point method for block angular optimization. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 1(4) </volume> <pages> 583-602, </pages> <year> 1991. </year>
Reference-contexts: Fork-Join coordination is quite popular in parallel optimization; it is employed by many decomposition algorithms such as: the Dantzig-Wolfe decomposition [19], the Schultz-Meyer shifted barrier 6 method <ref> [92] </ref>, the Parallel Variable Distribution [33], and Parallel Constraint Distribution [32] algorithms of Ferris and Mangasarian, and the Diagonal Quadratic Approximation method of Mulvey and Ruszczynski [71]. Methods that employ fork-join coordination are popular in other areas of scientific computing, as well. <p> Then the problem decomposes into independent block subproblems, the solution of which is used to redefine the penalty in the next iteration. In this general framework one can place the logarithmic shifted barrier algorithm of Schultz and Meyer [91], <ref> [92] </ref>, the smooth linear-quadratic penalty method of Zenios, Pinar and Dembo [107], [81], and the Diagonal Quadratic Approximation method of Mulvey and Ruszczynski [71], [89]. Several other decomposition methods take advantage of the matrix structure of the problem.
Reference: [93] <author> J.E. Spingarn. </author> <title> Partial inverse of a monotone operator. </title> <journal> Applied Mathematics and Optimization, </journal> <volume> 10 </volume> <pages> 247-265, </pages> <year> 1983. </year>
Reference-contexts: In his Ph.D. thesis [25] Eckstein demonstrates that, for the minimization problem shown above, some other decomposition algorithms, among them the algorithm of Han and Lou [47], Spingarn's Method of Partial Inverses <ref> [93] </ref>, [94] and Golshtein's Block Method of Convex Programming [44], [45], are instances of the Douglas-Rachford algorithm. He then proceeds to derive ADI methods for monotropic programming; in [26] he discusses their fine-grain implementation on the Connection Machine CM-2 parallel computer.
Reference: [94] <author> J.E. Spingarn. </author> <title> Applications of the method of partial inverses to convex programming: decomposition. </title> <journal> Mathematical Programming, </journal> <volume> 32 </volume> <pages> 199-223, </pages> <year> 1985. </year>
Reference-contexts: In his Ph.D. thesis [25] Eckstein demonstrates that, for the minimization problem shown above, some other decomposition algorithms, among them the algorithm of Han and Lou [47], Spingarn's Method of Partial Inverses [93], <ref> [94] </ref> and Golshtein's Block Method of Convex Programming [44], [45], are instances of the Douglas-Rachford algorithm. He then proceeds to derive ADI methods for monotropic programming; in [26] he discusses their fine-grain implementation on the Connection Machine CM-2 parallel computer.
Reference: [95] <author> G. Stephanopoulos and A.W. Westerberg. </author> <title> The use of Hestenes' method of multipliers to resolve dual gaps in engineering system optimization. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 15 </volume> <pages> 285-309, </pages> <year> 1975. </year>
Reference-contexts: [i] would be a minimizer with respect to an objective function that would include the Jacobi-like term (D [i] x [i] ) D [j] x t The application of linearization to the Augmented Lagrangian is one of the key ideas in the approximate method of multipliers of Stephanopoulos and Westerberg <ref> [95] </ref> and in the Diagonal Quadratic Approximation method of Mulvey and Ruszczynski [71]. Chapter 4 The Activity-and-Resource Proximization splitting (ARP) 4.1 Overview In this chapter we study the Activity-and-Resource Proximization splitting (ARP), the first of the three block-decomposable splittings for the convex block-angular problem (CBA).
Reference: [96] <institution> Thinking Machines Corporation, </institution> <address> Cambridge, MA. </address> <booktitle> The Connection Machine CM-5 technical summary, </booktitle> <year> 1991. </year>
Reference-contexts: The article by Best et al. [12] presents the parallel I/O modes of the machine, while the article by Hillis and Tucker [49] provides a general overview and presents examples of current applications of the CM-5. For details the interested reader can also refer to the CM-5 technical summary <ref> [96] </ref> and the CMMD library guide [97]. 90 7.3 The suite of test problems To assess the relative performance of the three splittings we used MNETGEN [4], a derivative of NETGEN [57], to generate three hundred random multicommodity network problems (MC), one hundred with linear objectives and two hundred with quadratic
Reference: [97] <institution> Thinking Machines Corporation, </institution> <address> Cambridge, MA. </address> <note> CMMD reference manual, version 3.0, </note> <year> 1993. </year>
Reference-contexts: For details the interested reader can also refer to the CM-5 technical summary [96] and the CMMD library guide <ref> [97] </ref>. 90 7.3 The suite of test problems To assess the relative performance of the three splittings we used MNETGEN [4], a derivative of NETGEN [57], to generate three hundred random multicommodity network problems (MC), one hundred with linear objectives and two hundred with quadratic objective functions.
Reference: [98] <author> C.D. Thomborson. </author> <title> Does your workstation computation belong on a vector supercomputer ? Communications of the ACM, </title> <booktitle> 36(11) </booktitle> <pages> 41-49, </pages> <year> 1993. </year>
Reference-contexts: However, the consensus is that "to exploit more parallelism, programmers should design new parallel algorithms, instead of parallelizing sequential algorithms" [104], especially since many large-scale sequential code libraries do not parallelize/vectorize well: a recent survey <ref> [98] </ref> reports that a flow simulation model, typical of numerical models in geophysical sciences, runs only three times faster on a vector Cray Y-MP that on an IBM RS-6000 workstation! The non-vectorizable part of the code is a library routine for solving a finite-difference discretization of a PDE, which accounts for
Reference: [99] <author> P.L. Toint and D. Tuyttens. LSSNO, </author> <title> a FORTRAN subroutine for solving large scale nonlinear network optimization problems. </title> <type> Technical Report 90/11, </type> <institution> Dept. of Mathematics, Facultes Universitaires de la Paix, </institution> <address> Namur, Belgium, </address> <year> 1990. </year> <month> 127 </month>
Reference-contexts: In the case of (MC) one might be able to solve the quadratic network subproblems faster by employing specialized algorithms, such as the two-segment linearization simplex method of Meyer and Kamesam [54] or the quasi-Newton method of Toint and Tuyttens [100], <ref> [99] </ref>. Table 1 displays the characteristics of the test problems. Five instances were randomly generated for each case; the table reports average characteristics.
Reference: [100] <author> P.L. Toint and D. Tuyttens. </author> <title> On large scale nonlinear network optimization. </title> <journal> Mathematical Programming, </journal> <volume> 48 </volume> <pages> 125-159, </pages> <year> 1990. </year>
Reference-contexts: In the case of (MC) one might be able to solve the quadratic network subproblems faster by employing specialized algorithms, such as the two-segment linearization simplex method of Meyer and Kamesam [54] or the quasi-Newton method of Toint and Tuyttens <ref> [100] </ref>, [99]. Table 1 displays the characteristics of the test problems. Five instances were randomly generated for each case; the table reports average characteristics.
Reference: [101] <author> H. Uzawa. </author> <title> Iterative methods for concave programming. </title> <editor> In K.J. Arrow, L. Hurwicz, and H. Uzawa, editors, </editor> <booktitle> Studies in linear and nonlinear programming, </booktitle> <pages> pages 154-165. </pages> <publisher> Stanford University Press, Stanford, </publisher> <address> CA, </address> <year> 1958. </year>
Reference-contexts: We make the following basic assumption. Assumption 2.2.1 Problem (9) admits a lagrangean saddlepoint. Thus, a way of solving (9) is by finding a saddle-point of the Lagrangian <ref> [101] </ref>, or, for reasons of computational stability [9], of the augmented Lagrangian L (x; z; p) := G 1 (x) + G 2 (z) + p T (Ax + b Bz) + 2 2 in which , the penalty parameter, is any positive scalar.
Reference: [102] <author> R. J.-B. Wets. </author> <title> The aggregation principle in scenario analysis and stochastic optimization. In Algorithms and model formulations in mathematical programming, </title> <booktitle> NATO ASI Series F vol. </booktitle> <volume> 51, </volume> <pages> pages 91-114. </pages> <publisher> Springer Verlag, </publisher> <year> 1989. </year>
Reference-contexts: This formulation results in coupling matrices that are quite sparse: for a three-stage problem, Figure 2 shows the decision tree, and Figure 3 shows the structure of the constraint matrix. Our presentation of the scenario analysis problem is based on material in Wets <ref> [102] </ref> and Ruszczynski [89]. <p> The Progressive Hedging algorithm of Rockafellar and Wets <ref> [88, 102] </ref> is a version of the ADI algorithm applied to scenario analysis problems. When used to solve partial differential equations, the ADI method is usually called the Douglas-Rachford method, because it corresponds to the method in [24] by these two authors. <p> Then, the relative performance of the three splittings on scenario problems has to be investigated, as well as their comparison with other specialized algorithms, such as the Diagonal Quadratic Approximation (Mulvey and Ruszczynski [71]) and Progressive Hedging (Rockafellar and Wets <ref> [88, 102] </ref>, Mulvey and Vladimirou [72, 73, 74]).
Reference: [103] <author> J. Worlton. </author> <title> Towards a science of parallel computation. </title> <booktitle> In Computational mechanics advances and trends, </booktitle> <volume> AMD vol. 75, </volume> <pages> pages 23-35. </pages> <publisher> ASME, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: Thus, if s = 0, the speedup is almost linear, if we allow a small margin for communications overhead. Otherwise, if, say, 10% of the operations must be performed sequentially, then the maximum 8 speedup is 10. Over the years there have been re-evaluations [46] and extensions <ref> [103] </ref> of Amdahl's Law, in an effort to incorporate parallel environment factors, such as the process granularity, the synchronization overhead etc.
Reference: [104] <author> M-Y. Wu and D.D. Gajski. </author> <title> Computer-aided programming for message-passing systems: problems and a solution. </title> <booktitle> In Proceedings of the IEEE, special issue on supercomputer technology, </booktitle> <pages> pages 1983-1991. </pages> <publisher> IEEE Press, </publisher> <month> December </month> <year> 1989. </year>
Reference-contexts: We define the (absolute) speedup [79], <ref> [104] </ref>, [11, chapter 1] as the ratio s fl T fl (1) 7 the relative speedup s p as the ratio s p := T p and the efficiency p as p := p (3) The value of p is usually less that one, indicating efficiency loss due to a number <p> However, the consensus is that "to exploit more parallelism, programmers should design new parallel algorithms, instead of parallelizing sequential algorithms" <ref> [104] </ref>, especially since many large-scale sequential code libraries do not parallelize/vectorize well: a recent survey [98] reports that a flow simulation model, typical of numerical models in geophysical sciences, runs only three times faster on a vector Cray Y-MP that on an IBM RS-6000 workstation! The non-vectorizable part of the code
Reference: [105] <author> A. Zakarian. </author> <title> Parallel solution of multicommodity network flow programs via nonlinear Jacobi algorithms. </title> <type> Unpublished manuscript, </type> <institution> University of Wisconsin-Madison, Computer Sciences Dept., </institution> <year> 1994. </year>
Reference-contexts: Of interest is the fact that the MNETGEN problems with 64 blocks were, on the average, easier to solve than those with 48 blocks. A similar case is reported by Zakarian <ref> [105] </ref> for a nonlinear Jacobi method, and may be attributed to the fact that the solutions found for the 64-block problems had, on average, fewer positive p multipliers than the ones for the 48-block problems (22 vs. 27); this may be a characteristic of the MNETGEN generator. 7.5.2 The Peaceman-Rachford method
Reference: [106] <author> S.A. Zenios. </author> <title> On the fine-grain decomposition of multicommodity transportation problem. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 1(4) </volume> <pages> 643-669, </pages> <year> 1991. </year>
Reference-contexts: Several other decomposition methods take advantage of the matrix structure of the problem. Zenios <ref> [106] </ref> specializes the row-action method of Censor and Lent [16] to quadratic (MC) ; the algorithm iterates on each constraint in an almost cycling fashion, adjusting primal and dual variables to preserve complementarity and constraint satisfaction. <p> An 8-commodity problem with 32 nodes and 256 arcs per network was solved in 25 seconds on a 4K CM-2 <ref> [106, Table 1] </ref>.
Reference: [107] <author> S.A. Zenios, M.C~ . Pinar, </author> <title> and R.S. Dembo. A smooth penalty function algorithm for network-structured problems. </title> <type> Technical Report 90-12-05, </type> <institution> Decision Sciences Department, The Wharton School, University of Pennsylvania, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: In this general framework one can place the logarithmic shifted barrier algorithm of Schultz and Meyer [91], [92], the smooth linear-quadratic penalty method of Zenios, Pinar and Dembo <ref> [107] </ref>, [81], and the Diagonal Quadratic Approximation method of Mulvey and Ruszczynski [71], [89]. Several other decomposition methods take advantage of the matrix structure of the problem.
References-found: 107

