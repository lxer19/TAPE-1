URL: http://www.icsi.berkeley.edu/~nikki/papers/ICASSP98_multiband_submitted.ps.Z
Refering-URL: http://www.icsi.berkeley.edu/~nikki/papers/
Root-URL: http://www.icsi.berkeley.edu
Email: Email: fnikki, morgang@icsi.berkeley.edu  
Phone: Tel: (510) 643-9153, FAX: (510) 643-7684,  
Title: TRANSMISSIONS AND TRANSITIONS: A STUDY OF TWO COMMON ASSUMPTIONS IN MULTI-BAND ASR  
Author: Nikki Mirghafori and Nelson Morgan 
Address: 1947 Center St, Berkeley, CA 94704  Berkeley, CA 94720  
Affiliation: International Computer Science Institute,  University of California at Berkeley, EECS Department,  
Abstract: Is multi-band ASR inherently inferior to a full-band approach because phonetic information is lost due to the division of the frequency space into sub-bands? Do the phonetic transitions in sub-bands occur at different times? The first statement is a common objection of the critics of multi-band ASR, and the second, a common assumption by multi-band researchers. This paper is dedicated to finding answers to both these questions. To study the first point, we calculate phonetic feature transmission for sub-bands. Not only do we fail to substantiate the above objection, but we observe the contrary. We confirm the second hypothesis by analyzing the phonetic transition lags in each sub-band. These results reinforce our view that multi-band speech analysis provides useful information for ASR, particularly when band merging takes place at the end state for a phonetic or syllabic model, allowing sub-bands to be independently time-aligned within the model. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Jont B. Allen. </author> <title> How do humans process and recognize speech? IEEE Trans. </title> <booktitle> on Speech and Audio Proc., </booktitle> <volume> 2(4) </volume> <pages> 567-577, </pages> <month> Oct </month> <year> 1994. </year>
Reference-contexts: 1. INTRODUCTION There has been much interest generated in the speech recognition community on multi-band ASR since Jont Allen's cogent retelling of Harvey Fletcher's work on articulation index <ref> [4, 1] </ref>. The main idea of this approach is to divide the signal into separate spectral bands, process each independently (typically generating state probabilities or likelihoods for each), and then merge the information streams, as shown in Figure 1. <p> of the feature set. * Rao and Pearlman [9] have proven theoretically, and shown with simulations, that auto-regressive spectral estimation from sub-bands offers a gain over full-band auto-regressive spectral estimation. * Multi-band ASR is well suited for taking advantage of par allel architectures. * Human speech perception may be similar <ref> [4, 1] </ref>. The most common objection to the use of separate statistical models for each band has been that important information in the form of correlation between bands may be lost.
Reference: [2] <author> Herve Bourlard and Stephane Dupont. </author> <title> Subband-based speech recognition. </title> <booktitle> In ICASSP, </booktitle> <volume> volume II, </volume> <pages> pages 125-128, </pages> <month> May </month> <year> 1997. </year>
Reference-contexts: Some of the motivations for this multi-band approach are: * If the speech signal has different signal-to-noise ratios per band, multi-band ASR shows graceful degradation <ref> [2, 10] </ref>. * It has been posited that acoustic evidence for sound unit identities occur at different times in different parts of the spectrum, particularly in the presence of reverberation or unusually slow or rapid speech. * Statistical modeling may be improved by simpler and less variable signals and lower dimensionality <p> In the second part of the paper, we focus our attention on the following: some multi-band researchers <ref> [2, 10, 8] </ref> have postulated that transitions in sub-bands occur asynchronously,and that a phone or syllable level merging of multi-band streams is necessary to permit independent alignment for each band within the merged unit.
Reference: [3] <author> Herve Bourlard and Nelson Morgan. </author> <title> Connectionist Speech Recognition A Hybrid Approach. </title> <publisher> Kluwer Academic Press, </publisher> <year> 1994. </year>
Reference-contexts: The database is phonetically hand-transcribed. For the purposes of this study, we use approximately two hours of the database for training and cross validation, and forty minutes as a test set. Our baseline full-band system is an HMM/MLP based <ref> [3] </ref> system. We train the MLP phonetic probability estimator on a nine-frame window of 8th-order RASTA-PLP [5], energy, and delta RASTA-PLP features for every 25 ms window, stepped every 10 ms.
Reference: [4] <author> Harvey Fletcher. </author> <title> Speech and Hearing in Communication. </title> <publisher> Krieger, </publisher> <address> New York, </address> <year> 1953. </year>
Reference-contexts: 1. INTRODUCTION There has been much interest generated in the speech recognition community on multi-band ASR since Jont Allen's cogent retelling of Harvey Fletcher's work on articulation index <ref> [4, 1] </ref>. The main idea of this approach is to divide the signal into separate spectral bands, process each independently (typically generating state probabilities or likelihoods for each), and then merge the information streams, as shown in Figure 1. <p> of the feature set. * Rao and Pearlman [9] have proven theoretically, and shown with simulations, that auto-regressive spectral estimation from sub-bands offers a gain over full-band auto-regressive spectral estimation. * Multi-band ASR is well suited for taking advantage of par allel architectures. * Human speech perception may be similar <ref> [4, 1] </ref>. The most common objection to the use of separate statistical models for each band has been that important information in the form of correlation between bands may be lost.
Reference: [5] <author> Hynek Hermansky and Nelson Morgan. </author> <title> RASTA processing of speech. </title> <journal> IEEE Transactions on Speech and Audio Processing, </journal> <volume> 2(4) </volume> <pages> 578-589, </pages> <month> Oct </month> <year> 1994. </year>
Reference-contexts: Our baseline full-band system is an HMM/MLP based [3] system. We train the MLP phonetic probability estimator on a nine-frame window of 8th-order RASTA-PLP <ref> [5] </ref>, energy, and delta RASTA-PLP features for every 25 ms window, stepped every 10 ms.
Reference: [6] <author> Richard P. Lippmann. </author> <title> Speech recognition by machines and humans. </title> <journal> Speech Communication, </journal> <volume> 22(1) </volume> <pages> 1-15, </pages> <year> 1997. </year>
Reference-contexts: For example, band 2 transmits 87% of the frontness features that are transmitted by the full-band system. 4. There is much redundancy in phonetic information content in the sub-bands, as the sum of information transmission over all bands far exceeds 100%. Lippmann <ref> [6] </ref> has highlighted this redundancy as a source of human robustness to speech degradations. In the next section, we examine the transition asynchrony hypothesis. 4.
Reference: [7] <author> George A. Miller and Patricia E. </author> <title> Nicely. An analysis of perceptual confusions among some English consonants. </title> <journal> JASA, </journal> <volume> 27(2) </volume> <pages> 338-352, </pages> <month> Mar </month> <year> 1955. </year>
Reference-contexts: In particular, we analyze the phonetic feature transmission pattern in each sub-band, the merged multi-band, and full-band probability streams. As discussed in Section 3, we use methods similar to those of Miller & Nicely <ref> [7] </ref> and calculate confusion matrices for phone and feature classes, and use mutual information as a measure of information transmission in a channel. <p> IS PHONETIC INFORMATION LOST? 3.1. Experimental Setup The first question we want to answer is whether any phonetic feature information is lost in multi-band ASR. For this analysis we use phone and broad category confusion matrices, as in the seminal studies of Miller and Nicely <ref> [7] </ref> on human speech recognition. A confusion matrix (CM) is simply an extended matrix of hits and misses for all classes, as in Table 1. The column headings represent the features we intend to transmit, and the row headings correspond to the received features. <p> To summarize the confusion matrix, we calculate mutual information (MI) for each CM <ref> [7] </ref> as P p ij , where i is the feature we would like to transmit, and j is the feature that is perceived. <p> The results are consistent with our knowledge of acoustic phonetics, as, for example, we would expect the low frequency band to contain the most information about voicing. Comparing our results with <ref> [7] </ref>, we observe similar patterns also for fricatives and nasals. vowel consonant silence vowel 74393 6962 1816 consonant 6738 61030 5055 silence 2321 8922 49281 Table 3: An example of a feature-based confusion matrix. 3.
Reference: [8] <author> Nikki Mirghafori. </author> <title> An alternative approach to automatic speech recognition using sub-band linguistic categories. </title> <note> Thesis Proposal (http://www.icsi.berkeley.edu/nikki/papers/thesis prop.ps), </note> <month> Dec </month> <year> 1996. </year>
Reference-contexts: In the second part of the paper, we focus our attention on the following: some multi-band researchers <ref> [2, 10, 8] </ref> have postulated that transitions in sub-bands occur asynchronously,and that a phone or syllable level merging of multi-band streams is necessary to permit independent alignment for each band within the merged unit.
Reference: [9] <author> Sudhakar Rao and Wiliam A. Pearlman. </author> <title> Analysis of linear prediction, coding, and spectral estimation from subbands. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 42(4) </volume> <pages> 1160-1178, </pages> <month> Jul </month> <year> 1996. </year>
Reference-contexts: evidence for sound unit identities occur at different times in different parts of the spectrum, particularly in the presence of reverberation or unusually slow or rapid speech. * Statistical modeling may be improved by simpler and less variable signals and lower dimensionality of the feature set. * Rao and Pearlman <ref> [9] </ref> have proven theoretically, and shown with simulations, that auto-regressive spectral estimation from sub-bands offers a gain over full-band auto-regressive spectral estimation. * Multi-band ASR is well suited for taking advantage of par allel architectures. * Human speech perception may be similar [4, 1].
Reference: [10] <author> Sangita Tibrewala and Hynek Hermansky. </author> <title> Sub-band based recognition of noisy speech. </title> <booktitle> In ICASSP, </booktitle> <volume> volume II, </volume> <pages> pages 1255-1258, </pages> <month> May </month> <year> 1997. </year>
Reference-contexts: Some of the motivations for this multi-band approach are: * If the speech signal has different signal-to-noise ratios per band, multi-band ASR shows graceful degradation <ref> [2, 10] </ref>. * It has been posited that acoustic evidence for sound unit identities occur at different times in different parts of the spectrum, particularly in the presence of reverberation or unusually slow or rapid speech. * Statistical modeling may be improved by simpler and less variable signals and lower dimensionality <p> In the second part of the paper, we focus our attention on the following: some multi-band researchers <ref> [2, 10, 8] </ref> have postulated that transitions in sub-bands occur asynchronously,and that a phone or syllable level merging of multi-band streams is necessary to permit independent alignment for each band within the merged unit.
References-found: 10

