URL: http://www.mli.gmu.edu/~kaufman/papers/97-3.ps
Refering-URL: http://www.mli.gmu.edu/kpubs.html
Root-URL: 
Title: Data Mining and Knowledge Discovery: A Review of Issues and a Multistrategy Approach  
Author: Ryszard S. Michalski and Kenneth A. Kaufman 
Date: March 1997  
Pubnum: P97-3  
Abstract-found: 0
Intro-found: 1
Reference: <author> Baim, P.W., </author> <title> The PROMISE Method for Selecting Most Relevant Attributes for Inductive Learning Systems, </title> <type> Report No. </type> <institution> UIUCDCS-F-82-898, Department of Computer Science, University of Illinois, Urbana, </institution> <year> 1982. </year>
Reference-contexts: This can be done by applying one of many methods for attribute selection, such as Gain Ratio (Quinlan, 1993) or Promise level <ref> (Baim, 1982) </ref>. Generating new attributes: The problem is to generate additional columns that correspond to new attributes generated by a constructive induction procedure. <p> For instance, one such operator climbs an attributes generalization hierarchy to build more general decision rules (Kaufman and Michalski, 1996). GENATR operators generate new attribute sets by creating new attributes (Bloedorn and Michalski, 1996), selecting the most representative attributes from the original set <ref> (Baim, 1982) </ref>, or by abstracting attributes (Kerber, 1992). <p> To this end, one may use many different criteria for evaluating the relevance of an attribute for a given classification problem, such as gain ratio (Quinlan, 1993), gini index (Breiman et al, 1984), PROMISE <ref> (Baim, 1982) </ref>, and chisquare analysis (Hart, 1984; Mingers, 1989). These criteria evaluate attributes on the basis of their expected global performance, which means that those attributes with the highest ability to discriminate among all classes are selected as the most relevant. <p> This way, the recognition of Q will require more tests than necessary, but at no expense to the recognition of other letters. INLEN supports both global and local attribute evaluation criteria for selecting the most relevant attributes. The former is based on the PROMISE methodology <ref> (Baim, 1982) </ref>, while the latter employs a variation of PROMISE that is oriented toward the maximum performance of some attribute value, rather than on the attributes global performance. 2 7 9 . 2 Generating New Attributes When the original representation space is weakly relevant to the problem at hand, or the
Reference: <author> Bentrup, J.A., Mehler, G.J. and Riedesel, J.D., </author> <title> INDUCE 4: A Program for Incrementally Learning Structural Descriptions From Examples, Reports of the Intelligent Systems Group, </title> <type> ISG 87-2. </type> <institution> UIUCDCS-F-87-958, Department of Computer Science, University of Illinois, Urbana, </institution> <year> 1987. </year>
Reference: <author> Bergadano, F., Matwin, S., Michalski, R.S. and Zhang, J., </author> <title> Learning Two-Tiered Descriptions of Flexible Concepts: The POSEIDON System, </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> pp. 5-43, </pages> <year> 1992. </year> <note> 3 3 Bloedorn, </note> <author> E. and Michalski, </author> <title> R.S., The AQ17-DCI System for Data-Driven Constructive Induction and Its Application to the Analysis of World Economics, </title> <booktitle> Proceedings of the 9th International Symposium on Methodologies for Intelligent Systems, </booktitle> <address> Zakopane, Poland, </address> <year> 1996. </year>
Reference-contexts: In some applications, especially those involving learning rules from noisy data or learning flexible concepts (Michalski, 1990), it may be advantageous to learn descriptions that are incomplete and/or inconsistent <ref> (Bergadano et al, 1992) </ref>. Attributional descriptions can be visualized by mapping them into a planar representation of a discrete multidimensional space (a diagram) spanned over the given attributes (Michalski, 1978), (Wnek et al, 1990). For example, Figure 2 shows a diagrammatic visualization of the rules from Figure 1.
Reference: <author> Bloedorn, E., Wnek, J. and Michalski, </author> <title> R.S., Multistrategy Constructive Induction, </title> <booktitle> Proceedings of the Second International Workshop on Multistrategy Learning, </booktitle> <address> Harpers Ferry, WV, </address> <pages> pp. 188-203, </pages> <year> 1993. </year>
Reference-contexts: An example of a constructive induction program is AQ17 <ref> (Bloedorn, Wnek and Michalski, 1993) </ref>, which performs all three types of improvements of the original representation space. <p> Generating new attributes: The problem is to generate additional columns that correspond to new attributes generated by a constructive induction procedure. These new attributes are created by using the problems background knowledge and/or special heuristic procedures as described in papers on constructive induction <ref> (e.g., Bloedorn, Wnek and Michalski, 1993) </ref>. Clustering: The problem is to automatically partition the rows of the table into groups that correspond to conceptual clusters, that is, sets of entities with high conceptual cohesiveness (Michalski, Stepp and Diday, 1981).
Reference: <author> Bongard, N., </author> <title> Pattern Recognition, Spartan Books, New York (a translation from Russian), </title> <year> 1970. </year>
Reference-contexts: These functions may be simple, e.g., a product or sum of a set of the original attributes, or very complex, e.g., a Boolean attribute based on the presence or absence of a straight line or circle in an image <ref> (Bongard, 1970) </ref>. Finally, in some situations, it will be desirable to abstract some attributes, that is, to group some attribute values into units, and thus reduce the attributes 9 range of possible values. A quantization of continuous attributes is an example of such an operation.
Reference: <author> Bratko, I., Muggleton, S. and Karalic, A., </author> <title> Applications of Inductive Logic Programming, </title> <editor> In Michalski, R.S., Bratko, I. and Kubat, M. (eds.), </editor> <booktitle> Machine Learning and Data Mining: Methods and Applications, </booktitle> <address> London, </address> <publisher> John Wiley & Sons, </publisher> <year> 1997. </year>
Reference: <author> Breiman, L., Friedman, J.H., Olshen, R.A. and Stone, C.J., </author> <title> Classification and Regression Trees, </title> <address> Belmont, CA, </address> <publisher> Wadsworth Int. Group, </publisher> <year> 1984. </year>
Reference-contexts: To this end, one may use many different criteria for evaluating the relevance of an attribute for a given classification problem, such as gain ratio (Quinlan, 1993), gini index <ref> (Breiman et al, 1984) </ref>, PROMISE (Baim, 1982), and chisquare analysis (Hart, 1984; Mingers, 1989). These criteria evaluate attributes on the basis of their expected global performance, which means that those attributes with the highest ability to discriminate among all classes are selected as the most relevant.
Reference: <author> Carpineto, C. and Romano, G., </author> <title> Some Results on Lattice-based Discovery in Databases, </title> <booktitle> Workshop on Statistics, Machine Learning and Knowledge Discovery in Databases, Heraklion, </booktitle> <pages> pp. 216-221, </pages> <year> 1995a. </year>
Reference: <author> Carpineto, C. and Romano, G., </author> <title> Automatic Construction of Navigable Concept Networks Characterizing Text Databases, </title> <editor> Gori, M. and Soda, G. (eds.), </editor> <booktitle> Topics in Artificial Intelligence, </booktitle> <publisher> LNAI 992-Springer, </publisher> <pages> pp. 67-78, </pages> <year> 1995b. </year>
Reference: <author> Collins, A. and Michalski, </author> <title> R.S., Toward a Formal Theory of Human Plausible Reasoning, </title> <booktitle> Proceedings of the Third Annual Conference of the Cognitive Science Society, </booktitle> <address> Berkeley, CA, </address> <year> 1981. </year>
Reference: <author> Collins, A. and Michalski, </author> <title> R.S., The Logic of Plausible Reasoning: A Core Theory, </title> <journal> Cognitive Science, </journal> <volume> 13, </volume> <pages> pp. 1-49, </pages> <year> 1989. </year>
Reference-contexts: GENEVE operators generate events, facts or examples that satisfy given rules, select the most representative events from a given set (Michalski and Larson, 1978), determine examples that are similar to a given example <ref> (Collins and Michalski, 1989) </ref>, or predict the value of a given variable using an expert system shell or a decision structure.
Reference: <author> Daniel, C. and Wood, </author> <title> F.S., Fitting Equations to Data, </title> <address> New York, </address> <publisher> John Wiley & Sons, </publisher> <year> 1980. </year>
Reference: <author> Davis, J., CONVART: </author> <title> A Program for Constructive Induction on TimeDependent Data, M.S. </title> <type> Thesis, </type> <institution> Department of Computer Science, University of Illinois, Urbana, </institution> <year> 1981. </year>
Reference-contexts: Inherent in a timestamped representation are many attributes that can be generated through constructive induction, for example, date of the highest temperature, the minimum population growth rate during some period, weediness on date of planting, etc. CONVART <ref> (Davis, 1981) </ref> uses user-provided and default system suggestions to search for useful timedependent attributes that are added to the representation space. It uses the items on the suggestion list to generate new attributes and to test them for likely relevance to the problem.
Reference: <editor> Diday, E. (ed.), </editor> <booktitle> Proceedings of the Conference on Data Analysis, Learning Symbolic and Numeric Knowledge, </booktitle> <publisher> Nova Science Publishers, Inc., </publisher> <address> Antibes, </address> <year> 1989. </year>
Reference: <author> Dieterrich, T. and Michalski, </author> <title> R.S., Learning to Predict Sequences, </title> <booktitle> Chapter in Machine Learning: An Artificial Intelligence Approach Vol. </booktitle> <volume> 2. </volume> <editor> Michalski, R.S., Carbonell, J.G. and Mitchell, T.M. (eds.), </editor> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 63-106, </pages> <year> 1986. </year>
Reference-contexts: But is there a consistent pattern? To determine such a pattern, one can employ different descriptive models, and instantiate the models to fit the particular sequence. The instantiated model that best fits the data is then used for prediction. Such a method is described in <ref> (Dieterrich and Michalski, 1986) </ref>. The method employs three descriptive modelsperiodic, decomposition and DNF. The periodic model is used to detect repeating patterns in a sequence. For example, Figure 4 depicts a recurring pattern that alternates T-shaped and Ishaped objects.
Reference: <author> Dontas, K., APPLAUSE: </author> <title> An Implementation of the Collins-Michalski Theory of Plausible Reasoning, M.S. </title> <type> Thesis, </type> <institution> Computer Science Department, The University of Tennessee, Knoxville, TN, </institution> <year> 1988. </year>
Reference: <editor> Dubois, D., Prade, H. and Yager, R.R. (eds.), </editor> <booktitle> Readings in Fuzzy Sets and Intelligent Systems, </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year> <note> 3 4 Evangelos S. </note> <editor> and Han, J. (eds.), </editor> <booktitle> Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, </booktitle> <address> Portland, OR, </address> <year> 1996. </year>
Reference: <author> Falkenhainer, B.C. and Michalski, </author> <title> R.S., Integrating Quantitative and Qualitative Discovery in the ABACUS System, </title> <editor> In Kodratoff, Y. and Michalski, R.S. (eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach Vol. III, </booktitle> <address> San Mateo, CA, </address> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 153-190, </pages> <year> 1990. </year>
Reference-contexts: Integrating qualitative and quantitative discovery, i.e., determining sets of equations that fit a given set of data points, and qualitative conditions for the application of these equations <ref> (e.g., Falkenhainer and Michalski, 1990) </ref>. Qualitative prediction, i.e., discovering patterns in sequences or processes and using these patterns to qualitatively predict the possible continuation of the given sequences or processes (e.g., Davis, 1981; Michalski, Ko and Chen, 1985; 1986; Dieterrich and Michalski, 1986). <p> This is an example of quantitative discovery. The application of machine learning to quantitative discovery was pioneered by the BACON system (Langley, Bradshaw and Simon, 1983), and then explored by many systems since, such as COPER (Kokar, 1986), FAHRENHEIT (Zytkow, 1987), and ABACUS <ref> (Falkenhainer and Michalski, 1990) </ref>. Similar problems have been explored independently by Zagoruiko (1972) under the name of empirical prediction. Some equations may not apply directly to data, because of an inappropriate value of a constant, or different equations may apply under different qualitative conditions. <p> Individual branches may be assigned a set of attribute values. Leaves may be assigned a set of decisions (Imam and Michalski, 1993; Imam, 1995). GENEQ operators generate equations characterizing numerical data sets and qualitatively describing the conditions under which these equations apply <ref> (e.g., Falkenhainer and Michalski, 1990) </ref>. GENHIER operators build conceptual clusters or hierarchies. They are based on the program CLUSTER methodology (Michalski, Stepp and Diday, 1981). The operator in INLEN is based on the reimplementation in C of the program CLUSTER/2 (Stepp, 1984).
Reference: <author> Fayyad, U., Piatetsky-Shapiro, G. and Smyth, P., </author> <title> Knowledge Discovery and Data Mining: Toward a Unifying Framework, </title> <booktitle> Proceedings of the Second International Conference on Knowledge Discovery and Data Mining. </booktitle> <address> Portland, OR, </address> <pages> pp. 82-88, </pages> <year> 1996. </year>
Reference-contexts: The INLEN methodology for intelligent data exploration directly reflects the aims of the current research on data mining and knowledge discovery. In this context, it may be useful to explain the distinction between the concepts of data mining and knowledge discovery, as proposed in <ref> (Fayyad, Piatetsky-Shapiro and Smyth, 1996) </ref>.
Reference: <editor> Fayyad, U.M. Piatetsky-Shapiro, G. Smyth, P. and Uhturusamy, R. (eds.), </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <address> San Mateo, CA, </address> <publisher> AAAI Press, </publisher> <year> 1996. </year>
Reference-contexts: The INLEN methodology for intelligent data exploration directly reflects the aims of the current research on data mining and knowledge discovery. In this context, it may be useful to explain the distinction between the concepts of data mining and knowledge discovery, as proposed in <ref> (Fayyad, Piatetsky-Shapiro and Smyth, 1996) </ref>.
Reference: <author> Feelders, A. </author> <title> Learning from Biased Data Using Mixture Models, </title> <booktitle> Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, </booktitle> <address> Portland, OR, </address> <pages> pp. 102-107, </pages> <year> 1996. </year>
Reference-contexts: Learning from biased data, i.e., learning from a data set that does not reflect the actual distribution of events <ref> (e.g., Feelders, 1996) </ref>.
Reference: <author> Forsyth, R. and Rada, R., </author> <title> Machine Learning: </title> <booktitle> Applications in Expert Systems and Information Retrieval, </booktitle> <publisher> Pittman, </publisher> <year> 1986. </year>
Reference: <author> Greene, G., </author> <title> The Abacus.2 System for Quantitative Discovery: Using Dependencies to Discover NonLinear Terms, Reports of the Machine Learning and Inference Laboratory, MLI 88-4, Machine Learning and Inference Laboratory, </title> <institution> George Mason University, Fairfax, VA, </institution> <year> 1988. </year>
Reference: <author> Hart, A., </author> <title> Experience in the Use of an Inductive System in Knowledge Engineering, </title> <editor> In M Bramer (Ed.), </editor> <booktitle> Research and Developments in Expert Systems, </booktitle> <address> Cambridge, </address> <publisher> Cambridge University Press, </publisher> <year> 1984. </year>
Reference: <author> Hong, J., Mozetic, I. and Michalski, </author> <title> R.S., AQ15: Incremental Learning of Attribute-Based Descriptions from Examples: The Method and Users Guide, Reports of the Intelligent Systems Group, </title> <type> ISG 86-5, </type> <institution> UIUCDCS-F-86-949, Department of Computer Science, University of Illinois, Urbana, </institution> <year> 1986. </year>
Reference: <author> Hunt, E., Marin, J. and Stone, P., </author> <title> Experiments in Induction, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1966. </year>
Reference: <author> Imam, </author> <title> I.F., Discovering Task-Oriented Decision Structures from Decision Rules, </title> <type> Ph.D. dissertation, </type> <institution> School of Information Technology and Engineering, George Mason University, Fairfax, VA, </institution> <year> 1995. </year>
Reference-contexts: Such rules can be often simplified by detecting superfluous conditions in them (e.g., Quinlan, 1993). The opposite process of transforming a ruleset into a decision tree is not so direct <ref> (Imam, 1995) </ref>, because a rule representation is more powerful than a tree representation. The term more powerful means in this context that a decision tree representing a given ruleset may require superfluous conditions (e.g., Michalski, 1990). <p> Another weakness of decision trees is that they may become unwieldy and incomprehensible because of their limited knowledge representational power. To overcome the above limitations, a new approach has been developed that creates task-oriented decision structures from decision rules <ref> (Imam, 1995, Michalski and Imam, 1997) </ref>.
Reference: <author> Imam, I.F. and Michalski, </author> <title> R.S., Should Decision Trees be Learned from Examples or From Decision Rules? Proceedings of the Seventh International Symposium on Methodologies for Intelligent Systems (ISMIS-93), </title> <address> Trondheim, Norway, </address> <year> 1993. </year>
Reference-contexts: GENTREE operators build a decision structure from a given set of decision rules <ref> (e.g., Imam and Michalski, 1993) </ref>, or from examples (e.g., Quinlan, 1993). A decision structure is a generalization of the concept of a decision tree in which nodes can be assigned an attribute or a function of attributes. Individual branches may be assigned a set of attribute values. <p> a predictive accuracy of 91.8% (when AQDT built an equivalent decision structure by combining some branches, the number of leaves was reduced to 8), while the decision tree learned by C4.5 from the same set of training examples had 8 nodes and 15 leaves, with a predictive accuracy of 85.7% <ref> (Imam and Michalski, 1993) </ref>. 2 6 This methodology directly fits the philosophy of INLEN.
Reference: <author> Kaufman, K., </author> <title> Comparing International Development Patterns Using Multi-Operator Learning and Discovery Tools, </title> <booktitle> Proceedings of the AAAI-94 Workshop on Knowledge Discovery in Databases, </booktitle> <address> Seattle, WA, </address> <pages> pp. 431-440, </pages> <year> 1994. </year> <note> 3 5 Kaufman, </note> <author> K.A. and Michalski, </author> <title> R.S., EMERALD: An Integrated System of Machine Learning and Discovery Programs to Support Education and Experimental Research, Reports of the Machine Learning and Inference Laboratory, MLI 93-10, Machine Learning and Inference Laboratory, </title> <institution> George Mason University, Fairfax, VA, </institution> <year> 1993. </year>
Reference-contexts: One experiment focused on distinguishing between development patterns in Eastern Europe and East Asia, first by identifying such patterns, and then by generating discriminant rules <ref> (Kaufman, 1994) </ref>. A conceptual clustering operator determined a way of grouping the countries, based on each countrys change in the percentage of its population in the labor force between 1980 and 1990. In this classification, the typical Eastern European country and the typical East Asian country fell into separate groups.
Reference: <author> Kaufman, K. and Michalski, </author> <title> R.S., A Method for Reasoning with Structured and Continuous Attributes in the INLEN-2 Knowledge Discovery System, </title> <booktitle> Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining, </booktitle> <address> Portland, OR, </address> <pages> pp. 232-237, </pages> <year> 1996. </year>
Reference-contexts: Learning concepts at different levels of generality, i.e., learning descriptions that involve concepts from different levels of generalization hierarchies representing background knowledge <ref> (e.g., Kaufman and Michalski, 1996) </ref>. Integrating qualitative and quantitative discovery, i.e., determining sets of equations that fit a given set of data points, and qualitative conditions for the application of these equations (e.g., Falkenhainer and Michalski, 1990). <p> TRANSFORM operators perform various transformations on the knowledge segments, e.g., generalization or specialization, abstraction or concretion, optimization of given rules, etc. according to user-provided criteria. For instance, one such operator climbs an attributes generalization hierarchy to build more general decision rules <ref> (Kaufman and Michalski, 1996) </ref>. GENATR operators generate new attribute sets by creating new attributes (Bloedorn and Michalski, 1996), selecting the most representative attributes from the original set (Baim, 1982), or by abstracting attributes (Kerber, 1992). <p> Such nodes should reflect the interests of a given application <ref> (Kaufman and Michalski, 1996) </ref>. To illustrate this idea, consider is a frozen dessert, which is a dessert, which is a type of food. <p> This attribute was presented as a nominal attribute in the initial dataset. In order to examine how the structuring of attributes affects knowledge discovery, INLEN was applied to 3 0 identical data sets with and without the Religion attribute being structured <ref> (Kaufman and Michalski, 1996) </ref>. A portion of the attribute domain structure is shown in Figure 12.
Reference: <author> Kaufman, K., Michalski, R.S. and Kerschberg, L., </author> <title> Mining for Knowledge in Data: Goals and General Description of the INLEN System, </title> <editor> In Piatetsky-Shapiro, G. and Frawley, W.J. (Eds.), </editor> <booktitle> Knowledge Discovery in Databases, </booktitle> <address> Menlo Park, CA: </address> <publisher> AAAI Press, </publisher> <pages> pp. 449-462, </pages> <year> 1991. </year>
Reference-contexts: To facilitate their use, the concept of a knowledge segment was introduced <ref> (Kaufman, Michalski and Kerschberg, 1991) </ref>. A knowledge segment is a structure that links one or more relational tables from the database with one or more structures from the knowledge base.
Reference: <author> Kerber, R., ChiMerge: </author> <title> Discretization of Numeric Attributes. </title> <booktitle> Proceedings of the Tenth National Conference on Artificial Intelligence (AAAI-92), </booktitle> <address> San Jose, CA, </address> <pages> pp. 123-127, </pages> <year> 1992. </year> <title> Kodratoff, Y, Introduction to Machine Learning, </title> <publisher> Pittman, </publisher> <year> 1988. </year>
Reference-contexts: For instance, one such operator climbs an attributes generalization hierarchy to build more general decision rules (Kaufman and Michalski, 1996). GENATR operators generate new attribute sets by creating new attributes (Bloedorn and Michalski, 1996), selecting the most representative attributes from the original set (Baim, 1982), or by abstracting attributes <ref> (Kerber, 1992) </ref>.
Reference: <author> Kokar, </author> <title> M.M., Coper: A Methodology for Learning Invariant Functional Descriptions, Chapter in Machine Learning: A Guide to Current Research, </title> <editor> Michalski, R.S., Mitchell, T.M and Carbonell, J.G. (Eds.), </editor> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA 1986. </address>
Reference-contexts: This is an example of quantitative discovery. The application of machine learning to quantitative discovery was pioneered by the BACON system (Langley, Bradshaw and Simon, 1983), and then explored by many systems since, such as COPER <ref> (Kokar, 1986) </ref>, FAHRENHEIT (Zytkow, 1987), and ABACUS (Falkenhainer and Michalski, 1990). Similar problems have been explored independently by Zagoruiko (1972) under the name of empirical prediction.
Reference: <editor> Kodratoff, Y. and Michalski, R.S. (eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, Vol. III. </booktitle> <address> San Mateo, CA, </address> <note> Morgan Kaufmann., 1990 Lakshminarayan, </note> <author> K., Harp, S.A., Goldman, R. and Samad, T., </author> <title> Imputation of Missing Data Using Machine Learning Techniques, </title> <booktitle> Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, </booktitle> <address> Portland, OR, </address> <pages> pp. 140-145, </pages> <year> 1996. </year>
Reference: <author> Langley, P., Bradshaw G.L. and Simon, H.A., </author> <title> Rediscovering Chemistry with the BACON System, Machine Learning: An Artificial Intelligence Approach, </title> <editor> Michalski, R.S., Carbonell, J.G. and Mitchell, T.M. (Eds.), </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <pages> pp. 307-329, </pages> <year> 1983. </year> <title> Larson, J.B, INDUCE-1: An Interactive Inductive Inference Program in VL21 Logic System, </title> <type> Report No. 876, </type> <institution> Department of Computer Science, University of Illinois, Urbana, </institution> <year> 1977. </year>
Reference-contexts: This is an example of quantitative discovery. The application of machine learning to quantitative discovery was pioneered by the BACON system <ref> (Langley, Bradshaw and Simon, 1983) </ref>, and then explored by many systems since, such as COPER (Kokar, 1986), FAHRENHEIT (Zytkow, 1987), and ABACUS (Falkenhainer and Michalski, 1990). Similar problems have been explored independently by Zagoruiko (1972) under the name of empirical prediction.
Reference: <author> Maloof, M.A. and Michalski, </author> <title> R.S, Learning Evolving Concepts Using Partial Memory Approach, </title> <booktitle> Working Notes of the 1995 AAAI Fall Symposium on Active Learning, </booktitle> <address> Boston, MA, </address> <pages> pp. 70-73, </pages> <year> 1995. </year>
Reference-contexts: For example, the area of interest of a user is often an evolving concept (e.g., Widmer and Kubat, 1996). Learning concepts from data arriving over time, i.e., incremental learning in which currently held hypotheses characterizing concepts may need to be updated to account for the new data <ref> (e.g., Maloof and Michalski, 1995) </ref>. Learning from biased data, i.e., learning from a data set that does not reflect the actual distribution of events (e.g., Feelders, 1996).
Reference: <author> Michael, J., </author> <title> Validation, Verification and Experimentation with Abacus2, Reports of the Machine Learning and Inference Laboratory, MLI 91-8, Machine Learning and Inference Laboratory, </title> <institution> George Mason University, Fairfax, VA, </institution> <year> 1991. </year>
Reference: <author> Michalski, </author> <title> R.S., A Planar Geometrical Model for Representing MultiDimensional Discrete Spaces and Multiple-Valued Logic Functions, ISG Report No. </title> <type> 897, </type> <institution> Department of Computer Science, University of Illinois, Urbana, </institution> <year> 1978. </year> <title> 3 6 Michalski, R.S., Inductive Learning as Rule-Guided Generalization and Conceptual Simplification of Symbolic Descriptions: Unifying Principles and a Methodology, </title> <booktitle> Workshop on Current Developments in Machine Learning, </booktitle> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1980. </year>
Reference-contexts: Attributional descriptions can be visualized by mapping them into a planar representation of a discrete multidimensional space (a diagram) spanned over the given attributes <ref> (Michalski, 1978) </ref>, (Wnek et al, 1990). For example, Figure 2 shows a diagrammatic visualization of the rules from Figure 1. The diagram in Figure 2 was generated by the concept visualization program DIAV (Wnek et al, 1990; Wnek, 1995). <p> The four shaded areas marked Class1 (A) represent rule A, and the shaded area marked Class 1 (B) represents rule B. In such a diagram, conjunctive rules correspond to certain regular arrangements of cells that are easy to recognize <ref> (Michalski, 1978) </ref>. The diagrammatic visualization can be used for displaying the target concept (the concept to be learned), the training examples (the examples and counterexamples of the concept), 5 and the actual concept learned by a method. <p> Most such cases are those that are either most typical or most extreme (assuming that there is not too much noise in the data). One method for determining the latter ones, the socalled method of outstanding representatives, is described in <ref> (Michalski and Larson, 1978) </ref>. 2 . 5 Integration of Qualitative and Quantitative Discovery In a database that contains numerical attributes, a useful discovery might be an equation binding these attributes. <p> When a datatable is very large, is it important to concentrate the analysis on a representative sample. The method of outstanding representatives selects examples (tuples) that are most different from the other examples <ref> (Michalski and Larson, 1978) </ref>. Attribute selection: When there are many columns (attributes) in the GDT, it is often desirable to reduce the data table by removing columns that correspond to the least relevant attributes for a designated learning task. <p> GENEVE operators generate events, facts or examples that satisfy given rules, select the most representative events from a given set <ref> (Michalski and Larson, 1978) </ref>, determine examples that are similar to a given example (Collins and Michalski, 1989), or predict the value of a given variable using an expert system shell or a decision structure.
Reference: <author> Michalski, </author> <title> R.S., A Theory and Methodology of Inductive Learning, In Michalski, R.S. </title>
Reference: <editor> Carbonell, J.G. and Mitchell, T.M. (eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <address> Palo Alto: </address> <publisher> Tioga Publishing, </publisher> <pages> pp. 83-129, </pages> <year> 1983. </year> <title> Michalski, R.S, Learning Flexible Concepts: Fundamental Ideas and a Method Based on Two-tiered Representation, </title> <editor> In Kodratoff, Y. and Michalski, R.S. (eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, Vol. III, </booktitle> <address> San Mateo, CA, </address> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 63-102, </pages> <year> 1990. </year>
Reference: <author> Michalski, </author> <title> R.S., Searching for Knowledge in a World Flooded with Facts, in Applied Stochastic Models and Data Analysis, </title> <journal> Vol. </journal> <volume> 7, </volume> <pages> pp. </pages> <address> 153-l63, </address> <month> January, </month> <year> 1991. </year>
Reference-contexts: To facilitate their use, the concept of a knowledge segment was introduced <ref> (Kaufman, Michalski and Kerschberg, 1991) </ref>. A knowledge segment is a structure that links one or more relational tables from the database with one or more structures from the knowledge base.
Reference: <author> Michalski, </author> <title> R.S., Inferential Theory of Learning: Developing Foundations for Multistrategy Learning, </title> <editor> In Michalski, R.S. and Tecuci, G. (eds.), </editor> <title> Machine Learning: A Multistrategy Approach, </title> <address> San Francisco: </address> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 3-61, </pages> <year> 1994. </year>
Reference-contexts: These may be initial attributes, given a priori, or additional ones generated through a process of constructive induction <ref> (e.g., Wnek and Michalski, 1994) </ref>. Each 1 3 attribute is assigned a domain and a type. The domain specifies the set of all legal values that the attribute can be assigned in the table. The type defines the ordering (if any) of the values in the domain.
Reference: <author> Michalski, </author> <title> R.S., Baskin, A.B. and Spackman, K.A., A Logic-based Approach to Conceptual Database Analysis, </title> <booktitle> Sixth Annual Symposium on Computer Applications in Medical Care (SCAMC-6), </booktitle> <institution> George Washington University, Medical Center, </institution> <address> Washington, DC, </address> <pages> pp. 792-796, </pages> <year> 1982. </year>
Reference: <editor> Michalski, R.S., Carbonell, J.G. and Mitchell, T.M. (eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <address> Palo Alto, CA, </address> <publisher> Tioga Publishing, </publisher> <year> 1983. </year>
Reference-contexts: In some methods, BK may include more information, e.g., constraints on the interrelationship between various attributes, rules for generating higher level concepts, new attributes, as well as some initial hypothesis <ref> (Michalski, 1983) </ref>. Learned rules are usually consistent and complete with regard to the input data. This means that they completely and correctly classify all the original training examples. Sections 5 and 8 present consistent and complete example solutions from the inductive concept learning program AQ15c (Wnek et al, 1995). <p> Such differences are expressed in the form of rules that define properties that characterize one group but not the other. These operators are based on programs for learning discriminant concept descriptions. 6 Section 5 will illustrate these two types of descriptions. For more details and their definitions see <ref> (Michalski, 1983) </ref>. Basic methods for concept learning assume that examples do not have errors, that all attributes have a specified value in them, that all examples are located in the same database, and that concepts to be learned have a precise (crisp) description that does not change over time. <p> Each rule shows all characteristics common to a given category, that is, it represents a characteristic description of a category <ref> (Michalski, 1983) </ref>. (Note that some of the conditions in these rules appear to be redundant. For example, the last condition of the Class 2 rule says that Loaners is yes or no. <p> To create a description that points out the most significant distinctions, one needs to apply the operator that creates discriminant descriptions <ref> (Michalski, 1983) </ref>. The operator (GENRULE) is applied to the extended data table in Figure 7, using the Group column as its output attribute.
Reference: <editor> Michalski, R.S., Carbonell, J.G. and Mitchell, T.M. (eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach Vol. 2, </booktitle> <address> San Mateo, CA, </address> <publisher> Morgan Kaufmann, </publisher> <year> 1986. </year>
Reference-contexts: But is there a consistent pattern? To determine such a pattern, one can employ different descriptive models, and instantiate the models to fit the particular sequence. The instantiated model that best fits the data is then used for prediction. Such a method is described in <ref> (Dieterrich and Michalski, 1986) </ref>. The method employs three descriptive modelsperiodic, decomposition and DNF. The periodic model is used to detect repeating patterns in a sequence. For example, Figure 4 depicts a recurring pattern that alternates T-shaped and Ishaped objects. <p> The third model, the DNF (disjunctive normal form) or catchall model, tries to capture general properties characterizing the whole sequence. For example, for the sequence in 1 1 The program SPARC/G <ref> (Michalski, Ko and Chen, 1986) </ref> employs these three descriptive models to detect patterns in a sequence of arbitrary objects, and then uses the patterns to predict a plausible continuation for the sequence. <p> Each 1 3 attribute is assigned a domain and a type. The domain specifies the set of all legal values that the attribute can be assigned in the table. The type defines the ordering (if any) of the values in the domain. For example, the AQ15 learning program <ref> (Michalski et al, 1986) </ref> allows four types of attributes: nominal (no order), linear (total order), cyclic (cyclic total order), and structured (hierarchical order; see Kaufman and Michalski, 1996). The attribute type determines the kinds of operations that are allowed on this attributes values during a learning process. <p> A specific operator may generate rules characterizing a set of facts, discriminating between groups of facts, characterizing a sequence of events, and determining differences between sequences, based on programs such as AQ15c (Wnek et al, 1995) and SPARC/G <ref> (Michalski, Ko and Chen, 1986) </ref>. A KGO for learning rules can usually work in either incremental or batch mode.
Reference: <author> Michalski, </author> <title> R.S. and Imam, I.F., On Learning Decision Structures, </title> <journal> Fundamenta Matematicae, Polish Academy of Sciences, </journal> <note> 1997 (in press). </note>
Reference-contexts: by AQDT-2 for a wind bracing design problem had 5 nodes and 9 leaves, with a predictive accuracy of 88.7% when tested against a new set of data, while the decision tree generated by the popular program C4.5 had 17 nodes and 47 leaves with a predictive accuracy of 84% <ref> (Michalski and Imam, 1997) </ref>.
Reference: <author> Michalski, R.S., Kaufman, K. and Wnek, J., </author> <title> The AQ Family of Learning Programs: A Review of Recent Developments and an Exemplary Application, Reports of the Machine Learning and Inference Laboratory, MLI 91-11, Machine Learning and Inference Laboratory, </title> <institution> George Mason University, Fairfax, VA, </institution> <year> 1991. </year>
Reference-contexts: To facilitate their use, the concept of a knowledge segment was introduced <ref> (Kaufman, Michalski and Kerschberg, 1991) </ref>. A knowledge segment is a structure that links one or more relational tables from the database with one or more structures from the knowledge base.
Reference: <author> Michalski, R.S., Kerschberg, L., Kaufman, K. and Ribeiro, J., </author> <title> Mining for Knowledge in Databases: The INLEN Architecture, Initial Implementation and First Results, </title> <journal> Journal of Intelligent Information Systems: Integrating AI and Database Technologies, </journal> <volume> 1, </volume> <pages> pp. 85-113, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: The main constraint is that these descriptions should be easy to understand and interpret by an expert in the given domain, i.e., they should satisfy the principle of comprehensibility (Michalski, 1993). Our first efforts in developing a methodology for multistrategy data exploration have been implemented in the system INLEN <ref> (Michalski et al, 1992) </ref>. The system combines a range of machine learning methods and tools with more traditional data analysis techniques. These tools provide a user with the capability to make different kinds of data explorations and to derive different kinds of knowledge from a database.
Reference: <author> Michalski, R. S., Ko, H. and Chen, K., SPARC/E(V.2), </author> <title> An Eleusis Rule Generator and Game Player, </title> <journal> Reports of the Intelligent Systems Group, </journal> <volume> ISG No. </volume> <pages> 85-11, </pages> <institution> UIUCDCS-F-85-941, Department of Computer Science, University of Illinois, Urbana, </institution> <year> 1985. </year>
Reference: <author> Michalski, R.S., Ko, H. and Chen, K., </author> <title> Qualitative Prediction: A Method and a Program SPARC/G, Chapter in Expert Systems, </title> <editor> Guetler, C. (ed.), </editor> <publisher> Academic Press, Inc., </publisher> <address> London, </address> <year> 1986. </year> <title> 3 7 Michalski, R.S. and Larson, J.B., Selection of Most Representative Training Examples and Incremental Generation of VL1 Hypotheses: The Underlying Methodology and the Description of Programs ESEL and AQ11, </title> <type> Report No. 867, </type> <institution> Department of Computer Science, University of Illinois, Urbana, </institution> <year> 1978. </year>
Reference-contexts: But is there a consistent pattern? To determine such a pattern, one can employ different descriptive models, and instantiate the models to fit the particular sequence. The instantiated model that best fits the data is then used for prediction. Such a method is described in <ref> (Dieterrich and Michalski, 1986) </ref>. The method employs three descriptive modelsperiodic, decomposition and DNF. The periodic model is used to detect repeating patterns in a sequence. For example, Figure 4 depicts a recurring pattern that alternates T-shaped and Ishaped objects. <p> The third model, the DNF (disjunctive normal form) or catchall model, tries to capture general properties characterizing the whole sequence. For example, for the sequence in 1 1 The program SPARC/G <ref> (Michalski, Ko and Chen, 1986) </ref> employs these three descriptive models to detect patterns in a sequence of arbitrary objects, and then uses the patterns to predict a plausible continuation for the sequence. <p> Each 1 3 attribute is assigned a domain and a type. The domain specifies the set of all legal values that the attribute can be assigned in the table. The type defines the ordering (if any) of the values in the domain. For example, the AQ15 learning program <ref> (Michalski et al, 1986) </ref> allows four types of attributes: nominal (no order), linear (total order), cyclic (cyclic total order), and structured (hierarchical order; see Kaufman and Michalski, 1996). The attribute type determines the kinds of operations that are allowed on this attributes values during a learning process. <p> A specific operator may generate rules characterizing a set of facts, discriminating between groups of facts, characterizing a sequence of events, and determining differences between sequences, based on programs such as AQ15c (Wnek et al, 1995) and SPARC/G <ref> (Michalski, Ko and Chen, 1986) </ref>. A KGO for learning rules can usually work in either incremental or batch mode.
Reference: <author> Michalski, R.S. and McCormick, B.H., </author> <title> Interval Generalization of Switching Theory, </title> <booktitle> Proceedings of the 3rd Annual Houston Conference on Computer and System Science, </booktitle> <address> Houston, TX, </address> <year> 1971. </year>
Reference: <author> Michalski, R.S. , Mozetic, I., Hong, J. and Lavrac, N., </author> <title> The AQ15 Inductive Learning System: An Overview and Experiments, </title> <type> ISG Report 86-20, </type> <institution> UIUCDCS-R-86-1260, Department of Computer Science, University of Illinois, Urbana, </institution> <year> 1986. </year>
Reference-contexts: But is there a consistent pattern? To determine such a pattern, one can employ different descriptive models, and instantiate the models to fit the particular sequence. The instantiated model that best fits the data is then used for prediction. Such a method is described in <ref> (Dieterrich and Michalski, 1986) </ref>. The method employs three descriptive modelsperiodic, decomposition and DNF. The periodic model is used to detect repeating patterns in a sequence. For example, Figure 4 depicts a recurring pattern that alternates T-shaped and Ishaped objects. <p> The third model, the DNF (disjunctive normal form) or catchall model, tries to capture general properties characterizing the whole sequence. For example, for the sequence in 1 1 The program SPARC/G <ref> (Michalski, Ko and Chen, 1986) </ref> employs these three descriptive models to detect patterns in a sequence of arbitrary objects, and then uses the patterns to predict a plausible continuation for the sequence. <p> Each 1 3 attribute is assigned a domain and a type. The domain specifies the set of all legal values that the attribute can be assigned in the table. The type defines the ordering (if any) of the values in the domain. For example, the AQ15 learning program <ref> (Michalski et al, 1986) </ref> allows four types of attributes: nominal (no order), linear (total order), cyclic (cyclic total order), and structured (hierarchical order; see Kaufman and Michalski, 1996). The attribute type determines the kinds of operations that are allowed on this attributes values during a learning process. <p> A specific operator may generate rules characterizing a set of facts, discriminating between groups of facts, characterizing a sequence of events, and determining differences between sequences, based on programs such as AQ15c (Wnek et al, 1995) and SPARC/G <ref> (Michalski, Ko and Chen, 1986) </ref>. A KGO for learning rules can usually work in either incremental or batch mode.
Reference: <author> Michalski, R.S., Stepp, R. and Diday, E., </author> <title> A Recent Advance in Data Analysis: Clustering Objects into Classes Characterized by Conjunctive Concepts, </title> <editor> Chapter in Kanal, L. and Rosenfeld, A. (eds.), </editor> <booktitle> Progress in Pattern Recognition, </booktitle> <volume> Vol. 1, </volume> <pages> pp. 33-55, </pages> <year> 1981. </year>
Reference-contexts: A classification quality criterion used in conceptual clustering may involve a variety of factors, such as the fit of a cluster description to the data (called sparseness), the simplicity of the description, and other properties of the entities or the concepts that describe them <ref> (Michalski, Stepp and Diday, 1981) </ref>. An example of conceptual clustering is presented in Section 5. Some new ideas on employing conceptual clustering for structuring text databases and creating concept lattices for discovering dependencies in data are in (Carpineto and Romano, 1995a; 1995b). <p> Clustering: The problem is to automatically partition the rows of the table into groups that correspond to conceptual clusters, that is, sets of entities with high conceptual cohesiveness <ref> (Michalski, Stepp and Diday, 1981) </ref>. Such a clustering operator will generate an additional column in the table that corresponds to a new attribute cluster name. The values of this attribute for each tuple in the table indicate the assigned class of the entity. <p> GENEQ operators generate equations characterizing numerical data sets and qualitatively describing the conditions under which these equations apply (e.g., Falkenhainer and Michalski, 1990). GENHIER operators build conceptual clusters or hierarchies. They are based on the program CLUSTER methodology <ref> (Michalski, Stepp and Diday, 1981) </ref>. The operator in INLEN is based on the reimplementation in C of the program CLUSTER/2 (Stepp, 1984). TRANSFORM operators perform various transformations on the knowledge segments, e.g., generalization or specialization, abstraction or concretion, optimization of given rules, etc. according to user-provided criteria.
Reference: <author> Mingers, J., </author> <title> An Empirical Comparison of Selection Measures for Decision-Tree Induction, </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> pp. 319-342, </pages> <year> 1989. </year>
Reference: <author> Morgenthaler, S. and Tukey, J.W., </author> <title> The Next Future of Data Analysis, </title> <editor> In Diday, E. (ed.), </editor> <booktitle> Proceedings of the Conference on Data Analysis, Learning Symbolic and Numeric Knowledge, </booktitle> <publisher> Nova Science Publishers, Inc., </publisher> <address> Antibes, </address> <year> 1989. </year> <title> Pawlak, Z, Rough Sets: Theoretical Aspects of Reasoning about Data, </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Dordrecht, </address> <year> 1991. </year>
Reference: <author> Quinlan, J.R., </author> <title> Induction of Decision Trees, </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> pp. 81-106, </pages> <year> 1986. </year>
Reference-contexts: One simple and popular form of attributional description is a decision or classification tree. In such a tree, nodes correspond to attributes, branches stemming from the nodes correspond to attribute values, and leaves correspond to individual classes <ref> (e.g., Quinlan, 1986) </ref>. A decision tree can be transformed into a set of decision rules (a ruleset) by traversing all paths from the root to individual leaves. Such rules can be often simplified by detecting superfluous conditions in them (e.g., Quinlan, 1993). <p> For example, if an attribute (test) assigned to a high-level node in the tree is impossible or too costly to measure, the decision tree offers no alternative course of action other than probabilistic reasoning <ref> (Quinlan, 1986) </ref>. In contrast, a human making the decision would probably search for alternative tests to perform. People can do this because they typically store decision knowledge in a declarative form.
Reference: <author> Quinlan, J.R., </author> <title> Probabilistic Decision Trees, </title> <editor> In Y. Kodratoff and R.S. Michalski, (Eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, Volume III, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <pages> pp. 140-152, </pages> <year> 1990. </year>
Reference: <author> Quinlan, J.R., C4.5: </author> <title> Programs for Machine Learning, </title> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA, </address> <year> 1993. </year>
Reference-contexts: A decision tree can be transformed into a set of decision rules (a ruleset) by traversing all paths from the root to individual leaves. Such rules can be often simplified by detecting superfluous conditions in them <ref> (e.g., Quinlan, 1993) </ref>. The opposite process of transforming a ruleset into a decision tree is not so direct (Imam, 1995), because a rule representation is more powerful than a tree representation. <p> This can be done by applying one of many methods for attribute selection, such as Gain Ratio <ref> (Quinlan, 1993) </ref> or Promise level (Baim, 1982). Generating new attributes: The problem is to generate additional columns that correspond to new attributes generated by a constructive induction procedure. <p> GENTREE operators build a decision structure from a given set of decision rules (e.g., Imam and Michalski, 1993), or from examples <ref> (e.g., Quinlan, 1993) </ref>. A decision structure is a generalization of the concept of a decision tree in which nodes can be assigned an attribute or a function of attributes. Individual branches may be assigned a set of attribute values. <p> To this end, one may use many different criteria for evaluating the relevance of an attribute for a given classification problem, such as gain ratio <ref> (Quinlan, 1993) </ref>, gini index (Breiman et al, 1984), PROMISE (Baim, 1982), and chisquare analysis (Hart, 1984; Mingers, 1989). These criteria evaluate attributes on the basis of their expected global performance, which means that those attributes with the highest ability to discriminate among all classes are selected as the most relevant.
Reference: <author> Reinke, R.E., </author> <title> Knowledge Acquisition and Refinement Tools for the ADVISE Meta-Expert System, </title> <type> Masters Thesis, </type> <institution> Department of Computer Science, University of Illinois, Urbana, </institution> <year> 1984. </year>
Reference-contexts: These operators can also be used to apply the rules to any given situation to determine a decision. The TEST operator implemented in INLEN is based on the ATEST program <ref> (Reinke, 1984) </ref>. VISUALIZE operators are used to present data and/or knowledge to the user in a convenient, easy-to-understand format (Wnek, 1995).
Reference: <author> Reinke, R.E. and Michalski, </author> <title> R.S., Incremental Learning of Concept Descriptions: A Method and Experimental Results, </title> <journal> Machine Intelligence, </journal> <volume> 11, </volume> <pages> pp. 263-288, </pages> <year> 1988. </year>
Reference: <author> Ribeiro, J., Kaufman, K. and Kerschberg, L., </author> <title> Knowledge Discovery from Multiple Databases, </title> <booktitle> Proceedings of the First International Conference on Knowledge Discovery and Data Mining, </booktitle> <address> Montreal PQ, </address> <pages> pp. 240-245, </pages> <year> 1995. </year>
Reference-contexts: Learning from incomplete data, i.e., learning from examples in which the values of some attributes are unknown (e.g., Dontas, 1988; Lakshminarayan et al, 1996). Learning from distributed data, i.e., learning from separate collections of data that must be brought together if the patterns within them are to be exposed <ref> (e.g., Ribeiro, Kaufman and Kerschberg, 1995) </ref>. Learning drifting or evolving concepts, i.e., learning concepts that are not stable but changing over time, randomly or in a certain general direction. For example, the area of interest of a user is often an evolving concept (e.g., Widmer and Kubat, 1996).
Reference: <author> Rosch, E., Mervis, C., Gray, W., Johnson, D. and Boyes-Braem, P., </author> <title> Basic Objects in Natural Categories, </title> <journal> Cognitive Psychology, </journal> <volume> 8, </volume> <pages> pp. 382-439, </pages> <year> 1976. </year>
Reference-contexts: Nodes marked by + and are values occurring in positive and negative examples, respectively. Cognitive scientists have noticed that people prefer certain nodes in a generalization hierarchy (concepts) over other nodes when creating descriptions <ref> (e.g., Rosch et al, 1976) </ref>. Factors that influence the choice of a concept (node) include the concept typicality (how common are a concepts features among its sibling concepts), and the context in which the concept is being used.
Reference: <author> Sharma, S., </author> <title> Applied Multivariate Techniques, </title> <publisher> London, John Wiley & Sons, Inc., </publisher> <year> 1996. </year> <note> 3 8 Slowinski, </note> <editor> R. (ed.), </editor> <title> Intelligent Decision Support: Handbook of Applications and Advances of the Rough Sets Theory, </title> <publisher> Dordrecht/Boston/London, Kluwer Academic Publishers, </publisher> <year> 1992. </year>
Reference: <author> Stepp, R., </author> <title> A Description and Users Guide for CLUSTER/2, A Program for Conjunctive Conceptual Clustering, </title> <type> Report No. </type> <institution> UIUCDCS-R-83-1084, Department of Computer Science, University of Illinois, Urbana, </institution> <year> 1984. </year>
Reference-contexts: GENHIER operators build conceptual clusters or hierarchies. They are based on the program CLUSTER methodology (Michalski, Stepp and Diday, 1981). The operator in INLEN is based on the reimplementation in C of the program CLUSTER/2 <ref> (Stepp, 1984) </ref>. TRANSFORM operators perform various transformations on the knowledge segments, e.g., generalization or specialization, abstraction or concretion, optimization of given rules, etc. according to user-provided criteria. For instance, one such operator climbs an attributes generalization hierarchy to build more general decision rules (Kaufman and Michalski, 1996). <p> The first operator is realized by the CLUSTER/2 program for conceptual clustering <ref> (Stepp, 1984) </ref>. The second operator is realized by the AQ15c rule learning program (Wnek et al, 1995). This section illustrates these operators through an application to a datatable characterizing hard drives (Figure 7). The datatable is based on information published in the October, 1994 issue of MacUser.
Reference: <author> Tukey, J.W., </author> <title> The Collected Works of John W. </title> <journal> Tukey, </journal> <volume> Vol. </volume> <booktitle> V, Philosophy and Principles of Data Analysis: </booktitle> <pages> 1965-1986, </pages> <editor> Jones, L.V. (ed.), </editor> <publisher> Wadsworth & Brooks/Cole, </publisher> <address> Monterey, CA, </address> <year> 1986. </year>
Reference: <editor> Van Mechelen, I., Hampton, J., Michalski, R.S. and Theuns, P. (eds.), </editor> <title> Categories and Concepts: Theoretical Views and Inductive Data Analysis, </title> <publisher> London, Academic Press, </publisher> <year> 1993. </year>
Reference: <author> Widmer, G. and Kubat, M., </author> <title> Learning in the Presence of Concept Drift and Hidden Concepts, </title> <journal> Machine Learning, </journal> <volume> 23, </volume> <pages> pp. 69-101, </pages> <year> 1996. </year>
Reference-contexts: Learning drifting or evolving concepts, i.e., learning concepts that are not stable but changing over time, randomly or in a certain general direction. For example, the area of interest of a user is often an evolving concept <ref> (e.g., Widmer and Kubat, 1996) </ref>. Learning concepts from data arriving over time, i.e., incremental learning in which currently held hypotheses characterizing concepts may need to be updated to account for the new data (e.g., Maloof and Michalski, 1995).
Reference: <author> Wnek, J., </author> <title> DIAV 2.0 User Manual, Specification and Guide through the Diagrammatic Visualization System, Reports of the Machine Learning and Inference Laboratory, </title> <type> MLI 95-5, </type> <institution> George Mason University, Fairfax, VA, </institution> <year> 1995. </year>
Reference-contexts: Learned rules are usually consistent and complete with regard to the input data. This means that they completely and correctly classify all the original training examples. Sections 5 and 8 present consistent and complete example solutions from the inductive concept learning program AQ15c <ref> (Wnek et al, 1995) </ref>. In some applications, especially those involving learning rules from noisy data or learning flexible concepts (Michalski, 1990), it may be advantageous to learn descriptions that are incomplete and/or inconsistent (Bergadano et al, 1992). <p> These operators perform symbolic and numerical data exploration tasks. They are based on various machine learning and inference programs, on conventional data exploration techniques, and on visualization operators for displaying graphically the results of exploration. The diagrammatic visualization method DIAV <ref> (Wnek ,1995) </ref> is used for displaying the effects of symbolic learning operations on data. <p> A specific operator may generate rules characterizing a set of facts, discriminating between groups of facts, characterizing a sequence of events, and determining differences between sequences, based on programs such as AQ15c <ref> (Wnek et al, 1995) </ref> and SPARC/G (Michalski, Ko and Chen, 1986). A KGO for learning rules can usually work in either incremental or batch mode. <p> These operators can also be used to apply the rules to any given situation to determine a decision. The TEST operator implemented in INLEN is based on the ATEST program (Reinke, 1984). VISUALIZE operators are used to present data and/or knowledge to the user in a convenient, easy-to-understand format <ref> (Wnek, 1995) </ref>. <p> The first operator is realized by the CLUSTER/2 program for conceptual clustering (Stepp, 1984). The second operator is realized by the AQ15c rule learning program <ref> (Wnek et al, 1995) </ref>. This section illustrates these operators through an application to a datatable characterizing hard drives (Figure 7). The datatable is based on information published in the October, 1994 issue of MacUser.
Reference: <author> Wnek, J., Kaufman, K., Bloedorn, E. and Michalski, </author> <title> R.S., Selective Induction Learning System AQ15c: The Method and Users Guide, Reports of the Machine Learning and Inference Laboratory, MLI 95-4, Machine Learning and Inference Laboratory, </title> <institution> George Mason University, Fairfax, VA, </institution> <year> 1995. </year>
Reference-contexts: Learned rules are usually consistent and complete with regard to the input data. This means that they completely and correctly classify all the original training examples. Sections 5 and 8 present consistent and complete example solutions from the inductive concept learning program AQ15c <ref> (Wnek et al, 1995) </ref>. In some applications, especially those involving learning rules from noisy data or learning flexible concepts (Michalski, 1990), it may be advantageous to learn descriptions that are incomplete and/or inconsistent (Bergadano et al, 1992). <p> These operators perform symbolic and numerical data exploration tasks. They are based on various machine learning and inference programs, on conventional data exploration techniques, and on visualization operators for displaying graphically the results of exploration. The diagrammatic visualization method DIAV <ref> (Wnek ,1995) </ref> is used for displaying the effects of symbolic learning operations on data. <p> A specific operator may generate rules characterizing a set of facts, discriminating between groups of facts, characterizing a sequence of events, and determining differences between sequences, based on programs such as AQ15c <ref> (Wnek et al, 1995) </ref> and SPARC/G (Michalski, Ko and Chen, 1986). A KGO for learning rules can usually work in either incremental or batch mode. <p> These operators can also be used to apply the rules to any given situation to determine a decision. The TEST operator implemented in INLEN is based on the ATEST program (Reinke, 1984). VISUALIZE operators are used to present data and/or knowledge to the user in a convenient, easy-to-understand format <ref> (Wnek, 1995) </ref>. <p> The first operator is realized by the CLUSTER/2 program for conceptual clustering (Stepp, 1984). The second operator is realized by the AQ15c rule learning program <ref> (Wnek et al, 1995) </ref>. This section illustrates these operators through an application to a datatable characterizing hard drives (Figure 7). The datatable is based on information published in the October, 1994 issue of MacUser.
Reference: <author> Wnek, J. and Michalski, </author> <title> R.S., Hypothesis-driven Constructive Induction in AQ17-HCI: </title>
References-found: 70

