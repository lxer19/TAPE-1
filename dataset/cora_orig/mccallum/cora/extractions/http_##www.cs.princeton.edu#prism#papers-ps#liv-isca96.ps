URL: http://www.cs.princeton.edu/prism/papers-ps/liv-isca96.ps
Refering-URL: http://www.cs.princeton.edu/prism/html/all-papers.html
Root-URL: http://www.cs.princeton.edu
Title: Understanding Application Performance on Shared Virtual Memory Systems  
Author: Liviu Iftode, Jaswinder Pal Singh and Kai Li 
Address: Princeton, NJ 08544 USA  
Affiliation: Department of Computer Science Princeton University  
Date: May 1996.  
Note: In Proceedings of the 23rd Annual International Symposium on Computer Architecture,  
Abstract: Many researchers have proposed interesting protocols for shared virtual memory (SVM) systems, and demonstrated performance improvements on parallel programs. However, there is still no clear understanding of the performance potential of SVM systems for different classes of applications. This paper begins to fill this gap, by studying the performance of a range of applications in detail and understanding it in light of application characteristics. We first develop a brief classification of the inherent data sharing patterns in the applications, and how they interact with system granularities to yield the communication patterns relevant to SVM systems. We then use detailed simulation to compare the performance of two SVM approaches| Lazy Released Consistency (LRC) and Automatic Update Release Consistency (AURC)|with each other and with an all-hardware CC-NUMA approach. We examine how performance is affected by problem size, machine size, key system parameters, and the use of less optimized program implementations. We find that SVM can indeed perform quite well for systems of at least up to 32 processors for several nontrivial applications. However, performance is much more variable across applications than on CC-NUMA systems, and the problem sizes needed to obtain good parallel performance are substantially larger. The hardware-assisted AURC system tends to perform significantly better than the all-software LRC under our system assumptions, particularly when realistic cache hierarchies are used. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. V. Adve, A. L. Cox, S. Dwarkadas, R. Rajamony, and W. Zwaenepoel. </author> <title> A Comparison of Entry Consistency and Lazy Release Consistency Implementation. </title> <booktitle> In The 2nd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: Papers that have proposed protocols or extensions have evaluated them with only a small set of usually quite simple programs. The programs, problem sizes and machine parameters used have also been limited and different across studies, making comparisons difficult. Despite a few comparative studies <ref> [6, 1] </ref> it is not well understood whether the SVM approach works well for important classes of applications, whether the approaches that use a small amount of hardware support have substantial advantages over all-software implementations, and how the answers to these questions are affected by architectural and application parameters.
Reference: [2] <author> J.K. Bennett, J.B. Carter, and W. Zwaenepoel. </author> <title> Adaptive Software Cache Management for Distributed Shared Memory Architectures. </title> <booktitle> In Proceedings of the 17th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 125-134, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Several previous studies have proposed classifications of data objects based on access patterns, although in different contexts. Gupta and Weber [16] proposed a classification to characterize cache invalidation patterns in shared memory multiprocessors. Bennett et al <ref> [2] </ref> expanded this classification and used it to perform adaptive software cache management on distributed shared memory machines. Our classification has a different purpose, to understand the observed performance of SVM systems, and is hence different from these.
Reference: [3] <author> B.N. Bershad, M.J. Zekauskas, </author> <title> and W.A. </title> <booktitle> Sawdon. The Midway Distributed Shared Memory System. In Proceedings of the IEEE COMPCON '93 Conference, </booktitle> <month> February </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Since shared virtual memory (SVM) was proposed [15], much research has concentrated on improving its performance through both relaxed consistency models <ref> [5, 13, 3] </ref> and architectural support. The goal has been to deliver performance close to that of a hardware cache-coherent shared address space machine at the same scale. While many SVM systems have been proposed, their performance on real applications is not well understood so far.
Reference: [4] <author> M. Blumrich, K. Li, R. Alpert, C. Dubnicki, E. Felten, and J. Sandberg. </author> <title> A Virtual Memory Mapped Network Interface for the SHRIMP Multicomputer. </title> <booktitle> In Proceedings of the 21st Annual Symposium on Computer Architecture, </booktitle> <pages> pages 142-153, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Automatic Update Release Consistency The AURC protocol [9] is also based on a lazy release consistency model, but it takes advantage of the automatic update hardware feature of virtual memory-mapped network interfaces <ref> [4, 14] </ref> to detect, propagate and merge writes by different processors to a page. <p> The page size is 4KB, so a page transfer takes 1K I/O bus cycles. The network interface has a mechanism that combines consecutive automatic updates to a single network packet, whose size cannot exceed 40 words. It is similar to the network interface of the SHRIMP multicomputer <ref> [4] </ref>. The network topology is a two-dimensional mesh. The simulators model contention everywhere (buses, memory, network interface) in great detail, except for contention in the network topology itself. We have tried to charge both the AURC and LRC protocols fairly for software operations.
Reference: [5] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and Performance of Munin. </title> <booktitle> In Proceedings of the Thirteenth Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Since shared virtual memory (SVM) was proposed [15], much research has concentrated on improving its performance through both relaxed consistency models <ref> [5, 13, 3] </ref> and architectural support. The goal has been to deliver performance close to that of a hardware cache-coherent shared address space machine at the same scale. While many SVM systems have been proposed, their performance on real applications is not well understood so far.
Reference: [6] <author> A.L. Cox, S. Dwarkadas, P. Keleher, H. Lu, R. Rajamony, and W. Zwaenepoel. </author> <title> Software Versus Hardware Shared-Memory Implementation: A Case Study. </title> <booktitle> In Proceedings of the 21st Annual Symposium on Computer Architecture, </booktitle> <pages> pages 106-117, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Papers that have proposed protocols or extensions have evaluated them with only a small set of usually quite simple programs. The programs, problem sizes and machine parameters used have also been limited and different across studies, making comparisons difficult. Despite a few comparative studies <ref> [6, 1] </ref> it is not well understood whether the SVM approach works well for important classes of applications, whether the approaches that use a small amount of hardware support have substantial advantages over all-software implementations, and how the answers to these questions are affected by architectural and application parameters.
Reference: [7] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Relaxed consistency models help to alleviate these problems by allowing the system to delay the coherence activity from when the modifications happen until the next synchronization point. For example, release consistency <ref> [7] </ref> is a relaxed model which guarantees memory consistency only after explicit synchronization points, distinguished as either acquire or release operations.
Reference: [8] <author> C. Holt, M. Heinrich, J.P. Singh, E. Rothberg, and J. Hen-nessy. </author> <title> The Effects of Latency, Occupancy and Bandwidth in Distributed Shared Memory Multiprocessors. </title> <type> Technical Report CSL-TR-95-660, </type> <institution> Stanford, </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: The hardware CC-NUMA simulator we use is a modified version of the detailed FlashLite simulator built for the Stanford FLASH machine. It is the same as the simulator used in <ref> [8] </ref>, and models a hardware rather than flexible communication controller. It uses almost exactly the same parameters as described above (with an 800MB/s memory bus; the I/O bus is irrelevant), and is used only as a reference point for comparison.
Reference: [9] <author> L. Iftode, C. Dubnicki, E. W. Felten, and Kai Li. </author> <title> Improving Release-Consistent Shared Virtual Memory using Automatic Update. </title> <booktitle> In The 2nd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: Third, even when consecutive diffs of the same page (from multiple synchronization intervals) are obtained from one place, they have to be obtained as separate diffs and applied individually by the faulting processor. Automatic Update Release Consistency The AURC protocol <ref> [9] </ref> is also based on a lazy release consistency model, but it takes advantage of the automatic update hardware feature of virtual memory-mapped network interfaces [4, 14] to detect, propagate and merge writes by different processors to a page.
Reference: [10] <author> L. Iftode, J.P. Singh, and K. Li. </author> <title> Irregular Applications under Software Shared Memory. </title> <type> Technical Report TR-514-96, </type> <institution> Princeton, NJ, </institution> <month> February </month> <year> 1996. </year>
Reference-contexts: Speedups can be increased in some cases by increasing the problem size, but not as effectively as for regular applications. This is because in the irregular applications the accesses do not tend to get less scattered when the problem size increases <ref> [17, 10] </ref>, and no thresholds are crossed that reduce fragmentation or false sharing. The inherent communication to computation ratio does not change so quickly either. <p> For example, further relaxing the consistency model in a transparent manner can help reduce false sharing in many situations [11]. It is also interesting to examine how to modify programs to perform better on SVM systems <ref> [10] </ref>. Acknowledgments We are grateful to the referees, whose comments helped improve the paper. Chris Holt helped us with hardware CC-NUMA simulations. We also thank Cezary Dubnicki and Stefanos Damianakis for their help, and Jim Philbin, Henry Cejtin, Ed Rothberg and Andrew Erlichson for generously providing us with simulation cycles.
Reference: [11] <author> L. Iftode, J.P. Singh, and K. Li. </author> <title> Scope Consistency: a Bridge between Release Consistency and Entry Consistency. </title> <booktitle> In Proceedings of the 8th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <month> June </month> <year> 1996. </year> <note> To appear. </note>
Reference-contexts: For example, further relaxing the consistency model in a transparent manner can help reduce false sharing in many situations <ref> [11] </ref>. It is also interesting to examine how to modify programs to perform better on SVM systems [10]. Acknowledgments We are grateful to the referees, whose comments helped improve the paper. Chris Holt helped us with hardware CC-NUMA simulations.
Reference: [12] <author> P. Keleher, A.L. Cox, S. Dwarkadas, and W. Zwaenepoel. TreadMarks: </author> <title> Distributed Shared Memory on Standard Workstations and Operating Systems. </title> <booktitle> In Proceedings of the Winter USENIX Conference, </booktitle> <pages> pages 115-132, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: Further details can be found in [13] and <ref> [12] </ref>. Such a delayed, multiple-writer protocol has several benefits. By delaying coherence until synchronization, it reduces the number of times data and coherence messages are communicated and hence the number of times expensive software protocol overhead is incurred.
Reference: [13] <author> P. Keleher, A.L. Cox, and W. Zwaenepoel. </author> <title> Lazy Consistency for Software Distributed Shared Memory. </title> <booktitle> In Proceedings of the 19th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Since shared virtual memory (SVM) was proposed [15], much research has concentrated on improving its performance through both relaxed consistency models <ref> [5, 13, 3] </ref> and architectural support. The goal has been to deliver performance close to that of a hardware cache-coherent shared address space machine at the same scale. While many SVM systems have been proposed, their performance on real applications is not well understood so far. <p> At the first access to a page following an invalidation, the page fault handler must collect the diffs for the page from the processors that have produced them, and apply these diffs locally in the proper causal order to reconstitute the coherent page. Further details can be found in <ref> [13] </ref> and [12]. Such a delayed, multiple-writer protocol has several benefits. By delaying coherence until synchronization, it reduces the number of times data and coherence messages are communicated and hence the number of times expensive software protocol overhead is incurred.
Reference: [14] <author> L. I. Kontothanassis and M. L. Scott. </author> <title> Using Memory-Mapped Network Interfaces to Improve t he Performance of Distributed Shared Memory. </title> <booktitle> In The 2nd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: Automatic Update Release Consistency The AURC protocol [9] is also based on a lazy release consistency model, but it takes advantage of the automatic update hardware feature of virtual memory-mapped network interfaces <ref> [4, 14] </ref> to detect, propagate and merge writes by different processors to a page.
Reference: [15] <author> K. Li and P. Hudak. </author> <title> Memory Coherence in Shared Virtual Memory Systems. </title> <booktitle> In Proceedings of the 5th Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 229-239, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: 1 Introduction Since shared virtual memory (SVM) was proposed <ref> [15] </ref>, much research has concentrated on improving its performance through both relaxed consistency models [5, 13, 3] and architectural support. The goal has been to deliver performance close to that of a hardware cache-coherent shared address space machine at the same scale. <p> Section 8 briefly examines robustness with respect to program optimizations (particularly data structuring and placement), as well as sensitivity to some detailed machine parameters. Finally, Section 9 summarizes our results and concludes the paper. 2 Shared Virtual Memory The basic idea in SVM <ref> [15] </ref> is to exploit the virtual memory mechanism to emulate a simple invalidation-based cache coherence protocol at the granularity of pages instead of cache lines. The large granularity of communication and coherence (an entire page) can affect the performance of any SVM by causing false sharing and fragmentation.
Reference: [16] <author> W. Weber and A. Gupta. </author> <title> Analysis of Cache Invalidation Patterns in Multiprocessors. </title> <booktitle> In The Third International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 243-256, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: For AURC (like CC-NUMA), the communication incurred also depends on how data are allocated and distributed among processor memories, but this is irrelevant in LRC (see Section 8). Several previous studies have proposed classifications of data objects based on access patterns, although in different contexts. Gupta and Weber <ref> [16] </ref> proposed a classification to characterize cache invalidation patterns in shared memory multiprocessors. Bennett et al [2] expanded this classification and used it to perform adaptive software cache management on distributed shared memory machines.
Reference: [17] <author> Steven Woo, Moriyoshi Ohara, Evan Torrie, Jaswinder Pal Singh, and Anoop Gupta. </author> <title> The SPLASH-2 Programs: Characterization and Methodological Considerations. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: This paper begins to answer these questions, using two different SVM systems as examples. To obtain good coverage, we use ten applications from the SPLASH-2 suite <ref> [17] </ref> that display a range of different communication patterns and granularities. We first classify their sharing patterns in a manner relevant for SVM systems. <p> MP pattern for a page is particularly bad because it means that means that diffs for the page will have to be obtained from multiple processors upon a page fault (especially after a barrier synchronization). 4.2 Applications The applications we use include a range of scientific, engineering and graphics programs <ref> [17] </ref>. We use them as written for hardware cache-coherent systems with cache line granularity, not attempting to reprogram them for SVM other than to change the padding amount for some obvious data structures. <p> We describe the applications themselves only very briefly, focusing mostly on their dominant sharing patterns relevant to SVM systems. More detailed descriptions can be found in the references cited in <ref> [17] </ref>. Barnes simulates the interactions among a system of particles over a number of time steps, using the Barnes-Hut hierarchical N-body method. The particle update and force calculation phases in the application are inherently 1P-MC sharing patterns. <p> Finally, to capture the effect of realistic situations in which the working set of an application does not fit in the cache, we ran Ocean and FFT with smaller cache sizes as recommended in <ref> [17] </ref> (we did not run the other two recommended applications, Radix and Raytrace, this way, the former because it performs so poorly and the latter because of simulation time). <p> Speedups can be increased in some cases by increasing the problem size, but not as effectively as for regular applications. This is because in the irregular applications the accesses do not tend to get less scattered when the problem size increases <ref> [17, 10] </ref>, and no thresholds are crossed that reduce fragmentation or false sharing. The inherent communication to computation ratio does not change so quickly either.
References-found: 17

