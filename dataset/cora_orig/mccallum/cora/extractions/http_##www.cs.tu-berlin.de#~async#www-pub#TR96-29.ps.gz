URL: http://www.cs.tu-berlin.de/~async/www-pub/TR96-29.ps.gz
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00147.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Decimatable Boltzmann Machines vs. Gibbs Sampling  
Author: Stefan Ruger Anton Weinberger, and Sebastian Wittchen 
Keyword: Boltzmann machines.  
Date: July 1996  
Address: Berlin  
Affiliation: Bericht 96-29 des Fachbereichs Informatik der Technischen Universitat Berlin  Technische Universitat  
Abstract: Exact Boltzmann learning can be done in certain restricted networks by the technique of decimation. We have enlarged the set of dec-imatable Boltzmann machines by introducing a new and more general decimation rule. We have compared solutions of a probability density estimation problem with decimatable Boltzmann machines to the results obtained by Gibbs sampling in unrestricted (non-decimatable) 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> David H. Ackley, Geoffrey E. Hinton, and Terrence J. Sejnowski. </author> <title> A learning algorithm for Boltzmann machines. </title> <journal> Cognitive Science, </journal> <volume> 9 </volume> <pages> 147-169, </pages> <year> 1985. </year>
Reference-contexts: 1 Introduction Boltzmann machines [3] have been the first stochastical neural networks for which a learning algorithm <ref> [1] </ref> has been defined. This learning rule is based upon local interactions of nodes. Generally, their calculation is intractable in the number of hidden nodes. However, Gibbs sampling with simulated annealing can estimate them sufficiently precise. <p> Then, with q = ff 7! P fl r (fffl), a suitable cost function is the so-called information gain E w := ff X r (fljff) log r (fljff) p w (fljff) : (4) Gradient descent w ab = r w E w yields the Boltzmann learning rule <ref> [1, 8] </ref> w ab = 2T hs a s b i fffl clamped hs a s b i q ff clamped X denotes the average of X with respect to r, which in turn is given empirically by a multi-set of input-output patterns fffl.
Reference: [2] <author> T. P. Eggarter. </author> <title> Cayley trees, the Ising problem, and the thermodynamic limit. </title> <journal> Physical Review B, </journal> <volume> 9(7) </volume> <pages> 2989-2992, </pages> <year> 1974. </year>
Reference-contexts: Especially in larger networks it is impossible to compute the expectation values directly from the Boltzmann-Gibbs distribution. Instead, they can be estimated through Gibbs sampling. However, this method turns out to be very computation-intensive. In the next section the technique of decimation <ref> [2, 4, 10] </ref> will be presented that allows the exact and efficient calculation of expectation values in certain Boltzmann machines. 2.2 Decimation in Boltzmann Trees We will describe the decimation rules proposed by [10] in a way that they are most easily extensible.
Reference: [3] <author> Geoffrey E. Hinton and Terrence J. Sejnowski. </author> <title> Optimal perceptual inference. </title> <booktitle> In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 448-453. </pages> <publisher> IEEE, </publisher> <year> 1983. </year> <month> 16 </month>
Reference-contexts: 1 Introduction Boltzmann machines <ref> [3] </ref> have been the first stochastical neural networks for which a learning algorithm [1] has been defined. This learning rule is based upon local interactions of nodes. Generally, their calculation is intractable in the number of hidden nodes. However, Gibbs sampling with simulated annealing can estimate them sufficiently precise.
Reference: [4] <author> C. Itzykson and J. Drouffe. </author> <title> Statistical Field Theory. </title> <publisher> Cambridge Uni--versity Press, </publisher> <year> 1991. </year>
Reference-contexts: Especially in larger networks it is impossible to compute the expectation values directly from the Boltzmann-Gibbs distribution. Instead, they can be estimated through Gibbs sampling. However, this method turns out to be very computation-intensive. In the next section the technique of decimation <ref> [2, 4, 10] </ref> will be presented that allows the exact and efficient calculation of expectation values in certain Boltzmann machines. 2.2 Decimation in Boltzmann Trees We will describe the decimation rules proposed by [10] in a way that they are most easily extensible.
Reference: [5] <author> Steffen L. Lauritzen and David J. Spiegelhalter. </author> <title> Local computations with probabilities on graphical structures and their application to expert systems (with discussion). </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> 50 </volume> <pages> 157-224, </pages> <year> 1988. </year>
Reference-contexts: The rest of the paper is organized as follows. In Section 3 we construct a pattern multi-set from a particular belief network that was introduced by Lauritzen and Spiegelhalter <ref> [5] </ref> as a didactical example. This example is not only rich enough to contain most of the interesting aspects of Boltzmann machines but also small enough to calculate all interesting properties exactly. <p> This can enormously reduce the complexity of determining the distribution. Now we introduce a fictitious medical example based on a belief network from <ref> [5] </ref>: Tuberculosis and lung cancer may be indicated by a positive chest X-ray. Both can cause shortness of breath (dyspna). The same is true for bronchitis. A recent visit to Asia increases the probability of tuberculosis, while smoking is a possible cause of both lung cancer and bronchitis.
Reference: [6] <author> Jyh-Han Lin and Jeffrey S. Vitter. </author> <title> Complexity issues in learning by neural nets. </title> <editor> In R. Rivest, D. Haussler, and M. Warmuth, editors, </editor> <booktitle> COLT89, </booktitle> <pages> pages 118-132. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1989. </year>
Reference-contexts: Weights that are not useful are not automatically set to zero. This is in accordance with the observation that, in general, the task of finding an optimal architecture is NP-hard <ref> [6] </ref>. 15 5 Conclusions Technically, decimation is much simpler to handle than Gibbs sampling: the interactions of nodes are simply calculated with an algorithm that is designed to yield exact results. With Gibbs sampling, parameters for the annealing schedule and for the sampling itself have to be determined. <p> Although the task of finding an optimal architecture may be intractable, as suggested by <ref> [6] </ref>, one would like to have an algorithm that finds a sufficiently good decimatable Boltzmann architecture. These issues are left for further studies.
Reference: [7] <author> William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. </author> <title> Numerical Recipes in C. </title> <publisher> Cambridge University Press, </publisher> <year> 1988. </year>
Reference-contexts: Thus the cost function E w may be determined in a tractable way. This allows to exploit many interesting modifications of gradient descent strategies like conjugate gradient, quasi-Newton methods <ref> [7] </ref> or stable dynamic parameter adaptation [9] in order to increase the speed of convergence. 10 3 Belief Networks and a Diagnosis Problem A variable X in a probability space is a set containing mutually exclusive and exhaustive states.
Reference: [8] <editor> Stefan M. Ruger. Ausgewahlte Kapitel der Theorie neuronaler Netze. </editor> <booktitle> Lecture notes, </booktitle> <address> Technische Universitat Berlin, </address> <year> 1996. </year>
Reference-contexts: When the network is in equilibrium, s follows a Boltzmann-Gibbs distribution <ref> [8] </ref> P w := s 7! Z w where Z w , also known as the partition sum, is a normalization factor: Z w = s2f1;1g N The problem to train the Boltzmann machine is one of supervised learning. <p> Then, with q = ff 7! P fl r (fffl), a suitable cost function is the so-called information gain E w := ff X r (fljff) log r (fljff) p w (fljff) : (4) Gradient descent w ab = r w E w yields the Boltzmann learning rule <ref> [1, 8] </ref> w ab = 2T hs a s b i fffl clamped hs a s b i q ff clamped X denotes the average of X with respect to r, which in turn is given empirically by a multi-set of input-output patterns fffl.
Reference: [9] <author> Stefan M. Ruger. </author> <title> Stable dynamic parameter adaptation. </title> <editor> In D. S. Touret-zky, M. C. Mozer, and M. E. Hasselmo, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <pages> pages 225-231. </pages> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: Thus the cost function E w may be determined in a tractable way. This allows to exploit many interesting modifications of gradient descent strategies like conjugate gradient, quasi-Newton methods [7] or stable dynamic parameter adaptation <ref> [9] </ref> in order to increase the speed of convergence. 10 3 Belief Networks and a Diagnosis Problem A variable X in a probability space is a set containing mutually exclusive and exhaustive states. For the sake of clarity we restrict ourselves to two possible states, true and false.
Reference: [10] <author> Lawrence Saul and Michael I. Jordan. </author> <title> Learning in Boltzmann trees. </title> <journal> Neural Computation, </journal> <volume> 6(6) </volume> <pages> 1174-1184, </pages> <year> 1994. </year> <title> 17 j j j j j j @ @ U a) D 1 = A 2 j j j j j @ @ I O j j j j j j j @ @ I U j j j j j j j I O j j jjjj j Q Q Q I O j j j j j j @ @ @ @ @ @ @ @ U f) D 6 j j j j j j jjjj j A </title>
Reference-contexts: Many efforts have been made since then to overcome the | even for modern computers | comparatively slow Gibbs sampling of a Boltzmann machine. One especially interesting fl Sekr. FR 5-9, Franklinstr. 28-29, 10 587 Berlin, async@cs.tu-berlin.de 1 technique presented by Saul and Jordan <ref> [10] </ref> is decimation: The interaction of two nodes can be calculated exactly by reducing the rest of the network such that the interaction is not disturbed. <p> Especially in larger networks it is impossible to compute the expectation values directly from the Boltzmann-Gibbs distribution. Instead, they can be estimated through Gibbs sampling. However, this method turns out to be very computation-intensive. In the next section the technique of decimation <ref> [2, 4, 10] </ref> will be presented that allows the exact and efficient calculation of expectation values in certain Boltzmann machines. 2.2 Decimation in Boltzmann Trees We will describe the decimation rules proposed by [10] in a way that they are most easily extensible. <p> In the next section the technique of decimation [2, 4, 10] will be presented that allows the exact and efficient calculation of expectation values in certain Boltzmann machines. 2.2 Decimation in Boltzmann Trees We will describe the decimation rules proposed by <ref> [10] </ref> in a way that they are most easily extensible. The basic idea behind decimation is to transform networks into reduced ones without changing their properties. A useful reduction for Boltzmann machines is to eliminate a node, with the simpler network retaining the same Boltzmann distribution for the remaining nodes. <p> Hence, we have v 0 cosh (v 12 + v 13 ) cosh (v 12 v 13 ) ! 5 which is equivalent to the result of <ref> [10] </ref>: tanh (v 0 23 ) = tanh (v 12 ) tanh (v 13 ) Thus decimation can be used to combine weights in series.
References-found: 10

