URL: http://vibes.cs.uiuc.edu/Publications/Theses/TaveraMS.ps.gz
Refering-URL: http://vibes.cs.uiuc.edu/Publications/publications.htm
Root-URL: http://www.cs.uiuc.edu
Title: THREE DIMENSIONAL SOUND FOR DATA PRESENTATION IN A VIRTUAL REALITY ENVIRONMENT  
Author: BY Luis Francisco Tavera 
Degree: Iberoamericana, 1987 THESIS Submitted in partial fulfillment of the requirements for the degree of Master of Science in Computer Science in the Graduate College of the  
Address: 1994 Urbana, Illinois  
Affiliation: Lic., Universidad  University of Illinois at Urbana-Champaign,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Aydt, R. A. SDDF: </author> <title> The Pablo Self-Describing Data Format. </title> <type> Tech. rep., </type> <institution> University of Illinois at Urbana-Champaign, Department of Computer Science, </institution> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: Specialized hardware is needed to provide the computational demands the task requires. One such piece of equipment is the Convolvotron which is used in the present work. It will be described in x3.3. 24 /* x [0] stores the most recent value of the input signal, x <ref> [1] </ref> the previous value, and so on. f [0] holds the first filter coefficient, f [length-1] the last. */ sum = 0; for (i = length - 1; i &gt; 0; i--) - sum += f [i] * x [i]; // multiply and accumulate x [i] = x [i-1]; // shift <p> When the instrumented application program is executed, it generates the desired performance data along with its normal output. The instrumentation library uses the Pablo Self-Defining Data Format (SDDF) <ref> [1] </ref> as its output format. An SDDF file is a self-contained unit that includes both data record structure definitions and data record instances.
Reference: [2] <author> Begault, D. R., and Wenzel, E. M. </author> <title> Techniques and applications for binaural sound manipulation in human-machine interfaces. </title> <journal> International Journal of Aviation Psychology 2, </journal> <volume> 1 (1992), </volume> <pages> 1-22. </pages>
Reference-contexts: However, a few applications of three-dimensional audio are begining to emerge (although, to the best of our knowledge, none uses TDS for data presentation in the way Avatar does). An interesting example, <ref> [2] </ref> investigates the use of spa-tialized sound in a system that helps air traffic controllers direct aircraft when landing. The system places the communications from incoming airplanes at locations that correspond to their physical position in the airport surroundings. <p> For values of the abscissa outside the specified interval, f is defined as 0. Figure 4.12 shows the definition of the example function f (v) = &gt; &lt; 2jvj if v 2 <ref> [2; 2] </ref> 0 otherwise (4:11) Because f is a piecewise linear function, it is entirely defined by an ordered collection of boundary points connected with straight lines.
Reference: [3] <author> Beshers, C., and Feiner, S. AutoVisual: </author> <title> Rule-Based Design of Interactive Multivariate Visualizations. </title> <journal> IEEE Computer Graphics and Applications 13, </journal> <month> 4 (July </month> <year> 1993), </year> <pages> 41-49. </pages>
Reference-contexts: Our eyes and our ears can work together in the difficult job of analyzing the behavior of a parallel program. 2 1.1 Related Research Other systems for multi-dimensional data visualization using virtual reality have been developed. The one that bears more similarity to Avatar is AutoVisual <ref> [3] </ref>. AutoVisual uses a "worlds within worlds" visualization metaphor that recursively nests three-dimensional coordinate systems to produce three-dimensional views of an N -dimensional space. In AutoVisual, users determine the nesting hierarchy of the dimensions to produce a particular visualization.
Reference: [4] <author> Blauert, J. </author> <title> Spatial Hearing. The Psychophysics of Human Sound Localization. </title> <publisher> The MIT Press, </publisher> <year> 1983. </year>
Reference-contexts: Similar ambiguities will occur at other (higher) frequencies for other azimuth values. 18 Most authors agree that inter-aural phase differences constitute the main cue used by our auditory sense to determine source azimuth for low frequency signals <ref> [4] </ref>. The minimal phase difference that allows a listener to correctly lateralize a sound corresponds to a time delay of approximately 10s for a 1000 Hz tone, which in turn corresponds to an azimuth of about 2 ffi for an external source [9].
Reference: [5] <author> Burgess, D. A. </author> <title> Techniques for low cost spatial audio. </title> <booktitle> In Proceedings of the ACM Symposium on User Interface Software and Technology (New York, </booktitle> <address> NY, 1992), </address> <publisher> ACM, </publisher> <pages> pp. 53-59. </pages>
Reference-contexts: The technology that allows real-time synthesis of three-dimensional sound is in an evolutionary state and only a very limited number of commercial sound spatialization systems are available. For these reasons, most of the work in the field has been about design and implementation of virtual acoustic displays [15] <ref> [5] </ref>. However, a few applications of three-dimensional audio are begining to emerge (although, to the best of our knowledge, none uses TDS for data presentation in the way Avatar does).
Reference: [6] <author> Cleveland, W., and McGill, M., Eds. </author> <title> Dynamic Graphics for Statistics. </title> <publisher> Wadsworth & Brooks/Cole, </publisher> <year> 1988. </year>
Reference-contexts: The motivation for the use of virtual reality as the presentation technique in Avatar, and the description of the virtual reality infrastructure are covered in x2.3 and x2.4. 2.2 Avatar's Data Presentation Metaphor The data presentation metaphor used in Avatar is an extension of a two dimensional scatterplot matrix <ref> [6] </ref> to three dimensional space. A scatterplot matrix is a square array of nfin scatterplots that presents all the possible pairwise correlations of points in n-dimensional space.
Reference: [7] <author> Foster, S. H. </author> <title> Convolvotron User's Manual. Crystal River Engineering, </title> <address> 12350 Ward's Ferry Road, Groveland, CA 95321. </address>
Reference-contexts: individual's HRTFs suggest that this does not affect localization accuracy in a significant way, at least not in a way that cannot be overcome by training [15]. 3.3 The Convolvotron The Convolvotron is a digital signal processing system capable of providing real-time three dimensional sound placement for a single listener <ref> [7] </ref>. Input to the system consists of up to four 25 analog audio signals, a set of pre-computed head-related finite impulse response filters that correspond to different values for source azimuth and elevation, and the desired location of the spatialized audio. <p> The Flock of Birds tracking system is operated through a library built at the University of North Carolina at 61 Chapel Hill. Sound generation is done using the Porsonify library developed at the University of Illinois at Urbana-Champaign [10], and audio spatialization is handled with the Convolvotron library <ref> [7] </ref>. 62 Chapter 5 Assessment of Usefulness To assess to what extent users are able to detect and interpret the position of a sound source in space, and to explore the feasibility of more sophisticated uses of three dimensional audio for data presentation, we conducted the following experiment.
Reference: [8] <author> Frederick P. Brooks, J. </author> <title> The Mythical Man-Month. </title> <publisher> Addison Wesley, </publisher> <year> 1975. </year>
Reference-contexts: All these factors can negatively affect the overall performance of a parallel program to the point in which it runs slower than an equivalent program in a single-processor computer. We can compare this effect to one observed by Brooks <ref> [8] </ref> in the context of software engineering and stated in the law that bears his name: "Adding manpower to a late 6 software project makes it later." In the case of the parallel computation this could translate to "the use of additional computational resources results in a decrease in performance." This
Reference: [9] <author> Klump, R. G., and Eady, H. R. </author> <title> Some measurements of interaural time difference thresholds. </title> <journal> Journal of the Acoustical Society of America 28 (1956), </journal> <pages> 859-860. 69 </pages>
Reference-contexts: The minimal phase difference that allows a listener to correctly lateralize a sound corresponds to a time delay of approximately 10s for a 1000 Hz tone, which in turn corresponds to an azimuth of about 2 ffi for an external source <ref> [9] </ref>. However, independent of sound frequency, inter-aural phase differences cannot account for front versus rear source location discrimination.
Reference: [10] <author> Madhyastha, T. M. Porsonify: </author> <title> A Portable System for Data Sonification. </title> <type> Tech. rep., </type> <institution> University of Illinois at Urbana-Champaign, Department of Computer Science, </institution> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: Positioning information gets to the Convolvotron through the host PC. The input audio signal is generated by the TG33 synthesizer under control of the Porsonify software <ref> [10] </ref> running on a Sun Sparcstation 10. A detailed description of the Convolvotron is given in x3.3. Porsonify is covered in x4.2.2.1. 14 15 Chapter 3 Three Dimensional Audio Our sense of hearing, as opposed to vision, is omnidirectional. <p> It does not determine the services a synthesizer should provide, only the way it should communicate with other devices. Thus, different MIDI products may have different capabilities, the most powerful of which are likely to be vendor-specific. This can create portability problems, complicating the sonification task. Porsonify <ref> [10] </ref>, the system used in the present thesis, follows the hardware synthesis approach to sound generation. It is a portable data sonification system with a client-server architecture similar to that of X windows. <p> Defining the Widget Control File. As we mentioned in x4.2.2.1, sound generation in Avatar is handled through sonic widgets provided by the Porsonify library <ref> [10] </ref>. Sonic widgets are configured through a widget control file or wcf. A wcf defines a set of parameterized messages that the sonic widget sends to a sound generating device, such as a MIDI synthesizer, to produce the desired sound. <p> The Flock of Birds tracking system is operated through a library built at the University of North Carolina at 61 Chapel Hill. Sound generation is done using the Porsonify library developed at the University of Illinois at Urbana-Champaign <ref> [10] </ref>, and audio spatialization is handled with the Convolvotron library [7]. 62 Chapter 5 Assessment of Usefulness To assess to what extent users are able to detect and interpret the position of a sound source in space, and to explore the feasibility of more sophisticated uses of three dimensional audio for
Reference: [11] <author> Noe, R. J. </author> <title> Pablo Instrumentation Environment Users Guide. </title> <type> Tech. rep., </type> <institution> University of Illinois at Urbana-Champaign, Department of Computer Science, </institution> <month> apr </month> <year> 1994. </year>
Reference-contexts: By analyzing how this data varies in time, one can detect, for example, phases with high message passing activity that may be indicative of some problem (e.g., poor data partitioning) within the application. To obtain performance data, the user first instruments her program with the Pablo instrumentation library <ref> [11] </ref>. The Pablo instrumentation library provides a collection of probes that can be inserted at selected points in a program to extract performance information from it. <p> The user interface is built using the X Window system and Motif widget set. Trace file input is handled with the Self Describing Data Format (SDDF) library developed at the University of Illinois at Urbana-Champaign <ref> [11] </ref>. The Flock of Birds tracking system is operated through a library built at the University of North Carolina at 61 Chapel Hill.
Reference: [12] <author> Rayleigh, L. </author> <title> On our perception of sound direction. </title> <booktitle> Philosophical Magazine 13 (1907), </booktitle> <pages> 214-232. </pages>
Reference-contexts: the human ear determines the direction of sound, and in x3.2 we introduce some techniques for three dimensional audio generation paying special attention to head-related transfer functions (HRTFs). 16 3.1 Spatial Sound Perception Spatial sound perception in humans has been a subject of study since the beginning of this century <ref> [12] </ref>. Although some aspects of it still remain unclear, it is generally accepted that this ability depends mainly on inter-aural time (phase) differences of the ear input signals, inter-aural intensity differences, and pinna response.
Reference: [13] <author> Shields, K. A. </author> <title> Performance analysis of parallel systems using virtual reality. </title> <type> Master's thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1994. </year>
Reference-contexts: Three-dimensional audio also can provide the user with information that facilitates the data exploration task. Further details regarding the use of sound in this context are covered in chapter 4. For an in-depth description of the visual aspects of the presentation metaphor see <ref> [13] </ref>. 2.4 Avatar's Hardware Infrastructure system is a Silicon Graphics (SGI) Iris 4D/440 four-processor computer with Reality Engine graphics and a Multi Channel Option (MCO) board. The Reality Engine is a high performance graphics subsystem capable of rendering up to 1.1M triangles per second.
Reference: [14] <author> Warren, R. M. </author> <title> Auditory Perception, A New Synthesis. </title> <publisher> Pergamon Press, </publisher> <year> 1982. </year>
Reference-contexts: This is simply the set for which d is an integer multiple of the wavelength of the sound wave. If a spherical model of the head is used <ref> [14] </ref> (radius = 8:5 cm), and the sound propagation media is air (sound propagation speed = 3:4 fi 10 4 cms 1 ), the minimal frequency in such a set is approximately 1.3 kHz for 90 ffi azimuth (in this case, the path-length difference between the two ears is one wavelength).
Reference: [15] <author> Wenzel, E. M. </author> <title> Localization in virtual acoustic displays. Presence 1, </title> <booktitle> 1 (Winter 1992), </booktitle> <pages> 80-107. </pages>
Reference-contexts: The technology that allows real-time synthesis of three-dimensional sound is in an evolutionary state and only a very limited number of commercial sound spatialization systems are available. For these reasons, most of the work in the field has been about design and implementation of virtual acoustic displays <ref> [15] </ref> [5]. However, a few applications of three-dimensional audio are begining to emerge (although, to the best of our knowledge, none uses TDS for data presentation in the way Avatar does). <p> Fortunately, results from experiments in which one listener uses another individual's HRTFs suggest that this does not affect localization accuracy in a significant way, at least not in a way that cannot be overcome by training <ref> [15] </ref>. 3.3 The Convolvotron The Convolvotron is a digital signal processing system capable of providing real-time three dimensional sound placement for a single listener [7].
Reference: [16] <author> Wightman, F. L., and Kistler, D. J. </author> <title> Headphone simulation of free-field listening. ii:psychophysical validation. </title> <journal> Journal of the Acoustical Society of America 85, </journal> <month> 2 (February </month> <year> 1989), </year> <pages> 868-878. </pages>
Reference-contexts: Furthermore, psychophysical experiments have shown that stimuli produced via HRTFs are correctly perceived as originating from the desired spatial positions <ref> [16] </ref>. Up to this point we have implicitly assumed that the head-related transfer functions used to spatialize sound for an individual are the ones corresponding to his or her ears. This is not always the case.
Reference: [17] <author> Wightman, F. L., and Kistler, D. J. </author> <title> Headphone simulation of free-field listening. i:stimulus synthesis. </title> <journal> Journal of the Acoustical Society of America 85, </journal> <month> 2 (February </month> <year> 1989), </year> <pages> 858-867. </pages>
Reference-contexts: The head-related transfer functions can be calculated with different techniques. To illustrate one of them, we briefly describe the procedure followed to compute the HRTFs used by the Convolvotron <ref> [17] </ref>. In this case, a pair of small microphones were inserted as close as possible to the eardrums inside the auditory canals of ten subjects. <p> [i] * x [i]; // multiply and accumulate x [i] = x [i-1]; // shift - 3.2.2 Usefulness of HRTFs Signals produced by use of HRTFs and headphones have been found not to differ more than 2 dB in magnitude and 10 ffi in phase relative to free-field generated ones <ref> [17] </ref>. Furthermore, psychophysical experiments have shown that stimuli produced via HRTFs are correctly perceived as originating from the desired spatial positions [16]. <p> It contains a Texas Instruments TMS320/C25 processor that acts as the main controller, memory for the TMS320, timing circuitry, and a DAC and a ADC. Both cards share two separate address and data buses. The HRTFs used by the Convolvotron are those computed by Wightman <ref> [17] </ref>. These cover a total of 144 source positions defined by azimuth values from 165 ffi to 180 ffi in 15 ffi increments, and elevation values from 36 ffi to 54 ffi in steps of 18 ffi .
References-found: 17

