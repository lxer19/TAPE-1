URL: http://www.cs.colostate.edu/~howe/papers/issre95.ps.gz
Refering-URL: http://www.cs.colostate.edu/~howe/pubs.html
Root-URL: 
Email: mraz@cs.colostate.edu howe@cs.colostate.edu avm@cs.colostate.edu  
Title: System Testing with an AI Planner Research Paper  
Author: Richard T. Mraz Adele Howe Anneliese von Mayrhauser Li Li 
Affiliation: College Dept. of Computer Science Colorado State University  
Note: Hunter  
Abstract: System testing of software with command language interfaces can be automated using grammar based test generation or through generating tests from an application domain specification. When viewing test case generation as constructing a sequence of commands to achieve a testing goal, AI planning systems appear very promising. This paper reports on automated test generation using an AI planning system and compares the results to tests generated by Sleuth, a tool for automated application domain testing. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anthony Barrett, Keith Golden, Scott Penberthy, and Daniel Weld. </author> <title> UCPOP User's Manual. </title> <institution> Dept of Computer Science and Engineering, University of Washington, </institution> <address> Seattle, WA, </address> <month> October </month> <year> 1993. </year> <note> TR 93-09-06. </note>
Reference-contexts: Postprocessor : Translates Planner output into Com mand Language Syntax. Command Preconditions Operator preconditions Command Postconditions Operator effects Intracommand Rules Operator preconditions Parameter Constraint Rules Preprocessor : Initial State Generator Preprocessor : Goal Generator Table 3: Domain Model Components and AI Planner Representation in the UCPOP planner <ref> [1] </ref>. The planner was selected because it is relatively easy to use and the software is easily obtained.
Reference: [2] <author> Franco Bazzichi and Ippolito Spadafora. </author> <title> "An Automatic Generator for Compiler Testing," </title> <journal> IEEE Transactions on Software Engineering, 1982:8(4), pp.343-353. </journal>
Reference-contexts: As an 1 aside, we also wanted to see whether the AI planner would come up with test cases that were unusual or would a need more detailed specification of test intent as compared to a test generation method using application domain information ([12, 15, 16]), or command language specification <ref> [2, 3] </ref>. Section 1 presents related work and provides background on domain based testing. Section 2 describes application domain testing [15] and Sleuth. This includes a description of domain analysis and the resulting domain model. <p> To automate this, we can represent each command using a grammar, generate commands from the grammar, and run the list of commands as the test case. When using grammars for test case generation, we also need to address command language semantics <ref> [2, 3, 5] </ref>. [6, 12] used attribute grammars for test case generation. Syntax and semantics of the command language were encoded as grammar productions and test case generation is a single stage algorithm.
Reference: [3] <author> A. Celentano and S. Crespi Reghizzi and P. Della Vigna and C. Ghezzi and G. Gramata and F. Savoretti. </author> <title> "Compiler Testing using a Sentence Generator," </title> <journal> Software-Practice and Experience, 1980:10, pp.987-918. </journal>
Reference-contexts: As an 1 aside, we also wanted to see whether the AI planner would come up with test cases that were unusual or would a need more detailed specification of test intent as compared to a test generation method using application domain information ([12, 15, 16]), or command language specification <ref> [2, 3] </ref>. Section 1 presents related work and provides background on domain based testing. Section 2 describes application domain testing [15] and Sleuth. This includes a description of domain analysis and the resulting domain model. <p> To automate this, we can represent each command using a grammar, generate commands from the grammar, and run the list of commands as the test case. When using grammars for test case generation, we also need to address command language semantics <ref> [2, 3, 5] </ref>. [6, 12] used attribute grammars for test case generation. Syntax and semantics of the command language were encoded as grammar productions and test case generation is a single stage algorithm.
Reference: [4] <author> Paul R. Cohen and Edward A. Feigenbaum. </author> <booktitle> Handbook of Artificial Intelligence, volume 3, chapter Planning and Problem Solving, </booktitle> <pages> pages 513-562. </pages> <publisher> William Kaufmann, Inc., </publisher> <address> Los Angeles, </address> <year> 1982. </year>
Reference-contexts: Approaches to automatic test generation include grammar based test generation and domain based testing. Alternatively, if we view test case generation as constructing a sequence of commands to achieve some goal (i.e., to test for correct behavior), then the problem is exactly that addressed by Artificial Intelligence Planning systems <ref> [4] </ref>; such systems generate sequences of operations, propagating constraints between different levels of abstraction and checking for possible harmful interactions. So, given knowledge about the target software and its expected input/output behavior, an AI planning system can be used to generate test cases from a test domain model.
Reference: [5] <author> A.G. Duncan and J.S. Hutchison, </author> <title> "Using Attributed Grammars to Test Designs and Implementations," </title> <booktitle> Proceedings of the Fifth International Conference on Software Engineering, </booktitle> <year> 1981, </year> <pages> pp. 170-177. </pages>
Reference-contexts: To automate this, we can represent each command using a grammar, generate commands from the grammar, and run the list of commands as the test case. When using grammars for test case generation, we also need to address command language semantics <ref> [2, 3, 5] </ref>. [6, 12] used attribute grammars for test case generation. Syntax and semantics of the command language were encoded as grammar productions and test case generation is a single stage algorithm.
Reference: [6] <author> Tsum S. Chow. </author> <title> "Testing Software Design Modeled by Finite State Machines," </title> <booktitle> Pro--ceedings of the First COMPSAC, </booktitle> <year> 1977, </year> <pages> pp. 58-64. </pages>
Reference-contexts: To automate this, we can represent each command using a grammar, generate commands from the grammar, and run the list of commands as the test case. When using grammars for test case generation, we also need to address command language semantics [2, 3, 5]. <ref> [6, 12] </ref> used attribute grammars for test case generation. Syntax and semantics of the command language were encoded as grammar productions and test case generation is a single stage algorithm.
Reference: [7] <author> Stephen Fickas and John Anderson. </author> <title> A proposed perspective shift: Viewing specification design as a planning problem. </title> <institution> Department of Computer and Information Science CIS-TR-88-15, University of Oregon, Eugene, </institution> <address> OR, </address> <month> November </month> <year> 1988. </year>
Reference-contexts: From its early days, planning was thought of as a type of automatic programming [11]. Recent uses of planning in software engineering are as a representation for specifications <ref> [7] </ref> and for supporting software reuse [8]. What makes planning an attractive paradigm for software engineering applications is its emphasis on goals and the similarity of plans to programs.
Reference: [8] <author> Karen Huff. </author> <title> Software adaptation. </title> <booktitle> In Working Notes of AAAI-92 Spring Symposium on Computational Considerations in Supporting Incremental Modification and Reuse, </booktitle> <pages> pages 63-66, </pages> <institution> Stanford University, </institution> <month> March </month> <year> 1992. </year> <journal> American Association for Artificial Intelligence. </journal>
Reference-contexts: From its early days, planning was thought of as a type of automatic programming [11]. Recent uses of planning in software engineering are as a representation for specifications [7] and for supporting software reuse <ref> [8] </ref>. What makes planning an attractive paradigm for software engineering applications is its emphasis on goals and the similarity of plans to programs.
Reference: [9] <author> Kenneth S. Rubin and Adele Goldberg. </author> <title> "Object Behavior Analysis," </title> <journal> Communications of the ACM, </journal> <volume> 35(9), </volume> <month> September </month> <year> 1992, </year> <pages> pp. 48-62. </pages>
Reference: [10] <author> StorageTek, </author> <title> StorageTek 4400 Operator's Guide, Host Software Component (VM) Rel 1.2.0, </title> <address> StorageTek, </address> <year> 1992. </year>
Reference-contexts: Traditional generation methods are procedural, emphasizing how generation has to proceed. In our experiments with an AI Planner, we used a subdomain of the StorageTek Robot Tape Library <ref> [10] </ref>. The Automated Cartridge System (ACS) stores tape cartridges in a 12-sided "silo" called a Library Storage Module (LSM). Each LSM contains a vision-assisted robot and storage for up to 6000 cartridges. Tapes occupy cells in the outer and inner panels.
Reference: [11] <author> Gerald A. Sussman. </author> <title> A computational model of skill acquisition. </title> <type> Technical Report Memo no. </type> <institution> AI-TR-297, MIT AI Lab, </institution> <year> 1973. </year>
Reference-contexts: It also supports regression testing [13]. 4 AI Planning Applied to Testing In the field of Artificial Intelligence, planning is the process of generating a sequence of actions to satisfy some goal or goals. From its early days, planning was thought of as a type of automatic programming <ref> [11] </ref>. Recent uses of planning in software engineering are as a representation for specifications [7] and for supporting software reuse [8]. What makes planning an attractive paradigm for software engineering applications is its emphasis on goals and the similarity of plans to programs.
Reference: [12] <author> Anneliese von Mayrhauser and Steward Crawford- Hines, </author> <title> "Automated Testing Support for a Robot Tape Library," </title> <booktitle> Proceedings of the Fourth International Software Reliability Engineering Conference, </booktitle> <month> November </month> <year> 1993, </year> <pages> pp. 6-14. </pages>
Reference-contexts: To automate this, we can represent each command using a grammar, generate commands from the grammar, and run the list of commands as the test case. When using grammars for test case generation, we also need to address command language semantics [2, 3, 5]. <ref> [6, 12] </ref> used attribute grammars for test case generation. Syntax and semantics of the command language were encoded as grammar productions and test case generation is a single stage algorithm. <p> the least of which is that for the average system tester these grammars are difficult to write and maintain and that the generation process does not follow the test engineers' thought processes, particularly in terms of testing goals and refinement of these goals in terms of successive levels of abstraction. <ref> [12, 15, 16] </ref> developed a test generation method that addresses the need of software testers for a tool that supports their thought process.
Reference: [13] <author> Anneliese von Mayrhauser, Richard T. Mraz, and Jeff Walls. </author> <title> "Domain Based Regression Testing," </title> <booktitle> (To be published) Proceedings of the International Conference on Software Maintenance, </booktitle> <month> Sept </month> <year> 1994. </year>
Reference-contexts: The last stage uses script parameter binding rules, parameter value sets, and parameter constraint rules to create a fully parameterized list of commands. This provides maximum reuse of commands at all levels of abstraction [14], and simplifies uniform testing across configurations and versions. It also supports regression testing <ref> [13] </ref>. 4 AI Planning Applied to Testing In the field of Artificial Intelligence, planning is the process of generating a sequence of actions to satisfy some goal or goals. From its early days, planning was thought of as a type of automatic programming [11].
Reference: [14] <author> Anneliese von Mayrhauser, Richard Mraz, Jeff Walls, and Pete Ocken. </author> <title> "Domain Based Testing: Increasing Test Case Reuse," (To be published) Proc. </title> <booktitle> of the International Conference on Computer Design, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: The second stage creates a command template. Parameters remain as place holders. The last stage uses script parameter binding rules, parameter value sets, and parameter constraint rules to create a fully parameterized list of commands. This provides maximum reuse of commands at all levels of abstraction <ref> [14] </ref>, and simplifies uniform testing across configurations and versions. It also supports regression testing [13]. 4 AI Planning Applied to Testing In the field of Artificial Intelligence, planning is the process of generating a sequence of actions to satisfy some goal or goals.
Reference: [15] <author> Anneliese von Mayrhauser, Jeff Walls, and Richard Mraz, </author> <title> "Testing Applications Using Domain Based Testing and Sleuth," </title> <booktitle> (To be published) Proceedings of the Fifth International Software Reliability Engineering Conference, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: Section 1 presents related work and provides background on domain based testing. Section 2 describes application domain testing <ref> [15] </ref> and Sleuth. This includes a description of domain analysis and the resulting domain model. Test criteria transform the domain model into a test subdomain which forms the basis of test generation. We also describe the test generation process and the mechanisms used. <p> Syntax and semantics of the command language were encoded as grammar productions and test case generation is a single stage algorithm. This poses several difficulties <ref> [15] </ref>, not the least of which is that for the average system tester these grammars are difficult to write and maintain and that the generation process does not follow the test engineers' thought processes, particularly in terms of testing goals and refinement of these goals in terms of successive levels of <p> the least of which is that for the average system tester these grammars are difficult to write and maintain and that the generation process does not follow the test engineers' thought processes, particularly in terms of testing goals and refinement of these goals in terms of successive levels of abstraction. <ref> [12, 15, 16] </ref> developed a test generation method that addresses the need of software testers for a tool that supports their thought process. <p> The domain model specifies all information needed for test generation. The results of test generation are test suites, T v j (j = 1; 2; : : : ; n; where n = total number of test). For details of the domain analysis and domain model see <ref> [15, 16] </ref>. Sleuth is the first tool to generate test cases based on a domain model. It uses the hybrid representation as shown in Table 1.
Reference: [16] <author> Anneliese von Mayrhauser, Jeff Walls, and Richard Mraz. "Sleuth: </author> <title> A Domain Based Testing Tool," (To be published) Proc. </title> <booktitle> of the International Conference on Computer Design, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: the least of which is that for the average system tester these grammars are difficult to write and maintain and that the generation process does not follow the test engineers' thought processes, particularly in terms of testing goals and refinement of these goals in terms of successive levels of abstraction. <ref> [12, 15, 16] </ref> developed a test generation method that addresses the need of software testers for a tool that supports their thought process. <p> The domain model specifies all information needed for test generation. The results of test generation are test suites, T v j (j = 1; 2; : : : ; n; where n = total number of test). For details of the domain analysis and domain model see <ref> [15, 16] </ref>. Sleuth is the first tool to generate test cases based on a domain model. It uses the hybrid representation as shown in Table 1.
References-found: 16

