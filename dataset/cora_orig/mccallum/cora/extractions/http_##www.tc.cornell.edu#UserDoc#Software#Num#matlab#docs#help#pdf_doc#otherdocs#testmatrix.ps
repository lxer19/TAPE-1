URL: http://www.tc.cornell.edu/UserDoc/Software/Num/matlab/docs/help/pdf_doc/otherdocs/testmatrix.ps
Refering-URL: http://www.tc.cornell.edu/UserDoc/Software/Num/matlab/docs/help/fulldocset.html
Root-URL: http://www.tc.cornell.edu
Title: ISSN 1360-1725 The Test Matrix Toolbox for Matlab (Version 3.0)  Numerical Analysis  
Author: N. J. Higham 
Affiliation: DEPARTMENTS OF MATHEMATICS  
Date: September 1995  
Pubnum: Report No. 276  
Abstract: Manchester Centre for Computational Mathematics Numerical Analysis Reports Reports available from: Department of Mathematics University of Manchester Manchester M13 9PL England And over the World-Wide Web from URLs http://www.ma.man.ac.uk/MCCM/MCCM.html ftp://ftp.ma.man.ac.uk/pub/narep 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Anderson, Z. Bai, C. H. Bischof, J. W. Demmel, J. J. Dongarra, J. J. Du Croz, A. Green-baum, S. J. Hammarling, A. McKenney, S. Ostrouchov, and D. C. Sorensen. </author> <note> LAPACK Users' Guide. </note> <institution> Society for Industrial and Applied Mathematics, </institution> <address> Philadelphia, PA, USA, </address> <year> 1992. </year> <note> ISBN 0-89871-294-7. xv+118+listings pp. </note>
Reference-contexts: Demmel and McKenney [7] present a suite of Fortran 77 codes for generating random square and rectangular matrices with prescribed singular values, eigenvalues, band structure, and other properties. This suite is part of the testing code for LAPACK <ref> [1] </ref>. Our focus is primarily on non-random matrices but we include a class of random matrices randsvd that has some of the features of the Demmel and McKenney test set. <p> In the output argument, in SAVIT saves, % and in function calls, x has the same shape as x0. % References: % <ref> [1] </ref> V.J. Torczon, Multi-directional search: A direct search algorithm for % parallel machines, Ph.D. Thesis, Rice University, Houston, Texas, 1989. % [2] V.J. Torczon, On the convergence of the multidirectional search % algorithm, SIAM J. Optimization, 1 (1991), pp. 123-145. % [3] N.J.
Reference: [2] <author> Zhaojun Bai. </author> <title> A collection of test matrices for large scale nonsymmetric eigenvalue problems (version 1.0). </title> <type> Manuscript, </type> <month> July </month> <year> 1994. </year>
Reference-contexts: We mention some other collections of test matrices that complement ours. The Harwell-Boeing collection of sparse matrices, largely drawn from practical problems, is presented by Duff, Grimes and Lewis [8], [9]. Bai <ref> [2] </ref> is building a collection of test matrices for the large-scale nonsymmetric eigenvalue problem. Zielke [46] gives various parametrized rectangular matrices of fixed dimension with known generalized inverses. <p> the Maple Symbolic Toolbox (the Matlab function inv produces nonzero, but tiny, upper triangular elements because of rounding errors): inverse (frank (8)) ans = [ -7, 8, -1, 0, 0, 0, 0, 0] [ -210, 240, -35, 6, -1, 0, 0, 0] [-2520, 2880, -420, 72, -15, 4, -1, 0] <ref> [-5040, 5760, -840, 144, -30, 8, -3, 2] </ref> In his 1958 paper, Frank commented "At the moment, the largest matrices resolved on the [Univac] 1103A are two 20 order matrices, one real symmetric and one complex. <p> In the output argument, in SAVIT saves, % and in function calls, x has the same shape as x0. % References: % [1] V.J. Torczon, Multi-directional search: A direct search algorithm for % parallel machines, Ph.D. Thesis, Rice University, Houston, Texas, 1989. % <ref> [2] </ref> V.J. Torczon, On the convergence of the multidirectional search % algorithm, SIAM J. Optimization, 1 (1991), pp. 123-145. % [3] N.J. Higham, Optimization by direct search in matrix computations, % SIAM J. Matrix Anal.
Reference: [3] <author> Richard Bartels and Barry Joe. </author> <title> On generating discrete linear l 1 test problems. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 10(3) </volume> <pages> 550-561, </pages> <year> 1989. </year>
Reference-contexts: an arbitrary parameter can be made complex by choosing a non-real value for the parameter. 7 As well as their obvious application to research in matrix computations we hope that the matrices presented here will be useful for constructing test problems in other areas, such as optimization (see, for example, <ref> [3] </ref>) and ordinary differential equations. We mention some other collections of test matrices that complement ours. The Harwell-Boeing collection of sparse matrices, largely drawn from practical problems, is presented by Duff, Grimes and Lewis [8], [9]. <p> Torczon, Multi-directional search: A direct search algorithm for % parallel machines, Ph.D. Thesis, Rice University, Houston, Texas, 1989. % [2] V.J. Torczon, On the convergence of the multidirectional search % algorithm, SIAM J. Optimization, 1 (1991), pp. 123-145. % <ref> [3] </ref> N.J. Higham, Optimization by direct search in matrix computations, % SIAM J. Matrix Anal.
Reference: [4] <author> James R. Bunch and Linda Kaufman. </author> <title> Some stable methods for calculating inertia and solving symmetric linear systems. </title> <journal> Math. Comp., </journal> <volume> 31(137) </volume> <pages> 163-179, </pages> <year> 1977. </year>
Reference-contexts: It produces the factorization P AP T = LDL T , where P is a permutation, L is unit lower triangular, and D is block diagonal with 1 fi 1 and 2 fi 2 diagonal blocks. The Bunch-Kaufman partial pivoting strategy is used <ref> [4] </ref>. * ge implements Gaussian elimination without pivoting. This routine is similar to lu, except that no row interchanges are done. Thus the routine computes, if possible, the LU factorization 21 A = LU of A 2 C nfin . <p> is the exact inverse as returned 27 by the Maple Symbolic Toolbox (the Matlab function inv produces nonzero, but tiny, upper triangular elements because of rounding errors): inverse (frank (8)) ans = [ -7, 8, -1, 0, 0, 0, 0, 0] [ -210, 240, -35, 6, -1, 0, 0, 0] <ref> [-2520, 2880, -420, 72, -15, 4, -1, 0] </ref> [-5040, 5760, -840, 144, -30, 8, -3, 2] In his 1958 paper, Frank commented "At the moment, the largest matrices resolved on the [Univac] 1103A are two 20 order matrices, one real symmetric and one complex.
Reference: [5] <author> Denise Chen and Cleve Moler. </author> <title> Symbolic Math Toolbox: User's Guide. The MathWorks, </title> <publisher> Inc., </publisher> <address> Natick, MA, USA, </address> <year> 1993. </year>
Reference-contexts: by looking at its characteristic polynomial: poly (F) 25 ans = Columns 1 through 7 0.0001 -0.0055 0.1035 -0.8310 2.9505 -4.5297 2.9505 Columns 8 through 11 -0.8310 0.1035 -0.0055 0.0001 The coefficients seem to be palindromic! As a check we use the function charpoly from Matlab 4's Maple Symbolic Toolbox <ref> [5] </ref> to compute the characteristic polynomial exactly: charpoly (F) ans = Any matrix whose characteristic polynomial n has a palindromic coefficient vector has eigen-values occurring in reciprocal pairs, since n () = n n (1=). In particular, it has determinant 1, and 1 is an eigenvalue when n is odd.
Reference: [6] <author> A. K. Cline and R. K. Rew. </author> <title> A set of counter-examples to three condition number estimators. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 4(4) </volume> <pages> 602-611, </pages> <year> 1983. </year>
Reference-contexts: can be proved; cf. [42].) As an illustration, here is the exact inverse as returned 27 by the Maple Symbolic Toolbox (the Matlab function inv produces nonzero, but tiny, upper triangular elements because of rounding errors): inverse (frank (8)) ans = [ -7, 8, -1, 0, 0, 0, 0, 0] <ref> [ -210, 240, -35, 6, -1, 0, 0, 0] </ref> [-2520, 2880, -420, 72, -15, 4, -1, 0] [-5040, 5760, -840, 144, -30, 8, -3, 2] In his 1958 paper, Frank commented "At the moment, the largest matrices resolved on the [Univac] 1103A are two 20 order matrices, one real symmetric and <p> The Matlab function rcond computes an upper bound for 1 (A) 1 = (kAk 1 kA 1 k 1 ) 1 using the LINPACK condition estimation algorithm. Although this algorithm is very reliable in general, parametrized matrices are known for which it can perform arbitrarily badly <ref> [6] </ref>. Here are two examples, from the toolbox routine condex. The underestimation ratio is approximately the same in both examples, but the second is probably the more serious because rcond does not detect any ill-conditioning whatsoever.
Reference: [7] <author> James W. Demmel and A. McKenney. </author> <title> A test matrix generation suite. </title> <type> Preprint MCS-P69-0389, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, IL, USA, </institution> <month> March </month> <year> 1989. </year> <pages> 16 pp. </pages> <note> LAPACK Working Note 9. </note>
Reference-contexts: Bai [2] is building a collection of test matrices for the large-scale nonsymmetric eigenvalue problem. Zielke [46] gives various parametrized rectangular matrices of fixed dimension with known generalized inverses. Demmel and McKenney <ref> [7] </ref> present a suite of Fortran 77 codes for generating random square and rectangular matrices with prescribed singular values, eigenvalues, band structure, and other properties. This suite is part of the testing code for LAPACK [1].
Reference: [8] <author> Iain S. Duff, Roger G. Grimes, and John G. Lewis. </author> <title> Sparse matrix test problems. </title> <journal> ACM Trans. Math. Software, </journal> <volume> 15(1) </volume> <pages> 1-14, </pages> <year> 1989. </year>
Reference-contexts: We mention some other collections of test matrices that complement ours. The Harwell-Boeing collection of sparse matrices, largely drawn from practical problems, is presented by Duff, Grimes and Lewis <ref> [8] </ref>, [9]. Bai [2] is building a collection of test matrices for the large-scale nonsymmetric eigenvalue problem. Zielke [46] gives various parametrized rectangular matrices of fixed dimension with known generalized inverses. <p> n I) the reciprocal pair property of the eigenvalues can be proved; cf. [42].) As an illustration, here is the exact inverse as returned 27 by the Maple Symbolic Toolbox (the Matlab function inv produces nonzero, but tiny, upper triangular elements because of rounding errors): inverse (frank (8)) ans = <ref> [ -7, 8, -1, 0, 0, 0, 0, 0] </ref> [ -210, 240, -35, 6, -1, 0, 0, 0] [-2520, 2880, -420, 72, -15, 4, -1, 0] [-5040, 5760, -840, 144, -30, 8, -3, 2] In his 1958 paper, Frank commented "At the moment, the largest matrices resolved on the [Univac] 1103A <p> the Maple Symbolic Toolbox (the Matlab function inv produces nonzero, but tiny, upper triangular elements because of rounding errors): inverse (frank (8)) ans = [ -7, 8, -1, 0, 0, 0, 0, 0] [ -210, 240, -35, 6, -1, 0, 0, 0] [-2520, 2880, -420, 72, -15, 4, -1, 0] <ref> [-5040, 5760, -840, 144, -30, 8, -3, 2] </ref> In his 1958 paper, Frank commented "At the moment, the largest matrices resolved on the [Univac] 1103A are two 20 order matrices, one real symmetric and one complex.
Reference: [9] <author> Iain S. Duff, Roger G. Grimes, and John G. Lewis. </author> <title> Users' guide for the Harwell-Boeing sparse matrix collection (release 1). </title> <type> Report RAL-92-086, </type> <institution> Atlas Centre, Rutherford Apple-ton Laboratory, Didcot, Oxon, UK, </institution> <month> December </month> <year> 1992. </year> <pages> 84 pp. </pages>
Reference-contexts: We mention some other collections of test matrices that complement ours. The Harwell-Boeing collection of sparse matrices, largely drawn from practical problems, is presented by Duff, Grimes and Lewis [8], <ref> [9] </ref>. Bai [2] is building a collection of test matrices for the large-scale nonsymmetric eigenvalue problem. Zielke [46] gives various parametrized rectangular matrices of fixed dimension with known generalized inverses.
Reference: [10] <author> W. H. Enright and J. D. Pryce. </author> <title> Two FORTRAN packages for assessing initial value methods. </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 13(1) </volume> <pages> 1-27, </pages> <year> 1987. </year>
Reference-contexts: In many areas of numerical analysis good test problems have been identified, and several collections of such problems have been published. For example, collections are available in the areas of nonlinear optimization [33], linear programming [13], [31], ordinary differential equations <ref> [10] </ref>, and partial differential equations [34]. Probably the most prolific devisers of test problems have been workers in matrix computations.
Reference: [11] <author> Werner L. Frank. </author> <title> Computing eigenvalues of complex matrices by determinant evaluation and by methods of Danilewski and Wielandt. </title> <journal> J. Soc. Indust. Appl. Math., </journal> <volume> 6 </volume> <pages> 378-392, </pages> <year> 1958. </year>
Reference-contexts: For n divisible by 4 the rank is only 3. 24 9.2 The Frank Matrix A famous test matrix for eigensolvers is the n fi n upper Hessenberg matrix F n introduced by Frank in 1958 <ref> [11] </ref>, illustrated for n = 8 by F = frank (8) F = 7 7 6 5 4 3 2 1 0 0 5 5 4 3 2 1 0 0 0 0 3 3 2 1 0 0 0 0 0 0 1 1 In evaluating three eigenvalue algorithms Frank <p> The reason is that rounding errors influence Matlab's evaluation of det (F T n ) much more than its evaluation of det (F n ); an illuminating discussion of this phenomenon is given by Frank <ref> [11] </ref> and Wilkinson [44, Section 8], [45, pp. 92-93].
Reference: [12] <author> F. R. Gantmacher. </author> <title> The Theory of Matrices, volume two. </title> <publisher> Chelsea, </publisher> <address> New York, </address> <year> 1959. </year> <note> ISBN 0-8284-0133-0. ix+276 pp. </note>
Reference-contexts: A totally positive matrix has distinct, real and positive eigenvalues and its ith eigenvector (corresponding to the ith largest eigenvalue) has exactly i 1 sign changes <ref> [12, Theorem 13, p. 105] </ref>; this property is important in testing regularization algorithms [16], [17]. <p> The user could, 1 cauchy (x,y) is totally positive if 0 &lt; x 1 &lt; &lt; x n and 0 &lt; y 1 &lt; &lt; y n [39, p. 295]. 2 vand (p) is totally positive if the p i satisfy 0 &lt; p 1 &lt; &lt; p n <ref> [12, p. 99] </ref>. 3 The new pascal (n,2) is generated by a call to rot90 and is "reverse upper triangular" instead of "reverse lower triangular" as in the Matlab 4.2 version. 12 of course, try see (full (A)) for a sparse matrix, but for large dimensions the storage and time required
Reference: [13] <author> David M. Gay. </author> <title> Electronic mail distribution of linear programming test problems. </title> <journal> Mathematical Programming Society COAL Newsletter, </journal> <volume> December:10-12, </volume> <year> 1985. </year>
Reference-contexts: In many areas of numerical analysis good test problems have been identified, and several collections of such problems have been published. For example, collections are available in the areas of nonlinear optimization [33], linear programming <ref> [13] </ref>, [31], ordinary differential equations [10], and partial differential equations [34]. Probably the most prolific devisers of test problems have been workers in matrix computations.
Reference: [14] <author> Gene H. Golub and Charles F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins University Press, </publisher> <address> Baltimore, MD, USA, </address> <note> second edition, 1989. ISBN 0-8018-3772-3 (hardback), 0-8018-3739-1 (paperback). xix+642 pp. </note>
Reference-contexts: Techniques for obtaining a triangular, orthogonal, or symmetric positive definite matrix that is related to a given matrix include * Bandwidth reduction using unitary transformations (see toolbox routine bandred). * LU, Cholesky, QR and polar decompositions (see lu, chol, qr and, from the toolbox, cholp, ge, gecp and poldec.) See <ref> [14] </ref> for details of these techniques. Another way to generate a new matrix is to perturb an existing one. One approach is to add a random perturbation. Another is to round the matrix elements to a certain number of binary places; this can be done using the toolbox routine chop. <p> Here, R is upper triangular with nonnegative diagonal elements and is a permutation matrix chosen to permute the largest diagonal element to the pivot position at each stage of the reduction (see <ref> [14, Section 4.2.9] </ref>). For the usual Cholesky factorization, as computed by chol, is the identity. <p> The criterion used to define the numerical rank is a simple one based on the diagonal elements of the upper triangular matrix from the QR factorization with column pivoting. The complete orthogonal decomposition is an important tool in rank-deficient least squares problems <ref> [14, Sec. 5.5.2] </ref>, [37]. * diagpiv implements the diagonal pivoting factorization with partial pivoting of a symmetric matrix A. <p> At the kth stage of the reduction of A to triangular form, row and column interchanges are used to bring the element of largest absolute value in the active submatrix to the pivot position (k; k) <ref> [14, Sec. 3.4.8] </ref>. * gj implements Gauss-Jordan elimination for solving a nonsingular linear system Ax = b. * poldec computes the polar decomposition A = U H 2 C mfin , where H 2 C nfin is Her-mitian positive semi-definite and U 2 C mfin has orthonormal columns or rows, according <p> [cond (A,1) 1/rcond (A) cond (A,1)*rcond (A)] ans = The QR decomposition with column pivoting of A 2 C mfin (m n) is a decomposition A = Q R i where is a permutation matrix chosen according to a certain pivoting strategy, Q is orthogonal, and R is upper triangular <ref> [14, Section 5.4.1] </ref>. This decomposition is often used to estimate the rank of A; in particular, jr nn j provides an upper bound for the smallest singular value min (A) of A that is usually at most a factor of 10 too big.
Reference: [15] <author> Robert T. Gregory and David L. Karney. </author> <title> A Collection of Matrices for Testing Computational Algorithms. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1969. </year> <note> ISBN 0-88275-649-4. ix+154 pp. Reprinted with corrections by Robert E. Krieger, </note> <institution> Huntington, </institution> <address> New York, </address> <year> 1978. </year>
Reference-contexts: An early survey of test matrices was given by Rutishauser [36]; most of the matrices he discusses come from continued fractions or moment problems. Two well-known books present collections of test matrices. Gregory and Karney <ref> [15] </ref> deal exclusively with the topic, while Westlake [43] gives an appendix of test matrices. In the 24 years since these books appeared several interesting matrices have been discovered (and in fact both books omit some worthy test matrices that were known at the time). <p> For a discussion of these techniques, and others, see <ref> [15, Chapter 2] </ref>.
Reference: [16] <author> Per Christian Hansen. </author> <title> Regularization tools. A Matlab package for analysis and solution of discrete ill-posed problems. </title> <type> Report UNIC-92-03, </type> <institution> UNIC, Technical University of Denmark, DK-2800 Lyngby, Denmark, </institution> <month> June </month> <year> 1992. </year> <month> 68 </month>
Reference-contexts: A totally positive matrix has distinct, real and positive eigenvalues and its ith eigenvector (corresponding to the ith largest eigenvalue) has exactly i 1 sign changes [12, Theorem 13, p. 105]; this property is important in testing regularization algorithms <ref> [16] </ref>, [17].
Reference: [17] <author> Per Christian Hansen. </author> <title> Test matrices for regularization methods. </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 16(2) </volume> <pages> 506-512, </pages> <year> 1995. </year>
Reference-contexts: A totally positive matrix has distinct, real and positive eigenvalues and its ith eigenvector (corresponding to the ith largest eigenvalue) has exactly i 1 sign changes [12, Theorem 13, p. 105]; this property is important in testing regularization algorithms [16], <ref> [17] </ref>.
Reference: [18] <author> Nicholas J. Higham. </author> <title> Computing the polar decomposition|with applications. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 7(4) </volume> <pages> 1160-1174, </pages> <month> October </month> <year> 1986. </year>
Reference-contexts: The factor U has the property that when m n it is the nearest matrix with orthonormal columns to A for both the 2-norm and the Frobenius norm: kA U k = minf kA Qk : Q fl Q = I; Q 2 C mfin g: For more details see <ref> [18] </ref> or [28]. * signm computes the matrix sign decomposition A = SN 2 C nfin , where S = sign (A) is the matrix sign function [25].
Reference: [19] <author> Nicholas J. Higham. </author> <title> A collection of test matrices in MATLAB. Numerical Analysis Report No. </title> <type> 172, </type> <institution> University of Manchester, </institution> <address> Manchester, England, </address> <month> July </month> <year> 1989. </year>
Reference-contexts: This document describes version 3.0 of the toolbox, dated September 19, 1995. 3 Release History The first release of this toolbox (version 1.0, July 4 1989) was described in a technical report <ref> [19] </ref>. The collection was subsequently published as ACM Algorithm 694 [21]. Prior to the current version, version 3.0, the most recent release was version 2.0 (November 14 1993) [24]. Version 2.0 incorporated many additions and improvements over version 1.3 and took full advantage of the features of Matlab 4.
Reference: [20] <author> Nicholas J. Higham. </author> <title> How accurate is Gaussian elimination? In D. </title> <editor> F. Griffiths and G. A. Watson, editors, </editor> <booktitle> Numerical Analysis 1989, Proceedings of the 13th Dundee Conference, volume 228 of Pitman Research Notes in Mathematics, </booktitle> <pages> pages 137-154. </pages> <publisher> Longman Scientific and Technical, Essex, </publisher> <address> UK, </address> <year> 1990. </year>
Reference-contexts: The routine is of pedagogical interest, but also of practical interest because there are certain classes of Ax = b problem where a more accurate solution is obtained from LU factorization when there are no row or column interchanges <ref> [20] </ref>, [26]. * gecp implements Gaussian elimination with complete pivoting. Thus it computes the factorization P AQ = LU of A 2 C nfin , where P and Q are permutation matrices and L and U are lower and upper triangular, respectively.
Reference: [21] <author> Nicholas J. Higham. </author> <title> Algorithm 694: A collection of test matrices in MATLAB. </title> <journal> ACM Trans. Math. Software, </journal> <volume> 17(3) </volume> <pages> 289-305, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: This document describes version 3.0 of the toolbox, dated September 19, 1995. 3 Release History The first release of this toolbox (version 1.0, July 4 1989) was described in a technical report [19]. The collection was subsequently published as ACM Algorithm 694 <ref> [21] </ref>. Prior to the current version, version 3.0, the most recent release was version 2.0 (November 14 1993) [24]. Version 2.0 incorporated many additions and improvements over version 1.3 and took full advantage of the features of Matlab 4.
Reference: [22] <author> Nicholas J. Higham. </author> <title> Estimating the matrix p-norm. </title> <journal> Numer. Math., </journal> <volume> 62 </volume> <pages> 539-555, </pages> <year> 1992. </year>
Reference-contexts: In fact, kAk p n for all 1 p 1. The proof relies on the convexity of the p-norm, which yields the inequality (see, <ref> [22] </ref>, for example) kAk p kAk 1 kAk 11=p (This inequality is well-known for p = 2.) For a magic square, kAk 1 = kAk 1 = n , so the inequality gives kAk p n .
Reference: [23] <author> Nicholas J. Higham. </author> <title> Optimization by direct search in matrix computations. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 14(2) </volume> <pages> 317-333, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: In general, mdsmax and nmsmax can be expected to perform better than adsmax since they use a more sophisticated method. These routines were developed during the work described in <ref> [23] </ref>. 15 Note: These routines, like fmins, are not competitive with more sophisticated methods such as (quasi-)Newton methods when applied to smooth problems. They are at their best when applied to non-smooth problems such as the one in the example below. <p> reflect, nf = 279, f = 2.0306e+000 (1.9%) Iter. 37, how = reflect, nf = 281, f = 2.0734e+000 (2.1%) Iter. 56, how = shrink, nf = 331, f = 2.1540e+000 (3.9%) Simplex size 6.5338e-004 &lt;= 1.0000e-003...quitting For more on the use of direct search in "automatic error analyis" see <ref> [23] </ref> or [26, Ch. 24]. 8 Miscellaneous Routines In addition to the test matrices and visualization routines, the Test Matrix Toolbox provides several routines that can be used to manipulate matrices or compute matrix functions or decompositions.
Reference: [24] <author> Nicholas J. Higham. </author> <title> The Test Matrix Toolbox for Matlab. Numerical Analysis Report No. </title> <type> 237, </type> <institution> Manchester Centre for Computational Mathematics, </institution> <address> Manchester, England, </address> <month> December </month> <year> 1993. </year> <pages> 76 pp. </pages>
Reference-contexts: The collection was subsequently published as ACM Algorithm 694 [21]. Prior to the current version, version 3.0, the most recent release was version 2.0 (November 14 1993) <ref> [24] </ref>. Version 2.0 incorporated many additions and improvements over version 1.3 and took full advantage of the features of Matlab 4.
Reference: [25] <author> Nicholas J. Higham. </author> <title> The matrix sign decomposition and its relation to the polar decomposition. </title> <journal> Linear Algebra and Appl., </journal> 212/213:3-20, 1994. 
Reference-contexts: norm: kA U k = minf kA Qk : Q fl Q = I; Q 2 C mfin g: For more details see [18] or [28]. * signm computes the matrix sign decomposition A = SN 2 C nfin , where S = sign (A) is the matrix sign function <ref> [25] </ref>. <p> The matrix sign function has several applications and is the subject of much recent research; see <ref> [25] </ref> for details and further references. Since sign (A) 2 = I, signm provides one way to generate involutary matrices.
Reference: [26] <author> Nicholas J. Higham. </author> <title> Accuracy and Stability of Numerical Algorithms. </title> <institution> Society for Industrial and Applied Mathematics, </institution> <address> Philadelphia, PA, USA, </address> <year> 1996. </year> <note> ISBN 0-89871-355-2. Approx xxiv+690 pp. In press. </note>
Reference-contexts: Minor bugs in other routines corrected. 2 Version 3.0 of the toolbox was developed in conjunction with the book Accuracy and Stability of Numerical Algorithms <ref> [26] </ref>. <p> The book contains a chapter A Gallery of Test Matrices which has sections * The Hilbert and Cauchy Matrices * Random Matrices * "Randsvd" Matrices * The Pascal Matrix * Tridiagonal Toeplitz Matrices * Companion Matrices * Notes and References * LAPACK * Problems Users of the toolbox should consult <ref> [26] </ref> for further information not contained in this document. 4 Quick Reference Tables This section contains quick reference tables for the Test Matrix Toolbox. All the M-files in the toolbox are listed by category, with a short description. <p> = 279, f = 2.0306e+000 (1.9%) Iter. 37, how = reflect, nf = 281, f = 2.0734e+000 (2.1%) Iter. 56, how = shrink, nf = 331, f = 2.1540e+000 (3.9%) Simplex size 6.5338e-004 &lt;= 1.0000e-003...quitting For more on the use of direct search in "automatic error analyis" see [23] or <ref> [26, Ch. 24] </ref>. 8 Miscellaneous Routines In addition to the test matrices and visualization routines, the Test Matrix Toolbox provides several routines that can be used to manipulate matrices or compute matrix functions or decompositions. <p> The methods are identical in exact arithmetic but produce different results in the presence of rounding errors <ref> [26] </ref>. * cholp computes the Cholesky factorization with pivoting T A = R fl R of a Hermitian positive semi-definite matrix A 2 C nfin . <p> The routine is of pedagogical interest, but also of practical interest because there are certain classes of Ax = b problem where a more accurate solution is obtained from LU factorization when there are no row or column interchanges [20], <ref> [26] </ref>. * gecp implements Gaussian elimination with complete pivoting. Thus it computes the factorization P AQ = LU of A 2 C nfin , where P and Q are permutation matrices and L and U are lower and upper triangular, respectively.
Reference: [27] <author> Nicholas J. Higham and Desmond J. Higham. </author> <title> Large growth factors in Gaussian elimination with pivoting. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 10(2) </volume> <pages> 155-164, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: partial pivoting; these include Wilkinson's classic example [45, p. 212] gfpp (7) ans = -1 1 0 0 0 0 1 -1 -1 -1 1 0 0 1 -1 -1 -1 -1 -1 1 1 as well as all members of the "2 n1 class" described by Higham and Higham <ref> [27] </ref>. <p> However, several of the matrices produced by orthog yield relatively large growth for complete pivoting: growth of order n=2 for real data, or n for a particular complex matrix <ref> [27] </ref>. n = 50; A = orthog (n, k); [L, U, P, Q, rho] = gecp (A); fprintf (' %g"n', rho) end 24.7028 25.3296 A = hadamard (64); [L, U, P, Q, rho] = gecp (A); rho rho = It is easy to show that complete pivoting suffers growth of at
Reference: [28] <author> Roger A. Horn and Charles R. Johnson. </author> <title> Matrix Analysis. </title> <publisher> Cambridge University Press, </publisher> <year> 1985. </year> <note> ISBN 0-521-30586-1. xiii+561 pp. </note>
Reference-contexts: A totally positive matrix has distinct, real and positive eigenvalues and its ith eigenvector (corresponding to the ith largest eigenvalue) has exactly i 1 sign changes [12, Theorem 13, p. 105]; this property is important in testing regularization algorithms [16], [17]. See <ref> [28] </ref> for further details of these matrix properties. defective: chebspec, gallery, gear, jordbloc, triw Hankel: dingdong, hilb, ipjfact Hessenberg: chow, frank, grcar, ohess, randsvd idempotent: invol involutary: invol, orthog, pascal normal (but not symmetric or orthogonal): circul 9 Matrix Inverse Ill-cond Rank Symm Pos Def Orth Eig augment p p cauchy <p> U has the property that when m n it is the nearest matrix with orthonormal columns to A for both the 2-norm and the Frobenius norm: kA U k = minf kA Qk : Q fl Q = I; Q 2 C mfin g: For more details see [18] or <ref> [28] </ref>. * signm computes the matrix sign decomposition A = SN 2 C nfin , where S = sign (A) is the matrix sign function [25].
Reference: [29] <author> Roger A. Horn and Charles R. Johnson. </author> <title> Topics in Matrix Analysis. </title> <publisher> Cambridge University Press, </publisher> <year> 1991. </year> <note> ISBN 0-521-30587-X. viii+607 pp. </note>
Reference-contexts: For an example of how the field of values gives insight into the problem of finding a nearest normal matrix see [35]. An excellent reference for the theory of the field of values is <ref> [29, Chapter 1] </ref>. The routine gersh plots the Gershgorin disks for an A 2 C nfin , which are the n disks D i = f z 2 C : jz a ii j j=1 ja ij j g in the complex plane.
Reference: [30] <author> W. Kahan. </author> <title> Numerical linear algebra. </title> <journal> Canadian Math. Bulletin, </journal> <volume> 9 </volume> <pages> 757-801, </pages> <year> 1966. </year>
Reference-contexts: This decomposition is often used to estimate the rank of A; in particular, jr nn j provides an upper bound for the smallest singular value min (A) of A that is usually at most a factor of 10 too big. Kahan <ref> [30] </ref> designed a matrix for which jr nn j can be approximately 2 n1 times bigger than min (A), and which thus shows the fallibility of the QR decomposition with column pivoting for revealing rank.
Reference: [31] <author> I. J. Lustig. </author> <title> An analysis of an available set of linear programming test problems. </title> <journal> Computers and Operations Research, </journal> <volume> 16 </volume> <pages> 173-184, </pages> <year> 1989. </year>
Reference-contexts: In many areas of numerical analysis good test problems have been identified, and several collections of such problems have been published. For example, collections are available in the areas of nonlinear optimization [33], linear programming [13], <ref> [31] </ref>, ordinary differential equations [10], and partial differential equations [34]. Probably the most prolific devisers of test problems have been workers in matrix computations.
Reference: [32] <author> Cleve B. Moler. </author> <title> MATLAB's magical mystery tour. </title> <journal> The MathWorks Newsletter, </journal> <volume> 7(1) </volume> <pages> 8-9, </pages> <year> 1993. </year>
Reference-contexts: this section we give examples of the use of the toolbox and explain some of the interesting properties of magic squares and the Frank matrix. 9.1 Magic Squares In the winter 1993 MathWorks Newsletter, Moler described some of the fascinating properties of magic squares, as embodied in Matlab's magic function <ref> [32] </ref>. Some further properties can be illustrated with the aid of the toolbox. Recall that a magic square is an n fi n matrix containing the integers from 1 to n 2 whose row and column sums are all the same.
Reference: [33] <author> J. J. More, B. S. Garbow, and K. E. Hillstrom. </author> <title> Testing unconstrained optimization software. </title> <journal> ACM Trans. Math. Software, </journal> <volume> 7 </volume> <pages> 17-41, </pages> <year> 1981. </year>
Reference-contexts: In many areas of numerical analysis good test problems have been identified, and several collections of such problems have been published. For example, collections are available in the areas of nonlinear optimization <ref> [33] </ref>, linear programming [13], [31], ordinary differential equations [10], and partial differential equations [34]. Probably the most prolific devisers of test problems have been workers in matrix computations.
Reference: [34] <author> John R. Rice and R. E. Boisvert. </author> <title> Solving Elliptic Problems using ELLPACK. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: In many areas of numerical analysis good test problems have been identified, and several collections of such problems have been published. For example, collections are available in the areas of nonlinear optimization [33], linear programming [13], [31], ordinary differential equations [10], and partial differential equations <ref> [34] </ref>. Probably the most prolific devisers of test problems have been workers in matrix computations. Indeed, in the 1950s and 1960s it was common for a whole paper to be devoted to a particular test matrix: typically its inverse or eigenvalues would be obtained in closed form.
Reference: [35] <author> A. Ruhe. </author> <title> Closest normal matrix finally found! BIT, </title> <booktitle> 27 </booktitle> <pages> 585-598, </pages> <year> 1987. </year> <month> 69 </month>
Reference-contexts: The circul matrix is normal, hence its field of values is the convex hull of the eigenvalues. For an example of how the field of values gives insight into the problem of finding a nearest normal matrix see <ref> [35] </ref>. An excellent reference for the theory of the field of values is [29, Chapter 1].
Reference: [36] <author> H. </author> <title> Rutishauser. On test matrices. In Programmation en Mathematiques Numeriques, </title> <booktitle> Besan~con, 1966, volume 7 (no. </booktitle> <institution> 165) of Editions Centre Nat. Recherche Sci., </institution> <address> Paris, </address> <pages> pages 349-365, </pages> <year> 1968. </year>
Reference-contexts: Indeed, in the 1950s and 1960s it was common for a whole paper to be devoted to a particular test matrix: typically its inverse or eigenvalues would be obtained in closed form. An early survey of test matrices was given by Rutishauser <ref> [36] </ref>; most of the matrices he discusses come from continued fractions or moment problems. Two well-known books present collections of test matrices. Gregory and Karney [15] deal exclusively with the topic, while Westlake [43] gives an appendix of test matrices. <p> The inverse of F n is lower Hessenberg. This can be seen using the following representation of F n noted by Rutishauser <ref> [36, Sec. 9] </ref>: F n = P C n P; where P is the identity with the order of its columns reversed (I = eye (n); P = I (:, n:-1:1) in Matlab notation) and C n = 6 6 6 1 1 1 . . . 3 7 7 5
Reference: [37] <author> G. W. Stewart. </author> <title> Updating a rank-revealing ULV decomposition. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 14(2) </volume> <pages> 494-499, </pages> <year> 1993. </year>
Reference-contexts: The criterion used to define the numerical rank is a simple one based on the diagonal elements of the upper triangular matrix from the QR factorization with column pivoting. The complete orthogonal decomposition is an important tool in rank-deficient least squares problems [14, Sec. 5.5.2], <ref> [37] </ref>. * diagpiv implements the diagonal pivoting factorization with partial pivoting of a symmetric matrix A.
Reference: [38] <author> J. Stoer and C. Witzgall. </author> <title> Transformations by diagonal matrices in a normed space. </title> <journal> Numer. Math., </journal> <volume> 4 </volume> <pages> 158-171, </pages> <year> 1962. </year>
Reference-contexts: This result is actually a special case of an apparently little-known 1962 result of Stoer and Witzgall, which says that the norm of a doubly stochastic matrix is 1 for any norm subordinate to a permutation-invariant absolute vector norm <ref> [38] </ref>. To estimate the eigenvalues of magic (n) we can apply Gershgorin's theorem. Unfortunately, the results are not very informative because the Gershgorin disks are all approximately the same, as is clear from the structure of the matrix; see Figure 9.1.
Reference: [39] <author> Olga Taussky and Marvin Marcus. </author> <title> Eigenvalues of finite matrices. </title> <editor> In John Todd, editor, </editor> <booktitle> Survey of Numerical Analysis, </booktitle> <pages> pages 279-313. </pages> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1962. </year>
Reference-contexts: For a sparse Matlab matrix, see simply displays a spy plot, which shows the sparsity pattern of the matrix. The user could, 1 cauchy (x,y) is totally positive if 0 &lt; x 1 &lt; &lt; x n and 0 &lt; y 1 &lt; &lt; y n <ref> [39, p. 295] </ref>. 2 vand (p) is totally positive if the p i satisfy 0 &lt; p 1 &lt; &lt; p n [12, p. 99]. 3 The new pascal (n,2) is generated by a call to rot90 and is "reverse upper triangular" instead of "reverse lower triangular" as in the Matlab
Reference: [40] <author> Lloyd N. Trefethen. </author> <title> Pseudospectra of matrices. </title> <editor> In D. F. Griffiths and G. A. Watson, editors, </editor> <booktitle> Numerical Analysis 1991, Proceedings of the 14th Dundee Conference, volume 260 of Pitman Research Notes in Mathematics, </booktitle> <pages> pages 234-266. </pages> <publisher> Longman Scientific and Technical, Essex, </publisher> <address> UK, </address> <year> 1992. </year>
Reference-contexts: For nonnormal matrices the *- pseudospectrum can take a wide variety of shapes and sizes, depending on the matrix and how nonnormal it is. Pseudospectra play an important role in many numerical problems. For full details see the work of Trefethen|in particular, <ref> [40] </ref> and [41]. The routine ps plots an approximation to the *-pseudospectrum fl * (A), which it obtains by computing the eigenvalues of a given number of random perturbations of A. The eigenvalues are plotted as crosses and the pseudo-eigenvalues as dots.
Reference: [41] <author> Lloyd N. Trefethen. </author> <title> Spectra and Pseudospectra: The Behavior of Non-Normal Matrices and Operators. </title> <note> Book in preparation. </note>
Reference-contexts: If A is Hermitian, the field of values is just a segment of the real line. For non-Hermitian A the field of values is usually two-dimensional and its shape and size gives some feel for the behaviour of the matrix. Trefethen <ref> [41] </ref> notes that the field of values is the largest reasonable answer to the question "Where in C does a matrix A `live' ?" and the spectrum is the smallest reasonable answer. Some examples of field of values plots are given in Figure 6.3. <p> For nonnormal matrices the *- pseudospectrum can take a wide variety of shapes and sizes, depending on the matrix and how nonnormal it is. Pseudospectra play an important role in many numerical problems. For full details see the work of Trefethen|in particular, [40] and <ref> [41] </ref>. The routine ps plots an approximation to the *-pseudospectrum fl * (A), which it obtains by computing the eigenvalues of a given number of random perturbations of A. The eigenvalues are plotted as crosses and the pseudo-eigenvalues as dots. Arguments to ps control the number and type of perturbations. <p> , is fl * (A) = f z : k (zI A) 1 k 2 * 1 g: An alternative way of viewing the pseudospectrum is to plot the function f (z) = k (zI A) 1 k 1 over the complex plane, where min denotes the smallest singular value <ref> [41] </ref>.
Reference: [42] <author> J. M. Varah. </author> <title> A generalization of the Frank matrix. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 7(3): </volume> <pages> 835-839, </pages> <year> 1986. </year>
Reference-contexts: 6 1 1 1 . . . 3 7 7 5 6 6 6 1 1 1 . . . n 1 3 7 7 5 (By manipulating the identity det (F n I) = det (C n I) the reciprocal pair property of the eigenvalues can be proved; cf. <ref> [42] </ref>.) As an illustration, here is the exact inverse as returned 27 by the Maple Symbolic Toolbox (the Matlab function inv produces nonzero, but tiny, upper triangular elements because of rounding errors): inverse (frank (8)) ans = [ -7, 8, -1, 0, 0, 0, 0, 0] [ -210, 240, -35, 6,
Reference: [43] <author> Joan R. Westlake. </author> <title> A Handbook of Numerical Matrix Inversion and Solution of Linear Equations. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1968. </year>
Reference-contexts: An early survey of test matrices was given by Rutishauser [36]; most of the matrices he discusses come from continued fractions or moment problems. Two well-known books present collections of test matrices. Gregory and Karney [15] deal exclusively with the topic, while Westlake <ref> [43] </ref> gives an appendix of test matrices. In the 24 years since these books appeared several interesting matrices have been discovered (and in fact both books omit some worthy test matrices that were known at the time).
Reference: [44] <author> J. H. Wilkinson. </author> <title> Error analysis of floating-point computation. </title> <journal> Numer. Math., </journal> <volume> 2 </volume> <pages> 319-340, </pages> <year> 1960. </year>
Reference-contexts: The difficulties encountered by Frank's codes were shown by Wilkinson <ref> [44, Section 8] </ref>, [45, pp. 92-93] to be caused by the inherent sensitivity of the eigenvalues to perturbations in the matrix. The Frank matrix is interesting to analyze using Matlab. <p> The reason is that rounding errors influence Matlab's evaluation of det (F T n ) much more than its evaluation of det (F n ); an illuminating discussion of this phenomenon is given by Frank [11] and Wilkinson <ref> [44, Section 8] </ref>, [45, pp. 92-93].
Reference: [45] <author> J. H. Wilkinson. </author> <title> The Algebraic Eigenvalue Problem. </title> <publisher> Oxford University Press, </publisher> <year> 1965. </year> <note> ISBN 0-19-853403-5 (hardback), 0-19-853418-3 (paperback). xviii+662 pp. </note>
Reference-contexts: The difficulties encountered by Frank's codes were shown by Wilkinson [44, Section 8], <ref> [45, pp. 92-93] </ref> to be caused by the inherent sensitivity of the eigenvalues to perturbations in the matrix. The Frank matrix is interesting to analyze using Matlab. <p> The reason is that rounding errors influence Matlab's evaluation of det (F T n ) much more than its evaluation of det (F n ); an illuminating discussion of this phenomenon is given by Frank [11] and Wilkinson [44, Section 8], <ref> [45, pp. 92-93] </ref>. The extreme sensitivity of det (F n ) to perturbations in F n is easy to see: if we change the (1; n) element from 1 to 1 + *, then det (F n ) changes from 1 to 1 + (1) n (n 1)!*. <p> The function gfpp generates n fi n matrices that produce the maximum growth of 2 n1 for Gaussian elimination with partial pivoting; these include Wilkinson's classic example <ref> [45, p. 212] </ref> gfpp (7) ans = -1 1 0 0 0 0 1 -1 -1 -1 1 0 0 1 -1 -1 -1 -1 -1 1 1 as well as all members of the "2 n1 class" described by Higham and Higham [27].
Reference: [46] <author> G. Zielke. </author> <title> Report on test matrices for generalized inverses. </title> <journal> Computing, </journal> <volume> 36 </volume> <pages> 105-162, </pages> <year> 1986. </year> <month> 70 </month>
Reference-contexts: We mention some other collections of test matrices that complement ours. The Harwell-Boeing collection of sparse matrices, largely drawn from practical problems, is presented by Duff, Grimes and Lewis [8], [9]. Bai [2] is building a collection of test matrices for the large-scale nonsymmetric eigenvalue problem. Zielke <ref> [46] </ref> gives various parametrized rectangular matrices of fixed dimension with known generalized inverses. Demmel and McKenney [7] present a suite of Fortran 77 codes for generating random square and rectangular matrices with prescribed singular values, eigenvalues, band structure, and other properties.
References-found: 46

