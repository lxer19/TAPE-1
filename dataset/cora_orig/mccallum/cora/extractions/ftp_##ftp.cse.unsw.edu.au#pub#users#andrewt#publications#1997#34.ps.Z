URL: ftp://ftp.cse.unsw.edu.au/pub/users/andrewt/publications/1997/34.ps.Z
Refering-URL: http://www.cse.unsw.edu.au/school/publications/1997/SCSE_publications.html
Root-URL: 
Email: Email: -bjtong,jas,anne-@cse.unsw.edu.au  
Title: Selectivity Estimation for Joins Using Systematic Sampling  
Author: Banchong Harangsri John Shepherd Anne Ngu 
Keyword: tuple thereafter. dom sampling.  
Note: the t_cross sampling.  
Address: Sydney 2052, AUSTRALIA.  
Affiliation: School of Computer Science and Engineering, The University of New South Wales,  
Abstract: We propose a new approach to the estimation of join selectivity. The technique, which we have called systematic sampling, is a novel variant of the sampling-based approach. Systematic sampling works as follows: Given a relation R of N tuples, with a join attribute that can be accessed in ascending/descending order via an index, if n is the number of tuples to be sampled from R, select a tuple at random from the first k = d N We first develop a theoretical foundation for systematic sampling which suggests that the method gives a more representative sample than the traditional simple random sampling. Subsequent experimental analysis on a range of synthetic relations confirms that the quality of sample relations (participating in a join) yielded by systematic sampling is higher than those produced by the traditional simple ran To ensure that the sample relations produced by the systematic sampling indeed assist in computation for more accurate join selectivities, we compare the systematic sampling with the most efficient simple random sampling called t_cross using a variety of star joins and a variety of relation configurations. The results demonstrate that with the same amounts of sampling, the systematic sampling can provide considerably more accurate join selectivities than n e tuples of R and every kth
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Bayer and E. M. McCreight. </author> <title> Organization and maintenance of large ordered indexes. </title> <publisher> Acta Informatica, Springer Verlag (Heidelberg, </publisher> <address> FRG and NewYork NY, USA) Verlag, 1(3): </address> <month> February </month> <year> 1972. </year> <note> Also published in/as: ACM SIGFIDET 1970, pp.107141. </note>
Reference-contexts: The ideal case for SYSSMP is where each join attribute participating in a star join has an index (such as a B + - tree <ref> [1] </ref>) on it which ensures that each relation is sorted on that attribute. However, achieving this would incur a prohibitive cost in building and maintaining indices.
Reference: [2] <author> W. G. Cochran. </author> <title> Sampling Techniques. </title> <publisher> John Wiley & Sons, Inc., </publisher> <address> second edition edition, </address> <year> 1963. </year>
Reference-contexts: Thus the proportion of R with value x is the selectivity Y of value x on R, namely, Y = A N , where A is the total number of tuples on R having x as values. The main theorem which is quoted from page 209 of reference <ref> [2] </ref> is applicable for the proportion (or mean) which we have just described above. <p> The proof is not given here since it was already shown in <ref> [2] </ref>.
Reference: [3] <author> P. J. Haas and A. N. Swami. </author> <title> Sequential Sampling Procedures for Query Size Estimation. </title> <booktitle> In ACM SIGMOD Conference on the Management of Data, </booktitle> <pages> pages 341350, </pages> <year> 1992. </year>
Reference-contexts: Good estimates for the cost of database operations are thus critical to the effective operation of query optimisers and ultimately of the database systems that rely on them. This paper proposes a novel sampling-based method to improve such cost estimation for the join operation. Most previous work <ref> [7, 9, 6, 3, 4] </ref> on sampling-based methods has focused on simple random sampling (SRS) whereby each unit (tuple) in the population (relation) of interest has an equal chance to be selected in the sample. Simple random sampling can be performed under two distinct regimes. <p> The second scheme does not allow replacement; any unit (tuple) already selected can not be selected again. This scheme which we call SRSWOR requires a more sophisticated data structure to do the sampling. The simple random sampling methods proposed in the literature <ref> [9, 6, 3] </ref> differ from one another primarily in their stopping conditions, i.e., when to stop sampling. Systematic sampling was first proposed by [12] in the context of multidatabase systems, and with unsorted data.
Reference: [4] <author> P. J. Haas and A. N. Swami. </author> <title> Sampling-Based Selectivity Estimation for Joins Using Augmented Frequency Value Statistics. </title> <booktitle> In The International Confererence on Data Engineering, </booktitle> <pages> pages 522531, </pages> <year> 1995. </year>
Reference-contexts: Good estimates for the cost of database operations are thus critical to the effective operation of query optimisers and ultimately of the database systems that rely on them. This paper proposes a novel sampling-based method to improve such cost estimation for the join operation. Most previous work <ref> [7, 9, 6, 3, 4] </ref> on sampling-based methods has focused on simple random sampling (SRS) whereby each unit (tuple) in the population (relation) of interest has an equal chance to be selected in the sample. Simple random sampling can be performed under two distinct regimes. <p> The most efficient SRS named t_cross was proposed by <ref> [4] </ref>. To see the effectiveness between t_cross and SYSSMP, we compare them using a variety of relation configurations and a variety of star joins, a join in which any join attribute of the two or more participating relations can join one another on a common join domain. <p> The reason star joins are used for comparison is that (1) they are amenable to simulation and analysis (2) they play a significant role in decision-support system applications and (3) Haas and Swami <ref> [4] </ref> also used star joins in their experiments. The ideal case for SYSSMP is where each join attribute participating in a star join has an index (such as a B + - tree [1]) on it which ensures that each relation is sorted on that attribute.
Reference: [5] <author> B. Harangsri, J. Shepherd, and A. Ngu. </author> <title> Selectivity Estimation for Joins using Systematic Sampling. </title> <type> Technical report, </type> <institution> The University of New South Wales, School of Computer Science and Engineering, </institution> <address> Sydney 2052, AUSTRALIA, </address> <year> 1997. </year>
Reference-contexts: Section 4 summarises the results of experiments which aimed to demonstrate the realistic case. Complete details of the experiments and their results for both the ideal and realistic cases can be found in <ref> [5] </ref>. Finally we summarise the work and give conclusive remarks in Section 5. 2. Rationale behind why systematic sampling works A good sample relation must be able to well capture the distribution of a join attribute of the source relation which participates in join queries. <p> Next is the second result which we have derived based also on Theorem 3.1. The proofs of this result and the remaining results in Section 3.2 are given in <ref> [5] </ref>.
Reference: [6] <author> W. Hou, G. Ozsoyoglu, and E. Dogdu. </author> <title> Error Constrained COUNT Query Evaluation in Relational Databases. </title> <booktitle> In ACM-SIGMOD Conference on the Management of Data, </booktitle> <pages> pages 278287, </pages> <year> 1991. </year>
Reference-contexts: Good estimates for the cost of database operations are thus critical to the effective operation of query optimisers and ultimately of the database systems that rely on them. This paper proposes a novel sampling-based method to improve such cost estimation for the join operation. Most previous work <ref> [7, 9, 6, 3, 4] </ref> on sampling-based methods has focused on simple random sampling (SRS) whereby each unit (tuple) in the population (relation) of interest has an equal chance to be selected in the sample. Simple random sampling can be performed under two distinct regimes. <p> The second scheme does not allow replacement; any unit (tuple) already selected can not be selected again. This scheme which we call SRSWOR requires a more sophisticated data structure to do the sampling. The simple random sampling methods proposed in the literature <ref> [9, 6, 3] </ref> differ from one another primarily in their stopping conditions, i.e., when to stop sampling. Systematic sampling was first proposed by [12] in the context of multidatabase systems, and with unsorted data.
Reference: [7] <author> W. Hou, G. Ozsoyoglu, and B. K. Taneja. </author> <title> Statistical Estimators for Relational Algebra Expressions. </title> <booktitle> In Proceedings of the ACM SIGACT-SIGMOD Symposium on Principles of Database Systems, </booktitle> <pages> pages 276287, </pages> <year> 1988. </year>
Reference-contexts: Good estimates for the cost of database operations are thus critical to the effective operation of query optimisers and ultimately of the database systems that rely on them. This paper proposes a novel sampling-based method to improve such cost estimation for the join operation. Most previous work <ref> [7, 9, 6, 3, 4] </ref> on sampling-based methods has focused on simple random sampling (SRS) whereby each unit (tuple) in the population (relation) of interest has an equal chance to be selected in the sample. Simple random sampling can be performed under two distinct regimes.
Reference: [8] <author> Y. E. Ioannidis and S. Christodoulakis. </author> <title> On the Propagation of Errors in the Size of Join Results. </title> <booktitle> In Proceedings of the ACM-SIGMOD Intl. Conf. on Management of Data, </booktitle> <pages> pages 268277, </pages> <year> 1991. </year>
Reference-contexts: 1. Introduction Query optimisers for database systems aim to determine the most efficient query execution plan to be executed by the database system. Choosing an efficient plan relies on cost estimates derived from the statistics maintained by the underlying database system. Work by <ref> [8] </ref> pointed out that inaccurate estimates derived from such statistics may cause the optimiser to choose a very poor plan. Although the initial error might be negligible for the first subplan (such as the first join/selection), the subsequent errors (errors in the next subplans) can grow very rapidly (i.e., exponentially).
Reference: [9] <author> R. J. Lipton, J. F. Naughton, and D. A. Schneider. </author> <title> Practical Selectivity Estimation through Adaptive Sampling. </title> <booktitle> In Proceedings of ACM SIGMOD, </booktitle> <pages> pages 112, </pages> <year> 1990. </year>
Reference-contexts: Good estimates for the cost of database operations are thus critical to the effective operation of query optimisers and ultimately of the database systems that rely on them. This paper proposes a novel sampling-based method to improve such cost estimation for the join operation. Most previous work <ref> [7, 9, 6, 3, 4] </ref> on sampling-based methods has focused on simple random sampling (SRS) whereby each unit (tuple) in the population (relation) of interest has an equal chance to be selected in the sample. Simple random sampling can be performed under two distinct regimes. <p> The second scheme does not allow replacement; any unit (tuple) already selected can not be selected again. This scheme which we call SRSWOR requires a more sophisticated data structure to do the sampling. The simple random sampling methods proposed in the literature <ref> [9, 6, 3] </ref> differ from one another primarily in their stopping conditions, i.e., when to stop sampling. Systematic sampling was first proposed by [12] in the context of multidatabase systems, and with unsorted data.
Reference: [10] <author> M. N. Murthy and T. J. Rao. </author> <title> Systematic Sampling with Illustrative Examples, </title> <booktitle> volume 6, chapter 7, </booktitle> <pages> pages 147185. </pages> <publisher> Elsevier Science Publishers, </publisher> <year> 1988. </year> <title> Handbook of Statistics. </title>
Reference-contexts: To ensure this, a sort over the entire population in our case, entire relation, in ascending or descending order must be done <ref> [10, 11] </ref>, which will then result in heterogeneous tuples within the same systematic sample relation. Next is the second result which we have derived based also on Theorem 3.1. The proofs of this result and the remaining results in Section 3.2 are given in [5]. <p> : + S 2 wsmp d = d X S 2 wsmp i Corollary 3.3 SYSSMP yields a more efficient sample re lation than SRSWOR if and only if T wsmp &gt; T (8) Again, to ascertain T wsmp &gt; T , relation R must be sorted in ascending/descending order <ref> [10, 11] </ref>.
Reference: [11] <author> R. L. Scheaffer, W. Mendenhall, and L. Ott. </author> <title> Elementary Survey Sampling. </title> <publisher> PWS-KENT Publishing Company, </publisher> <address> fourth edition, </address> <year> 1990. </year>
Reference-contexts: To ensure this, a sort over the entire population in our case, entire relation, in ascending or descending order must be done <ref> [10, 11] </ref>, which will then result in heterogeneous tuples within the same systematic sample relation. Next is the second result which we have derived based also on Theorem 3.1. The proofs of this result and the remaining results in Section 3.2 are given in [5]. <p> : + S 2 wsmp d = d X S 2 wsmp i Corollary 3.3 SYSSMP yields a more efficient sample re lation than SRSWOR if and only if T wsmp &gt; T (8) Again, to ascertain T wsmp &gt; T , relation R must be sorted in ascending/descending order <ref> [10, 11] </ref>.
Reference: [12] <author> Q. Zhu. </author> <title> An Integrated Method for Estimating Selectivities in a Multidatabase System. </title> <booktitle> In Proceedings of Distributed Computing (CASCON' 93), </booktitle> <volume> volume 2, </volume> <pages> pages 832847, </pages> <address> Toronto, Ontario, Canada, 2428, </address> <month> October </month> <year> 1993. </year>
Reference-contexts: This scheme which we call SRSWOR requires a more sophisticated data structure to do the sampling. The simple random sampling methods proposed in the literature [9, 6, 3] differ from one another primarily in their stopping conditions, i.e., when to stop sampling. Systematic sampling was first proposed by <ref> [12] </ref> in the context of multidatabase systems, and with unsorted data. In this paper, we suggest that a new systematic sampling method (SYSSMP) that exploits the sortedness of data can be profitably used in the context of query size estimation for conventional query optimisation.
References-found: 12

