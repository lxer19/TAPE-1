URL: http://www.cs.washington.edu/homes/eric/hpca/Complete.ps
Refering-URL: http://www.cs.washington.edu/homes/eric/hpca/outline.html
Root-URL: http://www.cs.washington.edu
Title: On the Performance Potential of Dynamic Cache Line Sizes  
Abstract: In this paper we present o*ine algorithms for determining the optimal sequence of loads, super-loads and bypasses for direct-mapped caches. We evaluate potential gains in terms of miss rate and bandwidth and find that in many cases optimal superloading can noticeably reduce the miss rate without appreciably increasing bandwidth. We also present an online algorithm for determining the sequence of loads and superloads. This algorithm operates by monitoring the reuse and conflicts of cache lines. Experimental results show comparable improvements to the optimal algorithm in terms of miss rates. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Craig Anderson and Jean-Loup Baer. </author> <title> Two techniques for improving performance on bus-based multiprocessors. </title> <journal> Future Generation Computer Systems, </journal> <volume> 11 </volume> <pages> 537-551, </pages> <year> 1995. </year>
Reference-contexts: The first cache implementation, named sector cache [5], did in fact use this principle. Predictors for sub-lines have been studied in [11] and several schemes for sub-line invalidation in the context of cache coherent multiprocessors have been proposed <ref> [1, 6] </ref>. 3 Algorithms for computing the optimal costs 3.1 Description of the algorithm We note first that the optimal cost of satisfying a sequence in the absence of superloads is, for a direct-mapped cache without bypassing, immediately determined by direct simulation.
Reference: [2] <author> Anonymous. </author> <title> Private communication. </title>
Reference-contexts: For superloads, the optimal algorithm is more complex. Interestingly, straightforward o*ine strategies such as majority voting (superload if the majority of the relatives will be next referenced before neighbors mapping to the same location) fail to be optimal, and relatively simple counterexamples suffice to demonstrate this <ref> [2] </ref>. For our optimal algorithm, we first decompose the optimal cost determination into separate computations for each superline, adding each of them together at the end to reconstruct the cost for the entire cache.
Reference: [3] <author> Lazlo Belady. </author> <title> A study of replacement algorithms for a virtual storage computer. </title> <journal> IBM Systems Journal, </journal> <volume> 5(2) </volume> <pages> 78-101, </pages> <year> 1966. </year>
Reference-contexts: the cost of a bypass is also C 1 , the same as the cost of a miss, and that there is no possible reuse of bypassed references through a bypass buffer. 2.2 Related work Belady's MIN algorithm is the most well-known o*ine algorithm for the study of memory hierarchies <ref> [3] </ref>. Developed in the context of paging systems, MIN gives the minimum number of page faults for a given program. Until recently, efficient implementations of MIN required two passes over the input string. A good example is the OPT stack algorithm [12]. <p> For a direct-mapped cache with bypassing, the optimal cost is readily determined by adapting the M IN algorithm of Belady <ref> [3] </ref>, here phrased as On a miss to block b, conflicting with a currently in the cache, fetch b; replace a if b will next be referenced before a will, and bypass otherwise. For superloads, the optimal algorithm is more complex.
Reference: [4] <author> Doug Burger and Todd M. Austin. </author> <title> The simplescalar tool set, version 2.0. </title> <type> Technical Report 1342, </type> <institution> Computer Science Department, University of Wisconsin, Madison, WI., </institution> <year> 1997. </year>
Reference-contexts: Traces were collected with the SimpleScalar simulator <ref> [4] </ref>. The default setting for the simulator, an 64-bit 256 register RISC machine, was used. The binaries for the benchmarks were those provided with SimpleScalar simulator. Our traces include only reads.
Reference: [5] <author> Charles Conti. </author> <title> Concepts for buffer storage. </title> <journal> Computer Group News, </journal> <volume> 2 </volume> <pages> 9-13, </pages> <year> 1969. </year>
Reference-contexts: Another approach is to start with a very large line size and be able to load only sub-lines when it appears that this is efficient. The first cache implementation, named sector cache <ref> [5] </ref>, did in fact use this principle.
Reference: [6] <author> Czarek Dubnicki and Thomas LeBlanc. </author> <title> Adjustable block size coherent caches. </title> <booktitle> In Proc. of 19th Int. Symp. on Computer Architecture, </booktitle> <pages> pages 170-180, </pages> <year> 1992. </year>
Reference-contexts: The first cache implementation, named sector cache [5], did in fact use this principle. Predictors for sub-lines have been studied in [11] and several schemes for sub-line invalidation in the context of cache coherent multiprocessors have been proposed <ref> [1, 6] </ref>. 3 Algorithms for computing the optimal costs 3.1 Description of the algorithm We note first that the optimal cost of satisfying a sequence in the absence of superloads is, for a direct-mapped cache without bypassing, immediately determined by direct simulation.
Reference: [7] <author> Antonio Gonzalez, Carlos Aliaga, and Mateo Valero. </author> <title> A data cache with multiple caching strategies tuned to different types of locality. </title> <booktitle> In Proc. 1995 Int. Conf. on Supercomputing, </booktitle> <pages> pages 338-347, </pages> <year> 1995. </year>
Reference-contexts: Taking advantage of reuse information is at the heart of other methods aimed at improving cache efficiency via bypassing [18], or at having two data caches one of which is devoted especially for those data that exhibit spatial locality <ref> [7, 14] </ref>. Of course the SLT and MAT schemes cited previously also monitor the same type of information. Loading and superloading start from the premise that one should look at multiples of the default line size.
Reference: [8] <author> John Hennessy and David Patterson. </author> <title> Computer architecture : a quantitative approach. </title> <publisher> Morgan Kaufmann, </publisher> <address> 2nd Edition, San Francisco, CA, </address> <year> 1996. </year>
Reference-contexts: It is well known <ref> [8] </ref> that for a given application and set-associativity, there exists an optimal cache line that will minimize the cache miss rate. This optimal cache line size depends not only on the cache size but also on the application. <p> To minimize bytes transferred, the optimal strategy for all applications is to use the smallest possible line size and perform no superloading. To minimize miss rate, the appropriate line size and extent of superloading depend on the spatial and temporal locality of the application <ref> [8] </ref>. We examine a range of optimal algorithms each optimizing within a specific tradeoff between latency and bandwidth. The Opt 1:1 algorithm minimizes the miss rate, defined as the sum of the number of loads and the number of superloads.
Reference: [9] <author> Teresa Johnson and Wen mei Hwu. </author> <title> Run-time adaptive cache hierarchy management via reference analysis. </title> <booktitle> In Proc. 24th Int. Symp. on Computer Architecture, </booktitle> <pages> pages 315-326, </pages> <year> 1997. </year>
Reference-contexts: For example, page promotion policies from a page to a superpage have been proposed with the goal of either facilitating superpage management [17] or to have better TLB coverage [15]. 3 Closer to our study is the work of Johnson et al. <ref> [10, 9] </ref> that investigates hardware assists, a Spatial Locality Detection Table (SLT) and a Memory Address Table (MAT), for dynamic fetch size choices. The goals of our study differ from theirs in two ways. <p> Second, our online algorithm focuses on the tradeoff between cache misses and bytes transferred between memory and cache while, because the machine simulated in <ref> [10, 9] </ref> has ample bandwidth, their emphasis is principally on the reduction of cache misses. Superlines are one mechanism to improve spatial locality. As we shall see, our proposed online algorithm monitors the reuse of lines in superlines in order to decide whether loads or superloads should be performed.
Reference: [10] <author> Teresa Johnson, Matthew Merten, and Wen mei Hwu. </author> <title> Run-time spatial locality detection and optimization. </title> <booktitle> In Proc. 30th Int. Symp. on Microarchitecture, </booktitle> <pages> pages 57-64, </pages> <year> 1997. </year>
Reference-contexts: For example, page promotion policies from a page to a superpage have been proposed with the goal of either facilitating superpage management [17] or to have better TLB coverage [15]. 3 Closer to our study is the work of Johnson et al. <ref> [10, 9] </ref> that investigates hardware assists, a Spatial Locality Detection Table (SLT) and a Memory Address Table (MAT), for dynamic fetch size choices. The goals of our study differ from theirs in two ways. <p> Second, our online algorithm focuses on the tradeoff between cache misses and bytes transferred between memory and cache while, because the machine simulated in <ref> [10, 9] </ref> has ample bandwidth, their emphasis is principally on the reduction of cache misses. Superlines are one mechanism to improve spatial locality. As we shall see, our proposed online algorithm monitors the reuse of lines in superlines in order to decide whether loads or superloads should be performed. <p> This counter corresponds to the following states and operations: 00 SL (Strong Load), 01 WL (Weak Load), 10 WSL (Weak Superload), 11 SSL (Strong Superload). The OCT differs from Tyson [18], which just used parts of the PC, and Johnson's MAT <ref> [10] </ref>, which just used the upper bits of the EA as the index in their lookup tables. <p> This is due to the observation that superloading is usually not profitable if 1 or more relatives are already present in the cache in the Opt 2:1 model. The OCT counters are initialized to weak superloads. The LSD is comparable to Johnson's SLDT <ref> [10] </ref> and Kumar's AST [11]. The LSD operates at a much finer grain than the SLDT. It restricts its prediction to a binary decision, unlike the AST which predicts a mask of 16 "lines" to be loaded in a "superline".
Reference: [11] <author> Sanjeev Kumar and Chris Wilkerson. </author> <title> Exploiting spatial locality in data caches using spatial footprints. </title> <booktitle> In Proc. 25th Int. Symp. on Computer Architecture, </booktitle> <pages> pages 357-368, </pages> <year> 1998. </year>
Reference-contexts: Another approach is to start with a very large line size and be able to load only sub-lines when it appears that this is efficient. The first cache implementation, named sector cache [5], did in fact use this principle. Predictors for sub-lines have been studied in <ref> [11] </ref> and several schemes for sub-line invalidation in the context of cache coherent multiprocessors have been proposed [1, 6]. 3 Algorithms for computing the optimal costs 3.1 Description of the algorithm We note first that the optimal cost of satisfying a sequence in the absence of superloads is, for a direct-mapped <p> The OCT differs from Tyson [18], which just used parts of the PC, and Johnson's MAT [10], which just used the upper bits of the EA as the index in their lookup tables. Kumar's SHT <ref> [11] </ref> considered several possible combinations of PC and EA with both infinite and finite storage. 4.3 Line Size Detector For the line size predictor, our knowledge mechanism will be called the Line Size Detector (LSD). <p> This is due to the observation that superloading is usually not profitable if 1 or more relatives are already present in the cache in the Opt 2:1 model. The OCT counters are initialized to weak superloads. The LSD is comparable to Johnson's SLDT [10] and Kumar's AST <ref> [11] </ref>. The LSD operates at a much finer grain than the SLDT. It restricts its prediction to a binary decision, unlike the AST which predicts a mask of 16 "lines" to be loaded in a "superline".
Reference: [12] <author> Richard Mattson, Jan Gecsei, Donald Slutz, and Irving Traiger. </author> <title> Evaluation techniques for storage hierarchies. </title> <journal> IBM Systems Journal, </journal> <volume> 9(2) </volume> <pages> 78-117, </pages> <year> 1970. </year>
Reference-contexts: Developed in the context of paging systems, MIN gives the minimum number of page faults for a given program. Until recently, efficient implementations of MIN required two passes over the input string. A good example is the OPT stack algorithm <ref> [12] </ref>. By using limited look-ahead windows and correcting the contents of the stack when necessary, a "one-pass" optimal algorithm can be devised for fully associative and set-associative caches [16].
Reference: [13] <author> Steven Przybylski, Mark Horowitz, and John Hennessy. </author> <title> Performance tradeoffs in cache design. </title> <booktitle> In Proc. 15th Int. Symp. on Computer Architecture, </booktitle> <pages> pages 290-298, </pages> <year> 1988. </year>
Reference-contexts: Results of the simulations, and their analysis, are given in Section 6. Section 7 concludes and suggests further study. 2 Motivation 2.1 Terminology and cost model There exists a large body of work devoted to the analysis of the numerous trade-offs involved in the design of efficient caches <ref> [13] </ref>. For a given real estate, i.e., cache capacity, and target cycle time, i.e., access time to the on-chip cache, one has to decide on line size, set-associativity, and various latency tolerance techniques.
Reference: [14] <author> Jude Rivers, Edward Tam, Gary Tyson, Edward Davidson, and Matthew Farrens. </author> <title> Utilizing reuse information in data cache management. </title> <booktitle> In Proc. 1998 Int. Conf. on Supercomputing, </booktitle> <pages> pages 449-456, </pages> <year> 1998. </year>
Reference-contexts: Taking advantage of reuse information is at the heart of other methods aimed at improving cache efficiency via bypassing [18], or at having two data caches one of which is devoted especially for those data that exhibit spatial locality <ref> [7, 14] </ref>. Of course the SLT and MAT schemes cited previously also monitor the same type of information. Loading and superloading start from the premise that one should look at multiples of the default line size.
Reference: [15] <author> Ted Romer, Wayne Ohlrich, Anna Karlin, and Brian Bershad. </author> <title> Reducing tlb and memory overhead using online superpage promotion. </title> <booktitle> In Proc. 22nd Int. Symp. on Computer Architecture, </booktitle> <pages> pages 176-187, </pages> <year> 1995. </year>
Reference-contexts: For example, page promotion policies from a page to a superpage have been proposed with the goal of either facilitating superpage management [17] or to have better TLB coverage <ref> [15] </ref>. 3 Closer to our study is the work of Johnson et al. [10, 9] that investigates hardware assists, a Spatial Locality Detection Table (SLT) and a Memory Address Table (MAT), for dynamic fetch size choices. The goals of our study differ from theirs in two ways.
Reference: [16] <author> Rabin Sugumar and Santosh Abraham. </author> <title> Efficient simulation of caches under optimal replacement with applications to miss characterization. </title> <booktitle> In Proc. SIGMETRICS Conf., </booktitle> <pages> pages 24-35, </pages> <year> 1993. </year> <month> 21 </month>
Reference-contexts: Until recently, efficient implementations of MIN required two passes over the input string. A good example is the OPT stack algorithm [12]. By using limited look-ahead windows and correcting the contents of the stack when necessary, a "one-pass" optimal algorithm can be devised for fully associative and set-associative caches <ref> [16] </ref>.
Reference: [17] <author> Madusudhan Talluri and Mark Hill. </author> <title> Surpassing the tlb performance of superpages with less operating system support. </title> <booktitle> In Proc. ASPLOS-VI, </booktitle> <pages> pages 171-182, </pages> <year> 1994. </year>
Reference-contexts: For example, page promotion policies from a page to a superpage have been proposed with the goal of either facilitating superpage management <ref> [17] </ref> or to have better TLB coverage [15]. 3 Closer to our study is the work of Johnson et al. [10, 9] that investigates hardware assists, a Spatial Locality Detection Table (SLT) and a Memory Address Table (MAT), for dynamic fetch size choices.
Reference: [18] <author> Gary Tyson, Matthew Farrens, John Matthews, and Andrew Pleskun. </author> <title> A modified approach to data cache management. </title> <booktitle> In Proc. 28th Int. Symp. on Microarchitecture, </booktitle> <pages> pages 93-103, </pages> <year> 1995. </year>
Reference-contexts: As we shall see, our proposed online algorithm monitors the reuse of lines in superlines in order to decide whether loads or superloads should be performed. Taking advantage of reuse information is at the heart of other methods aimed at improving cache efficiency via bypassing <ref> [18] </ref>, or at having two data caches one of which is devoted especially for those data that exhibit spatial locality [7, 14]. Of course the SLT and MAT schemes cited previously also monitor the same type of information. <p> This should reduce most harmful aliasing. For hysteresis, the OCT contains a 2-bit saturating counter for each reference. This counter corresponds to the following states and operations: 00 SL (Strong Load), 01 WL (Weak Load), 10 WSL (Weak Superload), 11 SSL (Strong Superload). The OCT differs from Tyson <ref> [18] </ref>, which just used parts of the PC, and Johnson's MAT [10], which just used the upper bits of the EA as the index in their lookup tables.
References-found: 18

