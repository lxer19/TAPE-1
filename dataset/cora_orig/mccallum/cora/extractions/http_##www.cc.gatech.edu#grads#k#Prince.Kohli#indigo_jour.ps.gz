URL: http://www.cc.gatech.edu/grads/k/Prince.Kohli/indigo_jour.ps.gz
Refering-URL: http://www.cs.gatech.edu/fac/Mustaque.Ahamad/pubs.html
Root-URL: 
Title: Indigo: User-level Support for Building Distributed Shared Abstractions  
Author: Prince Kohli Mustaque Ahamad Karsten Schwan 
Address: Atlanta, Georgia  
Note: This work was supported in part by the National Science Foundation grants CCR-9106627 and CDA-9501637 and ARPA contract DABT-63-95-C-0125. Contact author: Mustaque Ahamad,  
Affiliation: College of Computing Georgia Institute of Technology  College of Computing, Georgia Institute of Technology,  
Email: Email: mustaq@cc.gatech.edu.  
Phone: 30332-0280. Phone: (404)894-2593. Fax: (404)853-9378.  
Date: June 12, 1996  
Abstract: Distributed systems that consist of workstations connected by high performance interconnects offer computational power comparable to moderate size parallel machines. Middleware like Distributed Shared Memory (DSM) or Distributed Shared Objects (DSO) attempts to improve the programmability of such hardware by presenting to application programmers interfaces similar to those offered by shared memory machines. This paper presents the portable Indigo data sharing library which provides a small set of primitives with which arbitrary shared abstractions are easily and efficiently implemented across distributed hardware platforms. Sample shared abstractions implemented with Indigo include DSM as well as fragmented objects, where object state is split across different machines and where inter-fragment communications may be customized to application-specific consistency needs. The Indigo library's design and implementation are evaluated on two different target platforms, a workstation cluster and an IBM SP-2 machine. As part of this evaluation, a novel DSM system and consistency protocol are implemented and evaluated with several high performance applications. Application performance attained with the DSM system is compared to the performance experienced when utilizing the underlying basic message passing facilities or when employing Indigo to construct customized fragmented objects implementing the application's shared state. Such experimentation results in insights concerning the efficient implementation of DSM systems (e.g., how to deal with false sharing). It also leads to the conclusion that Indigo provides a sufficiently rich set of abstractions for efficient implementation of the next generation of parallel programming models for high performance machines. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Sarita V. Adve and Mark D. Hill. </author> <title> Weak ordering anew definition. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-14, </pages> <month> May </month> <year> 1990. </year>
Reference: [2] <author> Sarita V. Adve and Mark D. Hill. </author> <title> A unified formalization of four shared-memory models. </title> <type> Technical Report Computer Sciences Technical Report 1051, </type> <institution> University of Wisconsin, Madison, </institution> <month> September </month> <year> 1991. </year>
Reference: [3] <author> Mustaque Ahamad, Rida Bazzi, Ranjit John, Prince Kohli, and Gil Neiger. </author> <title> The power of processor consistency. </title> <booktitle> In Proceedings of the 5th ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1993. </year>
Reference: [4] <author> Mustaque Ahamad, Gil Neiger, Prince Kohli, James E. Burns, and Phillip W. Hutto. </author> <title> Causal memory: Definitions, implementation and programming. </title> <journal> Distributed Computing, </journal> <volume> 9(1) </volume> <pages> 37-49, </pages> <month> Aug </month> <year> 1995. </year>
Reference: [5] <author> H.E. Bal, M.F. Kaashoek, </author> <title> and A.S. Tanenbaum. Experience with distributed programming in Orca. </title> <booktitle> In In Intl. Conf. on Computer Languages, </booktitle> <year> 1990. </year>
Reference-contexts: We also describe how the applications are programmed in the three systems: ECM, FSO and MP. 1. The Traveling Salesperson Problem (TSP): Lock-based, small message size, data-dependent execution. The TSP algorithm is similar to the one used by Bal et al. <ref> [5] </ref> and uses a branch-and-bound method. It creates a statically defined job queue which is shared between the worker processes. The initial queue is built by unraveling all possible paths for the tour to a fixed depth.
Reference: [6] <author> J. K. Bennett, J. B. Carter, and W. Zwaenepoel. Munin: </author> <title> Distributed shared memory based on type-specific memory coherence. </title> <booktitle> In Proceedings of the 2nd ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 168-177, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: Demands for increased programmability of distributed memory machines have given rise to research efforts resulting in Distributed Shared Memory (DSM) libraries layered on top of the networking and virtual memory systems <ref> [6, 7, 14, 19, 23] </ref>. They have also resulted in the development of distributed object-oriented systems offering user programs network-wide access to shared services [12, 18, 33] as well as object-oriented concurrent programming layers on top of shared and distributed memory machines [10, 21, 34, 37]. <p> Solutions to the false sharing problem like the diff mechanism <ref> [6] </ref> can introduce additional copying and processing overheads. 3. Structure of shared state: DSM systems treat all shared information as an array of shared bytes, but these bytes are used to store information that has structure or type (e.g., fragmented objects like shared work queues). <p> Moreover, an innovative approach for FS handling is implemented that incurs the overheads associated with FS only when actual FS exists. A new approach to handle FS. Various software-based DSMs <ref> [6, 7] </ref> have used client-based diff 3 approaches, where the user process accessing shared objects computes the diff. However, since they make copies for all objects that could potentially be written, they actually introduce copying overhead even when there is no actual false sharing.
Reference: [7] <author> Brian N. Bershad and Matthew J. Zekauskas. Midway: </author> <title> Shared memory parallel programming with entry consistency for distributed memory multiprocessors. </title> <type> Technical Report CMU-CS-91-170, </type> <institution> Carnegie Mellon University, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: Demands for increased programmability of distributed memory machines have given rise to research efforts resulting in Distributed Shared Memory (DSM) libraries layered on top of the networking and virtual memory systems <ref> [6, 7, 14, 19, 23] </ref>. They have also resulted in the development of distributed object-oriented systems offering user programs network-wide access to shared services [12, 18, 33] as well as object-oriented concurrent programming layers on top of shared and distributed memory machines [10, 21, 34, 37]. <p> This may be unnecessary because the write is done inside a critical section and processes at other nodes will not be able to access the modified data item until control of the critical section is released <ref> [7, 20, 23] </ref>. 2. Granularity mismatch: in many DSM systems, consistency for shared data is provided at the level of a page. This implies that an update to a small amount of shared data may result in the transfer of an entire page. <p> Moreover, an innovative approach for FS handling is implemented that incurs the overheads associated with FS only when actual FS exists. A new approach to handle FS. Various software-based DSMs <ref> [6, 7] </ref> have used client-based diff 3 approaches, where the user process accessing shared objects computes the diff. However, since they make copies for all objects that could potentially be written, they actually introduce copying overhead even when there is no actual false sharing. <p> Page 4 37 92 Table 3: Communication times for 8 nodes on ECM, in seconds 4.2.3 Handling False Sharing An approach that maintains consistency at the level of user-defined objects and not pages will suffer from little or no false sharing (FS) (such an assumption has been made in Midway <ref> [7] </ref> that provides consistency at the level of user-defined objects). However, it is not possible to completely eliminate FS [8]. Our implementation makes the assumption that FS will not be common, and hence we optimize for the case of no FS. <p> These results again demonstrate that using type and structure information as in FSOs leads to better performance than using DSM. 5 Related Systems A number of systems that can support state sharing on a workstation cluster (for example, Ivy [29], TreadMarks [22], Midway <ref> [7] </ref> and Distributed Filaments [15]) have been proposed and implemented.
Reference: [8] <author> William J. Bolosky and Michael L. Scott. </author> <title> False sharing and its effect on shared memory performance. </title> <booktitle> In 4th Symposium on Experimental Distributed and Multiprocessor Systems, </booktitle> <pages> pages 57-71, </pages> <month> September </month> <year> 1993. </year> <note> Also available as MSR-TR-93-1, </note> <institution> Microsoft Res. Lab., </institution> <month> Sep. </month> <year> 1993. </year>
Reference-contexts: Although false sharing (FS) is expected to be rare because consistency is provided at the level of user-defined objects, it may not always be possible to eliminate it completely <ref> [8] </ref>. In this section, we use the Indigo-based ECM implementation to investigate some of the issues involved in FS and also explore several ways of handling it. Moreover, an innovative approach for FS handling is implemented that incurs the overheads associated with FS only when actual FS exists. <p> However, it is not possible to completely eliminate FS <ref> [8] </ref>. Our implementation makes the assumption that FS will not be common, and hence we optimize for the case of no FS. That is, in our implementation which does deal with FS, applications with no FS will not pay any penalty for its handling.
Reference: [9] <author> J. S. Chase, F. G. Amador, E. D. Lazowska, H. M. Levy, and R. J. Littlefield. </author> <title> The amber system: Parallel programming on a network of multiprocessors. </title> <booktitle> In Proceedings of the 12th Symposium on Operating System Principles, </booktitle> <year> 1989. </year>
Reference-contexts: Successive Over-Relaxation (SOR): Barrier-based, large messages, write-write false sharing. SOR is another iteration-based application that is frequently a kernel in many mathematical packages. The program is based on the parallel red/black SOR algorithm as described by Chase et al. <ref> [9] </ref>. At each iteration, a process writes some value into a set of locations in a matrix, the value being computed from the values of the four nearest neighbours in the grid. The communication is thus neighbour to neighbour.
Reference: [10] <author> Christian Clemencon, Bodhisattwa Mukherjee, and Karsten Schwan. </author> <title> Distributed shared abstractions (DSA) on large-scale multiprocessors. </title> <booktitle> In Proc. of the Fourth USENIX Symposium on Experiences with Distributed and Multiprocessor Systems, </booktitle> <pages> pages 227-246. </pages> <publisher> USENIX, </publisher> <month> September </month> <year> 1993. </year> <note> Also as TR# GIT-CC-93/25. </note>
Reference-contexts: They have also resulted in the development of distributed object-oriented systems offering user programs network-wide access to shared services [12, 18, 33] as well as object-oriented concurrent programming layers on top of shared and distributed memory machines <ref> [10, 21, 34, 37] </ref>. Our contribution to the emerging field of heterogeneous parallel programming is the development of the Indigo data sharing library. <p> DSM provides shared memory objects that can be read and written by processes executing at different nodes. On the other hand, in FSO, object state and functionality may be fragmented across multiple machines' memory units <ref> [10] </ref> in an application specific way. We use the term DSA to refer to shared objects that are either implemented as memory objects or as fragmented objects.
Reference: [11] <author> A. L. Cox, S. Dwarkadas, P. Keleher, H. Lu, R. Rajamony, and W. Zwaenepoel. </author> <title> Software versus hardware shared memory implementation a case study. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <year> 1994. </year>
Reference-contexts: This was sufficient in the DSA implementations and we did not find it necessary to associate thread or object specific handlers. We should note here that many DSMs need some way of detecting accesses to shared data (e.g., write to shared data). Several techniques exist for access detection <ref> [11, 39] </ref>. Due to that reason and also due to the fact that many DSAs do not need such mechanisms, access detection techniques are not part of Indigo.
Reference: [12] <author> Partha Dasgupta, Richard J. LeBlanc, and William F. Appelbe. </author> <title> The Clouds distributed operating system: A functional description, related work and implementation details. </title> <booktitle> In Proceedings of the Eighth International Conference on Distributed Computing Systems, </booktitle> <month> June </month> <year> 1988. </year>
Reference-contexts: They have also resulted in the development of distributed object-oriented systems offering user programs network-wide access to shared services <ref> [12, 18, 33] </ref> as well as object-oriented concurrent programming layers on top of shared and distributed memory machines [10, 21, 34, 37]. Our contribution to the emerging field of heterogeneous parallel programming is the development of the Indigo data sharing library.
Reference: [13] <author> Ed Felton. </author> <title> Best-first branch-and-bound on a hypercube. </title> <booktitle> In Third Conference on Hypercube Concurrent Computers and Applications, </booktitle> <address> Pasadena, CA. </address> <institution> ACM, Jet Propulsion Laboratories, </institution> <month> Jan. </month> <year> 1988. </year> <month> 30 </month>
Reference-contexts: This mismatch can cause additional communications, expensive buffering and copying of message data <ref> [13] </ref>, and increased communication latencies compared to equivalent implementations directly layered on message passing systems using only the `minimal' set of required communications. For example, on a write to a shared data item, some DSM systems invalidate or update copies of the data at other nodes.
Reference: [14] <author> I. Foster, C. Kesselman, and S. Tuecke. </author> <title> The Nexus task-parallel runtime system. </title> <booktitle> In Proceedings of the First International Workshop on Parallel Processing, </booktitle> <year> 1994. </year>
Reference-contexts: Demands for increased programmability of distributed memory machines have given rise to research efforts resulting in Distributed Shared Memory (DSM) libraries layered on top of the networking and virtual memory systems <ref> [6, 7, 14, 19, 23] </ref>. They have also resulted in the development of distributed object-oriented systems offering user programs network-wide access to shared services [12, 18, 33] as well as object-oriented concurrent programming layers on top of shared and distributed memory machines [10, 21, 34, 37]. <p> The breadth of support distinguishes Indigo from communication libraries like active messages [38] that do not offer the specific functionality required by DSM or FSO implementations. Similarly, in comparison with the Nexus library developed for concurrent C++ <ref> [14] </ref> and with the lower RPC-like layers of distributed object systems like Spring [33], Chorus [28], and distributed objects [34], Indigo attempts to combine both the functionality required by distributed objects and by distributed shared memory. <p> Tempest [19] also provides mechanisms for communication and synchronization in a parallel program. It differs from Indigo which considers access detection techniques needed by DSM implementations to be a matter of policy and thus a responsibility of the DSM layer whereas Tempest provides support for it. Nexus <ref> [14] </ref> supports distributed objects but it does not provide support for programming shared abstractions over a distributed system.
Reference: [15] <author> Vincent Freeh, David Lowenthal, and Gregory Andrews. </author> <title> Distributed filaments: Efficient fine-grain parallelism on a cluster of workstations. </title> <booktitle> In Proceedings of the First Symposium on Operating Systems Design and Implementation, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: The correctness of the algorithm is discussed in [24]. The invalidation done on acquiring a lock is similar to the consistency actions of Tread-Marks and the implicit invalidate protocol of Distributed Filaments <ref> [15] </ref>. However, compared to the latter, in the case when the data is indeed good, we are able to revalidate it using the timestamps. We have thus demonstrated how Indigo facilitates DSM implementations with the variety of calls it provides. <p> These results again demonstrate that using type and structure information as in FSOs leads to better performance than using DSM. 5 Related Systems A number of systems that can support state sharing on a workstation cluster (for example, Ivy [29], TreadMarks [22], Midway [7] and Distributed Filaments <ref> [15] </ref>) have been proposed and implemented.
Reference: [16] <author> Kourosh Gharachorloo, Daniel Lenoski, James Laudon, Phillip Gibbons, Anoop Gupta, and John Hennessy. </author> <title> Memory consistency and event ordering in scalable shared memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: This is most useful when some form of synchronization needs to be performed, like in RC (Release Consistent) memory <ref> [16] </ref>, where one needs to place some data at another node before transferring a lock. (There also exists a sync put validate call that is similar to sync put except that it also turns on the valid bit remotely.
Reference: [17] <author> James R. Goodman. </author> <title> Cache consistency and sequential consistency. </title> <type> Technical Report 1006, </type> <institution> University of Wisconsin, Madison, </institution> <month> February </month> <year> 1991. </year>
Reference: [18] <institution> X Consortium Working Group. </institution> <note> Fresco specification draft version 0.7, </note> <month> April </month> <year> 1994. </year>
Reference-contexts: They have also resulted in the development of distributed object-oriented systems offering user programs network-wide access to shared services <ref> [12, 18, 33] </ref> as well as object-oriented concurrent programming layers on top of shared and distributed memory machines [10, 21, 34, 37]. Our contribution to the emerging field of heterogeneous parallel programming is the development of the Indigo data sharing library.
Reference: [19] <author> Mark D. Hill, James R. Larus, and David A. Wood. </author> <title> Tempest: A substrate for portable parallel programs. </title> <booktitle> In COMPCON, </booktitle> <month> March </month> <year> 1995. </year>
Reference-contexts: Demands for increased programmability of distributed memory machines have given rise to research efforts resulting in Distributed Shared Memory (DSM) libraries layered on top of the networking and virtual memory systems <ref> [6, 7, 14, 19, 23] </ref>. They have also resulted in the development of distributed object-oriented systems offering user programs network-wide access to shared services [12, 18, 33] as well as object-oriented concurrent programming layers on top of shared and distributed memory machines [10, 21, 34, 37]. <p> Indigo's goals are similar to those of the Tempest library <ref> [19] </ref> but it differs from Tempest in its additional support for objects and in its exclusion of constructs required only for DSM implementations (e.g., read-write access detection). Such DSM-specific support is implemented above Indigo's level of abstraction. <p> The object fragments and the communication structure between them 28 can be implemented using Indigo calls. In addition, we explored shared abstractions for computational problems and compared their performance with message passing and a DSM system. In contrast, coarse-grain objects such as replicated files are investigated in [35]. Tempest <ref> [19] </ref> also provides mechanisms for communication and synchronization in a parallel program. It differs from Indigo which considers access detection techniques needed by DSM implementations to be a matter of policy and thus a responsibility of the DSM layer whereas Tempest provides support for it.
Reference: [20] <author> Ranjit John and Mustaque Ahamad. </author> <title> Evaluation of Causal Distributed Shared Memory for Data-race-free Programs. </title> <type> Technical Report GIT-CC-94/34, </type> <institution> College of Computing, Georgia Tech, </institution> <address> Atlanta, GA 30332-0280, </address> <year> 1994. </year>
Reference-contexts: This may be unnecessary because the write is done inside a critical section and processes at other nodes will not be able to access the modified data item until control of the critical section is released <ref> [7, 20, 23] </ref>. 2. Granularity mismatch: in many DSM systems, consistency for shared data is provided at the level of a page. This implies that an update to a small amount of shared data may result in the transfer of an entire page. <p> The efficiency of the library is the topic of the next section. One of the contributions of this paper is an innovative implementation of Extended Causal Memory (ECM) <ref> [20] </ref> using the Indigo library. This implementation takes advantage of Indigo's ability to provide arbitrary consistency for user-defined objects. We also explore how the calls provided by Indigo can be explicitly used to implement application-specific DSAs. <p> We claim that any one or a combination of these can easily be constructed with Indigo. We illustrate this by exploring the implementation of one memory system Extended Causal Memory (ECM) <ref> [20] </ref>. ECM provides benefits similar to other systems such as TreadMarks. It can be defined for all types of programs including ones with data races, and it exploits both 10 P 1 : w (x)1 r (y)0 weak-consistency and weak-ordering approaches to improve performance. ECM definition. <p> The algorithm shown in Figure 6 assumes that data-race free programs access shared objects stored in ECM. It has been shown that in such a case, consistency actions need to be performed only when synchronization operations are executed <ref> [20, 22] </ref>. In particular, vector timestamps received with synchronization variables are used to detect objects in the processor cache which may potentially be overwritten. Such objects, which have lower timestamps, 2 We chose the page-based approach because it is simpler to implement as compared to the compiler-based approaches used elsewhere. <p> This is also to be seen for ECM for EP and TSP applications. In fact, the differences in completion times between ECM and MP for these three applications are similar to differences between these systems when ECM is implemented at the operating system level <ref> [20] </ref>. For the other applications, the loss of performance in ECM is due to synchronization and false sharing.
Reference: [21] <author> Peter Karlin, Mani Chandy, and Carl Kesselman. </author> <title> The compositional C++ language definition. </title> <type> Technical Report CS-TR-92-02, </type> <institution> Califonia Institute of Technology, </institution> <month> March </month> <year> 1993. </year> <note> Revision 0.95. </note>
Reference-contexts: They have also resulted in the development of distributed object-oriented systems offering user programs network-wide access to shared services [12, 18, 33] as well as object-oriented concurrent programming layers on top of shared and distributed memory machines <ref> [10, 21, 34, 37] </ref>. Our contribution to the emerging field of heterogeneous parallel programming is the development of the Indigo data sharing library.
Reference: [22] <author> P. Keleher, S. Dwarkadas, A. Cox, and W. Zwaenepoel. Treadmarks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <institution> Technical Report Rice COMP TR93-214, Department of Computer Science, Rice University, </institution> <year> 1993. </year>
Reference-contexts: For example, using Indigo, we implement an extended causal memory model. This model makes use of synchronization interactions with consistency maintenance as is done in TreadMarks <ref> [22] </ref>. <p> The algorithm shown in Figure 6 assumes that data-race free programs access shared objects stored in ECM. It has been shown that in such a case, consistency actions need to be performed only when synchronization operations are executed <ref> [20, 22] </ref>. In particular, vector timestamps received with synchronization variables are used to detect objects in the processor cache which may potentially be overwritten. Such objects, which have lower timestamps, 2 We chose the page-based approach because it is simpler to implement as compared to the compiler-based approaches used elsewhere. <p> These results again demonstrate that using type and structure information as in FSOs leads to better performance than using DSM. 5 Related Systems A number of systems that can support state sharing on a workstation cluster (for example, Ivy [29], TreadMarks <ref> [22] </ref>, Midway [7] and Distributed Filaments [15]) have been proposed and implemented.
Reference: [23] <author> Pete Keleher, Alan L. Cox, and Willy Zwaenepoel. </author> <title> Lazy release consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th International Symposium of Computer Architecture, </booktitle> <year> 1992. </year>
Reference-contexts: Demands for increased programmability of distributed memory machines have given rise to research efforts resulting in Distributed Shared Memory (DSM) libraries layered on top of the networking and virtual memory systems <ref> [6, 7, 14, 19, 23] </ref>. They have also resulted in the development of distributed object-oriented systems offering user programs network-wide access to shared services [12, 18, 33] as well as object-oriented concurrent programming layers on top of shared and distributed memory machines [10, 21, 34, 37]. <p> This may be unnecessary because the write is done inside a critical section and processes at other nodes will not be able to access the modified data item until control of the critical section is released <ref> [7, 20, 23] </ref>. 2. Granularity mismatch: in many DSM systems, consistency for shared data is provided at the level of a page. This implies that an update to a small amount of shared data may result in the transfer of an entire page.
Reference: [24] <author> Prince Kohli. </author> <title> User-level State Sharing in Distributed Systems. </title> <type> PhD thesis, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <month> April </month> <year> 1996. </year>
Reference-contexts: The only difference is that the synchronization server waits for all processes that were to synchronize before sending the release message with a timestamp that is the component-wise maximum of the incoming timestamps. The correctness of the algorithm is discussed in <ref> [24] </ref>. The invalidation done on acquiring a lock is similar to the consistency actions of Tread-Marks and the implicit invalidate protocol of Distributed Filaments [15]. However, compared to the latter, in the case when the data is indeed good, we are able to revalidate it using the timestamps.
Reference: [25] <author> Prince Kohli, Gil Neiger, and Mustaque Ahamad. </author> <title> A characterization of scalable shared memories. </title> <booktitle> In Proceedings of the 22nd International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1993. </year>
Reference: [26] <author> Leslie Lamport. </author> <title> Time, clocks, and the ordering of events in a distributed system. </title> <journal> Communications of the ACM, </journal> <volume> 21(7) </volume> <pages> 558-565, </pages> <month> July </month> <year> 1978. </year>
Reference-contexts: For example, in ECM, the view of each processor includes all of its own accesses (reads and writes) but only the writes of other processors all of which are constrained to be placed in a causal order <ref> [26] </ref>. ECM thus places weaker constraints on the consistency of shared variables. Consider the example in reads the location written by the other processor.
Reference: [27] <author> Leslie Lamport. </author> <title> How to make a multiprocessor computer that correctly executes mul-tiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: It can be defined for all types of programs including ones with data races, and it exploits both 10 P 1 : w (x)1 r (y)0 weak-consistency and weak-ordering approaches to improve performance. ECM definition. In a strong memory model like Sequential Consistency (SC) <ref> [27] </ref>, the memory system performs all operations of all processors so that they appear to be executed in a serial order that respects program order. This ordered set of operations constitutes the view of each processor (i.e., how it perceives the operation of the memory system).
Reference: [28] <author> R. Lea, C Jacquemot, and E. Pillevesse. </author> <title> COOL: system support for distributed pro-gramming. </title> <journal> Communications of the ACM, </journal> <volume> 36(9) </volume> <pages> 37-46, </pages> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: Similarly, in comparison with the Nexus library developed for concurrent C++ [14] and with the lower RPC-like layers of distributed object systems like Spring [33], Chorus <ref> [28] </ref>, and distributed objects [34], Indigo attempts to combine both the functionality required by distributed objects and by distributed shared memory.
Reference: [29] <author> Kai Li. Ivy: </author> <title> A shared virtual memory system for parallel computing. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages II 94-101, </pages> <month> Aug </month> <year> 1988. </year>
Reference-contexts: These results again demonstrate that using type and structure information as in FSOs leads to better performance than using DSM. 5 Related Systems A number of systems that can support state sharing on a workstation cluster (for example, Ivy <ref> [29] </ref>, TreadMarks [22], Midway [7] and Distributed Filaments [15]) have been proposed and implemented.
Reference: [30] <author> Kai Li and Paul Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM TOCS, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference: [31] <author> Richard J. Lipton and Jonathan S. Sandberg. </author> <title> PRAM: A scalable shared memory. </title> <type> Technical Report CS-TR-180-88, </type> <institution> Princeton University, Department of Computer Science, </institution> <month> September </month> <year> 1988. </year>
Reference: [32] <author> F. Mattern. </author> <title> Virtual time and global states of distributed systems. </title> <booktitle> In Proceedings of the 1988 International Workshop on Parallel and Distributed Algorithms, </booktitle> <address> Bonas, France. </address> <publisher> North Holland, </publisher> <year> 1989. </year>
Reference-contexts: Data transfers are done only for the object data and not for pages. ECM consistency algorithm. In our implementation of ECM, all consistency-related actions for cached objects take place at synchronization points. Such consistency of shared objects relies on vector timestamps (V T S) <ref> [32] </ref> stored with each object. These timestamps are read from vector clocks that have one entry for each process in the application. A process increments its entry in its vector clock (V T p is p's clock) when a write dirties a cached object.
Reference: [33] <author> J. G. Mitchell, J. G. Gibbons, G. Hamilton, P. B. Kessler, Y. A. Khalidi, P. Kougiouris, P. W. Madany, M. N. Nelson, M. L. Powell, and S. R. Radia. </author> <title> An overview of the spring system. </title>
Reference-contexts: They have also resulted in the development of distributed object-oriented systems offering user programs network-wide access to shared services <ref> [12, 18, 33] </ref> as well as object-oriented concurrent programming layers on top of shared and distributed memory machines [10, 21, 34, 37]. Our contribution to the emerging field of heterogeneous parallel programming is the development of the Indigo data sharing library. <p> Similarly, in comparison with the Nexus library developed for concurrent C++ [14] and with the lower RPC-like layers of distributed object systems like Spring <ref> [33] </ref>, Chorus [28], and distributed objects [34], Indigo attempts to combine both the functionality required by distributed objects and by distributed shared memory.
Reference: [34] <author> Karsten Schwan and Win Bo. </author> <title> Topologies | Distributed objects on multicomputers. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 8(2) </volume> <pages> 111-157, </pages> <year> 1990. </year>
Reference-contexts: They have also resulted in the development of distributed object-oriented systems offering user programs network-wide access to shared services [12, 18, 33] as well as object-oriented concurrent programming layers on top of shared and distributed memory machines <ref> [10, 21, 34, 37] </ref>. Our contribution to the emerging field of heterogeneous parallel programming is the development of the Indigo data sharing library. <p> Similarly, in comparison with the Nexus library developed for concurrent C++ [14] and with the lower RPC-like layers of distributed object systems like Spring [33], Chorus [28], and distributed objects <ref> [34] </ref>, Indigo attempts to combine both the functionality required by distributed objects and by distributed shared memory. <p> It should be possible to exploit such structural information both to communicate object state changes only to those object fragments affected by them and to permit object programmers to implement object-specific consistency and granularity-correct consistency protocols <ref> [34] </ref>. The Indigo library described in this paper is used to address each of the three problems listed above. <p> Alternatively, for complex abstraction--specific computations, handlers may simply trigger application-level threads which can perform abstraction-specific operations of arbitrary complexity. Such association of a handler with incoming messages provides a form of active messages, the use of which has been shown to improve program performance and modularity <ref> [34, 38] </ref>. Event types. The following different types of events are recognized by Indigo (and can thus be "captured"). Each such event has an associated per-process handler that is executed when the event occurs in that process's domain. <p> The specific DSAs implemented in our work are fragmented shared objects (FSOs), similar to the `topology' objects described in <ref> [34] </ref>. Topologies are event-trigger-able fragmented shared objects (on a network in our case) where the events are generally associated with the arrival of remote invocations on local fragments. <p> Moreover, Indigo is portable as it runs on PVM which has been ported to a number of systems. Shapiro [35] proposed a system in which objects fragmented across nodes of a distributed system could be implemented. This work is similar to topologies <ref> [34] </ref>, which is the approach we take for implementing distributed shared abstractions. Indigo provides lower level support for data movement between shared abstraction caches and events that could be used to keep the caches consistent.
Reference: [35] <author> M. Shapiro. </author> <title> Structure and encapsulation in distributed systems: The proxy principle. </title> <booktitle> In Sixth International Conference on Distributed Computing Systems, </booktitle> <address> Boston, Mass., </address> <pages> pages 198-204. </pages> <publisher> IEEE, </publisher> <month> May </month> <year> 1986. </year>
Reference-contexts: Thus we are able to provide system support that matches the needs of various state sharing techniques that range from memory with various consistency needs to FSOs. Moreover, Indigo is portable as it runs on PVM which has been ported to a number of systems. Shapiro <ref> [35] </ref> proposed a system in which objects fragmented across nodes of a distributed system could be implemented. This work is similar to topologies [34], which is the approach we take for implementing distributed shared abstractions. <p> The object fragments and the communication structure between them 28 can be implemented using Indigo calls. In addition, we explored shared abstractions for computational problems and compared their performance with message passing and a DSM system. In contrast, coarse-grain objects such as replicated files are investigated in <ref> [35] </ref>. Tempest [19] also provides mechanisms for communication and synchronization in a parallel program. It differs from Indigo which considers access detection techniques needed by DSM implementations to be a matter of policy and thus a responsibility of the DSM layer whereas Tempest provides support for it.
Reference: [36] <author> V. S. Sunderam. </author> <title> PVM: A framework for parallel distributed computing. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2 </volume> <pages> 315-339, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: These measurements are done on network of Sun Sparc2s on a 10Mbps Ethernet. The hardware page size was 4 Kbytes and the main memory size of the nodes varied from 24-64 Mbytes. PVM <ref> [36] </ref> was used as the underlying communication medium. Though it is not the fastest such medium available, our aim was to understand the issues involved in building a system such as ours and thus, the ease of programming with PVM helps 4 . We ran the applications on 1-8 nodes.
Reference: [37] <author> A. Tanenbaum and S. Mullender. </author> <title> An overview of the amoeba distributed operating system. </title> <journal> Operating Systems Review, </journal> <volume> 15 </volume> <pages> 51-64, </pages> <month> July </month> <year> 1981. </year>
Reference-contexts: They have also resulted in the development of distributed object-oriented systems offering user programs network-wide access to shared services [12, 18, 33] as well as object-oriented concurrent programming layers on top of shared and distributed memory machines <ref> [10, 21, 34, 37] </ref>. Our contribution to the emerging field of heterogeneous parallel programming is the development of the Indigo data sharing library.
Reference: [38] <author> Deborah Wallach, Wilson Hsieh, Kirk Johnson, Frans Kaashoek, and William Weihl. </author> <title> Optimistic active messages: A mechanism for scheduling communication with computation. </title> <year> 1994. </year>
Reference-contexts: We use the term DSA to refer to shared objects that are either implemented as memory objects or as fragmented objects. The breadth of support distinguishes Indigo from communication libraries like active messages <ref> [38] </ref> that do not offer the specific functionality required by DSM or FSO implementations. <p> If such jobs exist, the latter passes them on, else the request is forwarded to the next node in the ring. When this abstraction is implemented, we realize that, besides providing simple data movement calls, some notion of active messages <ref> [38] </ref> is needed. In other words, we need the ability to asynchronously invoke handlers for events such as a put or a get when one process performs these operations on another process's shared memory cache. <p> Alternatively, for complex abstraction--specific computations, handlers may simply trigger application-level threads which can perform abstraction-specific operations of arbitrary complexity. Such association of a handler with incoming messages provides a form of active messages, the use of which has been shown to improve program performance and modularity <ref> [34, 38] </ref>. Event types. The following different types of events are recognized by Indigo (and can thus be "captured"). Each such event has an associated per-process handler that is executed when the event occurs in that process's domain.
Reference: [39] <author> M. Zekauskas, W. Sawdon, and B. Bershad. </author> <title> Software write detection for distributed shared memory. </title> <booktitle> In Proceedings of the First Symposium on Operating Systems Design and Implementation, </booktitle> <month> November </month> <year> 1994. </year> <month> 32 </month>
Reference-contexts: This was sufficient in the DSA implementations and we did not find it necessary to associate thread or object specific handlers. We should note here that many DSMs need some way of detecting accesses to shared data (e.g., write to shared data). Several techniques exist for access detection <ref> [11, 39] </ref>. Due to that reason and also due to the fact that many DSAs do not need such mechanisms, access detection techniques are not part of Indigo.
References-found: 39

