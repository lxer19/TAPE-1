URL: http://www.math.tau.ac.il/~mansour/papers/95focs-poly.ps.gz
Refering-URL: 
Root-URL: 
Title: Simple Learning Algorithms for Decision Trees and Multivariate Polynomials  
Author: Nader H. Bshouty Yishay Mansour 
Address: Calgary, Alberta, Canada  Tel-Aviv, Israel  
Affiliation: Department of Computer Science University of Calgary  Department of Computer Science Tel-Aviv University  
Abstract: There were two techniques in the literature for learning multivariate polynomials and decision trees. These are learning decision trees under the uniform distribution via the Fourier Spectrum [Kushilevitz, Mansour 93 and Jackson 94] and learning decision trees and multivariate polynomials under any distribution via Lattice Theory [Bshouty 94, Schapire and Sellie 93]. These two approaches are used for proving the learnability of many other interesting classes such as CDNF (poly size DNF and CNF) under any distribution and DNF and AC 0 under the uniform distribution. In this paper we develop a new approach. Learning decision trees and multivari-ate polynomials via multivariate interpolation. This is an algebraic approach that is a modification of the approaches used in [Roth, Benedek 89 and Bshouty 95] and is based on the divide and conquer technique for learning [Bshouty 95]. This new approach yelds simple learning algorithms for multivariate polynomials and decision trees over fields under any constant bounded product distribution. The output hypothesis is a (single) multivariate polynomial that is an *-approximation of the target under any constant bounded product distribution. Previous approaches do not achieve this property. The new approach also solves learnability of many other problems that cannot be solved using previous approaches. Problems include PAC-learning with membership queries of j-disjoint DNF (intersection of any j terms is empty) under any constant bounded product distribution, PAC-learning with membership queries of multivari-ate polynomial with bounded degree over any field under the constant bounded product distribution. Learning multivariate polynomials with bounded term size. This in particular gives a learning algorithm for O(log n)-depth decision tree from membership queries only and a new learning algorithm of any multivariate polynomial over sufficiently large fields from membership queries only. We also show that all the results we have for learning from membership queries only are the best that can be achieved. We also prove learnability of Xor of decision trees and sum of decision trees with leaves that contain values from some field. 
Abstract-found: 1
Intro-found: 1
Reference: [A88] <author> D. Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 319-342, </pages> <year> 1988. </year> <month> 11 </month>
Reference: [BT88] <author> M. Ben-Or, P. </author> <title> Tiwari. </title> <booktitle> A deterministic algorithm for sparse multivariate polyno-mial interpolation In Proceedings of the 20th Annual ACM Symposium on Theory of Computing. </booktitle> <pages> pages 301-309, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: We also show that the above field size q is tight, i.e., if jF j = !(q) then no polynomial time learning algorithm exists. This result is a generalization of the results in <ref> [BT88, CDG+91, Z90] </ref> for learning multivariate polynomials under any field. Previous algorithms for learning multivariate polynomial over finite fields F require asking membership queries with assignments in 3 some extension of the field F [CDG+91].
Reference: [Bl92] <author> A. Blum. </author> <title> Learning boolean functions in an infinite attribute space. </title> <booktitle> Machine Learning 9(4), </booktitle> <pages> pages 373-386. </pages> <year> 1992. </year>
Reference: [Bs93] <author> N. H. Bshouty. </author> <title> Exact learning via the monotone theory. </title> <booktitle> In Proceedings of the 34th Symposium on Foundations of Computer Science. </booktitle> <pages> pages 302-311, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: If f (a i ) is zero for all the assignments then with probability close to 1 we have f 0. For decision tree of depth O (log n) we can deterministicly zero-test by choosing the assignments to be the (n; O (log n))-universal set. For details see <ref> [Bs93] </ref>. We now show how to reduce zero-test to learning. Let f 0 = f . Since we can zero-test we can find the minimal i 1 such that f 0 j x 1 0;:::;:::;x i 1 0 0.
Reference: [B195] <author> N. H. Bshouty. </author> <title> Simple Learning Algorithms Using Divide and Conquer. </title> <booktitle> In Proceedings of the Annual ACM Workshop on Computational Learning Theory. </booktitle> <year> 1995. </year>
Reference: [B295] <author> N. H. Bshouty. </author> <title> A Note on Learning Multivariate Polynomials under the Uniform Distribution. </title> <booktitle> In Proceedings of the Annual ACM Workshop on Computational Learning Theory. </booktitle> <year> 1995. </year>
Reference: [CDG+91] <author> M. Clausen, A. Dress, J. Grabmeier, M. Karpinski. </author> <title> On zero-testing and interpolation of k-sparse multivariate polynomials over finite fields. </title> <journal> Theoretical Computer Science. </journal> <volume> 84. </volume> <pages> pages 151-164, </pages> <year> 1991. </year>
Reference-contexts: We also show that the above field size q is tight, i.e., if jF j = !(q) then no polynomial time learning algorithm exists. This result is a generalization of the results in <ref> [BT88, CDG+91, Z90] </ref> for learning multivariate polynomials under any field. Previous algorithms for learning multivariate polynomial over finite fields F require asking membership queries with assignments in 3 some extension of the field F [CDG+91]. <p> This result is a generalization of the results in [BT88, CDG+91, Z90] for learning multivariate polynomials under any field. Previous algorithms for learning multivariate polynomial over finite fields F require asking membership queries with assignments in 3 some extension of the field F <ref> [CDG+91] </ref>. In [CDG+91] it is shown that an extension n of the field is sufficient to interpolate any multivariate polynomial (when membership queries with assignments from an extension field are allowed). <p> This result is a generalization of the results in [BT88, CDG+91, Z90] for learning multivariate polynomials under any field. Previous algorithms for learning multivariate polynomial over finite fields F require asking membership queries with assignments in 3 some extension of the field F <ref> [CDG+91] </ref>. In [CDG+91] it is shown that an extension n of the field is sufficient to interpolate any multivariate polynomial (when membership queries with assignments from an extension field are allowed). Our result in (4.) improves this extension bound to 1 + log (n)= log jF j 1 + log n.
Reference: [GKS90] <author> D. Yu. Grigoriev, M. Karpinski, M. F. Singers. </author> <title> Fast parallel algorithms for sparse multivariate polynomial interpolation over finite fields. </title> <journal> SIAM J. of Comp. </journal> <volume> 19(6). </volume> <pages> pages 1059-1063, </pages> <year> 1990. </year>
Reference: [J94] <author> J. Jackson. </author> <title> An efficient membership-query algorithm for learning DNF with respect to the uniform distribution. </title> <booktitle> In Proceeding of the 35th Annual Symposium on Foundations of Computer Science, </booktitle> <year> 1994. </year>
Reference-contexts: 1 Introduction Two techniques were used in the literature for PAC-learning decision tree with membership queries. Kushilevitz and Mansour gave in [KM93] a technique for learning decision trees under the uniform distribution via the Fourier Spectrum. Jackson in <ref> [J94] </ref> extended the result to learning DNF under the uniform distribution. The output hypothesis is a majority of parities. Bshouty gave in [Bs94] a technique for learning decision trees under any distribution via the Monotone Theory. <p> Our contribution is to show the 2 same when the terms are not nesasary monotone. It is also known that any DNF is PAC-learnable with membership queries under constant bounded product distribution <ref> [J94] </ref>. In [J94] the output hypothesis is a majority of parities. Our contribution for j-disjoint DNF is to use an output hypothesis that is a parity of terms and to show that the output hypothesis is an * approximation of the target against any constant bounded distribution. <p> Our contribution is to show the 2 same when the terms are not nesasary monotone. It is also known that any DNF is PAC-learnable with membership queries under constant bounded product distribution <ref> [J94] </ref>. In [J94] the output hypothesis is a majority of parities. Our contribution for j-disjoint DNF is to use an output hypothesis that is a parity of terms and to show that the output hypothesis is an * approximation of the target against any constant bounded distribution.
Reference: [J95] <author> J. Jackson. </author> <title> Private communication. </title>
Reference-contexts: Schapire and Sellie gave in [SS93] a Lattice based algorithm for learning multivariate polynomials over the binary field under any distribution. In the former the output hypothesis for the decision tree is depth 3 formulas. Jackson <ref> [J95] </ref> generalizes his DNF learning algorithm from uniform distribution to any fixed constant bounded product distribution.
Reference: [KM93] <author> E. Kushilevitz and Y. Mansour. </author> <title> Learning decision trees using the Fourier spectrum. </title> <journal> SIAM J. Computing, </journal> <volume> 22(6) </volume> <pages> 1331-1348, </pages> <year> 1993. </year>
Reference-contexts: 1 Introduction Two techniques were used in the literature for PAC-learning decision tree with membership queries. Kushilevitz and Mansour gave in <ref> [KM93] </ref> a technique for learning decision trees under the uniform distribution via the Fourier Spectrum. Jackson in [J94] extended the result to learning DNF under the uniform distribution. The output hypothesis is a majority of parities. <p> The first result in (3.) is a generalization of the result in [Bs95]. In [Bs95] the learning algorithm uses membership and equivalence queries. The second result is a generalization of the result in <ref> [KM93] </ref> for learning boolean decision tree from membership queries. Result 3. also gives 4. An algorithm for learning any multivariate polynomial over fields of size q = n=(d (log n + log d)) from membership queries only.
Reference: [Ma92] <author> Y. Mansour. </author> <title> Randomized interpolation and approximation of sparse polynomials. </title> <booktitle> In Automata, Languages and Programming: 19th International Colloquim. </booktitle> <pages> pages 261-272, </pages> <month> July </month> <year> 1992. </year>
Reference: [RB89] <author> M. Ron Roth and G. Benedek. </author> <title> Interpolation and approximation of sparse mul-tivariate polynomials over gf(2). </title> <journal> SIAM J. Computing, </journal> <volume> 20(2) </volume> <pages> 291-314, </pages> <year> 1991. </year> <month> 12 </month>
Reference: [SS93] <author> R. E. Schapire, L. M. Sellie. </author> <title> Learning sparse multivariate polynomial over a field with queries and counterexamples. </title> <booktitle> In Proceedings of the Sixth Annual ACM Workshop on Computational Learning Theory. </booktitle> <month> July, </month> <year> 1993. </year>
Reference-contexts: Jackson in [J94] extended the result to learning DNF under the uniform distribution. The output hypothesis is a majority of parities. Bshouty gave in [Bs94] a technique for learning decision trees under any distribution via the Monotone Theory. Schapire and Sellie gave in <ref> [SS93] </ref> a Lattice based algorithm for learning multivariate polynomials over the binary field under any distribution. In the former the output hypothesis for the decision tree is depth 3 formulas. Jackson [J95] generalizes his DNF learning algorithm from uniform distribution to any fixed constant bounded product distribution. <p> The output hypotheses of the learning algorithm is a multivariate polynomial. It is known that multivariate polynomials (with monotone terms) are PAC-learnable with membership queries under any distribution <ref> [SS93] </ref>. Our contribution is to show the 2 same when the terms are not nesasary monotone. It is also known that any DNF is PAC-learnable with membership queries under constant bounded product distribution [J94]. In [J94] the output hypothesis is a majority of parities.
Reference: [Val84] <author> L. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> November </month> <year> 1984. </year>

References-found: 15

