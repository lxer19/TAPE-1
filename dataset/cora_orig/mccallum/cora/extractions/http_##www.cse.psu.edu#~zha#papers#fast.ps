URL: http://www.cse.psu.edu/~zha/papers/fast.ps
Refering-URL: http://www.cse.psu.edu/~zha/papers.html
Root-URL: http://www.cse.psu.edu
Title: FAST PARALLELIZABLE METHODS FOR THE HERMITIAN EIGENVALUE PROBLEM  
Author: HONGYUAN ZHA AND ZHENYUE ZHANG 
Abstract: We propose a quadratically convergent algorithm for computing the invariant sub-spaces of an Hermitian matrix. Each iteration of the algorithm consists of one matrix-matrix multiplication and one QR decomposition. We present an accurate convergence analysis of the algorithm without using the big O notation. We also propose a general framework based on implicit rational transformations which allow us to make connections with several existing algorithms and to derive classes of extensions to our basic algorithm with faster convergence rates. Several numerical examples are given which compare some aspects of the existing algorithms and the new algorithms. 1. Introduction. In [15] we proposed a cubically convergent algorithm for computing the two invariant subspaces of an Hermitian matrix A corresponding to the 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Auslander and A. Tsao. </author> <title> On parallelizable eigensolvers. </title> <booktitle> Advances in Applied Mathematics, </booktitle> <volume> 13 </volume> <pages> 253-261, </pages> <year> 1992. </year>
Reference-contexts: 1. Introduction. In [15] we proposed a cubically convergent algorithm for computing the two invariant subspaces of an Hermitian matrix A corresponding to the eigenvalues of A inside and outside the unit interval <ref> [1; 1] </ref>, respectively. There we also presented a detailed convergence analysis which proved the cubic convergence of the algorithm. <p> There we also presented a detailed convergence analysis which proved the cubic convergence of the algorithm. The derivation of the algorithm is inspired by the work in <ref> [1, 2, 3, 4, 6, 10, 11, 12] </ref> and the algorithm only uses matrix-matrix multiplications and QR decompositions as building blocks which are highly parallelizable primitive operations in libraries such as ScalaPack [14]. <p> To test the power of the implicit rational transformation framework, we will derive a simple version of the matrix sign function scheme from the general framework. We then discuss the relations of our new algorithms with Algorithm ISDA proposed in <ref> [1, 3, 10] </ref> and Algorithm Cubic proposed in [15]. We focus on the accuracy of the invariant subspaces that are computed by those algorithms for a variety of numerical examples. Remark. <p> Remark. It is easy to see that lim W k = P jj&lt;1 ; lim I W k = P jj&gt;1 ; where P jj&gt;1 = Q diag (0; I nr )Q H , the orthogonal projector to the invariant subspace of A corresponding to eigenvalues outside <ref> [1; 1] </ref>. Remark. <p> Then we show this framework can be used to derive several schemes for computing the matrix sign function [8, 9]. We will also relate our approach to that of ISDA proposed in <ref> [1, 3, 10] </ref>. 4.1. Implicit Rational Transformations. The key observation is that un derlying Algorithm Quad there is a fixed-point iteration going on with a rational iteration function. <p> It is easy to verify that function OE (x) is strictly monotonically increasing in the closed interval <ref> [0; 1] </ref>, and maps [0; 1] onto itself. It can also be verified that function OE (x) has three fixed points in [0; 1]: 0; 1=2; and 1. <p> It is easy to verify that function OE (x) is strictly monotonically increasing in the closed interval <ref> [0; 1] </ref>, and maps [0; 1] onto itself. It can also be verified that function OE (x) has three fixed points in [0; 1]: 0; 1=2; and 1. <p> It is easy to verify that function OE (x) is strictly monotonically increasing in the closed interval <ref> [0; 1] </ref>, and maps [0; 1] onto itself. It can also be verified that function OE (x) has three fixed points in [0; 1]: 0; 1=2; and 1. <p> It follows that the following fixed-point iteration x 1 = x; x k+1 = OE (x k ); k = 1; 2; : : :; converges to the function x k ! O (x) = &gt; &lt; 0; x 2 <ref> [0; 1=2) 1; x 2 (1=2; 1] </ref> Therefore the matrix sequence fW k g defined by W k+1 = OE (W k ); k = 1; 2; : : :; converges to the orthogonal projector: W k ! P (W 1 ; (1=2; 1]) j Q diag (I r ; 0)Q <p> Hermitian Eigenproblems 10 where W 1 = Q diag ( 1 ; : : : ; n )Q H is the eigendecomposition of W 1 with 1 1 r &gt; 1=2 &gt; r+1 n 0: It can be verified that the eigenvalues of W 1 are all in the interval <ref> [0; 1] </ref>. <p> 2.1 with k = 0, computes the QR decomposition of [I; A] H which gives the implicit rational transformation of A: W 1 = (I + A 2 ) 1 j f (A); f (x) = 1=(1 + x 2 ); f ((1; 1)) = (1=2; 1]; f ((1; 1) <ref> [1; 1] </ref>) = (0; 1=2): Therefore P (W 1 ; (1=2; 1]) = P jj&lt;1 . <p> A Comparison with ISDA. ISDA uses the following truncated Beta function B 1 (x) = x 2 (3 2x) as the iteration function <ref> [1, 3, 10] </ref>, and only matrix-matrix multiplication is needed in the iteration. The eigenvalues of A will be first transformed to the interval [0; 1]. 4 We notice that other higher order Beta functions can also be used if necessary [1, 3, 10]. <p> A Comparison with ISDA. ISDA uses the following truncated Beta function B 1 (x) = x 2 (3 2x) as the iteration function [1, 3, 10], and only matrix-matrix multiplication is needed in the iteration. The eigenvalues of A will be first transformed to the interval <ref> [0; 1] </ref>. 4 We notice that other higher order Beta functions can also be used if necessary [1, 3, 10]. <p> (x) = x 2 (3 2x) as the iteration function <ref> [1, 3, 10] </ref>, and only matrix-matrix multiplication is needed in the iteration. The eigenvalues of A will be first transformed to the interval [0; 1]. 4 We notice that other higher order Beta functions can also be used if necessary [1, 3, 10]. In this section we compare several properties of the functions OE (x) and B 1 (x), one is used as the iteration function in Algorithm Quad and the other is used as the iteration function in ISDA. <p> In this section we compare several properties of the functions OE (x) and B 1 (x), one is used as the iteration function in Algorithm Quad and the other is used as the iteration function in ISDA. First it can be verified that for x 2 <ref> [0; 1] </ref>, (25=27)OE (x) B 1 (x) 3OE (x): 4 As a side issue, this step can be accomplished using the QR decomposition of [I; A] H instead of explicitly estimating the largest and smallest eigenvalues of A using some version of Gershgorin Theorem and resorting to a linear transformation as <p> B 1 (x) 3OE (x): 4 As a side issue, this step can be accomplished using the QR decomposition of [I; A] H instead of explicitly estimating the largest and smallest eigenvalues of A using some version of Gershgorin Theorem and resorting to a linear transformation as is done in <ref> [1, 3] </ref>. <p> This means that OE (x) expels away any point close to 1=2 about 2=1:5 = 4=3 as fast as B 1 (x). We now compare the function OE 2 (x) and iterated version of B 1 (x) and found out that for x 2 <ref> [1=2; 1] </ref>, OE 2 (x) B 3 j B 1 (B 1 (B 1 (x))): For numerical examples, see Example 2 and Example 3 below. <p> The explanation for this phenomenon goes as follows: from the above discussion we already know that once the eigenvalues are mapped to the interval <ref> [0; 1] </ref>, Algorithm Quad can compute very accurate invariant subspaces. Therefore the problem seems to be in the first step of Algorithm Quad where the matrix A is transformed to the matrix W 1 . <p> Although we have tried larger matrices, we will use a 6 fi 6 matrix as a demonstration. The matrix A = QDQ T with D=[.6 .5+10^(-15) .5+10^(-15) .5-10^(-15) .5-10^(-15) .4] To make a fair comparison the eigenvalues of the matrix is chosen to be in <ref> [0; 1] </ref>. Example 3. In this example we consider another class of matrices with eigenval-ues D=[(1-rand (1,n/2))/2 (1+rand (1,n/2))/2 ] The matrix A with n = 200 is generated as A = QDQ H with Q random orthogonal matrices. <p> Example 2 in Section 4.5 clearly demonstrates that once the spectrum of A is mapped into the interval <ref> [0; 1] </ref>, several algorithms can resolve eigenvalues that are very close to each other. However, Example 1 shows that using the QR decomposition of [I; A] H to implicitly carry out this transformation is not very satisfactory for very close eigenvalues. <p> However, Example 1 shows that using the QR decomposition of [I; A] H to implicitly carry out this transformation is not very satisfactory for very close eigenvalues. We notice that in <ref> [1, 3, 10] </ref>, a linear mapping is constructed based on estimation of the eigenvalues of A using Gershgorin Theorem. Can a better method be devised? 4. Many papers have been written about how to parallelize matrix-matrix multiplication and QR decomposition.
Reference: [2] <author> Z. Bai, J. Demmel and M. Gu. </author> <title> Inverse free parallel spectral divide and conquer algorithms for nonsymmetric eigenproblems. </title> <type> Technical report CSD-94-793, </type> <institution> Computer Science Division, University of California at Berkeley, </institution> <year> 1994. </year>
Reference-contexts: There we also presented a detailed convergence analysis which proved the cubic convergence of the algorithm. The derivation of the algorithm is inspired by the work in <ref> [1, 2, 3, 4, 6, 10, 11, 12] </ref> and the algorithm only uses matrix-matrix multiplications and QR decompositions as building blocks which are highly parallelizable primitive operations in libraries such as ScalaPack [14]. <p> Similar analysis can also be carried out for the so-called inverse-free algorithms in <ref> [12, 2] </ref> in case of Hermitian matrices. Example 2. In this example we compare the speed of convergence of ISDA, Quad and Quart. Although we have tried larger matrices, we will use a 6 fi 6 matrix as a demonstration.
Reference: [3] <author> C. Bischof, S. Huss-Lederman, X. Sun, A. Tsao and T. Turnbull. </author> <title> Parallel studies of the Invariant Subspace Decomposition Approach for banded symmetric matrices. </title> <booktitle> Seventh SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> San Francisco, </address> <year> 1995. </year>
Reference-contexts: There we also presented a detailed convergence analysis which proved the cubic convergence of the algorithm. The derivation of the algorithm is inspired by the work in <ref> [1, 2, 3, 4, 6, 10, 11, 12] </ref> and the algorithm only uses matrix-matrix multiplications and QR decompositions as building blocks which are highly parallelizable primitive operations in libraries such as ScalaPack [14]. <p> To test the power of the implicit rational transformation framework, we will derive a simple version of the matrix sign function scheme from the general framework. We then discuss the relations of our new algorithms with Algorithm ISDA proposed in <ref> [1, 3, 10] </ref> and Algorithm Cubic proposed in [15]. We focus on the accuracy of the invariant subspaces that are computed by those algorithms for a variety of numerical examples. Remark. <p> Then we show this framework can be used to derive several schemes for computing the matrix sign function [8, 9]. We will also relate our approach to that of ISDA proposed in <ref> [1, 3, 10] </ref>. 4.1. Implicit Rational Transformations. The key observation is that un derlying Algorithm Quad there is a fixed-point iteration going on with a rational iteration function. <p> A Comparison with ISDA. ISDA uses the following truncated Beta function B 1 (x) = x 2 (3 2x) as the iteration function <ref> [1, 3, 10] </ref>, and only matrix-matrix multiplication is needed in the iteration. The eigenvalues of A will be first transformed to the interval [0; 1]. 4 We notice that other higher order Beta functions can also be used if necessary [1, 3, 10]. <p> (x) = x 2 (3 2x) as the iteration function <ref> [1, 3, 10] </ref>, and only matrix-matrix multiplication is needed in the iteration. The eigenvalues of A will be first transformed to the interval [0; 1]. 4 We notice that other higher order Beta functions can also be used if necessary [1, 3, 10]. In this section we compare several properties of the functions OE (x) and B 1 (x), one is used as the iteration function in Algorithm Quad and the other is used as the iteration function in ISDA. <p> B 1 (x) 3OE (x): 4 As a side issue, this step can be accomplished using the QR decomposition of [I; A] H instead of explicitly estimating the largest and smallest eigenvalues of A using some version of Gershgorin Theorem and resorting to a linear transformation as is done in <ref> [1, 3] </ref>. <p> However, Example 1 shows that using the QR decomposition of [I; A] H to implicitly carry out this transformation is not very satisfactory for very close eigenvalues. We notice that in <ref> [1, 3, 10] </ref>, a linear mapping is constructed based on estimation of the eigenvalues of A using Gershgorin Theorem. Can a better method be devised? 4. Many papers have been written about how to parallelize matrix-matrix multiplication and QR decomposition.
Reference: [4] <author> A.Y. Bulgakov and S.K. </author> <title> Godunov. Circular dichotomy of the spectrum of a matrix. </title> <journal> Siberian Mathematical Journal, </journal> <volume> 29 </volume> <pages> 734-744, </pages> <year> 1988. </year>
Reference-contexts: There we also presented a detailed convergence analysis which proved the cubic convergence of the algorithm. The derivation of the algorithm is inspired by the work in <ref> [1, 2, 3, 4, 6, 10, 11, 12] </ref> and the algorithm only uses matrix-matrix multiplications and QR decompositions as building blocks which are highly parallelizable primitive operations in libraries such as ScalaPack [14]. <p> Convergence Analysis. In this section we provide a detailed convergence analysis of Algorithm Quad. All the quantities in Algorithm Quad are carefully analyzed and rigorous bounds are given. Specifically we proved that Algorithm Quad converges quadratically. (see Theorem 3.1 and Theorem 3.2.) 1 1 We notice that in <ref> [4, 6, 11, 12] </ref> a very different convergence theory for their algorithms were developed based on matrix Green function theory. A Fast Parallelizable Method for Hermitian Eigenproblems 4 Theorem 3.1.
Reference: [5] <author> A.Edelman and W. Mascarenhas. </author> <title> On Parlett's matrix norm inequality for the Cholesky decomposition. Numerical Linear Algebra with Applications, </title> <booktitle> 2 </booktitle> <pages> 243-250, </pages> <year> 1995. </year>
Reference-contexts: It can be verified that (R (k) ) H R (k) = I + k is the Cholesky decomposition of I + k , a perturbation of the identity matrix. Using the perturbation bounds derived in <ref> [5] </ref>, we have R (k) = I + R k with kR k k 2 (2 log 2 n + 4)k k k 2 (2 log 2 n + 4)(2 + j 2 k : On the other hand, we have M H k = R (k) QD k Q H
Reference: [6] <author> S.K. </author> <title> Godunov. Problem of the dichotomy of the spectrum of a matrix. </title> <journal> Siberian Mathematical Journal, </journal> <volume> 27 </volume> <pages> 649-660, </pages> <year> 1986. </year>
Reference-contexts: There we also presented a detailed convergence analysis which proved the cubic convergence of the algorithm. The derivation of the algorithm is inspired by the work in <ref> [1, 2, 3, 4, 6, 10, 11, 12] </ref> and the algorithm only uses matrix-matrix multiplications and QR decompositions as building blocks which are highly parallelizable primitive operations in libraries such as ScalaPack [14]. <p> Convergence Analysis. In this section we provide a detailed convergence analysis of Algorithm Quad. All the quantities in Algorithm Quad are carefully analyzed and rigorous bounds are given. Specifically we proved that Algorithm Quad converges quadratically. (see Theorem 3.1 and Theorem 3.2.) 1 1 We notice that in <ref> [4, 6, 11, 12] </ref> a very different convergence theory for their algorithms were developed based on matrix Green function theory. A Fast Parallelizable Method for Hermitian Eigenproblems 4 Theorem 3.1.
Reference: [7] <author> G. H. Golub and C. F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins University Press, </publisher> <address> Baltimore, Maryland, 2nd edition, </address> <year> 1989. </year>
Reference: [8] <author> N.J. Higham. </author> <title> The matrix sign decomposition and its relation to the polar decomposition. </title> <journal> Linear Algebra and Its Applications, </journal> 212/213:3-20, 1994. 
Reference-contexts: We demonstrate by way of examples how the general framework can be used to derive iterative schemes with faster convergence rates. Then we show this framework can be used to derive several schemes for computing the matrix sign function <ref> [8, 9] </ref>. We will also relate our approach to that of ISDA proposed in [1, 3, 10]. 4.1. Implicit Rational Transformations. The key observation is that un derlying Algorithm Quad there is a fixed-point iteration going on with a rational iteration function.
Reference: [9] <author> J. Howland. </author> <title> The sign matrix and the separation of matrix eigenvalues. </title> <journal> Linear Algebra and Its Applications, </journal> <volume> 49 </volume> <pages> 221-232, </pages> <year> 1983. </year>
Reference-contexts: We demonstrate by way of examples how the general framework can be used to derive iterative schemes with faster convergence rates. Then we show this framework can be used to derive several schemes for computing the matrix sign function <ref> [8, 9] </ref>. We will also relate our approach to that of ISDA proposed in [1, 3, 10]. 4.1. Implicit Rational Transformations. The key observation is that un derlying Algorithm Quad there is a fixed-point iteration going on with a rational iteration function.
Reference: [10] <author> S. Huss-Lederman, A. Tsao and T. Turnbull. </author> <title> A parallelizable eigensolver for real diagonalizable matrices with real eigenvalues. </title> <note> To appear in SIAM Journal of Scientific Computing. </note>
Reference-contexts: There we also presented a detailed convergence analysis which proved the cubic convergence of the algorithm. The derivation of the algorithm is inspired by the work in <ref> [1, 2, 3, 4, 6, 10, 11, 12] </ref> and the algorithm only uses matrix-matrix multiplications and QR decompositions as building blocks which are highly parallelizable primitive operations in libraries such as ScalaPack [14]. <p> To test the power of the implicit rational transformation framework, we will derive a simple version of the matrix sign function scheme from the general framework. We then discuss the relations of our new algorithms with Algorithm ISDA proposed in <ref> [1, 3, 10] </ref> and Algorithm Cubic proposed in [15]. We focus on the accuracy of the invariant subspaces that are computed by those algorithms for a variety of numerical examples. Remark. <p> Then we show this framework can be used to derive several schemes for computing the matrix sign function [8, 9]. We will also relate our approach to that of ISDA proposed in <ref> [1, 3, 10] </ref>. 4.1. Implicit Rational Transformations. The key observation is that un derlying Algorithm Quad there is a fixed-point iteration going on with a rational iteration function. <p> A Comparison with ISDA. ISDA uses the following truncated Beta function B 1 (x) = x 2 (3 2x) as the iteration function <ref> [1, 3, 10] </ref>, and only matrix-matrix multiplication is needed in the iteration. The eigenvalues of A will be first transformed to the interval [0; 1]. 4 We notice that other higher order Beta functions can also be used if necessary [1, 3, 10]. <p> (x) = x 2 (3 2x) as the iteration function <ref> [1, 3, 10] </ref>, and only matrix-matrix multiplication is needed in the iteration. The eigenvalues of A will be first transformed to the interval [0; 1]. 4 We notice that other higher order Beta functions can also be used if necessary [1, 3, 10]. In this section we compare several properties of the functions OE (x) and B 1 (x), one is used as the iteration function in Algorithm Quad and the other is used as the iteration function in ISDA. <p> However, Example 1 shows that using the QR decomposition of [I; A] H to implicitly carry out this transformation is not very satisfactory for very close eigenvalues. We notice that in <ref> [1, 3, 10] </ref>, a linear mapping is constructed based on estimation of the eigenvalues of A using Gershgorin Theorem. Can a better method be devised? 4. Many papers have been written about how to parallelize matrix-matrix multiplication and QR decomposition.
Reference: [11] <author> A. Malyshev. </author> <title> Computing the invariant subspaces of a regular linear pencil of matrices. </title> <journal> Siberian Mathematical Journal, </journal> <volume> 30 </volume> <pages> 559-567, </pages> <year> 1989. </year>
Reference-contexts: There we also presented a detailed convergence analysis which proved the cubic convergence of the algorithm. The derivation of the algorithm is inspired by the work in <ref> [1, 2, 3, 4, 6, 10, 11, 12] </ref> and the algorithm only uses matrix-matrix multiplications and QR decompositions as building blocks which are highly parallelizable primitive operations in libraries such as ScalaPack [14]. <p> Convergence Analysis. In this section we provide a detailed convergence analysis of Algorithm Quad. All the quantities in Algorithm Quad are carefully analyzed and rigorous bounds are given. Specifically we proved that Algorithm Quad converges quadratically. (see Theorem 3.1 and Theorem 3.2.) 1 1 We notice that in <ref> [4, 6, 11, 12] </ref> a very different convergence theory for their algorithms were developed based on matrix Green function theory. A Fast Parallelizable Method for Hermitian Eigenproblems 4 Theorem 3.1.
Reference: [12] <author> A. Malyshev. </author> <title> Parallel algorithm for solving some spectral problems of linear algebra. </title> <journal> Linear Algebra and Its Applications, </journal> 188/189:489-520, 1993. 
Reference-contexts: There we also presented a detailed convergence analysis which proved the cubic convergence of the algorithm. The derivation of the algorithm is inspired by the work in <ref> [1, 2, 3, 4, 6, 10, 11, 12] </ref> and the algorithm only uses matrix-matrix multiplications and QR decompositions as building blocks which are highly parallelizable primitive operations in libraries such as ScalaPack [14]. <p> Convergence Analysis. In this section we provide a detailed convergence analysis of Algorithm Quad. All the quantities in Algorithm Quad are carefully analyzed and rigorous bounds are given. Specifically we proved that Algorithm Quad converges quadratically. (see Theorem 3.1 and Theorem 3.2.) 1 1 We notice that in <ref> [4, 6, 11, 12] </ref> a very different convergence theory for their algorithms were developed based on matrix Green function theory. A Fast Parallelizable Method for Hermitian Eigenproblems 4 Theorem 3.1. <p> Similar analysis can also be carried out for the so-called inverse-free algorithms in <ref> [12, 2] </ref> in case of Hermitian matrices. Example 2. In this example we compare the speed of convergence of ISDA, Quad and Quart. Although we have tried larger matrices, we will use a 6 fi 6 matrix as a demonstration.
Reference: [13] <author> P. Pandey, C. Kenney, and A. Laub. </author> <title> A parallel algorithm for the matrix sign function. </title> <journal> International Journal of High Speed Computing, </journal> <volume> 2 </volume> <pages> 181-191, </pages> <year> 1990. </year>
Reference-contexts: We now consider the following class of schemes in <ref> [13] </ref>: A k+1 = p p X 1 (A 2 i I) 1 ; A 1 = A; A Fast Parallelizable Method for Hermitian Eigenproblems 14 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 1 2 3 solid line -- graph of phi (x) dash-dot line --
Reference: [14] <author> ScaLAPACK, </author> <note> http://www.netlib.org/scalapack/index.html. </note>
Reference-contexts: The derivation of the algorithm is inspired by the work in [1, 2, 3, 4, 6, 10, 11, 12] and the algorithm only uses matrix-matrix multiplications and QR decompositions as building blocks which are highly parallelizable primitive operations in libraries such as ScalaPack <ref> [14] </ref>. In this paper we continue along the same line of research and concentrate on deriving new algorithms that can substantially reduce the amount of storage and the number of matrix-matrix multiplications in each iteration.
Reference: [15] <author> H. Zha and Z. Zhang. </author> <title> A cubically convergent parallelizable method for the Hermitian eigenvalue problem. </title> <type> CSE Technical Report CSE-96-035, </type> <institution> Department of Computer Science and Engineering, The Pennsylvania State University, </institution> <year> 1996. </year>
Reference-contexts: 1. Introduction. In <ref> [15] </ref> we proposed a cubically convergent algorithm for computing the two invariant subspaces of an Hermitian matrix A corresponding to the eigenvalues of A inside and outside the unit interval [1; 1], respectively. There we also presented a detailed convergence analysis which proved the cubic convergence of the algorithm. <p> The structure of the new algorithm is extremely simple which allows us to give a much refined convergence analysis of the algorithm in Section 3. In particular, we were able to get rid of all the big O expressions which were heavily used in <ref> [15] </ref>. The resulting bounds are cleaner and more concise. In Section 4, we analyze our proposed algorithm from the point of view of implicit rational transformations. This approach allows us to propose classes of extensions of our basic algorithm which have higher convergence rates. <p> To test the power of the implicit rational transformation framework, we will derive a simple version of the matrix sign function scheme from the general framework. We then discuss the relations of our new algorithms with Algorithm ISDA proposed in [1, 3, 10] and Algorithm Cubic proposed in <ref> [15] </ref>. We focus on the accuracy of the invariant subspaces that are computed by those algorithms for a variety of numerical examples. Remark. <p> Remark. Notice that W k converges to an orthogonal projection, i.e., P jj&lt;1 , which can be written as P jj&lt;1 = V V H with V orthonormal. In <ref> [15, Lemma 2.1] </ref>, we proved that if V H = QR is its QR decomposition with column pivoting, then oe min (R) (r!(n r)!=n!); where r is the rank of P jj&lt;1 . <p> Comparison of Algorithm Cubic, Algorithm Quad and Algorithm Quart. In this section we compare the accuracy and convergence behavior of the two algorithms: Algorithm Cubic and Algorithm Quad. For a detailed discussion of Algorithm Cubic, the reader is referred to <ref> [15] </ref>. The main difference between the two algorithms is in Step 2, where the matrices W k and Z k are constructed.
Reference: [16] <author> Z. Zhang and H. Zha. </author> <title> A Parallelizable Method for Computing the Singular Subspaces of a General Matrix CSE Technical Report CSE-96-040, </title> <institution> Department of Computer Science and Engineering, The Pennsylvania State University, </institution> <year> 1996. </year>
Reference-contexts: Remark. We want to emphasize that when the matrix A is non-Hermitian, then all the algorithms proposed in the sequel can be converted into algorithms for computing the singular subspaces of A <ref> [16] </ref>. 2. The Algorithm. Our purpose is to derive an algorithm which uses as few number of matrix-matrix multiplications as possible in each iteration that computes fl Department of Computer Science and Engineering, The Pennsylvania State University, University Park, PA 16802, zha@cse.psu.edu.
References-found: 16

