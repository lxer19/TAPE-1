URL: http://vismod.www.media.mit.edu/~jdavis/NewPapers/pui98.ps
Refering-URL: http://vismod.www.media.mit.edu/~jdavis/
Root-URL: http://www.media.mit.edu
Email: jdavis@media.mit.edu, bobick@media.mit.edu  
Title: Virtual PAT: A Virtual Personal Aerobics Trainer  
Author: James W. Davis Aaron F. Bobick 
Address: 20 Ames Street, Cambridge, MA 02139  
Affiliation: MIT Media Lab  
Abstract: A prototype system for implementing a virtual Personal Aerobics Trainer (PAT) is presented. Unlike workout video tapes or TV exercise shows, this system allows the user to create and personalize an aerobics session to meet the user's needs and desires. Various media technology and computer vision algorithms are used to enhance the interaction of a virtual instructor by enabling it to watch and talk to the user. Throughout the paper, we report the system components and discuss the advantages, problems, and extensions of the design. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Blumberg, B. </author> <title> Old tricks, new dogs: ethology and interactive creatures. </title> <type> PhD dissertation, </type> <institution> MIT Media Lab, MIT, </institution> <year> 1996. </year>
Reference-contexts: It might even be 1 All media components were developed using SGI's Digital Media utilities/libraries. quite fun to have virtual cartoonish-like characters as the instructors. Each character could have their own "attitude" and behavior <ref> [1] </ref>, which would possibly increase the entertainment value of the system. But in the current system, we chose to use stored movie clips for simplicity. Also, to date we have only recorded one instructor though the system is designed to have multiple instructors from which the user can choose.
Reference: [2] <author> A. Bobick, S. Intille, J. Davis, F. Baird, L. Cambell, Y. Ivanov, C. Pinhanez, A. Schutte, and A. Wilson. </author> <title> The KidsRoom: Action recognition in an interactive story environment. </title> <note> Presence (to appear). </note>
Reference-contexts: This vision technology is different from many other sensing technologies in that the user need not wear puter vision modules within the PAT system. any special clothing/devices or be tethered to machines with bundles of wires. This enables the experience to be more natural and desirable <ref> [2, 9] </ref>. We now explain the design of the system. 2. System design The PAT system is a modular design of media and vision components connected to a controller (See Figure 2). <p> We use real-time computer vision techniques to "watch" the user and determine if he/she is currently performing the same move as the instructor. A background-subtracted image of the room yielding only a silhouette of the person <ref> [4, 2, 9] </ref> is used as the input to the vision system. 3.1. Robust silhouette extraction tech nique The silhouetting method used in this system is based on the optical blocking (or eclipsing) of specialized non-visible light (e.g. infrared light) rather than the color differences between the person and background.
Reference: [3] <author> Davis, J. and A. Bobick. </author> <title> The representation and recognition of human movement using temporal templates. </title> <booktitle> In Proc. Comp. Vis. and Pattern Rec., </booktitle> <pages> pages 928-934, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: For recognition of these moves, statistical pattern recognition techniques are applied to moment-based feature descriptors of the templates. The system employs user training to get a measure of the variation which arise from different people. This approach easily extends to multiple camera views of the person; see <ref> [3] </ref> for details on the algorithm. 4. Future possibilities Presently, the system is designed for a single user. To increase the usefulness of the system, it could be designed to run with multiple people (as in a class).
Reference: [4] <author> Davis, J. and A. Bobick. </author> <title> A robust human-silhouette extraction technique for interactive virtual environments. In IFIP Workshop on Modelling and Motion Capture Techniques for Virtual Environments, </title> <address> Geneva, Switzer-land, </address> <month> Nov. </month> <year> 1998. </year>
Reference-contexts: We use real-time computer vision techniques to "watch" the user and determine if he/she is currently performing the same move as the instructor. A background-subtracted image of the room yielding only a silhouette of the person <ref> [4, 2, 9] </ref> is used as the input to the vision system. 3.1. Robust silhouette extraction tech nique The silhouetting method used in this system is based on the optical blocking (or eclipsing) of specialized non-visible light (e.g. infrared light) rather than the color differences between the person and background. <p> The infrared light is not visible to the human visual system and thus one sees only video projected on the display screen (as shown in Figure 5 (a)). We point the reader to <ref> [4] </ref> for further details on the approach. This silhouette form is then further processed to recognize the movements of the user. 3.2. Motion Templates Recently, we have developed real-time computer vision methods for recognizing large-scale body movements such as aerobic exercises.
Reference: [5] <author> Ishii, H., and B. Ullmer. </author> <title> Tangible bits: towards seamless interfaces between people, bits and atoms. </title> <booktitle> In CHI'97, </booktitle> <pages> pages 234 - 241, </pages> <year> 1997. </year>
Reference-contexts: The method also permits the display of video graphics behind the user on the background. Figure 4 shows the environment configuration. This system is similar in nature to the infrared designs of <ref> [7, 5] </ref>. We can see the silhouetting process in Figure 5. Figure 5 (a) shows a standard camera view of someone standing in front of the back-wall projection screen with graphics displayed.
Reference: [6] <author> C. S. Pinhanez, K. Mase, and A. F. Bobick. </author> <title> Interval scripts: A design paradigm for story-based interactive systems. </title> <booktitle> In CHI'97, </booktitle> <pages> pages 287-294, </pages> <address> Atlanta, Georgia, </address> <month> Mar. </month> <year> 1997. </year>
Reference-contexts: The system was designed so that each session is guided from a simple script which controls the flow of the session (similar to <ref> [6] </ref>). Included in the script are the names for the workout moves, the time allotted for each move, the choice of music (song titles) for the workout, and the instructor (by name) to run the session. This allows the user to easily choose their own tailored workout.
Reference: [7] <author> Rekimoto, J. and N. </author> <title> Matsushita. Perceptual surfaces: towards a human and object sensitive interactive display. </title> <booktitle> In Workshop on Perceptual User Interfaces (PUI-97), </booktitle> <pages> pages 30 - 32, </pages> <month> October </month> <year> 1997. </year>
Reference-contexts: The method also permits the display of video graphics behind the user on the background. Figure 4 shows the environment configuration. This system is similar in nature to the infrared designs of <ref> [7, 5] </ref>. We can see the silhouetting process in Figure 5. Figure 5 (a) shows a standard camera view of someone standing in front of the back-wall projection screen with graphics displayed.
Reference: [8] <author> Scheirer, E. </author> <title> Tempo and beat analysis of acoustic musical signals. </title> <journal> Acoustic Society of America, </journal> <volume> 103(1) </volume> <pages> 588-601, </pages> <month> Jan </month> <year> 1998. </year>
Reference-contexts: This way, the computer can watch for the beat events while not actually playing audible notes. We note that there exists methods for accomplishing the above beat track synchronization using other musical media forms (e.g. a CD); such a method is found in <ref> [8] </ref>. At times, there can be a bit of a jump when looping the movie clips in synchronization with the beat track of the music. The actual time measured between beats in a song and also the response time of the movie adjustment can sometimes vary slightly.
Reference: [9] <author> Wren, C., Azarbayejani, A., Darrell, T., and A. Pent-land. Pfinder: </author> <title> Real-time tracking of the human body. </title> <booktitle> In SPIE Conference on Integration Issues in Large Commercial Media Delivery Systems, </booktitle> <year> 1995. </year> <month> 6 </month>
Reference-contexts: This vision technology is different from many other sensing technologies in that the user need not wear puter vision modules within the PAT system. any special clothing/devices or be tethered to machines with bundles of wires. This enables the experience to be more natural and desirable <ref> [2, 9] </ref>. We now explain the design of the system. 2. System design The PAT system is a modular design of media and vision components connected to a controller (See Figure 2). <p> We use real-time computer vision techniques to "watch" the user and determine if he/she is currently performing the same move as the instructor. A background-subtracted image of the room yielding only a silhouette of the person <ref> [4, 2, 9] </ref> is used as the input to the vision system. 3.1. Robust silhouette extraction tech nique The silhouetting method used in this system is based on the optical blocking (or eclipsing) of specialized non-visible light (e.g. infrared light) rather than the color differences between the person and background.
References-found: 9

