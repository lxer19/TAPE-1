URL: http://www-swiss.ai.mit.edu/~pas/p/hpdc4.ps.Z
Refering-URL: http://www-swiss.ai.mit.edu/~pas/biblio.html
Root-URL: 
Email: pas@ai.mit.edu  
Title: Parallel Simulation of Subsonic Fluid Dynamics on a Cluster of Workstations  
Author: Panayotis A. Skordos 
Date: 1995  
Address: Washington D.C., USA  545 Technology Square NE43-432, Cambridge, MA 02139 USA  
Affiliation: Computing,  Massachusetts Institute of Technology  
Note: Proceedings of Fourth IEEE International Symposium on High Performance Distributed  
Abstract: An effective approach of simulating subsonic fluid dynamics on a cluster of non-dedicated workstations is presented. The approach is applied to simulate the flow of air in wind instruments. The use of local-interaction methods and coarse-grain decompositions lead to small communication requirements. The automatic migration of processes from busy hosts to free hosts enables the use of non-dedicated workstations. Simulations of 2D flow achieve 80% parallel efficiency (speedup/processors) using 20 HP-Apollo workstations. Detailed measurements of the parallel efficiency of 2D and 3D simulations are presented, and a theoretical model of efficiency is developed and compared against the measurements. Two numerical methods of fluid dynamics are tested: explicit finite differences, and the lattice Boltzmann method. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Peyret and T. D. Taylor, </author> <title> Computational Methods For Fluid Flow. </title> <publisher> Springer-Verlag, </publisher> <address> New York, N.Y., </address> <year> 1990. </year>
Reference-contexts: A jet of air enters through an opening on the left wall, and impinges the sharp edge in front of it. Air exits from the simulation through the opening on the right. The success of the present approach depends critically on the use of local-interaction methods (explicit numerical methods <ref> [1] </ref>) because the communication capability of the Ethernet network is small. Explicit methods are inherently parallel, have small communication requirements and small cost per integration step, but require small integration time steps for numerical stability. By contrast, implicit methods [1] are challenging to parallelize, have large communication requirements and large cost <p> depends critically on the use of local-interaction methods (explicit numerical methods <ref> [1] </ref>) because the communication capability of the Ethernet network is small. Explicit methods are inherently parallel, have small communication requirements and small cost per integration step, but require small integration time steps for numerical stability. By contrast, implicit methods [1] are challenging to parallelize, have large communication requirements and large cost per integration step, but they can use much larger integration time steps 1 than explicit methods. <p> If we wish to model the passage of acoustic waves at the finest grid resolution x, then we must choose the time step t ~ x=c s which is very small and also turns out to be sufficient for the numerical stability of explicit methods for the present problem (see <ref> [1] </ref> and section 6). On the other hand, if we do not insist on modeling acoustic waves at the finest grid resolution, then we can use larger time steps with implicit methods. <p> We use explicit numerical methods (explicit finite differences and the lattice Boltzmann method) to calculate the future values of density and velocity at each fluid node using the present values of density and velocity at this node and at neighboring nodes. The finite difference method <ref> [1] </ref> has the following form: * Calculate V x ; V y (inner) * Communicate: send/recv V x ; V y (boundary) * Calculate (inner) * Communicate: send/recv (boundary) * Filter ; V x ; V y (inner) For numerical stability reasons, the density is updated using the values of velocity <p> A filter is also included (fourth-order numerical viscosity <ref> [15, 16, 1] </ref>) in order to mitigate the nonlinear instabilities of compressible flow at high Reynolds number. The same filter is used both for the finite difference method and for the lattice Boltzmann method.
Reference: [2] <author> C. H. Cap and V. Strumpen, </author> <title> "Efficient parallel computing in distributed workstation environments," </title> <journal> Parallel Computing, </journal> <volume> vol. 19, no. 11, </volume> <pages> pp. 1221-1234, </pages> <year> 1993. </year>
Reference-contexts: discussed as generally as possible within the context of local-interaction problems, and the specifics of fluid dynamics are limited to sections 2 and 6. 1.1 Comparison with other work The suitability of local-interaction algorithms for parallel computing on a cluster of workstations has been demonstrated in previous works such as <ref> [2] </ref>, [3] and elsewhere. Cap&Strumpen [2] present the PARFORM system and simulate the unsteady heat equation using explicit finite differences. Chase&et al. [3] present the AMBER parallel system, and solve Laplace's equation using Successive Over-Relaxation. <p> within the context of local-interaction problems, and the specifics of fluid dynamics are limited to sections 2 and 6. 1.1 Comparison with other work The suitability of local-interaction algorithms for parallel computing on a cluster of workstations has been demonstrated in previous works such as <ref> [2] </ref>, [3] and elsewhere. Cap&Strumpen [2] present the PARFORM system and simulate the unsteady heat equation using explicit finite differences. Chase&et al. [3] present the AMBER parallel system, and solve Laplace's equation using Successive Over-Relaxation. The present work clarifies further the importance of local-interaction methods for parallel systems with small communication capacity. <p> An alternative approach that has been used elsewhere is the dynamic allocation of processor workload. In the present context, dynamic allocation means to enlarge and to shrink the subregions which are assigned to each workstation depending on the CPU load of the workstation (Cap&Strumpen <ref> [2] </ref>). Although this approach is important in various applications (Blumofe&Park [5]), it seems unnecessary for simulating fluid flow problems with static geometry and may lead to large overhead.
Reference: [3] <author> J. Chase, F. Amador, E. Lazowska, H. Levy, and R. Littlefield, </author> <title> "The Amber system: Parallel programming on a network of multiprocessors," </title> <journal> ACM SIGOPS Operating Systems Review, </journal> <volume> vol. 23, no. 5, </volume> <pages> pp. 147-158, </pages> <year> 1989. </year>
Reference-contexts: as generally as possible within the context of local-interaction problems, and the specifics of fluid dynamics are limited to sections 2 and 6. 1.1 Comparison with other work The suitability of local-interaction algorithms for parallel computing on a cluster of workstations has been demonstrated in previous works such as [2], <ref> [3] </ref> and elsewhere. Cap&Strumpen [2] present the PARFORM system and simulate the unsteady heat equation using explicit finite differences. Chase&et al. [3] present the AMBER parallel system, and solve Laplace's equation using Successive Over-Relaxation. <p> and 6. 1.1 Comparison with other work The suitability of local-interaction algorithms for parallel computing on a cluster of workstations has been demonstrated in previous works such as [2], <ref> [3] </ref> and elsewhere. Cap&Strumpen [2] present the PARFORM system and simulate the unsteady heat equation using explicit finite differences. Chase&et al. [3] present the AMBER parallel system, and solve Laplace's equation using Successive Over-Relaxation. The present work clarifies further the importance of local-interaction methods for parallel systems with small communication capacity.
Reference: [4] <author> G. Fox, M. Johnson, G. Lyzenga, S. Otto, J. Salmon, and D. Walker, </author> <title> Solving Problems on Concurrent Processors, </title> <journal> vol. </journal> <volume> 1. </volume> <publisher> Prentice-Hall Inc., </publisher> <year> 1988. </year>
Reference-contexts: The model of parallel efficiency which is developed in section 8 is based on ideas which have been discussed previously, for example in Fox et al. <ref> [4] </ref> and elsewhere. Here, the model is derived in a clear way, and the predictions of the model are compared against experimental measurements of parallel efficiency. Regarding the problem of using non-dedicated workstations, this problem is handled by employing automatic process migration from busy hosts to free hosts. <p> Our measurements show that during normal execution, all the parallel processes are very close in time, starting and ending each computational cycle almost at the same time. The communication of data between processes is organized using a well-known programming technique which is called "padding" or "ghost cells" (Fox <ref> [4] </ref>, Camp [12]). Specifically, we pad each subregion with one or more layers of extra nodes on the outside depending on how far the local-interaction rule extends to (for example, one layer in the present fluid flow simulations).
Reference: [5] <author> R. Blumofe and D. Park, </author> <title> "Scheduling large-scale parallel computations on networks of workstations," </title> <booktitle> in Proceedings of High Performance Distributed Computing 94, </booktitle> <address> San Franscisco, </address> <publisher> Califor-nia, </publisher> <pages> pp. 96-105, </pages> <year> 1994. </year>
Reference-contexts: In the present context, dynamic allocation means to enlarge and to shrink the subregions which are assigned to each workstation depending on the CPU load of the workstation (Cap&Strumpen [2]). Although this approach is important in various applications (Blumofe&Park <ref> [5] </ref>), it seems unnecessary for simulating fluid flow problems with static geometry and may lead to large overhead. For such problems, it may be more effective to use large fixed-size subre-gions per processor, and to apply automatic migration of processes from busy hosts to free hosts.
Reference: [6] <author> V. S. Sunderam, </author> <title> "A framework for parallel distributed computing," </title> <journal> Concurrency: Practice and Experience, </journal> <volume> vol. 2, no. 4, </volume> <pages> pp. 315-339, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: The present approach does not address the issues of high-level programming, parallel languages, inhomogeneous clusters of workstations, and distributed computing of general problems. Efforts along these directions are the PVM system <ref> [6] </ref>, the Linda system [7], the packages of Kohn&Baden [8] and Chesshire&Naik [9] that facilitate parallel decomposition, the Orca language for distributed computing [10], etc. 2 Examples of flow simulations The present distributed system has been applied to simulate the flow of air and the generation of tones in wind instruments
Reference: [7] <author> N. Carriero, D. Gelernter, D. Kaminsky, and J. Westbrook, </author> <title> Adaptive Parallelism with Piranha. </title> <type> Report No. </type> <institution> YALEU/DCS/RR-954, Department of Computer Science, Yale University, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: The present approach does not address the issues of high-level programming, parallel languages, inhomogeneous clusters of workstations, and distributed computing of general problems. Efforts along these directions are the PVM system [6], the Linda system <ref> [7] </ref>, the packages of Kohn&Baden [8] and Chesshire&Naik [9] that facilitate parallel decomposition, the Orca language for distributed computing [10], etc. 2 Examples of flow simulations The present distributed system has been applied to simulate the flow of air and the generation of tones in wind instruments such as the recorder
Reference: [8] <author> S. Kohn and S. Baden, </author> <title> A robust parallel programming model for dynamic non-uniform scientific computations. </title> <type> Report CS94-354, </type> <institution> University of California, </institution> <address> San Diego, </address> <year> 1994. </year>
Reference-contexts: The present approach does not address the issues of high-level programming, parallel languages, inhomogeneous clusters of workstations, and distributed computing of general problems. Efforts along these directions are the PVM system [6], the Linda system [7], the packages of Kohn&Baden <ref> [8] </ref> and Chesshire&Naik [9] that facilitate parallel decomposition, the Orca language for distributed computing [10], etc. 2 Examples of flow simulations The present distributed system has been applied to simulate the flow of air and the generation of tones in wind instruments such as the recorder and the flute (flue pipes).
Reference: [9] <author> G. Chesshire and V. Naik, </author> <title> "An environment for parallel and distributed computation with application to overlapping grids," </title> <journal> IBM Journal Research and Development, </journal> <volume> vol. 38, no. 3, </volume> <pages> pp. 285-300, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: The present approach does not address the issues of high-level programming, parallel languages, inhomogeneous clusters of workstations, and distributed computing of general problems. Efforts along these directions are the PVM system [6], the Linda system [7], the packages of Kohn&Baden [8] and Chesshire&Naik <ref> [9] </ref> that facilitate parallel decomposition, the Orca language for distributed computing [10], etc. 2 Examples of flow simulations The present distributed system has been applied to simulate the flow of air and the generation of tones in wind instruments such as the recorder and the flute (flue pipes).
Reference: [10] <author> H. Bal, F. Kaashoek, and A. Tanenbaum, "Orca: </author> <title> A languate for parallel programming of distributed systems," </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> vol. 18, no. 3, </volume> <pages> pp. 190-205, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Efforts along these directions are the PVM system [6], the Linda system [7], the packages of Kohn&Baden [8] and Chesshire&Naik [9] that facilitate parallel decomposition, the Orca language for distributed computing <ref> [10] </ref>, etc. 2 Examples of flow simulations The present distributed system has been applied to simulate the flow of air and the generation of tones in wind instruments such as the recorder and the flute (flue pipes).
Reference: [11] <author> A. Hirschberg, </author> <title> Wind Instruments. </title> <institution> Eindhoven Institute of Technology, Report R-1290-D, </institution> <year> 1994. </year>
Reference-contexts: The oscillations are reenforced by a nonlinear feedback from the acoustic waves to the jet of air. The mechanism of flow-generated sound in wind instruments is a 100-year-old problem whose details are still a subject of active research <ref> [11] </ref>. impinging the sharp edge. The outlet of the simulation is located at the top of the picture. The present parallel system can simulate the flow of air inside flue pipes using uniform orthogonal grids as large as 1200 fi 1200 in two dimensions (1:5 million nodes) and even larger.
Reference: [12] <author> W. Camp, S. Plimpton, B. Hendrickson, and R. Leland, </author> <title> "Massively parallel methods for engineering and science problems," </title> <journal> Communications of the ACM, </journal> <volume> vol. 37, no. 4, </volume> <pages> pp. 31-41, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: The communication of data between processes is organized using a well-known programming technique which is called "padding" or "ghost cells" (Fox [4], Camp <ref> [12] </ref>). Specifically, we pad each subregion with one or more layers of extra nodes on the outside depending on how far the local-interaction rule extends to (for example, one layer in the present fluid flow simulations).
Reference: [13] <author> F. Douglis, </author> <title> Transparent Process Migration in the Sprite Operating System. </title> <type> Report No. </type> <institution> UCB/CSD 90/598, Computer Science Division (EECS), University of California Berkeley, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: It is straightforward because the processes are programmed to deal with the migration themselves. By contrast, process migration in a general computing environment such as a distributed operating system <ref> [13] </ref> can be a very challenging task. 5.1 Sharing the network A related issue to sharing the workstations with other users, is sharing the network and the file server.
Reference: [14] <author> G. Batchelor, </author> <title> An Introduction to Fluid Dynamics. </title> <publisher> Cambridge University Press, </publisher> <year> 1967. </year>
Reference-contexts: must save their state one after the other in an orderly fashion, allowing sufficient time gaps between, so that other programs can use the network and the file system. 6 Numerical algorithms In our simulations of subsonic flow, we solve numerically the compressible Navier Stokes equations (in the adiabatic form <ref> [14] </ref>). Three fluid variables are involved: the fluid density , and the components of the fluid velocity V x ; V y in the x,y directions respectively. We employ a uniform grid of fluid nodes which looks very much like the grid of nodes in figure 3.
Reference: [15] <author> P. Skordos, </author> <title> Modeling flue pipes: subsonic flow, lattice Boltzmann, and parallel distributed computers. </title> <institution> Department of Electrical Engineering and Computer Science, MIT, </institution> <type> Ph.D. Dissertation, </type> <month> Jan-uary </month> <year> 1995. </year>
Reference-contexts: A filter is also included (fourth-order numerical viscosity <ref> [15, 16, 1] </ref>) in order to mitigate the nonlinear instabilities of compressible flow at high Reynolds number. The same filter is used both for the finite difference method and for the lattice Boltzmann method. <p> The same filter is used both for the finite difference method and for the lattice Boltzmann method. The lattice Boltzmann method is a recently-developed method for simulating subsonic flow which is competitive with finite differences in terms of numerical accuracy <ref> [17, 15] </ref> and has slightly better stability properties. The lattice Boltzmann method uses two kinds of variables to represent the fluid: the traditional fluid variables ; V x ; V y and another set of variables called populations F i .
Reference: [16] <author> P. Skordos and G. Sussman, </author> <title> "Comparison between subsonic flow simulation and physical measurements of flue pipes," </title> <booktitle> in Proceedings of ISMA 95, International Symposium on Musical Acoustics, </booktitle> <address> Le Normont, France, </address> <month> July, </month> <year> 1995. </year>
Reference-contexts: A filter is also included (fourth-order numerical viscosity <ref> [15, 16, 1] </ref>) in order to mitigate the nonlinear instabilities of compressible flow at high Reynolds number. The same filter is used both for the finite difference method and for the lattice Boltzmann method.
Reference: [17] <author> P. Skordos, </author> <title> "Initial and boundary conditions for the lattice Boltzmann method," </title> <journal> Physical Review E, </journal> <volume> vol. 48, no. 6, </volume> <pages> pp. 4823-4842, </pages> <month> December </month> <year> 1993. </year> <month> 13 </month>
Reference-contexts: The same filter is used both for the finite difference method and for the lattice Boltzmann method. The lattice Boltzmann method is a recently-developed method for simulating subsonic flow which is competitive with finite differences in terms of numerical accuracy <ref> [17, 15] </ref> and has slightly better stability properties. The lattice Boltzmann method uses two kinds of variables to represent the fluid: the traditional fluid variables ; V x ; V y and another set of variables called populations F i . <p> In two dimensional problems, both methods communicate 3 variables per fluid node. 7 Performance measurements We measure the performance of the distributed system when using the finite difference method and the lattice Boltzmann method to simulate Hagen-Poiseuille flow through a rectangular channel <ref> [17] </ref>. The goal of testing both methods is to examine the performance of the parallel system on two similar, but slightly different parallel algorithms.
References-found: 17

