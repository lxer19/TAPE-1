URL: http://www.ai.mit.edu/people/deniz/ps/yuretphd.ps.gz
Refering-URL: http://www.ai.mit.edu/people/deniz/
Root-URL: 
Title: Discovery of Linguistic Relations Using Lexical Attraction  
Author: by Deniz Yuret Patrick H. Winston Arthur C. Smith 
Degree: Submitted to the Department of Electrical Engineering and Computer Science in partial fulfillment of the requirements for the degree of Doctor of Philosophy at the  All rights reserved. Author  Certified by  Ford Professor  Thesis Supervisor Accepted by  Chairman, Department Committee on Graduate Students  
Date: May 1998  May 15, 1998  
Affiliation: MASSACHUSETTS INSTITUTE OF TECHNOLOGY  c Massachusetts Institute of Technology 1998.  Department of Electrical Engineering and Computer Science  of Artificial Intelligence and Computer Science  
Abstract-found: 0
Intro-found: 0
Reference: <author> Baker, J. </author> <year> 1979. </year> <title> Trainable grammars for speech recognition. </title> <booktitle> In Speech communication papers presented at the 97th Meeting of the Acoustical Society, </booktitle> <pages> 547-550. </pages>
Reference-contexts: The Viterbi algorithm selects the most probable analysis of a sentence given a model (Viterbi 1967). The Baum-Welch algorithm estimates the parameters of the model given a sequence of training data (Baum 1972). These algorithms are generalized to work with probabilistic context free grammars in addition to HMM's <ref> (Baker 1979) </ref>. The Baum-Welch is sometimes called the forward-backward algorithm in the context of HMM's and the inside-outside algorithm in the context of PCFG's. For a detailed description of these algorithms, see (Rabiner & Juang 1986; Lari & Young 1990; Charniak 1993).
Reference: <author> Baum, L. E. </author> <year> 1972. </year> <title> An inequality and associated maximization technique in statistical estimation for probabilistic functions of markov processes. </title> <booktitle> Inequalities 3 </booktitle> <pages> 1-8. </pages>
Reference-contexts: The pillars of this approach are two algorithms for training and processing with probabilistic language models. The Viterbi algorithm selects the most probable analysis of a sentence given a model (Viterbi 1967). The Baum-Welch algorithm estimates the parameters of the model given a sequence of training data <ref> (Baum 1972) </ref>. These algorithms are generalized to work with probabilistic context free grammars in addition to HMM's (Baker 1979). The Baum-Welch is sometimes called the forward-backward algorithm in the context of HMM's and the inside-outside algorithm in the context of PCFG's.
Reference: <author> Beeferman, D.; Berger, A.; and Lafferty, J. </author> <year> 1997. </year> <title> A model of lexical attraction and repulsion. </title> <booktitle> In ACL/EACL '97. </booktitle>
Reference-contexts: In fact, Beefer-man et al. report that words can continue to show selectional influence for a window of several hundred words <ref> (Beeferman, Berger, & Lafferty 1997) </ref>. However, the degree of the dependency falls exponentially with distance. That justifies the choice of the n-gram models to relate dependency to proximity. Nevertheless, using the previous n 1 words as context is against our linguistic intuition.
Reference: <author> Briscoe, T., and Waegner, N. </author> <year> 1992. </year> <title> Robust stochastic parsing using the inside-outside algorithm. </title> <booktitle> In AAAI '92 Workshop on Probabilistically-Based Natural Language Processing Techniques, </booktitle> <pages> 39-53. </pages>
Reference-contexts: Their method was tested on small artificial languages and only worked when the grammar space was fairly restricted. Briscoe and Waegner started with a partial initial grammar and achieved good results training on a corpus of unbracketed text <ref> (Briscoe & Waegner 1992) </ref>. Pereira and Schabes started with all possible Chomsky normal form rules with a restricted number of nonterminals and trained on the Air Travel Information System spoken language corpus (Pereira & Schabes 1992).
Reference: <author> Brown, P. F., et al. </author> <year> 1992. </year> <title> Class-based n-gram models of natural language. </title> <booktitle> Computational Linguistics 18(4) </booktitle> <pages> 467-479. </pages>
Reference: <author> Carroll, G., and Charniak, E. </author> <year> 1992a. </year> <title> Learning probabilistic dependency grammars from labeled text. In Probabilistic Approaches to Natural Language, </title> <booktitle> Papers from 1992 AAAI Fall Symposium, </booktitle> <pages> 25-31. </pages>
Reference: <author> Carroll, G., and Charniak, E. </author> <year> 1992b. </year> <title> Two experiments on learning probabilistic dependency grammars from corpora. </title> <booktitle> In Workshop Notes, Statistically Based NLP Techniqies, AAAI, </booktitle> <pages> 1-13. </pages>
Reference: <author> Charniak, E. </author> <year> 1993. </year> <title> Statistical language learning. </title> <publisher> MIT Press. </publisher>
Reference-contexts: Figure 4-3 gives the accuracy and entropy results from this work. In general these approaches fail on unsupervised acquisition because of the large size of the search space, the complexity of the estimation algorithm and the problem of local maxima. Charniak provides a detailed review of this work <ref> (Charniak 1993) </ref>. More recent work has focused on improving the efficiency of the training methods (Stolcke 1994; Chen 1996; de Marcken 1996). maxima. de Marcken (de Marcken 1995) has an excellent critique on why the current approaches to unsupervised language learning fail.
Reference: <author> Charniak, E. </author> <year> 1997. </year> <title> Statistical parsing with a context-free grammar and word statistics. </title> <booktitle> In AAAI'97. </booktitle>
Reference: <author> Chen, S. F. </author> <year> 1996. </year> <title> Building probabilistic models for natural language. </title> <type> Ph.D. Dissertation, </type> <institution> Harvard University. </institution>
Reference: <author> Chomsky, N. </author> <year> 1957. </year> <title> Syntactic Structures. </title> <publisher> Mouton. </publisher>
Reference-contexts: Even though there are cases where either syntax or semantics alone is enough to get a unique interpretation, in general we need both. What we need from semantics in particular is the likelihood of various relations between words. 1 Sentence (3) is from Chomsky <ref> (Chomsky 1957) </ref>. Sentence (1) is attributed to Lenat. Sentence (2) is from Schank (Schank & Colby 1973). 9 Language acquisition Children start mapping words to concepts before they have a full grasp of syntax.
Reference: <author> Chomsky, N. </author> <year> 1965. </year> <title> Aspects of the theory of syntax. </title> <publisher> MIT Press. </publisher>
Reference-contexts: It follows that the context of a word would be better determined by its linguistic relations rather than according to a fixed pattern. 24 Words in direct syntactic relation have strong dependencies. Chomsky defines such dependencies as selectional relations <ref> (Chomsky 1965) </ref>. Subject and verb, for example, have a selectional relation, and so do verb and object. Subject and object, on the other hand, are assumed to be chosen independently of one another. It should be noted that this independence is only an approximation.
Reference: <author> Collins, M. J. </author> <year> 1996. </year> <title> A new statistical parser based on bigram lexical dependencies. </title> <booktitle> In Proceedings of the 34th Annual Meeting of the ACL. </booktitle> <volume> 53 Cormen, </volume> <editor> T. H.; Leiserson, C. E.; and Rivest, R. L. </editor> <year> 1990. </year> <title> Introduction to Algorithms. </title> <publisher> MIT Press and McGraw-Hill. </publisher>
Reference: <author> Cover, T. M., and Thomas, J. A. </author> <year> 1991. </year> <title> Elements of Information Theory. </title> <publisher> John Wiley and Sons, Inc. </publisher> <editor> de Marcken, C. G. </editor> <year> 1995. </year> <title> On the unsupervised acquisition of phrase-structure grammars. </title> <booktitle> In Third Workshop on Very Large Corpora. </booktitle> <editor> de Marcken, C. G. </editor> <year> 1996. </year> <title> Unsupervised language acquisition. </title> <type> Ph.D. Dissertation, </type> <institution> MIT. </institution>
Reference: <author> Fujisaki, T.; Jelinek, F.; et al. </author> <year> 1989. </year> <title> A probabilistic parsing method for sentence disambiguation. </title> <booktitle> In Proceedings of the 1st International Workshop on Parsing Technologies, </booktitle> <pages> 85-94. </pages>
Reference: <author> Gaifman, H. </author> <year> 1965. </year> <title> Dependency systems and phrase-structure systems. </title> <booktitle> Information and Control 8 </booktitle> <pages> 304-337. </pages>
Reference-contexts: seems more plausible for John to be in the red dress. (7) John met the woman in the red dress in the afternoon (8) John met the woman in the afternoon in the red dress ? Gaifman gave the first formal analysis of dependency structures that satisfy the planarity condition <ref> (Gaifman 1965) </ref>. His paper gives a natural correspondence between dependency systems and phrase-structure systems and shows that the dependency model characterized by planarity is context-free. Sleator and Temperley show that their planar model is also context-free even though it allows cycles (Sleator & Temperley 1991).
Reference: <author> Graham, R. L.; Knuth, D. E.; and Patashnik, O. </author> <year> 1994. </year> <title> Concrete Mathematics. </title> <publisher> Addison-Wesley, </publisher> <address> 2 edition. </address>
Reference-contexts: This is a recurrence with 3-fold convolution. The general expression for a recurrence with m-fold convolution is C (mn; n)=(mn n + 1) where C is the binomial coefficient <ref> (Graham, Knuth, & Patashnik 1994, p. 361) </ref>. Therefore f (n) = C (3n; n)=(2n + 1). The first few values of f (n) are: 1, 1, 3, 12, 55, 273, 1428. Figure 3-1 shows the possible dependency structures with up to four words.
Reference: <author> Harary, F. </author> <year> 1969. </year> <title> Graph Theory. </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: P (L) = 1=jLj, where jLj is the number of possible dependency structures. Without the planarity condition, the number of possible dependency structures for an n word sentence would be given by Cayley's formula: n n2 <ref> (Harary 1969) </ref>. The encoding of the dependency structure would then take O (n log n) bits. However, the encoding of planar dependency structures is linear in the number of words as the following theorem shows.
Reference: <author> Hudson, R. A. </author> <year> 1984. </year> <title> Word Grammar. </title> <publisher> B. Blackwell. </publisher>
Reference-contexts: This property is called planarity (Sleator & Temperley 1993), projectivity (Mel'cuk 1988), or adjacency <ref> (Hudson 1984) </ref> by various researchers. The examples below illustrate the planarity of English. In sentence (7), it is easily seen that the woman was in the red dress and the meeting was in the afternoon. However, in sentence (8) the same interpretation is not possible.
Reference: <author> Jelinek, F. </author> <year> 1985. </year> <title> Markov source modeling of text generation. </title> <editor> In Skwirzinski, J. K., ed., </editor> <booktitle> The Impact of Processing Techniques on Communications. </booktitle> <publisher> Martinus Nijhoff. </publisher> <pages> 569-598. </pages>
Reference: <author> Lari, K., and Young, S. </author> <year> 1990. </year> <title> The estimation of stochastic context-free grammars using the inside-outside algorithm. </title> <booktitle> Computer Speech and Language 4(1) </booktitle> <pages> 35-56. </pages>
Reference-contexts: Early work focused on optimizing the parameters for hand-built grammars (Jelinek 1985; Fujisaki, Jelinek, & others 1989; Sharman, Jelinek, & Mercer 1990). Lari and Young used the inside-outside algorithm for grammar induction using an artificially generated language <ref> (Lari & Young 1990) </ref>. Their algorithm is only practical for small category sets and does not scale up to a realistic grammar of natural language. Carroll and Charniak used an incremental approach where new rules are generated when existing rules fail to parse a sentence (Carroll & Charniak 1992a; 1992b).
Reference: <author> Lee, L. </author> <year> 1997. </year> <title> Similarity-based approaches to natural language processing. </title> <type> Ph.D. Dissertation, </type> <institution> Harvard University. </institution>
Reference: <author> Magerman, D. M. </author> <year> 1995. </year> <title> Statistical decision-tree models for parsing. </title> <booktitle> In Proceedings of the 33rd Annual Meeting of the ACL. </booktitle>
Reference: <author> Mel'cuk, I. A. </author> <year> 1988. </year> <title> Dependency Syntax: Theory and Practice. </title> <type> SUNY. 54 Pearl, </type> <institution> J. </institution> <year> 1988. </year> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Pereira, F. C., and Schabes, Y. </author> <year> 1992. </year> <title> Inside-outside reestimation from partially bracketed corpora. </title> <booktitle> In Proceedings of the 30th Annual Meeting of the Association for Computational Linguist, </booktitle> <pages> 128-135. </pages>
Reference-contexts: Pereira and Schabes started with all possible Chomsky normal form rules with a restricted number of nonterminals and trained on the Air Travel Information System spoken language corpus <ref> (Pereira & Schabes 1992) </ref>. They achieved good results training with the bracketed corpus but 47 the program showed no improvement in accuracy when trained with raw text. Even though the entropy improved, the bracketing accuracy stayed around 37% for raw text.
Reference: <author> Pereira, F., and Tishby, N. </author> <year> 1992. </year> <title> Distributional similarity, phase transitions and hierarchical clustering. In Probabilistic Approaches to Natural Language, </title> <booktitle> Papers from 1992 AAAI Fall Symposium, </booktitle> <pages> 108-112. </pages>
Reference-contexts: Pereira and Schabes started with all possible Chomsky normal form rules with a restricted number of nonterminals and trained on the Air Travel Information System spoken language corpus <ref> (Pereira & Schabes 1992) </ref>. They achieved good results training with the bracketed corpus but 47 the program showed no improvement in accuracy when trained with raw text. Even though the entropy improved, the bracketing accuracy stayed around 37% for raw text.
Reference: <author> Quirk, R.; Greenbaum, S.; Leech, G.; and Svartvik, J. </author> <year> 1985. </year> <title> A Comprehensive Grammar of the English Language. </title> <publisher> Longman. </publisher>
Reference-contexts: N = 100; 000 * many people died in the clashes in the west in september . * N = 1; 000; 000 * many people died in the clashes in the west in september . * English preposition of is particularly ambiguous in its semantic function <ref> (Quirk et al. 1985) </ref>. It can be used in a function similar to that of the genitive (the gravity of the earth ~ the earth's gravity), or in partitive constructions (bottle of wine) among others. 18 The two sentences in Figure 2-5 are syntactically identical.
Reference: <author> Rabiner, L., and Juang, B. </author> <year> 1986. </year> <title> An introduction to hidden markov models. </title> <journal> IEEE ASSP Magazine 4-16. </journal>
Reference: <author> Schank, R. C., and Colby, K. M. </author> <year> 1973. </year> <title> Computer Models of Thought and Language. </title> <publisher> Freeman. </publisher>
Reference-contexts: What we need from semantics in particular is the likelihood of various relations between words. 1 Sentence (3) is from Chomsky (Chomsky 1957). Sentence (1) is attributed to Lenat. Sentence (2) is from Schank <ref> (Schank & Colby 1973) </ref>. 9 Language acquisition Children start mapping words to concepts before they have a full grasp of syntax. At that stage, the problem facing the child is not unlike the disambiguation problem in sentences like (1) and (2).
Reference: <author> Shannon, C. E. </author> <year> 1948. </year> <title> A mathematical theory of communication. </title> <journal> The Bell System Technical Journal 27. </journal>
Reference: <author> Shannon, C. E. </author> <year> 1951. </year> <title> Prediction and entropy of printed english. </title> <journal> The Bell System Technical Journal 30 </journal> <pages> 50-64. </pages>
Reference: <author> Sharman, R.; Jelinek, F.; and Mercer, R. </author> <year> 1990. </year> <title> Generating a grammar for statistical training. </title> <booktitle> In Proceedings of the Third DARPA Speech and Natural Language Workshop, </booktitle> <pages> 267-274. </pages>
Reference: <author> Sleator, D., and Temperley, D. </author> <year> 1991. </year> <title> Parsing english with a link grammar. </title> <type> Technical Report CMU-CS-91-196, CMU. </type>
Reference-contexts: Mel'cuk discusses important properties of syntactic relations in his book on dependency formalism (Mel'cuk 1988). A large scale implementation of English syntax based on a similar formalism by Sleator and Temperley uses 107 different types of syntactic relations such as subject-verb, verb-object, and determiner-noun <ref> (Sleator & Temperley 1991) </ref>. I did not differentiate between different types of syntactic relations in this work. The goal of the learning program described in Chapter 4 is to correctly determine whether or not two words in a sentence are syntactically related. <p> His paper gives a natural correspondence between dependency systems and phrase-structure systems and shows that the dependency model characterized by planarity is context-free. Sleator and Temperley show that their planar model is also context-free even though it allows cycles <ref> (Sleator & Temperley 1991) </ref>. Lexical attraction is symmetric Lexical attraction between two words is symmetric. The mutual information is the same no matter which direction the dependency goes. This directly follows from Bayes' rule.
Reference: <author> Sleator, D., and Temperley, D. </author> <year> 1993. </year> <title> Parsing english with a link grammar. </title> <booktitle> In Third international workshop on parsing technologies. </booktitle>
Reference-contexts: This property is called planarity <ref> (Sleator & Temperley 1993) </ref>, projectivity (Mel'cuk 1988), or adjacency (Hudson 1984) by various researchers. The examples below illustrate the planarity of English. In sentence (7), it is easily seen that the woman was in the red dress and the meeting was in the afternoon.
Reference: <author> Stolcke, A. </author> <year> 1994. </year> <title> Bayesian learning of probabilistic language models. </title> <type> Ph.D. Dissertation, </type> <institution> University of California at Berkeley. </institution> <note> 55 Viterbi, </note> <author> A. J. </author> <year> 1967. </year> <title> Error bounds for convolutional codes and an asymptotically optimal decoding algorithm. </title> <journal> IEEE Transactions on Information Processing 13 </journal> <pages> 260-269. </pages>
Reference: <author> Zipf, G. K. </author> <year> 1949. </year> <title> Human Behavior and the Principle of Least Effort. </title> <publisher> Addison-Wesley. </publisher> <pages> 56 </pages>
References-found: 36

