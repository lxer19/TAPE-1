URL: ftp://synapse.cs.byu.edu/pub/papers/vanhorn_2.ps
Refering-URL: ftp://synapse.cs.byu.edu/pub/papers/details.html
Root-URL: 
Email: email: vanhorn@bert.cs.byu.edu, martinez@cs.byu.edu  
Title: The Design and Evaluation of a Rule Induction Algorithm  
Author: Kevin S. Van Horn and Tony R. Martinez 
Address: Provo, UT 84602  
Affiliation: Computer Science Department Brigham Young University  
Abstract: technical report BYU-CS-93-11 June 1993 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Blumer, A., et al. </author> <year> (1989.) </year> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the ACM 36, </journal> <pages> 929-965. </pages>
Reference-contexts: A method for choosing one of the rule lists output by G. There is a well-known trade-off between low empirical error and simplicity of hypothesis, as there is a tendency to fit noise or statistical flukes of the training sample when excessively complex hypotheses are allowed <ref> [1, 5, 9, 11] </ref>. A number of other machine-learning algorithms have used this two-part strategy; see, e.g., [11] or the literature on tree induction algorithms [2, 6]. The two parts are quite independent, and one can be changed without affecting the other. <p> Since () is monotonically decreasing in *, we can compute estErr (s; e) using a binary search of the interval <ref> [0; 1] </ref>. It remains only to discuss the computation of () and ~ N . For any single hypothesis h 0 with error *, the number of training examples misclassified out of m total examples is a random variable following the binomial distribution Binom (m; *). <p> With this in mind we set ~ N = N 0 for some 0 &lt; fi &lt; 1. The above estimate of jRj assumed there were only two classes. Theoretical results using the Vapnik-Chervonenkis dimension to bound the difference between empirical error and actual error <ref> [1] </ref>, applied to conjunctive concepts [4], suggest that this difference does not depend on the number of classes. Thus we compute ~ N as described above even when there are more than two classes. <p> In the upper-bound computation, counters [i; 0] and <ref> [i; 1] </ref> are replaced by a single counter [i], for each Boolean attribute i. The statement "increase [i; x i ]" is replaced by "if (x i ) increase [i]".
Reference: [2] <author> Breiman, L., et al. </author> <year> (1984.) </year> <title> Classification and Regression Trees. </title> <address> Belmont, CA: </address> <publisher> Wadsworth. </publisher>
Reference-contexts: A number of other machine-learning algorithms have used this two-part strategy; see, e.g., [11] or the literature on tree induction algorithms <ref> [2, 6] </ref>. The two parts are quite independent, and one can be changed without affecting the other. The second part is discussed in Section 6; for now we concentrate on algorithm G. In this section and Section 4 we assume that the training sample is consistent. <p> In this section we describe how BBG trades off the conflicting goals of low empirical error and low complexity to choose one of the h i output by G. In the literature can be found a number of methods for handling this trade-off; these include cross-validation <ref> [2, 11] </ref>, using a separate hold-out set on which to test the sequence of hypotheses produced [3], the minimum description-length principle [7], Vapnik's structural risk minimization [9, 10], and heuristic estimates of the actual error of the best hypothesis found for a given size bound [6].
Reference: [3] <author> Devroye, L. </author> <year> (1988.) </year> <title> Automatic pattern recognition: a study of the probability of error. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence 10, </journal> <pages> 530-543. </pages>
Reference-contexts: In the literature can be found a number of methods for handling this trade-off; these include cross-validation [2, 11], using a separate hold-out set on which to test the sequence of hypotheses produced <ref> [3] </ref>, the minimum description-length principle [7], Vapnik's structural risk minimization [9, 10], and heuristic estimates of the actual error of the best hypothesis found for a given size bound [6]. Any of these methods can be used in combination with algorithm G. BBG uses the last approach mentioned.
Reference: [4] <author> Haussler, D. </author> <year> (1988.) </year> <title> Quantifying inductive bias: AI learning algorithms and Valiant's learning framework. </title> <booktitle> Artificial Intelligence 36, </booktitle> <pages> 177-221. </pages>
Reference-contexts: The above estimate of jRj assumed there were only two classes. Theoretical results using the Vapnik-Chervonenkis dimension to bound the difference between empirical error and actual error [1], applied to conjunctive concepts <ref> [4] </ref>, suggest that this difference does not depend on the number of classes. Thus we compute ~ N as described above even when there are more than two classes. The reader may be concerned at the unrealistic assumptions and rough approximations used in the derivation of estErr ().
Reference: [5] <author> Haussler, D. </author> <year> (1992.) </year> <title> Decision theoretic generalizations of the PAC model for neural net and other learning applications. </title> <booktitle> Information and Computation 100, </booktitle> <pages> 78-150. </pages>
Reference-contexts: A method for choosing one of the rule lists output by G. There is a well-known trade-off between low empirical error and simplicity of hypothesis, as there is a tendency to fit noise or statistical flukes of the training sample when excessively complex hypotheses are allowed <ref> [1, 5, 9, 11] </ref>. A number of other machine-learning algorithms have used this two-part strategy; see, e.g., [11] or the literature on tree induction algorithms [2, 6]. The two parts are quite independent, and one can be changed without affecting the other.
Reference: [6] <author> Quinlan, J. R. </author> <year> (1993.) </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: A number of other machine-learning algorithms have used this two-part strategy; see, e.g., [11] or the literature on tree induction algorithms <ref> [2, 6] </ref>. The two parts are quite independent, and one can be changed without affecting the other. The second part is discussed in Section 6; for now we concentrate on algorithm G. In this section and Section 4 we assume that the training sample is consistent. <p> this trade-off; these include cross-validation [2, 11], using a separate hold-out set on which to test the sequence of hypotheses produced [3], the minimum description-length principle [7], Vapnik's structural risk minimization [9, 10], and heuristic estimates of the actual error of the best hypothesis found for a given size bound <ref> [6] </ref>. Any of these methods can be used in combination with algorithm G. BBG uses the last approach mentioned. <p> We wanted to compare its performance on the same data sets with that of some well-respected and widely-known learning algorithm; for this purpose we chose Quinlan's C4.5. We compiled the code that came with his book <ref> [6] </ref> on DECstation 5000's and HP 710's. Since C4.5 can produce either decision trees (the c4.5 program) or rule lists (the c4.5rules program), we tested both alternatives. The default parameters were always used with these programs. For BBG we set the memory limit and time limit t as follows.
Reference: [7] <author> Quinlan, J. R., & Rivest, R. L. </author> <year> (1989.) </year> <title> Inferring decision trees using the Minimum Description Length Principle. </title> <booktitle> Information and Computation 80, </booktitle> <pages> 227-248. </pages>
Reference-contexts: In the literature can be found a number of methods for handling this trade-off; these include cross-validation [2, 11], using a separate hold-out set on which to test the sequence of hypotheses produced [3], the minimum description-length principle <ref> [7] </ref>, Vapnik's structural risk minimization [9, 10], and heuristic estimates of the actual error of the best hypothesis found for a given size bound [6]. Any of these methods can be used in combination with algorithm G. BBG uses the last approach mentioned.
Reference: [8] <author> Tou, J., & Gonzalez, R. </author> <year> (1974.) </year> <title> Pattern Recognition Principles. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: Note that, as is a common assumption in the pattern-recognition literature <ref> [8] </ref>, we assume that the relationship between a description x and its class c is in general a stochastic one, and not necessarily deterministic.
Reference: [9] <author> Vapnik, V. N. </author> <year> (1982.) </year> <title> Estimation of Dependences Based on Empirical Data. </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: A method for choosing one of the rule lists output by G. There is a well-known trade-off between low empirical error and simplicity of hypothesis, as there is a tendency to fit noise or statistical flukes of the training sample when excessively complex hypotheses are allowed <ref> [1, 5, 9, 11] </ref>. A number of other machine-learning algorithms have used this two-part strategy; see, e.g., [11] or the literature on tree induction algorithms [2, 6]. The two parts are quite independent, and one can be changed without affecting the other. <p> In the literature can be found a number of methods for handling this trade-off; these include cross-validation [2, 11], using a separate hold-out set on which to test the sequence of hypotheses produced [3], the minimum description-length principle [7], Vapnik's structural risk minimization <ref> [9, 10] </ref>, and heuristic estimates of the actual error of the best hypothesis found for a given size bound [6]. Any of these methods can be used in combination with algorithm G. BBG uses the last approach mentioned. <p> BBG uses the last approach mentioned. The various h i are compared according to a heuristic estimate of their actual error, computed from their empirical error and size, and the best is chosen. (This method bears some resemblance to Vapnik's structural risk minimization <ref> [9, 10] </ref>.) In particular let s i be the size of h i and e i = errs (h i ; S).
Reference: [10] <author> Vapnik, V. N. </author> <year> (1989.) </year> <title> Inductive principles of the search for empirical dependences. </title> <booktitle> In Proceedings of the 2nd Annual Workshop on Computational Learning Theory. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In the literature can be found a number of methods for handling this trade-off; these include cross-validation [2, 11], using a separate hold-out set on which to test the sequence of hypotheses produced [3], the minimum description-length principle [7], Vapnik's structural risk minimization <ref> [9, 10] </ref>, and heuristic estimates of the actual error of the best hypothesis found for a given size bound [6]. Any of these methods can be used in combination with algorithm G. BBG uses the last approach mentioned. <p> BBG uses the last approach mentioned. The various h i are compared according to a heuristic estimate of their actual error, computed from their empirical error and size, and the best is chosen. (This method bears some resemblance to Vapnik's structural risk minimization <ref> [9, 10] </ref>.) In particular let s i be the size of h i and e i = errs (h i ; S).
Reference: [11] <author> Weiss, S., & Kulikowski, C. </author> <year> (1991.) </year> <title> Computer Systems That Learn. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher> <pages> 17 </pages>
Reference-contexts: A method for choosing one of the rule lists output by G. There is a well-known trade-off between low empirical error and simplicity of hypothesis, as there is a tendency to fit noise or statistical flukes of the training sample when excessively complex hypotheses are allowed <ref> [1, 5, 9, 11] </ref>. A number of other machine-learning algorithms have used this two-part strategy; see, e.g., [11] or the literature on tree induction algorithms [2, 6]. The two parts are quite independent, and one can be changed without affecting the other. <p> A number of other machine-learning algorithms have used this two-part strategy; see, e.g., <ref> [11] </ref> or the literature on tree induction algorithms [2, 6]. The two parts are quite independent, and one can be changed without affecting the other. The second part is discussed in Section 6; for now we concentrate on algorithm G. <p> In this section we describe how BBG trades off the conflicting goals of low empirical error and low complexity to choose one of the h i output by G. In the literature can be found a number of methods for handling this trade-off; these include cross-validation <ref> [2, 11] </ref>, using a separate hold-out set on which to test the sequence of hypotheses produced [3], the minimum description-length principle [7], Vapnik's structural risk minimization [9, 10], and heuristic estimates of the actual error of the best hypothesis found for a given size bound [6].
References-found: 11

