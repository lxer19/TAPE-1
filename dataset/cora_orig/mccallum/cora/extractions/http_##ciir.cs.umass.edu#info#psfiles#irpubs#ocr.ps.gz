URL: http://ciir.cs.umass.edu/info/psfiles/irpubs/ocr.ps.gz
Refering-URL: http://ciir.cs.umass.edu/info/psfiles/irpubs/irnew.html
Root-URL: 
Title: An Evaluation of Information Retrieval Accuracy with Simulated OCR Output  
Author: W.B. Croft S.M. Harding K. Taghva and J. Borsack 
Address: Nevada, Las Vegas  
Affiliation: Computer Science Department University of Massachusetts, Amherst Information Science Research Institute University of  
Abstract: Optical Character Recognition (OCR) is a critical part of many text-based applications. Although some commercial systems use the output from OCR devices to index documents without editing, there is very little quantitative data on the impact of OCR errors on the accuracy of a text retrieval system. Because of the difficulty of constructing test collections to obtain this data, we have carried out evaluations using simulated OCR output on a variety of databases. The results show that high quality OCR devices have little effect on the accuracy of retrieval, but low quality devices used with databases of short documents can result in significant degradation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N. J. Belkin and W.B. Croft. </author> <title> Information filtering and information retrieval: Two sides of the same coin. </title> <journal> Communications of the ACM, </journal> <volume> 35(12) </volume> <pages> 29-38, </pages> <year> 1992. </year>
Reference-contexts: The study reported here focuses on the impact of the word accuracy rates of the OCR system. To generate a simulated OCR database, then, an IR test database is indexed using standard techniques such as tokenization, stemming, and stopword removal <ref> [6, 1] </ref>. During this process, the text of a document is randomly assigned to page groups, and index words are randomly discarded according to the error rates for that page Croft, Harding, Taghva and Borsack group and word length.
Reference: [2] <author> J. P. Callan, W. B. Croft, and S. M. Harding. </author> <title> The INQUERY retrieval system. </title> <booktitle> In Proceedings of the Third International Conference on Database and Expert Systems Applications, </booktitle> <pages> pages 78-83, </pages> <address> Valencia, Spain, 1992. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Word positions, which are used in proximity operators, were counted whether discarded or not. The results of this process on four test collections are given in the next section. 3 The Experiments The experiments were done using the IN-QUERY information retrieval system developed at the University of Massachusetts <ref> [2] </ref>. INQUERY is based on a probabilistic model of retrieval, has a number of advanced features, and has consistently achieved excellent results at the ARPA-sponsored TREC and TIPSTER evaluations (see [4] for an overview of the TREC evaluation).
Reference: [3] <author> Edward A. Fox. </author> <title> Characterization of two new experimental collections in computer and information science containing textual and bibliographic concepts. </title> <type> Technical Report 83-561, </type> <institution> Department of Computer Science, Cornell University, </institution> <address> Ithaca, NY 14853, </address> <month> September </month> <year> 1983. </year>
Reference-contexts: Four test collections were used in these experiments. The collections were selected to represent a range of sources, document sizes, and query sizes. The CACM collection is a small collection of Computer Science Abstracts <ref> [3] </ref> and has been a standard benchmark for a number of years. NPL is a larger collection of short documents and short queries that has been used in a variety of IR experiments. WEST is a collection of long, full-text, legal information, specifically case law.
Reference: [4] <author> D. </author> <title> Harman. </title> <booktitle> Overview of the first TREC conference. In Proceedings of the 16 th ACM SIGIR International Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 36-47, </pages> <year> 1993. </year>
Reference-contexts: INQUERY is based on a probabilistic model of retrieval, has a number of advanced features, and has consistently achieved excellent results at the ARPA-sponsored TREC and TIPSTER evaluations (see <ref> [4] </ref> for an overview of the TREC evaluation). For the purposes of these experiments, the main features of INQUERY are that it does automatic indexing and produces ranked lists of documents in response to a query. These are features that are common to many recent information retrieval systems. <p> The WSJ collection is the largest number of documents, which are moderate length, full-text articles from the Wall St. Journal. The WSJ queries are also the longest of any collection. The WSJ collection is a subset of the TIPSTER collection described in <ref> [4] </ref>. In general, we would expect OCR errors to have more impact on the collections of short documents, since long documents would have much more redundant information. This is one of the factors that is tested in the experiments.
Reference: [5] <author> S. Rice, J. Kanai, and T. Nartker. </author> <title> An evaluation of OCR accuracy. </title> <booktitle> In UNLV Information Science Research Institute Annual Report, </booktitle> <pages> pages 9-20, </pages> <year> 1993. </year>
Reference-contexts: In this paper, we describe our first approach to obtaining accuracy data using simulated OCR output for a range of databases. The simulation is done using data about word error rates for a variety of devices tested at the UNLV Information Science Research Institute (ISRI) <ref> [5] </ref>. Although the simulation is not completely accurate, it is the first study about OCR and retrieval effectiveness where the results have some basis particular query. on actual OCR data. In the next section, we describe how the simulation was done. <p> The results of the experiments are summarized in the fourth section, and the final section suggests future directions for this work. 2 The OCR Simulation The data that was used for the simulation was a study of character and word error rates for a range of OCR devices and software <ref> [5] </ref>. The study was done using a sample of 460 document pages from a Department of Energy test database. The word error rates that were reported in this study are not uniformly distributed throughout the document.
Reference: [6] <author> Gerard Salton and Michael J. McGill. </author> <title> Introduction to Modern Information Retrieval. </title> <publisher> McGraw-Hill, </publisher> <year> 1983. </year>
Reference-contexts: IR system is typically measured using precision and recall 1 with a test collection 1 Precision is the percentage of retrieved documents that are relevant, and recall is the percentage of relevant documents that are retrieved, for a consisting of a document database, queries, and relevance judgements for those queries <ref> [6] </ref>. Despite the fact that there are commercial retrieval systems that use OCR input, the lack of availability of test collections means that there is very little published data about the effect on retrieval accuracy. <p> The study reported here focuses on the impact of the word accuracy rates of the OCR system. To generate a simulated OCR database, then, an IR test database is indexed using standard techniques such as tokenization, stemming, and stopword removal <ref> [6, 1] </ref>. During this process, the text of a document is randomly assigned to page groups, and index words are randomly discarded according to the error rates for that page Croft, Harding, Taghva and Borsack group and word length.
Reference: [7] <author> K. Taghva. </author> <title> Results of applying probabilistic ir to ocr text. </title> <address> pages 1-20, </address> <year> 1994. </year>
Reference-contexts: In this study, we ignore errors caused by incorrect zoning, that is, attempting to do OCR on figures, maps, etc. A recent study has shown that the type of OCR errors generated by incorrect zoning can have an impact on retrieval performance <ref> [7] </ref>. In other words, the zoning accuracy of an OCR system is an important factor in determining retrieval performance independent of the word accuracy of the system. This happens because zoning errors can generate large numbers of "misspelled" words that affect a retrieval system's probability estimates.
Reference: [8] <author> K. Taghva, J. Borsack, A. Condit, and S. Erva. </author> <title> The effects of noisy data on text retrieval. </title> <booktitle> In UNLV Information Science Research Institute Annual Report, </booktitle> <pages> pages 71-80, </pages> <year> 1993. </year> <title> An Evaluation of Information Retrieval Accuracy with Simulated OCR Output </title>
Reference-contexts: Despite the fact that there are commercial retrieval systems that use OCR input, the lack of availability of test collections means that there is very little published data about the effect on retrieval accuracy. In a recent study, Taghva, Borsack, Condit and Erva <ref> [8] </ref> did a comparison of the output of a retrieval system using a document database created using scanning and OCR, and the same database with errors removed by editing. The comparison was done by comparing the overlap of the retrieved documents for a set of test queries.
References-found: 8

