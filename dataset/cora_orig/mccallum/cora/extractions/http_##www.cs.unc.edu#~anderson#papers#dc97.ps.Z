URL: http://www.cs.unc.edu/~anderson/papers/dc97.ps.Z
Refering-URL: http://www.cs.unc.edu/~anderson/papers.html
Root-URL: http://www.cs.unc.edu
Title: Using Local-Spin k-Exclusion Algorithms to Improve Wait-Free Object Implementations  
Author: James H. Anderson Mark Moir 
Date: November 1995 Revised November 1996, February 1997  
Address: Chapel Hill, NC 27599  Pittsburgh Pittsburgh, PA 15260  
Affiliation: Department of Computer Science The University of North Carolina  Department of Computer Science The University of  
Abstract: We present the first shared-memory algorithms for k-exclusion in which all process blocking is achieved through the use of "local-spin" busy waiting. Such algorithms are designed to reduce interconnect traffic, which is important for good performance. Our k-exclusion algorithms are starvation-free, and are designed to be fast in the absence of contention, and to exhibit scalable performance as contention rises. In contrast, all previous starvation-free k-exclusion algorithms require unrealistic operations or generate excessive interconnect traffic under contention. We also show that efficient, starvation-free k-exclusion algorithms can be used to reduce the time and space overhead associated with existing wait-free shared object implementations, while still providing some resilience to delays and failures. The resulting "hybrid" object implementations combine the advantages of local-spin spin locks, which perform well in the absence of process delays (caused, for example, by preemptions), and wait-free algorithms, which effectively tolerate such delays. We present performance results that confirm that this technique can improve the performance of existing wait-free shared object implementations. These results also show that lock-based implementations can be susceptible to severe performance degradation under multiprogramming, while our hybrid implementations are not. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Afek, D. Dolev, E. Gafni, M. Merritt, and N. Shavit, </author> <title> "A Bounded, First-In, First-Enabled Solution to the `-Exclusion Problem", </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 16(3), </volume> <year> 1994, </year> <pages> pp. 939-953. </pages>
Reference-contexts: Also, in some cases, they concentrated on providing "fairness" to processes, which our algorithms do not. (See <ref> [1] </ref> for details.) 4 The algorithm presented in [23] actually solves mutual exclusion, and not k-exclusion; it is an elementary exercise to modify this algorithm so that it solves k-exclusion [19]. 2 Time Complexity Starvation Reference With Contention Without Instructions Used Free? Fischer et al. [14] 1 fi (1) Large Critical <p> k-exclusion [19]. 2 Time Complexity Starvation Reference With Contention Without Instructions Used Free? Fischer et al. [14] 1 fi (1) Large Critical Sections Yes Fischer et al. [15] 1 fi (1) Large Critical Sections Yes Dolev et al. [13] 1 fi (N 2 ) Safe Bits Yes Afek et al. <ref> [1] </ref> 1 fi (N ) Atomic Read and Write Yes Peterson [23] (CC) fi (N 3 N k 2 ) fi (N ) Atomic Read and Write No Peterson [23] (DSM) 1 fi (N 2 N k) Atomic Read and Write No Burns and Peterson [9] 1 fi (N ) Atomic
Reference: [2] <author> J. Anderson, </author> <title> "Composite Registers", </title> <booktitle> Distributed Computing , 6, </booktitle> <year> 1993, </year> <pages> pp. 141-154. </pages>
Reference-contexts: However, in practice it can be desirable to run more than one process on each processor. In our experiments, we consider scenarios in which processes share processors by multiprogramming. In order to test 9 Exceptions include objects, such as snapshot objects <ref> [2] </ref>, in which operations return state information that is (N ) in size.
Reference: [3] <author> J. Anderson, </author> <title> "A Fine-Grained Solution to the Mutual Exclusion Problem", </title> <journal> Acta Informatica, </journal> <volume> 30(3), </volume> <year> 1993, </year> <pages> pp. 249-265. </pages>
Reference-contexts: The algorithm of [23] allows individual processes to starve even if no failures occur.) Our decision to distinguish between local and remote accesses of shared memory is motivated by recent work on local-spin spin locks <ref> [3, 7, 17, 20, 25, 26] </ref>. In such locks, the impact of the processor-to-memory bottleneck is minimized by ensuring that processes busy wait only on locally-accessible shared variables.
Reference: [4] <author> J. Anderson and M. Moir, </author> <title> "Using k-Exclusion to Implement Resilient, Scalable Shared Objects", </title> <booktitle> Proceedings of the 13th Annual ACM Symposium on Principles of Distributed Computing, ACM, </booktitle> <address> New York, </address> <year> 1994, </year> <pages> pp. 141-150. </pages>
Reference-contexts: In addition, the first author was supported by an Alfred P. Sloan Research Fellowship, and the second author was supported by a UNC Alumni Fellowship. A preliminary version of this work appeared in the Proceedings of the 13th Annual ACM Symposium on Principles of Distributed Computing, August 1994 <ref> [4] </ref>. reasons for these requirements are explained later. First, progress must be guaranteed in the face of up to k 1 undetectable process halting failures. k-exclusion algorithms that satisfy this property are said to be starvation-free. <p> This is achieved by designing algorithms in which all busy waiting is performed on locally-accessible shared variables that are statically allocated to processes. The algorithms presented here are much simpler and more efficient than the distributed shared-memory algorithms we presented in the preliminary version of this paper <ref> [4] </ref>. Our approach here is the same as that of Section 4.1. In particular, we inductively reduce the problem of implementing (N; k)-exclusion to that of implementing (k + 1; k)-exclusion. We present two algorithms for (k + 1; k)-exclusion, both of which have constant time complexity.
Reference: [5] <author> J. Anderson and M. Moir, </author> <title> "Universal Constructions for Large Objects", </title> <booktitle> Proceedings of the Ninth International Workshop on Distributed Algorithms, Lecture Notes in Computer Science 972, </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1995, </year> <pages> pp. 168-182. </pages>
Reference-contexts: To our knowledge, the experiments we present are the first to demonstrate the advantages that resilient object implementations can have over lock-based implementations in systems that are multiprogrammed. (All previously published performance evaluations of resilient objects that we know of assume a one-process-per-processor model of computation <ref> [5, 18] </ref>.) The remainder of this paper is organized as follows. In Section 2, we present definitions and notation that will be used in the rest of the paper.
Reference: [6] <author> J. Anderson and J.-H. Yang, </author> <title> "Time/Contention Tradeoffs for Multiprocessor Synchronization", </title> <journal> Information and Computation, </journal> <volume> 124(1), </volume> <year> 1996, </year> <pages> pp. 68-84. </pages>
Reference-contexts: Anderson [7], by Graunke and Thakkar [17], and by Mellor-Crummey and Scott [20]; the time complexity measure we use to formalize this notion was proposed by Anderson and Yang <ref> [6, 26] </ref>. 3 We should point out that the designers of previous k-exclusion algorithms were not concerned with minimizing remote memory references.
Reference: [7] <author> T. Anderson, </author> <title> "The Performance of Spin Lock Alternatives for Shared-Memory Multiprocessors", </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(1), </volume> <year> 1990, </year> <pages> pp. 6-16. </pages>
Reference-contexts: The algorithm of [23] allows individual processes to starve even if no failures occur.) Our decision to distinguish between local and remote accesses of shared memory is motivated by recent work on local-spin spin locks <ref> [3, 7, 17, 20, 25, 26] </ref>. In such locks, the impact of the processor-to-memory bottleneck is minimized by ensuring that processes busy wait only on locally-accessible shared variables. <p> In practice, a shared variable can be made locally-accessible by storing it in a local cache line or in a local partition of distributed shared memory. Performance studies presented in <ref> [7, 17, 20, 25, 26] </ref> show that minimizing remote memory accesses is important for scalable performance in the design of synchronization algorithms. <p> The k-exclusion algorithms we present employ only local spins for process blocking. The first few algo 1 The k-assignment problem was originally posed by Attiya et al. in [8]; they called it "slotted k-exclusion". 2 This importance of minimizing remote references in busy-waiting algorithms was recognized by T. Anderson <ref> [7] </ref>, by Graunke and Thakkar [17], and by Mellor-Crummey and Scott [20]; the time complexity measure we use to formalize this notion was proposed by Anderson and Yang [6, 26]. 3 We should point out that the designers of previous k-exclusion algorithms were not concerned with minimizing remote memory references. <p> First, however, we explain the key insight on which all of our k-exclusion algorithms are based. On first thought, it may seem that the k-exclusion problem could be efficiently solved by simply modifying a queue-based spin lock <ref> [7, 17, 20] </ref> so that a process waits in the queue only if k other processes are already in their critical sections. Before giving our first algorithm, we explain why this simple approach is problematic. Consider the simple (unrealistic) queue-based (N; k)-exclusion algorithm in Figure 2. <p> The number of participating processes is varied, and the priority queue operations are equally divided among these processes. Previous experiments involving scalable synchronization constructs have assumed that each process runs on a dedicated processor <ref> [7, 17, 20, 25] </ref>. However, in practice it can be desirable to run more than one process on each processor. In our experiments, we consider scenarios in which processes share processors by multiprogramming.
Reference: [8] <author> H. Attiya, A. Bar-Noy, D. Dolev, D. Peleg, and R. Reischuk, </author> <title> "Renaming in an Asynchronous Environment", </title> <journal> Journal of the ACM 37(3), </journal> <year> 1990, </year> <pages> pp. 524-548. </pages>
Reference-contexts: The k-exclusion algorithms we present employ only local spins for process blocking. The first few algo 1 The k-assignment problem was originally posed by Attiya et al. in <ref> [8] </ref>; they called it "slotted k-exclusion". 2 This importance of minimizing remote references in busy-waiting algorithms was recognized by T.
Reference: [9] <author> J. Burns and G. Peterson, </author> <title> "The Ambiguity of Choosing", </title> <booktitle> Proceedings of the Eighth Annual ACM Symposium on Principles of Distributed Computing, ACM, </booktitle> <address> New York, </address> <year> 1989, </year> <pages> pp. 145-157. </pages>
Reference-contexts: The second requirement is that, before a process p enters its critical section, p is assigned an identifier (or "name") from f0 : : : k 1g that is not assigned to any other process until p leaves its critical section. Following the terminology of Burns and Peterson <ref> [9] </ref>, we call this variant of the k-exclusion problem the k-assignment problem. 1 In this paper, we study starvation-free k-assignment algorithms for shared-memory multiprocessors. <p> Bits Yes Afek et al. [1] 1 fi (N ) Atomic Read and Write Yes Peterson [23] (CC) fi (N 3 N k 2 ) fi (N ) Atomic Read and Write No Peterson [23] (DSM) 1 fi (N 2 N k) Atomic Read and Write No Burns and Peterson <ref> [9] </ref> 1 fi (N ) Atomic Read and Write Yes Gottlieb et al. [16] 1 fi (1) Fetch-and-Add No CC Algs: Thm. 3 fi (k log (N=k)) fi (1) Fetch-and-Add, Test-and-Set Yes Thm. 4 fi (c) fi (1) Fetch-and-Add, Test-and-Set Yes DSM Algs: Thm. 7 fi (k log (N=k)) fi (1)
Reference: [10] <author> K. Chandy and J. Misra, </author> <title> Parallel Program Design: A Foundation, </title> <publisher> Addison-Wesley, </publisher> <year> 1988. </year>
Reference-contexts: We prove a program correct by proving that its correctness conditions hold in all histories that start from a state that satisfies the initial conditions of the program. When reasoning about programs, we define safety properties using invariant and unless assertions and progress properties using leads-to assertions <ref> [10] </ref>. A state assertion 5 is an invariant iff it holds in each state of every history.
Reference: [11] <author> M. Choy and A. Singh, </author> <title> "Adaptive Solutions to the Mutual Exclusion Problem", </title> <journal> Distributed Computing, </journal> <volume> 8(1), </volume> <year> 1994, </year> <pages> pp. 1-17. </pages>
Reference-contexts: Theorem 2: Using fetch-and-add, (N; k)-exclusion can be implemented on a cache-coherent machine with time complexity 7kdlog 2 (N=k)e and with space complexity O (N ). 2 7 Tree-based algorithms for mutual exclusion have been presented previously by Yang and Anderson [26], and by Choy and Singh <ref> [11] </ref>.
Reference: [12] <author> E. Dijkstra, </author> <title> "Solution of a Problem in Concurrent Programming Control", </title> <journal> Communications of the ACM , 8(9), </journal> <note> 1965, p. 569. </note>
Reference-contexts: 1 Introduction The k-exclusion problem was posed by Fischer et al. [14] as a generalization of the well-known mutual exclusion problem <ref> [12] </ref>. In the k-exclusion problem, the objective is to design a set of N &gt; k processes, each of which has a "critical section" of code. Each process can enter its critical section repeatedly, and at most k processes may be in their critical sections at any time.
Reference: [13] <author> D. Dolev, E. Gafni, and N. Shavit, </author> <title> "Towards a Non-atomic Era: l-Exclusion as a Test Case", </title> <booktitle> Proceedings of the 20th ACM Symposium on Theory of Computing, </booktitle> <year> 1988, </year> <pages> pp. 78-92. </pages>
Reference-contexts: is an elementary exercise to modify this algorithm so that it solves k-exclusion [19]. 2 Time Complexity Starvation Reference With Contention Without Instructions Used Free? Fischer et al. [14] 1 fi (1) Large Critical Sections Yes Fischer et al. [15] 1 fi (1) Large Critical Sections Yes Dolev et al. <ref> [13] </ref> 1 fi (N 2 ) Safe Bits Yes Afek et al. [1] 1 fi (N ) Atomic Read and Write Yes Peterson [23] (CC) fi (N 3 N k 2 ) fi (N ) Atomic Read and Write No Peterson [23] (DSM) 1 fi (N 2 N k) Atomic Read
Reference: [14] <author> M. Fischer, N. Lynch, J. Burns, and A. Borodin, </author> <title> "Resource Allocation with Immunity to Process Failure", </title> <booktitle> Proceedings of the 20th Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <year> 1979, </year> <pages> pp. 234-254. </pages>
Reference-contexts: 1 Introduction The k-exclusion problem was posed by Fischer et al. <ref> [14] </ref> as a generalization of the well-known mutual exclusion problem [12]. In the k-exclusion problem, the objective is to design a set of N &gt; k processes, each of which has a "critical section" of code. <p> Observe that all previously published algorithms have very high time complexity under contention, and that most have high time complexity even in the absence of contention. 3 In addition, the algorithms of <ref> [14] </ref> and [15] assume the existence of large mutually exclusive critical sections that are executed atomically, despite the fact that processes may fail, and the algorithms of [16, 23] 4 are not starvation-free. (The algorithm of [16] ensures that, provided there are no process failures, any contending process eventually enters its <p> our algorithms do not. (See [1] for details.) 4 The algorithm presented in [23] actually solves mutual exclusion, and not k-exclusion; it is an elementary exercise to modify this algorithm so that it solves k-exclusion [19]. 2 Time Complexity Starvation Reference With Contention Without Instructions Used Free? Fischer et al. <ref> [14] </ref> 1 fi (1) Large Critical Sections Yes Fischer et al. [15] 1 fi (1) Large Critical Sections Yes Dolev et al. [13] 1 fi (N 2 ) Safe Bits Yes Afek et al. [1] 1 fi (N ) Atomic Read and Write Yes Peterson [23] (CC) fi (N 3 N
Reference: [15] <author> M. Fischer, N. Lynch, J. Burns, and A. Borodin, </author> <title> "Distributed FIFO Allocation of Identical Resources Using Small Shared Space", </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 11(1), </volume> <year> 1989, </year> <pages> pp. 90-114. </pages>
Reference-contexts: Observe that all previously published algorithms have very high time complexity under contention, and that most have high time complexity even in the absence of contention. 3 In addition, the algorithms of [14] and <ref> [15] </ref> assume the existence of large mutually exclusive critical sections that are executed atomically, despite the fact that processes may fail, and the algorithms of [16, 23] 4 are not starvation-free. (The algorithm of [16] ensures that, provided there are no process failures, any contending process eventually enters its critical section. <p> presented in [23] actually solves mutual exclusion, and not k-exclusion; it is an elementary exercise to modify this algorithm so that it solves k-exclusion [19]. 2 Time Complexity Starvation Reference With Contention Without Instructions Used Free? Fischer et al. [14] 1 fi (1) Large Critical Sections Yes Fischer et al. <ref> [15] </ref> 1 fi (1) Large Critical Sections Yes Dolev et al. [13] 1 fi (N 2 ) Safe Bits Yes Afek et al. [1] 1 fi (N ) Atomic Read and Write Yes Peterson [23] (CC) fi (N 3 N k 2 ) fi (N ) Atomic Read and Write No
Reference: [16] <author> A. Gottlieb, B. Lubachevsky, and L. Rudoph, </author> <title> "Basic Techniques for the Efficient Coordination of Very Large Numbers of Cooperating Sequential Processors", </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 5(2), </volume> <year> 1983, </year> <pages> pp. 164-189. </pages>
Reference-contexts: time complexity under contention, and that most have high time complexity even in the absence of contention. 3 In addition, the algorithms of [14] and [15] assume the existence of large mutually exclusive critical sections that are executed atomically, despite the fact that processes may fail, and the algorithms of <ref> [16, 23] </ref> 4 are not starvation-free. (The algorithm of [16] ensures that, provided there are no process failures, any contending process eventually enters its critical section. <p> time complexity even in the absence of contention. 3 In addition, the algorithms of [14] and [15] assume the existence of large mutually exclusive critical sections that are executed atomically, despite the fact that processes may fail, and the algorithms of [16, 23] 4 are not starvation-free. (The algorithm of <ref> [16] </ref> ensures that, provided there are no process failures, any contending process eventually enters its critical section. However, this algorithm does not satisfy the starvation-freedom property considered in this paper, because a single process failure can prevent other processes from reaching their critical sections. <p> Write Yes Peterson [23] (CC) fi (N 3 N k 2 ) fi (N ) Atomic Read and Write No Peterson [23] (DSM) 1 fi (N 2 N k) Atomic Read and Write No Burns and Peterson [9] 1 fi (N ) Atomic Read and Write Yes Gottlieb et al. <ref> [16] </ref> 1 fi (1) Fetch-and-Add No CC Algs: Thm. 3 fi (k log (N=k)) fi (1) Fetch-and-Add, Test-and-Set Yes Thm. 4 fi (c) fi (1) Fetch-and-Add, Test-and-Set Yes DSM Algs: Thm. 7 fi (k log (N=k)) fi (1) Fetch-and-Add, Test-and-Set Yes Thm. 8 fi (c) fi (1) Fetch-and-Add, Test-and-Set Yes Thm.
Reference: [17] <author> G. Graunke and S. Thakkar, </author> <title> "Synchronization Algorithms for Shared-Memory Multiprocessors", </title> <journal> IEEE Computer, </journal> <volume> 23, </volume> <year> 1990, </year> <pages> pp. 60-69. </pages>
Reference-contexts: The algorithm of [23] allows individual processes to starve even if no failures occur.) Our decision to distinguish between local and remote accesses of shared memory is motivated by recent work on local-spin spin locks <ref> [3, 7, 17, 20, 25, 26] </ref>. In such locks, the impact of the processor-to-memory bottleneck is minimized by ensuring that processes busy wait only on locally-accessible shared variables. <p> In practice, a shared variable can be made locally-accessible by storing it in a local cache line or in a local partition of distributed shared memory. Performance studies presented in <ref> [7, 17, 20, 25, 26] </ref> show that minimizing remote memory accesses is important for scalable performance in the design of synchronization algorithms. <p> The first few algo 1 The k-assignment problem was originally posed by Attiya et al. in [8]; they called it "slotted k-exclusion". 2 This importance of minimizing remote references in busy-waiting algorithms was recognized by T. Anderson [7], by Graunke and Thakkar <ref> [17] </ref>, and by Mellor-Crummey and Scott [20]; the time complexity measure we use to formalize this notion was proposed by Anderson and Yang [6, 26]. 3 We should point out that the designers of previous k-exclusion algorithms were not concerned with minimizing remote memory references. <p> First, however, we explain the key insight on which all of our k-exclusion algorithms are based. On first thought, it may seem that the k-exclusion problem could be efficiently solved by simply modifying a queue-based spin lock <ref> [7, 17, 20] </ref> so that a process waits in the queue only if k other processes are already in their critical sections. Before giving our first algorithm, we explain why this simple approach is problematic. Consider the simple (unrealistic) queue-based (N; k)-exclusion algorithm in Figure 2. <p> The number of participating processes is varied, and the priority queue operations are equally divided among these processes. Previous experiments involving scalable synchronization constructs have assumed that each process runs on a dedicated processor <ref> [7, 17, 20, 25] </ref>. However, in practice it can be desirable to run more than one process on each processor. In our experiments, we consider scenarios in which processes share processors by multiprogramming.
Reference: [18] <author> M. Herlihy, </author> <title> "A Methodology for Implementing Highly Concurrent Data Objects", </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 15(5), </volume> <year> 1993, </year> <pages> pp. 745-770. </pages>
Reference-contexts: To demonstrate the utility of the technique described above, we used Herlihy's wait-free universal construction <ref> [18] </ref> to implement a priority queue, and then used some of our k-assignment algorithms to improve its performance and reduce its space requirements. We conducted performance experiments using these implementations on a cache-coherent multiprocessor and on a distributed shared memory multiprocessor. <p> To our knowledge, the experiments we present are the first to demonstrate the advantages that resilient object implementations can have over lock-based implementations in systems that are multiprogrammed. (All previously published performance evaluations of resilient objects that we know of assume a one-process-per-processor model of computation <ref> [5, 18] </ref>.) The remainder of this paper is organized as follows. In Section 2, we present definitions and notation that will be used in the rest of the paper. <p> All of our experiments have the same structure, so before we present any specific details, we give a brief overview. On both machines, we implemented a shared priority queue 10 using the local-spin queue lock of Mellor-Crummey and Scott [20], and Herlihy's universal wait-free object construction <ref> [18] </ref>. To test the performance of our algorithms we also used the "inductive fast path" method of Figure 7 with each level implemented using the algorithm in Figure 3 on the Sequent Symmetry, and in Figure 8 on the BBN GP1000. <p> By definition, such objects will not scale well, even at lower levels of resiliency. 10 We used the same priority queue code as that presented in <ref> [18] </ref> 19 the performance of each method under varying levels of multiprogramming, we fix the number of processors (to 10 on the Sequent Symmetry and to 40 on the BBN GP1000) 11 and vary the number of processes.
Reference: [19] <author> N. Lynch, </author> <title> "Distributed Algorithms", </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, Calif., </address> <year> 1996. </year>
Reference-contexts: Also, in some cases, they concentrated on providing "fairness" to processes, which our algorithms do not. (See [1] for details.) 4 The algorithm presented in [23] actually solves mutual exclusion, and not k-exclusion; it is an elementary exercise to modify this algorithm so that it solves k-exclusion <ref> [19] </ref>. 2 Time Complexity Starvation Reference With Contention Without Instructions Used Free? Fischer et al. [14] 1 fi (1) Large Critical Sections Yes Fischer et al. [15] 1 fi (1) Large Critical Sections Yes Dolev et al. [13] 1 fi (N 2 ) Safe Bits Yes Afek et al. [1] 1 <p> other than reads and writes.) Theorem 1: Using fetch-and-add, (N; k)-exclusion can be implemented on a cache-coherent machine with time complexity 7 (N k) and space complexity O (N ). 2 On the surface, this algorithm is similar to Peterson's mutual exclusion [23] and k-exclusion (posed as an exercise in <ref> [19] </ref>) algorithms, in which processes go through a series of levels, each of which stops one process.
Reference: [20] <author> J. Mellor-Crummey and M. Scott, </author> <title> "Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors", </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(1), </volume> <year> 1991, </year> <pages> pp. 21-65. </pages>
Reference-contexts: The algorithm of [23] allows individual processes to starve even if no failures occur.) Our decision to distinguish between local and remote accesses of shared memory is motivated by recent work on local-spin spin locks <ref> [3, 7, 17, 20, 25, 26] </ref>. In such locks, the impact of the processor-to-memory bottleneck is minimized by ensuring that processes busy wait only on locally-accessible shared variables. <p> In practice, a shared variable can be made locally-accessible by storing it in a local cache line or in a local partition of distributed shared memory. Performance studies presented in <ref> [7, 17, 20, 25, 26] </ref> show that minimizing remote memory accesses is important for scalable performance in the design of synchronization algorithms. <p> Performance studies presented in [7, 17, 20, 25, 26] show that minimizing remote memory accesses is important for scalable performance in the design of synchronization algorithms. In particular, studies presented in <ref> [20] </ref> show that, on both classes of machines, mutual exclusion algorithms that ensure that all busy-wait loops involve only locally-accessible shared variables perform better than similar algorithms that do not. The k-exclusion algorithms we present employ only local spins for process blocking. <p> The first few algo 1 The k-assignment problem was originally posed by Attiya et al. in [8]; they called it "slotted k-exclusion". 2 This importance of minimizing remote references in busy-waiting algorithms was recognized by T. Anderson [7], by Graunke and Thakkar [17], and by Mellor-Crummey and Scott <ref> [20] </ref>; the time complexity measure we use to formalize this notion was proposed by Anderson and Yang [6, 26]. 3 We should point out that the designers of previous k-exclusion algorithms were not concerned with minimizing remote memory references. <p> Our performance experiments also demonstrate the advantages of resilient object implementations over lock-based ones. In particular, they show that resilient implementations effectively tolerate the relatively long delays due to preemption under multiprogramming, while a state-of-the-art mutual exclusion algorithm <ref> [20] </ref> does not. <p> First, however, we explain the key insight on which all of our k-exclusion algorithms are based. On first thought, it may seem that the k-exclusion problem could be efficiently solved by simply modifying a queue-based spin lock <ref> [7, 17, 20] </ref> so that a process waits in the queue only if k other processes are already in their critical sections. Before giving our first algorithm, we explain why this simple approach is problematic. Consider the simple (unrealistic) queue-based (N; k)-exclusion algorithm in Figure 2. <p> All of our experiments have the same structure, so before we present any specific details, we give a brief overview. On both machines, we implemented a shared priority queue 10 using the local-spin queue lock of Mellor-Crummey and Scott <ref> [20] </ref>, and Herlihy's universal wait-free object construction [18]. To test the performance of our algorithms we also used the "inductive fast path" method of Figure 7 with each level implemented using the algorithm in Figure 3 on the Sequent Symmetry, and in Figure 8 on the BBN GP1000. <p> The number of participating processes is varied, and the priority queue operations are equally divided among these processes. Previous experiments involving scalable synchronization constructs have assumed that each process runs on a dedicated processor <ref> [7, 17, 20, 25] </ref>. However, in practice it can be desirable to run more than one process on each processor. In our experiments, we consider scenarios in which processes share processors by multiprogramming.
Reference: [21] <author> M. Moir and J. Anderson, </author> <title> "Fast, Long-Lived Renaming", </title> <booktitle> Proceedings of the Eighth International Workshop on Distributed Algorithms, Lecture Notes in Computer Science 857, </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1994, </year> <pages> pp. 141-155. </pages>
Reference-contexts: Therefore, this paper is almost entirely devoted to starvation-free algorithms for k-exclusion. Our conversion of k-exclusion algorithms to k-assignment algorithms involves the use of a simple long-lived renaming algorithm <ref> [21, 22] </ref> that allows each process to acquire a name before entering its critical section, and to release that name upon exiting its critical section. <p> Unless stated otherwise, we assume that k &gt; 0, N &gt; k, and that p, q, and r range over 0::N 1. 2 3 k-Assignment As explained in the introduction, the k-assignment problem can be solved by combining a solution to the long-lived renaming problem <ref> [21, 22] </ref> with a program that solves the k-exclusion problem. In the long-lived renaming problem, processes repeatedly acquire and release unique names from a fixed name space. respectively. The renaming mechanism employs a sequence of test-and-set bits, one per name.
Reference: [22] <author> M. Moir and J. Anderson, </author> <title> "Wait-Free Algorithms for Fast, Long-Lived Renaming", </title> <booktitle> Science of Computer Programming 25, </booktitle> <year> 1995, </year> <pages> pp. 1-39. </pages>
Reference-contexts: Therefore, this paper is almost entirely devoted to starvation-free algorithms for k-exclusion. Our conversion of k-exclusion algorithms to k-assignment algorithms involves the use of a simple long-lived renaming algorithm <ref> [21, 22] </ref> that allows each process to acquire a name before entering its critical section, and to release that name upon exiting its critical section. <p> Unless stated otherwise, we assume that k &gt; 0, N &gt; k, and that p, q, and r range over 0::N 1. 2 3 k-Assignment As explained in the introduction, the k-assignment problem can be solved by combining a solution to the long-lived renaming problem <ref> [21, 22] </ref> with a program that solves the k-exclusion problem. In the long-lived renaming problem, processes repeatedly acquire and release unique names from a fixed name space. respectively. The renaming mechanism employs a sequence of test-and-set bits, one per name. <p> The bit X [j] is associated with name j, where 0 j &lt; k. A process that has obtained name j releases it by simply clearing X [j] (line 5). The renaming algorithm employed in Figure 1 is presented in more detail and proved correct in <ref> [22] </ref>. As the correctness proof of [22] shows, if a process is about to test-and-set X [i], then :X [j] holds for some j where i j &lt; k. <p> A process that has obtained name j releases it by simply clearing X [j] (line 5). The renaming algorithm employed in Figure 1 is presented in more detail and proved correct in <ref> [22] </ref>. As the correctness proof of [22] shows, if a process is about to test-and-set X [i], then :X [j] holds for some j where i j &lt; k. Thus, if a process has unsuccessfully tested bits X [0] through X [k 2], then :X [k 1] holds, so the kth test-and-set will succeed.
Reference: [23] <author> G. Peterson, </author> <title> "Myths about the Mutual Exclusion Problem", </title> <journal> Information Processing Letters, </journal> <volume> 12(3), </volume> <year> 1981, </year> <pages> pp. 115-116. </pages>
Reference-contexts: time complexity under contention, and that most have high time complexity even in the absence of contention. 3 In addition, the algorithms of [14] and [15] assume the existence of large mutually exclusive critical sections that are executed atomically, despite the fact that processes may fail, and the algorithms of <ref> [16, 23] </ref> 4 are not starvation-free. (The algorithm of [16] ensures that, provided there are no process failures, any contending process eventually enters its critical section. <p> However, this algorithm does not satisfy the starvation-freedom property considered in this paper, because a single process failure can prevent other processes from reaching their critical sections. The algorithm of <ref> [23] </ref> allows individual processes to starve even if no failures occur.) Our decision to distinguish between local and remote accesses of shared memory is motivated by recent work on local-spin spin locks [3, 7, 17, 20, 25, 26]. <p> Also, in some cases, they concentrated on providing "fairness" to processes, which our algorithms do not. (See [1] for details.) 4 The algorithm presented in <ref> [23] </ref> actually solves mutual exclusion, and not k-exclusion; it is an elementary exercise to modify this algorithm so that it solves k-exclusion [19]. 2 Time Complexity Starvation Reference With Contention Without Instructions Used Free? Fischer et al. [14] 1 fi (1) Large Critical Sections Yes Fischer et al. [15] 1 fi <p> Used Free? Fischer et al. [14] 1 fi (1) Large Critical Sections Yes Fischer et al. [15] 1 fi (1) Large Critical Sections Yes Dolev et al. [13] 1 fi (N 2 ) Safe Bits Yes Afek et al. [1] 1 fi (N ) Atomic Read and Write Yes Peterson <ref> [23] </ref> (CC) fi (N 3 N k 2 ) fi (N ) Atomic Read and Write No Peterson [23] (DSM) 1 fi (N 2 N k) Atomic Read and Write No Burns and Peterson [9] 1 fi (N ) Atomic Read and Write Yes Gottlieb et al. [16] 1 fi (1) <p> fi (1) Large Critical Sections Yes Dolev et al. [13] 1 fi (N 2 ) Safe Bits Yes Afek et al. [1] 1 fi (N ) Atomic Read and Write Yes Peterson <ref> [23] </ref> (CC) fi (N 3 N k 2 ) fi (N ) Atomic Read and Write No Peterson [23] (DSM) 1 fi (N 2 N k) Atomic Read and Write No Burns and Peterson [9] 1 fi (N ) Atomic Read and Write Yes Gottlieb et al. [16] 1 fi (1) Fetch-and-Add No CC Algs: Thm. 3 fi (k log (N=k)) fi (1) Fetch-and-Add, Test-and-Set Yes Thm. 4 fi <p> those that follow, we only list atomic operations other than reads and writes.) Theorem 1: Using fetch-and-add, (N; k)-exclusion can be implemented on a cache-coherent machine with time complexity 7 (N k) and space complexity O (N ). 2 On the surface, this algorithm is similar to Peterson's mutual exclusion <ref> [23] </ref> and k-exclusion (posed as an exercise in [19]) algorithms, in which processes go through a series of levels, each of which stops one process.
Reference: [24] <author> R. Wisniewski, L. Kontothanassis, and M. Scott, </author> <title> "Scalable Spin Locks for Multiprogrammed Systems", </title> <type> Technical Report 454, </type> <institution> Computer Science Department, University of Rochester, </institution> <year> 1993. </year>
Reference-contexts: To address this problem, other researchers have suggested using a modified kernel interface that releases any critical section acquired by a preempted process <ref> [24] </ref>. However, operating-system-based solutions such as this entail added scheduling overhead and limit portability. Such operating system support can be avoided if the (user-level) code that processes execute to access objects is tolerant of delays. Algorithms that are delay-tolerant in this way are called "resilient".
Reference: [25] <author> J.-H. Yang and J. Anderson, </author> <title> "Fast, Scalable Synchronization with Minimal Hardware Support", </title> <booktitle> Proceedings of the 12th Annual ACM Symposium on Principles of Distributed Computing, ACM, </booktitle> <address> New York, </address> <year> 1993, </year> <pages> pp. 171-182. </pages>
Reference-contexts: The algorithm of [23] allows individual processes to starve even if no failures occur.) Our decision to distinguish between local and remote accesses of shared memory is motivated by recent work on local-spin spin locks <ref> [3, 7, 17, 20, 25, 26] </ref>. In such locks, the impact of the processor-to-memory bottleneck is minimized by ensuring that processes busy wait only on locally-accessible shared variables. <p> In practice, a shared variable can be made locally-accessible by storing it in a local cache line or in a local partition of distributed shared memory. Performance studies presented in <ref> [7, 17, 20, 25, 26] </ref> show that minimizing remote memory accesses is important for scalable performance in the design of synchronization algorithms. <p> The number of participating processes is varied, and the priority queue operations are equally divided among these processes. Previous experiments involving scalable synchronization constructs have assumed that each process runs on a dedicated processor <ref> [7, 17, 20, 25] </ref>. However, in practice it can be desirable to run more than one process on each processor. In our experiments, we consider scenarios in which processes share processors by multiprogramming.
Reference: [26] <author> J.-H. Yang and J. Anderson, </author> <title> "A Fast, Scalable Mutual Exclusion Algorithm", </title> <journal> Distributed Computing, </journal> <volume> 9, </volume> <year> 1995, </year> <pages> pp. 51-60. </pages>
Reference-contexts: The algorithm of [23] allows individual processes to starve even if no failures occur.) Our decision to distinguish between local and remote accesses of shared memory is motivated by recent work on local-spin spin locks <ref> [3, 7, 17, 20, 25, 26] </ref>. In such locks, the impact of the processor-to-memory bottleneck is minimized by ensuring that processes busy wait only on locally-accessible shared variables. <p> In practice, a shared variable can be made locally-accessible by storing it in a local cache line or in a local partition of distributed shared memory. Performance studies presented in <ref> [7, 17, 20, 25, 26] </ref> show that minimizing remote memory accesses is important for scalable performance in the design of synchronization algorithms. <p> Anderson [7], by Graunke and Thakkar [17], and by Mellor-Crummey and Scott [20]; the time complexity measure we use to formalize this notion was proposed by Anderson and Yang <ref> [6, 26] </ref>. 3 We should point out that the designers of previous k-exclusion algorithms were not concerned with minimizing remote memory references. <p> Theorem 2: Using fetch-and-add, (N; k)-exclusion can be implemented on a cache-coherent machine with time complexity 7kdlog 2 (N=k)e and with space complexity O (N ). 2 7 Tree-based algorithms for mutual exclusion have been presented previously by Yang and Anderson <ref> [26] </ref>, and by Choy and Singh [11].
References-found: 26

