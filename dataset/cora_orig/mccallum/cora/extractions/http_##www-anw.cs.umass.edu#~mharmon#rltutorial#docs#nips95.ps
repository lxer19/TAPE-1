URL: http://www-anw.cs.umass.edu/~mharmon/rltutorial/docs/nips95.ps
Refering-URL: http://www-anw.cs.umass.edu/~mharmon/rltutorial/tut.html
Root-URL: 
Title: Residual Advantage Learning Applied to a Differential Game  
Author: Mance E. Harmon Leemon C. Baird III 
Keyword: Category: Control, Navigation, and Planning Keywords: Reinforcement Learning, Advantage Updating, Dynamic Programming, Differential Games  
Address: WL/AAAT Bldg.  OH 45433-7301  2354 Fairchild Dr. Suite 6K41, USAFA, CO 80840-6234  
Affiliation: Wright Laboratory  U.S.A.F. Academy  
Note: May 1995: Submitted to Neural Information Processing Systems Conference (NIPS*95), Denver, CO, 27 November-2  Avionics Circle Wright-Patterson Air Force Base,  
Email: harmonme@aa.wpafb.mil  baird@cs.usafa.af.mil  
Phone: 635 2185  
Date: December 1995.  
Abstract: An application of reinforcement learning to a differential game is presented. The reinforcement learning system uses a recently developed algorithm, the residual form of advantage learning. The game is a Markov decision process (MDP) with continuous states and nonlinear dynamics. The game consists of two players, a missile and a plane; the missile pursues the plane and the plane evades the missile. On each time step each player chooses one of two possible actions; turn left or turn right 90 degrees. Reinforcement is given only when the missile hits the plane or the plane reaches an escape distance from the missile. The advantage function is stored in a single-hidden-layer sigmoidal network. The reinforcement learning algorithm for optimal control is modified for differential games in order to find the minimax point, rather than the maximum. As far as we know, this is the first time that a reinforcement learning algorithm with guaranteed convergence for general function approximation systems has been demonstrated to work with a general neural network. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Baird, L.C. </author> <year> (1993). </year> <title> Advantage updating Wright-Patterson Air Force Base, </title> <institution> OH. (Wright Laboratory Technical Report WL-TR-93-1146, </institution> <note> available from the Defense Technical information Center, </note> <institution> Cameron Station, </institution> <address> Alexandria, VA 22304-6145). </address>
Reference-contexts: This is the first time that a reinforcement learning algorithm with guaranteed convergence for general function approximation systems has been demonstrated to work with a general neural network. 2 BACKGROUND 2.1 Advantage Updating The advantage updating algorithm <ref> (Baird, 1993) </ref> is a reinforcement learning algorithm in which two types of information are stored. For each state x, the value V (x) is stored, representing an estimate of the total discounted return expected when starting in state x and performing optimal actions. <p> Advantage updating has been shown to learn faster than Q-learning (Watkins, 1989), especially for continuous-time problems <ref> (Baird, 1993, Harmon, Baird, & Klopf, 1995) </ref>. 2.2 Advantage Learning Advantage learning improves on advantage updating by requiring only a single function to be stored, the advantage function A (x,u).
Reference: <author> Boyan, J.A., and A.W.Moore. </author> <year> (1995). </year> <title> Generalization in reinforcement learning: Safely approximating the value function. </title> <editor> In Tesauro, G., Touretzky, D.S., and Leen, T.K. (eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 7. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge MA. </address>
Reference-contexts: and could also be called the direct implementation of incremental value iteration, Q-learning, and advantage learning. 3.2 Residual Gradient Algorithms Reinforcement learning algorithms can be guaranteed to converge for lookup tables, yet be unstable for function-approximation systems that have even a small amount of generalization when using the direct implementation <ref> (Boyan, 95) </ref>. To find an algorithm that is more stable than the direct algorithm, it is useful to specify the exact goal for the learning system.
Reference: <author> Isaacs, </author> <title> Rufus (1965). Differential games. </title> <address> New York: </address> <publisher> John Wiley and Sons, Inc. </publisher>
Reference-contexts: u - g D D 1 1 (7) where the inner &lt;&gt; is the expected value over all possible results of performing a given action u in a given state x, and the outer &lt;&gt; is the expected value over all possible states and actions. 4 DIFFERENTIAL GAMES Differential games <ref> (Isaacs, 1965) </ref> are played in continuous time, or use sufficiently small time steps to approximate continuous time. Both players evaluate the given state and simultaneously execute an action, with no knowledge of the other player's selected action.
Reference: <author> Harmon, M.E., Baird, L.C, & Klopf, A.H. </author> <year> (1995). </year> <title> Advantage updating applied to a differential game. </title> <editor> In Tesauro, G., Touretzky, D.S., and Leen, T.K. (eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 7. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge MA. </address>
Reference: <author> Millington, P. J. </author> <year> (1991). </year> <title> Associative reinforcement learning for optimal control. </title> <type> Unpublished master's thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, MA. </address>
Reference: <author> Rajan, N., Prasad, U. R., and Rao, N. J. </author> <year> (1980). </year> <title> Pursuit-evasion of two aircraft in a horizontal plane. </title> <journal> Journal of Guidance and Control. </journal> <volume> 3(3), May-June, </volume> <pages> 261-267. </pages>
Reference: <author> Rumelhart, D., Hinton, G., & Williams, R. </author> <year> (1986). </year> <title> Learning representations by backpropagating errors. </title> <journal> Nature. </journal> <volume> 323 , 9 October, </volume> <pages> 533-536. </pages>
Reference-contexts: x V x R V x ( ) ( ) ( ) -( ) + + [ ] 1 a a g (3) If V (x) is represented by a function-approximation system other than a lookup table, update (3) can be implemented directly by combining it with the backpropagation algorithm <ref> (Rumelhart, Hinton, & Williams, 86) </ref>.
Reference: <author> Watkins, C. J. C. H. </author> <year> (1989). </year> <title> Learning from delayed rewards. </title> <type> Doctoral thesis, </type> <institution> Cambridge University, </institution> <address> Cambridge, England. </address>
Reference-contexts: Advantage updating has been shown to learn faster than Q-learning <ref> (Watkins, 1989) </ref>, especially for continuous-time problems (Baird, 1993, Harmon, Baird, & Klopf, 1995). 2.2 Advantage Learning Advantage learning improves on advantage updating by requiring only a single function to be stored, the advantage function A (x,u).
References-found: 8

