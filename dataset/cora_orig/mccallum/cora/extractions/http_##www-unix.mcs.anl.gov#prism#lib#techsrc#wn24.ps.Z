URL: http://www-unix.mcs.anl.gov/prism/lib/techsrc/wn24.ps.Z
Refering-URL: http://www-unix.mcs.anl.gov/prism/lib/tech.html
Root-URL: http://www.mcs.anl.gov
Title: On the Design of a Tridiagonalization Routine for Banded Matrices  
Author: Christian Bischof Xiaobai Sun 
Abstract: This paper discusses scalability and data layout issues arising in the development of a parallel algorithm for reducing a banded matrix to tridiagonal form. As it turns out, balancing the memory and computational complexity of the reduction of the matrix and the accumulation of the associated orthogonal matrix is the key to scalability and sustained performance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. DuCroz, A. Greenbaum, S. Hammarling, A. McKenney, S. Ostrouchov, and D. Sorensen, </author> <title> LAPACK User's Guide, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1992. </year>
Reference-contexts: The only exception is the first processor which contains one more column. Such a block column of A can be conveniently stored in LAPACK packed format <ref> [1] </ref>, allowing for the use of BLAS-2 kernels. Preliminary performance results of the serial version of the resulting band reduction algorithm are presented in [3]. Assuming that A is distributed over p processors, each processor contains roughly r := n=(b fl p) blocks columns.
Reference: [2] <author> C. Bischof, S. Huss-Lederman, X. Sun, and A. Tsao, </author> <title> The PRISM project: Infrastructure and algorithms for parallel eigensolvers, </title> <booktitle> in Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <address> Washington, DC, 1994, </address> <publisher> IEEE Computer Society, </publisher> <pages> pp. 123-131. </pages>
Reference-contexts: Another approach is the successive bandreduction (SBR) approach [4], which in its simplest form first reduces a matrix to narrow banded form and from there to tridiagonal form. As is shown in <ref> [2] </ref>, such an approach can also advantageously used for a "rank-revealing tridiagonalization" as it is, for example, required, in the invariant subspace decomposition approach (ISDA) [10] for the solution of the symmetric eigenproblem.
Reference: [3] <author> C. Bischof, X. Sun, and B. Lang, </author> <title> Parallel tridiagonalization through two-step band reduction, </title> <booktitle> in Proceedings of Scalable High Performance Computing Conference, </booktitle> <publisher> IEEE Computer Society Press, </publisher> <month> May </month> <year> 1994, </year> <pages> pp. 23-27. </pages>
Reference-contexts: Such a block column of A can be conveniently stored in LAPACK packed format [1], allowing for the use of BLAS-2 kernels. Preliminary performance results of the serial version of the resulting band reduction algorithm are presented in <ref> [3] </ref>. Assuming that A is distributed over p processors, each processor contains roughly r := n=(b fl p) blocks columns. <p> This situation is typical, as the bandwidth is usually dictated by "local" considerations in many matrices arising in numerical PDE solutions, or by blocking considerations in the SBR approach <ref> [3] </ref>. 4 Accumulating the Orthogonal Transformations We allocate separate processors for A and Q for reasons to be seen. The total operation count for the reduction of A is 6bn (n 1) flops, whereas the application of the orthogonal transformations to a matrix Q invoves 2n 2 (n 1) flops. <p> Lastly, we mention that the scheme discussed here does not incorporate the blocking of the orthogonal updates of Q suggested in <ref> [3] </ref>. The reason is that the blocking scheme requires us to complete nb full sweeps (where nb is the desired block size) before we can begin with the update of Q.
Reference: [4] <author> C. H. Bischof and X. Sun, </author> <title> A framework for band reduction and tridiagonalization of symmetric matrices, </title> <type> Tech. Rep. </type> <institution> MCS-P298-0392, Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1992. </year>
Reference-contexts: 1 Introduction Reduction to tridiagonal form is a major step in eigenvalue computations for symmetric matrices. If the matrix is full, the conventional Householder tridiagonalization approach [7, p. 276] or block variants thereof [6] is the usual method of choice. Another approach is the successive bandreduction (SBR) approach <ref> [4] </ref>, which in its simplest form first reduces a matrix to narrow banded form and from there to tridiagonal form.
Reference: [5] <author> C. H. Bischof, X. Sun, A. Tsao, and T. Turnbull, </author> <title> A study of the invariant subspace decomposition algorithm for banded symmetric matrices, </title> <booktitle> in Proceedings of the Fifth SIAM Conference on Applied Linear Algebra, </booktitle> <editor> J. G. Lewis, ed., </editor> <year> 1994, </year> <pages> pp. 321-325. </pages>
Reference-contexts: In addition to the SBR approach, the reduction of a banded matrix to tridiagonal form arises in the banded eigenvalue problem, and as the final step in block Lanczos methods for the solution of large sparse eigenvalue problems. A banded variant of the ISDA approach <ref> [5] </ref> also requires the ability to reduce the bandwidth of banded matrices, with tridiagonalization being one particular approach to doing so.
Reference: [6] <author> J. J. Dongarra, S. J. Hammarling, and D. C. Sorensen, </author> <title> Block reduction of matrices to condensed form for eigenvalue computations, </title> <type> Tech. Rep. </type> <institution> MCS-TM-99, Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <month> September </month> <year> 1987. </year>
Reference-contexts: 1 Introduction Reduction to tridiagonal form is a major step in eigenvalue computations for symmetric matrices. If the matrix is full, the conventional Householder tridiagonalization approach [7, p. 276] or block variants thereof <ref> [6] </ref> is the usual method of choice. Another approach is the successive bandreduction (SBR) approach [4], which in its simplest form first reduces a matrix to narrow banded form and from there to tridiagonal form.
Reference: [7] <author> G. H. Golub and C. F. V. Loan, </author> <title> Matrix Computations, </title> <publisher> The Johns Hopkins University Press, </publisher> <year> 1983. </year>
Reference-contexts: 1 Introduction Reduction to tridiagonal form is a major step in eigenvalue computations for symmetric matrices. If the matrix is full, the conventional Householder tridiagonalization approach <ref> [7, p. 276] </ref> or block variants thereof [6] is the usual method of choice. Another approach is the successive bandreduction (SBR) approach [4], which in its simplest form first reduces a matrix to narrow banded form and from there to tridiagonal form.
Reference: [8] <author> W. Gropp, E. Lusk, and A. Skjellum, </author> <title> Using MPI Portable Parallel Programming with the Message Passing Interface, </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: If we give high priority to the first column of a SYM update, and all "first columns" have been updated, we can use the capability of MPI <ref> [8] </ref> to send "scattered" vectors with equal stride to shift all leading columns one to the left while the nodes are still busy computing, thus masking this computational cost. Here we assume that a processor can send and receive messages while computing, which is true on recent multiprocessors computers.
Reference: [9] <author> B. Lang, </author> <title> A parallel algorithm for reducing symmetric matrices banded matrices to tridiagonal form, </title> <journal> SIAM Journal on Scientific Computing, </journal> <volume> 14 (1993), </volume> <pages> pp. 1320-1338. </pages>
Reference-contexts: In contrast, employing the algorithm developed independently by Muraka and Horikoshi [11] and Lang <ref> [9] </ref>, the reduction of an n fi n matrix of bandwidth b takes 6bn (n 1) flops, and the accumulation of the associated transformation matrix Q on the fly requires 2n 2 (n 1) flops. <p> Cass Avenue, Argonne, IL 60439, bischof@mcs.anl.gov z Department of Computer Science, Duke University, Durham, NC 27708-0129, xiaobai@cs.duke.edu 1 Fig. 1. First Sweep Fig. 2. Second Sweep 2 Bandreduction and Bulge Chasing Let A be an n fi n matrix of semibandwidth b. Employing Lang's <ref> [9] </ref> and Muraka and Horikoshi's [11] algorithm, the reduction of A to tridiagonal form proceeds in "sweeps", where each sweep consists of the reduction of a column in the original band of A to tridiagonal form, and the chasing of the "bulges' one column down along the diagonal.
Reference: [10] <author> S. Lederman, A. Tsao, and T. Turnbull, </author> <title> A parallelizable eigensolver for real diagonalizable matrices with real eigenvalues, </title> <type> Tech. Rep. </type> <institution> TR-91-042, Supercomputing Research Center, Institute for Defense Analysis, Bowie, MD, </institution> <year> 1991. </year>
Reference-contexts: As is shown in [2], such an approach can also advantageously used for a "rank-revealing tridiagonalization" as it is, for example, required, in the invariant subspace decomposition approach (ISDA) <ref> [10] </ref> for the solution of the symmetric eigenproblem. In addition to the SBR approach, the reduction of a banded matrix to tridiagonal form arises in the banded eigenvalue problem, and as the final step in block Lanczos methods for the solution of large sparse eigenvalue problems.
Reference: [11] <author> K. Muraka and K. Horikoshi, </author> <title> A new method for the tridiagonalization of the symmetric band matrix, </title> <booktitle> Information Processing in Japan, 15 (1975), </booktitle> <pages> pp. 108-112. 6 </pages>
Reference-contexts: In contrast, employing the algorithm developed independently by Muraka and Horikoshi <ref> [11] </ref> and Lang [9], the reduction of an n fi n matrix of bandwidth b takes 6bn (n 1) flops, and the accumulation of the associated transformation matrix Q on the fly requires 2n 2 (n 1) flops. <p> Cass Avenue, Argonne, IL 60439, bischof@mcs.anl.gov z Department of Computer Science, Duke University, Durham, NC 27708-0129, xiaobai@cs.duke.edu 1 Fig. 1. First Sweep Fig. 2. Second Sweep 2 Bandreduction and Bulge Chasing Let A be an n fi n matrix of semibandwidth b. Employing Lang's [9] and Muraka and Horikoshi's <ref> [11] </ref> algorithm, the reduction of A to tridiagonal form proceeds in "sweeps", where each sweep consists of the reduction of a column in the original band of A to tridiagonal form, and the chasing of the "bulges' one column down along the diagonal.
References-found: 11

