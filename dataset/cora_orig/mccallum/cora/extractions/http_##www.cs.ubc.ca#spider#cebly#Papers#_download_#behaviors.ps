URL: http://www.cs.ubc.ca/spider/cebly/Papers/_download_/behaviors.ps
Refering-URL: http://www.cs.ubc.ca/spider/cebly/papers.html
Root-URL: 
Email: fbacchus@logos.uwaterloo.ca  cebly@cs.ubc.cs  grove@research.nj.nec.com  
Title: Rewarding Behaviors  
Author: Fahiem Bacchus Craig Boutilier Adam Grove 
Address: Waterloo, Ontario Canada, N2L 3G1  Vancouver, B.C. Canada, V6T 1Z4  4 Independence Way Princeton NJ 08540, USA  
Affiliation: Dept. Computer Science University of Waterloo  Dept. Computer Science University of British Columbia  NEC Research Institute  
Abstract: Markov decision processes (MDPs) are a very popular tool for decision theoretic planning (DTP), partly because of the well-developed, expressive theory that includes effective solution techniques. But the Markov assumptionthat dynamics and rewards depend on the current state only, and not on history is often inappropriate. This is especially true of rewards: we frequently wish to associate rewards with behaviors that extend over time. Of course, such reward processes can be encoded in an MDP should we have a rich enough state space (where states encode enough history). However it is often difficult to hand craft suitable state spaces that encode an appropriate amount of history. We consider this problem in the case where non-Markovian rewards are encoded by assigning values to formulas of a temporal logic. These formulas characterize the value of temporally extended behaviors. We argue that this allows a natural representation of many commonly encountered non-Markovian rewards. The main result is an algorithm which, given a decision process with non-Markovian rewards expressed in this manner, automatically constructs an equivalent MDP (with Markovian reward structure), allowing optimal policy con struction using standard techniques.
Abstract-found: 1
Intro-found: 1
Reference: [AH90] <author> R. Alur and T. Henzinger. </author> <title> Real-time logics: complexity and expressiveness. </title> <address> LICS-90, Philadelphia, </address> <year> 1990. </year>
Reference-contexts: For this reason, it will be more natural to encode our reward formulas using a past or backward-looking temporal logic rather than the usual future or forward logics like LTL, CTL [Eme90] or MTL <ref> [AH90] </ref>. In Section 3, we describe a number of interesting and useful classes of target behaviors and show how they can be encoded by TERFs. In Section 4, we consider the problem of constructing optimal policies for NMRDPs.
Reference: [BD94] <author> C. Boutilier and R. Dearden. </author> <title> Using abstractions for decision-theoretic planning with time constraints. </title> <booktitle> AAAI-94, </booktitle> <address> pp.1016-1022, Seattle, </address> <year> 1994. </year>
Reference-contexts: Much work in decision theoretic planning (DTP), generally aimed at addressing these issues, has adopted the theory of Markov decision processes (MDPs) as the underlying conceptual and computational model <ref> [DKKN93, TR94, BD94, BDG95] </ref>. MDPs allow one to formulate problems in which an agent is involved in an on-going, process-oriented interaction with the environment and receives rewards at various system states. This generalizes the classical goal-oriented view of planning [BP95]. <p> Complete observability entails that the agent always knows what state it is in. We assume that the state space is characterized by a set of features, or logical propositions. This allows actions to be described compactly using probabilistic STRIPS rules <ref> [KHW94, BD94] </ref>, Bayes nets [DK89, BDG95] or other action representations. A real-valued reward function R reflects the objectives, tasks and goals to be accomplished by the agent, with R (s) denoting the (immediate) utility of being in state s.
Reference: [BDG95] <author> C. Boutilier, R. Dearden, and M. Goldszmidt. </author> <title> Exploiting structure in policy construction. </title> <booktitle> IJCAI-95, </booktitle> <address> pp.1104-1111, Montreal, </address> <year> 1995. </year>
Reference-contexts: Much work in decision theoretic planning (DTP), generally aimed at addressing these issues, has adopted the theory of Markov decision processes (MDPs) as the underlying conceptual and computational model <ref> [DKKN93, TR94, BD94, BDG95] </ref>. MDPs allow one to formulate problems in which an agent is involved in an on-going, process-oriented interaction with the environment and receives rewards at various system states. This generalizes the classical goal-oriented view of planning [BP95]. <p> Complete observability entails that the agent always knows what state it is in. We assume that the state space is characterized by a set of features, or logical propositions. This allows actions to be described compactly using probabilistic STRIPS rules [KHW94, BD94], Bayes nets <ref> [DK89, BDG95] </ref> or other action representations. A real-valued reward function R reflects the objectives, tasks and goals to be accomplished by the agent, with R (s) denoting the (immediate) utility of being in state s. <p> Another interesting idea is to use compact representations of MDPs to obviate the need for computation involving individual states. For instance, Bayes net representations have been used to specify actions for MDPs in <ref> [BDG95] </ref>, and can be exploited in policy construction. Given an NMRDP specified in this way, we could produce new Bayes net action descriptions involving an expanded set of variables (or propositions) that render the underlying reward process Markovian, rather than expanding states explicitly.
Reference: [Bel57] <author> R. E. Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, Princeton, </publisher> <year> 1957. </year>
Reference-contexts: Temporally extended goals of this nature have been examined to some extent in the literature [HH92, Dru89, Kab90, GK91], but not in the context of generating effective policies. The key difficulty with non-Markovian rewards is that standard optimization techniques, most based on Bellman's <ref> [Bel57] </ref> dynamic programming principle, cannot be used. One way of dealing with this predicament is to formulate an equivalent decision problem in which the rewards are Markovian. <p> A policy is optimal if V (s) V 0 (s) for all s 2 S and policies 0 . Techniques for constructing optimal policies in the case of discounted rewards have been well-studied, and include algorithms such as value iteration <ref> [Bel57] </ref> and policy iteration [How60]. It should be noted that each of these algorithms exploits the Markovian nature of the reward process.
Reference: [BP95] <author> C. Boutilier and M. L. Puterman. </author> <title> Process-oriented planning and average-reward optimality. </title> <booktitle> IJCAI-95, </booktitle> <address> pp.1096-1103, Montreal, </address> <year> 1995. </year>
Reference-contexts: MDPs allow one to formulate problems in which an agent is involved in an on-going, process-oriented interaction with the environment and receives rewards at various system states. This generalizes the classical goal-oriented view of planning <ref> [BP95] </ref>. Instead of classical plans, one considers the more flexible concept of a policy, namely a mapping from each state to the action that should be executed in that state. <p> In particular, one can augment the state space of the underlying system by adding variables that keep track of the history relevant to the reward function. For instance, Boutilier and Puterman <ref> [BP95] </ref> suggest straightforward ways of encoding reward functions that involve simple requests. This approach has the advantage that existing optimization methods for MDPs can be used.
Reference: [DK89] <author> T. Dean and K. </author> <title> Kanazawa. A model for reasoning about persistence and causation. </title> <journal> Comp. Intel., </journal> <volume> 5 </volume> <pages> 142-150, </pages> <year> 1989. </year>
Reference-contexts: Complete observability entails that the agent always knows what state it is in. We assume that the state space is characterized by a set of features, or logical propositions. This allows actions to be described compactly using probabilistic STRIPS rules [KHW94, BD94], Bayes nets <ref> [DK89, BDG95] </ref> or other action representations. A real-valued reward function R reflects the objectives, tasks and goals to be accomplished by the agent, with R (s) denoting the (immediate) utility of being in state s.
Reference: [DKKN93] <author> T. Dean, L. P. Kaelbling, J. Kirman, and A. Nicholson. </author> <title> Planning with deadlines in stochastic domains. </title> <booktitle> AAAI-93, </booktitle> <address> pp.574-579, Washington, D.C., </address> <year> 1993. </year>
Reference-contexts: Much work in decision theoretic planning (DTP), generally aimed at addressing these issues, has adopted the theory of Markov decision processes (MDPs) as the underlying conceptual and computational model <ref> [DKKN93, TR94, BD94, BDG95] </ref>. MDPs allow one to formulate problems in which an agent is involved in an on-going, process-oriented interaction with the environment and receives rewards at various system states. This generalizes the classical goal-oriented view of planning [BP95].
Reference: [Dru89] <author> M. Drummond. </author> <title> Situated control rules. </title> <type> KR-89, </type> <institution> pp.103-113, Toronto, </institution> <year> 1989. </year>
Reference-contexts: Typical forms of desirable temporally extended behaviors include response to requests, bounded response, lack of response, maintaining safety constraints, and so on. Temporally extended goals of this nature have been examined to some extent in the literature <ref> [HH92, Dru89, Kab90, GK91] </ref>, but not in the context of generating effective policies. The key difficulty with non-Markovian rewards is that standard optimization techniques, most based on Bellman's [Bel57] dynamic programming principle, cannot be used.
Reference: [Eme90] <author> E. A. Emerson. </author> <title> Temporal and modal logic. </title> <editor> In J. van Leeuwen, ed., </editor> <booktitle> Handbook Theor. </booktitle> <institution> Comp. Sci., Vol.B, pp.997-1072, </institution> <year> 1990. </year>
Reference-contexts: For this reason, it will be more natural to encode our reward formulas using a past or backward-looking temporal logic rather than the usual future or forward logics like LTL, CTL <ref> [Eme90] </ref> or MTL [AH90]. In Section 3, we describe a number of interesting and useful classes of target behaviors and show how they can be encoded by TERFs. In Section 4, we consider the problem of constructing optimal policies for NMRDPs. <p> This accords well with our view of reward processes because, in most contexts, rewards should be earned based on what has actually happened. We present a past version of LTL <ref> [Eme90] </ref> called PLTL. <p> It is well-known that the modalities in LTL can be decomposed into present and future components <ref> [Eme90] </ref>. Similarly, modalities of PLTL can be decomposed into present and past components. For example, 2 is equivalent to fl 2^. That is, 2 is true iff is true of the current state and 2 is true of the previous state.
Reference: [GK91] <author> P. Godefroid and F. Kabanza. </author> <title> An efficient reactive planner for synthesizing reactive plans. </title> <booktitle> AAAI-91, </booktitle> <address> pp.640-645, </address> <year> 1991. </year>
Reference-contexts: Typical forms of desirable temporally extended behaviors include response to requests, bounded response, lack of response, maintaining safety constraints, and so on. Temporally extended goals of this nature have been examined to some extent in the literature <ref> [HH92, Dru89, Kab90, GK91] </ref>, but not in the context of generating effective policies. The key difficulty with non-Markovian rewards is that standard optimization techniques, most based on Bellman's [Bel57] dynamic programming principle, cannot be used.
Reference: [HH92] <author> P. Haddawy and S. Hanks. </author> <title> Representations for decision-theoretic planning: Utility functions for deadline goals. </title> <address> KR-92, pp.71-82, Cambridge, </address> <year> 1992. </year>
Reference-contexts: Typical forms of desirable temporally extended behaviors include response to requests, bounded response, lack of response, maintaining safety constraints, and so on. Temporally extended goals of this nature have been examined to some extent in the literature <ref> [HH92, Dru89, Kab90, GK91] </ref>, but not in the context of generating effective policies. The key difficulty with non-Markovian rewards is that standard optimization techniques, most based on Bellman's [Bel57] dynamic programming principle, cannot be used.
Reference: [How60] <author> R. A. Howard. </author> <title> Dynamic Programming and Markov Processes. </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1960. </year>
Reference-contexts: way, we obtain a compact representation of the required history-dependent policy by considering only relevant history, and can produce this policy using computationally-effective MDP algorithms. 2 Non-Markovian Rewards 2.1 Markov Decision Processes Much recent work in DTP considers planning problems that can be modeled by completely observable Markov Decision Processes <ref> [How60, Put94] </ref>. In this model, we assume that there is a finite set of system states S, a set of actions A, and a reward function R. <p> The expected value of a fixed policy at any given state s can be shown to satisfy <ref> [How60] </ref>: V (s) = R (s) + fi t2S The value of at any initial state s can be computed by solving this system of linear equations. A policy is optimal if V (s) V 0 (s) for all s 2 S and policies 0 . <p> A policy is optimal if V (s) V 0 (s) for all s 2 S and policies 0 . Techniques for constructing optimal policies in the case of discounted rewards have been well-studied, and include algorithms such as value iteration [Bel57] and policy iteration <ref> [How60] </ref>. It should be noted that each of these algorithms exploits the Markovian nature of the reward process.
Reference: [HU79] <author> J. E. Hopcroft and J. D. Ullman. </author> <title> Introduction to Automata Theory, Languages and Computation. </title> <publisher> Addison-Wesley, </publisher> <year> 1979. </year>
Reference-contexts: If minimality is important, a separate step after Phase II is required. Fortunately, minimizing G ES can be performed using a variant of standard algorithms for minimizing finite state automata <ref> [HU79] </ref>. We defer discussion to the full paper, but note that the complexity of doing this is only polynomial in the size of G ES .
Reference: [Kab90] <author> F. Kabanza. </author> <title> Synthesis of reactive plans for multi-path environments. </title> <booktitle> AAAI-90, </booktitle> <address> pp.164-169, </address> <year> 1990. </year>
Reference-contexts: Typical forms of desirable temporally extended behaviors include response to requests, bounded response, lack of response, maintaining safety constraints, and so on. Temporally extended goals of this nature have been examined to some extent in the literature <ref> [HH92, Dru89, Kab90, GK91] </ref>, but not in the context of generating effective policies. The key difficulty with non-Markovian rewards is that standard optimization techniques, most based on Bellman's [Bel57] dynamic programming principle, cannot be used.
Reference: [KHW94] <author> N. Kushmerick, S. Hanks and D. Weld. </author> <title> An algorithm for probabilistic least-commitment planning. </title> <booktitle> AAAI-94, </booktitle> <address> pp.1073-1078, Seattle, </address> <year> 1994. </year>
Reference-contexts: Complete observability entails that the agent always knows what state it is in. We assume that the state space is characterized by a set of features, or logical propositions. This allows actions to be described compactly using probabilistic STRIPS rules <ref> [KHW94, BD94] </ref>, Bayes nets [DK89, BDG95] or other action representations. A real-valued reward function R reflects the objectives, tasks and goals to be accomplished by the agent, with R (s) denoting the (immediate) utility of being in state s.
Reference: [LDK95] <author> M. Littman, T. L. Dean and L. P. Kaelbling. </author> <title> On the complexity of solving Markov decision problems. </title> <address> UAI-95, pp.394-402, Montreal, </address> <year> 1995. </year>
Reference-contexts: Since we must ensure that es's label is a correct assertion about its history, in the expanded MDP any transition from an extended version of s (es , say) to es must satisfy 5 See <ref> [LDK95] </ref> on the complexity of solving MDPs. Generally, the state space is problematic in planning problems because it grows exponentially with the number of atomic propositions. Adding history naively to the domain exacerbates this problem considerably. the historical constraints imposed by .
Reference: [Put94] <author> M. L. Puterman. </author> <title> Markov Decision Processes: Discrete Stochastic Dynamic Programming. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: Effective optimization methods exist for computing policies such that an agent executing the policy will maximize its accumulated reward over time <ref> [Put94] </ref>. The fundamental assumption underling the formulation of a planning problem as an MDP is that the system dynamics and rewards are Markovian. <p> way, we obtain a compact representation of the required history-dependent policy by considering only relevant history, and can produce this policy using computationally-effective MDP algorithms. 2 Non-Markovian Rewards 2.1 Markov Decision Processes Much recent work in DTP considers planning problems that can be modeled by completely observable Markov Decision Processes <ref> [How60, Put94] </ref>. In this model, we assume that there is a finite set of system states S, a set of actions A, and a reward function R. <p> Techniques for constructing optimal policies in the case of discounted rewards have been well-studied, and include algorithms such as value iteration [Bel57] and policy iteration [How60]. It should be noted that each of these algorithms exploits the Markovian nature of the reward process. We refer to <ref> [Put94] </ref> for an excellent treatment of MDPs and associated computational methods. 2.2 A Temporal Logic of the Past To reward agents for (temporally extended) behaviors, as opposed to simply reaching certain states, we need a means to specify rewards for specific trajectories through the state space.
Reference: [Sch87] <author> M. J. Schoppers. </author> <title> Universal plans for reactive robots in unpredictable environments. </title> <journal> IJCAI-87, </journal> <pages> 1039-1046, </pages> <address> Milan, </address> <year> 1987. </year>
Reference-contexts: A stationary Markovian policy is a mapping : S ! A, where (s) denotes the action an agent should perform whenever it is in state s. One might think of such policies as reactive or universal plans <ref> [Sch87] </ref>. Given an MDP, an agent ought to adopt a policy that maximizes the expected value over its (potentially infinite) trajectory through the state space.
Reference: [TR94] <author> J. Tash and S. Russell. </author> <title> Control strategies for a stochastic planner. </title> <booktitle> AAAI-94, </booktitle> <pages> 1079-1085, </pages> <address> Seattle, </address> <year> 1994. </year>
Reference-contexts: Much work in decision theoretic planning (DTP), generally aimed at addressing these issues, has adopted the theory of Markov decision processes (MDPs) as the underlying conceptual and computational model <ref> [DKKN93, TR94, BD94, BDG95] </ref>. MDPs allow one to formulate problems in which an agent is involved in an on-going, process-oriented interaction with the environment and receives rewards at various system states. This generalizes the classical goal-oriented view of planning [BP95].
References-found: 19

