URL: http://reconalpha.ius.cs.cmu.edu/~hamps/pubs.htmd/icassp93.ps.Z
Refering-URL: http://reconalpha.ius.cs.cmu.edu/~hamps/pubs.htmd/index.html
Root-URL: 
Email: hamps@faraday.ece.cmu.edu and kumar@gauss.ece.cmu.edu  
Title: Differential Learning Leads to Efficient Neural Network Classifiers probabilistic learning strategy for neural network classifiers
Author: J. B. Hampshire II and B. V. K. Vijaya Kumar 
Keyword: Summary  
Affiliation: Department of Electrical Computer Engineering Carnegie Mellon University  
Address: Minneapolis, MN  Pittsburgh, PA 15213-3890  
Note: June 19, 1992 11:22 am. Original Submission for ICASSP-93, April 27 30, 1993,  It proves that the current  Page 1 of 3  
Abstract: We outline a differential theory of learning for statistical pattern classification. When applied to neural networks, the theory leads to an efficient differential learning strategy based on classification figure-of-merit (CFM) objective functions [5]. Differential learning guarantees the highest probability of generalization for a classifier with limited functional complexity, trained with a limited number of examples. The theory is significant for this and two other reasons: We demonstrate the importance of differential learning's efficiency with a simple pattern recognition task that lends itself to closed-form analysis. We conclude with a practical application of the theory in which a differentially trained perceptron diagnoses a crippling joint disorder from magnetic resonance images better than both its probabilistically trained counterpart and more complex probabilistically trained multi-layer perceptrons. The recent renaissance of connectionism has led to a considerable amount of research regarding generalization in neural network pattern classifiers that are trained in a supervised fashion. Most of this research has been done by computational learning theorists and statisticians intent on matching the functional complexity of the classifier with the size of the training sample in order to avoid the well-known curse of dimensionality (see for example the work of Barron, Baum, Haussler, and Vapnik much of which is summarized in [8]). Yet relatively little attention has been paid to the effect that the objective function (used to drive the supervised learning procedure) has on discrimination and generalization [6, 1, 5, 7, 2, 3]. fl Copyright c fl1992 by J. B. Hampshire II and B. V. K. V. Kumar: all rights reserved. Copyright is automatically extended to IEEE if this submission is accepted for presentation/publication. This research was funded by the Air Force Office of Scientific Research (grant AFOSR-89-0551) and supported by a supercomputing grant from the National Science Foundation's Pittsburgh Supercomputing Center (grant CCR920002P). The views and conclusions contained in this submission are the authors' and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Air Force, the National Science Foundation, or the U.S. Government. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Barnard and D. Casasent. </author> <title> A Comparison between Criterion Functions for Linear Classifiers, with an Application to Neural Nets. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 19(5) </volume> <pages> 1030-1041, </pages> <month> September </month> <year> 1989. </year>
Reference: [2] <author> A. El-Jaroudi and J. Makhoul. </author> <title> A New Error Criterion for Posterior Probability Estimation with Neural Nets. </title> <booktitle> In IEEE Proceedings of the 1990 International Joint Conference on Neural Networks, </booktitle> <volume> Vol. 3, </volume> <pages> pages 185-192, </pages> <address> San Diego, </address> <month> June </month> <year> 1990. </year>
Reference: [3] <author> H. Gish. </author> <title> A Minimum Classification Error, Maximum likelihood, Neural Network. </title> <booktitle> In Proceedings of the 1991 IEEE International, Conference on Acoustics, Speech, and Signal Processing Vol. </booktitle> <volume> 2, </volume> <pages> pages 289-292, </pages> <year> 1991. </year>
Reference: [4] <author> J. B. </author> <title> Hampshire II. A Differential Theory of Learning for Statistical Pattern Recognition with Connectionist Models. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, Department of Electrical & Computer Engineering, Hammerschlag Hall, </institution> <address> Pittsburgh, PA 15213-3890, </address> <year> 1992. </year> <title> Manuscript in progress. </title>
Reference: [5] <author> J. B. Hampshire II and A. H. Waibel. </author> <title> A Novel Objective Function for Improved Phoneme Recognition Using Time-Delay Neural Networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 1(2) </volume> <pages> 216-228, </pages> <month> June </month> <year> 1990. </year> <booktitle> A revised and extended version of work first presented at the 1989 International Joint Conference on Neural Networks, </booktitle> <volume> vol. I, </volume> <pages> pp. 235-241. </pages>
Reference: [6] <author> G. E. Hinton. </author> <title> Connectionist Learning Procedures. </title> <editor> In J. G. Carbonell, editor, </editor> <booktitle> Machine Learning: Paradigms and Methods, </booktitle> <pages> pages 185-234. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year> <note> Based on the Carnegie Mellon University technical report (CMU-CS-87-115) of the same title. </note>
Reference: [7] <author> J. R. Movellan. </author> <title> Error Functions to Improve Noise Resistance and Generalization in Backpropagation Networks. </title> <booktitle> In IEEE Proceedings of the 1990 International Joint Conference on Neural Networks, </booktitle> <volume> Vol. 1, </volume> <pages> pages 557-560, </pages> <address> Washington, DC, </address> <month> January </month> <year> 1990. </year>
Reference: [8] <author> B. K. Natarajan. </author> <title> Machine Learning: A Theoretical Approach. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year> <note> Page 3 of 3 </note>
References-found: 8

