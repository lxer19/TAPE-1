URL: ftp://dimacs.rutgers.edu/pub/dimacs/TechnicalReports/TechReports/1997/97-23.ps.gz
Refering-URL: http://dimacs.rutgers.edu/TechnicalReports/1997.html
Root-URL: http://www.cs.rutgers.edu
Email: E-mail:rvdb@princeton.edu  
Title: Extension of Piyavskii's Algorithm to Continuous Global Optimization  
Author: by Robert J. Vanderbei ; 
Note: 2 Research supported by the NSF through grant CCR-9403789 and by AFOSR through grant F49620-95-1-0351.  
Address: Princeton, NJ 08544  
Affiliation: Statistics and Operations Research Princeton University  
Abstract: DIMACS Technical Report 97-23 June 1997 DIMACS is a partnership of Rutgers University, Princeton University, AT&T Labs, Bellcore, and Bell Labs. DIMACS is an NSF Science and Technology Center, funded under contract STC-91-19999; and also receives support from the New Jersey Commission on Science and Technology. 
Abstract-found: 1
Intro-found: 1
Reference: [GJE97] <author> E. Gourdin, B. Jaumard, and R. Ellaia. </author> <title> Global optimization of Holder functions. </title> <journal> J. of Global Optimization, </journal> <note> 1997. To appear. </note>
Reference-contexts: We carry out this program for one such algorithm in the next section. 4. Extension of Piyavskii's Algorithm The first algorithm for Lipschitz global optimization was published in [Piy67] (see also [Piy72]) and was independently rediscovered by [Shu72] and [Tim77]. Recently, <ref> [GJE97] </ref> gave an extension of Piyavskii's algorithm to Holder continuous functions.
Reference: [HJL91] <author> P. Hansen, B. Jaumard, and S.H. Lu. </author> <title> On the number of iterations of Piyavskii's global optimization algorithm. </title> <journal> Mathematics of Operations Research, </journal> <volume> 16 </volume> <pages> 334-350, </pages> <year> 1991. </year>
Reference-contexts: In this manner, one can develop efficient algorithms for finding a solution that is within a prespecified tolerance of an optimal solution in just a finite number of iterations. The papers <ref> [HJL91] </ref>, [HJL92a], and [HJL92b] give an extensive survey of algorithms for Lipschitz global optimization all of which are based on this idea. Of course, the methods generally assume a priori knowledge of the parameter K. This can be problematic. <p> The algorithm then reduces exactly to Piyavskii's algorithm for Lipschitz global optimization. The analysis of the extended Piyavskii algorithm is essentially the same as the usual analysis of Piyavskii's algorithm in the Lipschitz case. We refer the reader to <ref> [HJL91] </ref> for details for that case. Of course, to implement Piyavskii's algorithm it must be possible to find an (*; K)-pair. Finding such a pair is very much analogous to finding the K for a Lipschitz continuous function.
Reference: [HJL92a] <author> P. Hansen, B. Jaumard, and S.H. Lu. </author> <title> Global optimization of univariate Lipschitz functions: I. Survey and properties. </title> <journal> Mathematical Programming, </journal> <volume> 55 </volume> <pages> 251-272, </pages> <year> 1992. </year>
Reference-contexts: We leave to others the interesting task of applying the main idea to the many and varied algorithms that already exist for Lipschitz global optimization. These algorithms are surveyed in <ref> [HJL92a] </ref> and [Pin96]. The problem then is, given a continuous function f on [a; b], find x fl that attains the global maximum: max f (x): If f is Lipschitz with a known Lipschitz constant (or overestimate thereof), we call the problem a Lipschitz global optimization problem. 2. <p> In this manner, one can develop efficient algorithms for finding a solution that is within a prespecified tolerance of an optimal solution in just a finite number of iterations. The papers [HJL91], <ref> [HJL92a] </ref>, and [HJL92b] give an extensive survey of algorithms for Lipschitz global optimization all of which are based on this idea. Of course, the methods generally assume a priori knowledge of the parameter K. This can be problematic.
Reference: [HJL92b] <author> P. Hansen, B. Jaumard, and S.H. Lu. </author> <title> Global optimization of univariate Lipschitz functions: II. New algorithms and computational comparison. </title> <journal> Mathematical Programming, </journal> <volume> 55 </volume> <pages> 273-292, </pages> <year> 1992. </year>
Reference-contexts: In this manner, one can develop efficient algorithms for finding a solution that is within a prespecified tolerance of an optimal solution in just a finite number of iterations. The papers [HJL91], [HJL92a], and <ref> [HJL92b] </ref> give an extensive survey of algorithms for Lipschitz global optimization all of which are based on this idea. Of course, the methods generally assume a priori knowledge of the parameter K. This can be problematic.
Reference: [Pin96] <author> J.D. Pinter. </author> <title> Global Optimization in Action Continuous and Lipschitz Optimization: Algorithms, Implementations and Applications. </title> <publisher> Kluwer, </publisher> <address> Dordrecht, </address> <year> 1996. </year>
Reference-contexts: We leave to others the interesting task of applying the main idea to the many and varied algorithms that already exist for Lipschitz global optimization. These algorithms are surveyed in [HJL92a] and <ref> [Pin96] </ref>. The problem then is, given a continuous function f on [a; b], find x fl that attains the global maximum: max f (x): If f is Lipschitz with a known Lipschitz constant (or overestimate thereof), we call the problem a Lipschitz global optimization problem. 2.
Reference: [Piy67] <author> S.A. Piyavskii. </author> <title> An algorithm for finding the absolute minimum of a function. Theory of Optimal Solutions, </title> <booktitle> 2 </booktitle> <pages> 13-24, </pages> <year> 1967. </year> <type> IK Akad. </type> <institution> Nauk USSR, Kiev. </institution>
Reference-contexts: Hence, it is possible to extend essentially every algorithm for Lipschitz global optimization to continuous global optimization. We carry out this program for one such algorithm in the next section. 4. Extension of Piyavskii's Algorithm The first algorithm for Lipschitz global optimization was published in <ref> [Piy67] </ref> (see also [Piy72]) and was independently rediscovered by [Shu72] and [Tim77]. Recently, [GJE97] gave an extension of Piyavskii's algorithm to Holder continuous functions.
Reference: [Piy72] <author> S.A. Piyavskii. </author> <title> An algorithm for finding the absolute extremum of a function. </title> <journal> USSR Comput. Math. Math. Phys., </journal> <volume> 12 </volume> <pages> 57-67, </pages> <year> 1972. </year>
Reference-contexts: Hence, it is possible to extend essentially every algorithm for Lipschitz global optimization to continuous global optimization. We carry out this program for one such algorithm in the next section. 4. Extension of Piyavskii's Algorithm The first algorithm for Lipschitz global optimization was published in [Piy67] (see also <ref> [Piy72] </ref>) and was independently rediscovered by [Shu72] and [Tim77]. Recently, [GJE97] gave an extension of Piyavskii's algorithm to Holder continuous functions.
Reference: [Shu72] <author> B.O. Shubert. </author> <title> A sequential method seeking the global maximum of a function. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 9 </volume> <pages> 379-388, </pages> <year> 1972. </year>
Reference-contexts: We carry out this program for one such algorithm in the next section. 4. Extension of Piyavskii's Algorithm The first algorithm for Lipschitz global optimization was published in [Piy67] (see also [Piy72]) and was independently rediscovered by <ref> [Shu72] </ref> and [Tim77]. Recently, [GJE97] gave an extension of Piyavskii's algorithm to Holder continuous functions.
Reference: [Tim77] <author> L.N. Timonov. </author> <title> An algorithm for search of a global extremum.Engrg. </title> <journal> Cybernetics, </journal> <volume> 15 </volume> <pages> 38-44, </pages> <year> 1977. </year>
Reference-contexts: We carry out this program for one such algorithm in the next section. 4. Extension of Piyavskii's Algorithm The first algorithm for Lipschitz global optimization was published in [Piy67] (see also [Piy72]) and was independently rediscovered by [Shu72] and <ref> [Tim77] </ref>. Recently, [GJE97] gave an extension of Piyavskii's algorithm to Holder continuous functions.
Reference: [Van91] <author> R.J. Vanderbei. </author> <title> Uniform continuity is almost Lipschitz continuity. </title> <type> Technical Report SOR-91-11, </type> <institution> Statistics and Operations Research Series, Princeton University, </institution> <year> 1991. </year>
Reference-contexts: This theorem was proved in the unpublished technical report <ref> [Van91] </ref>. For completeness, we repeat the proof here. Proof. Suppose that f is uniformly continuous on D and fix * &gt; 0. Then, there exists a ffi &gt; 0 such that jf (z) f (z 0 )j &lt; * whenever kz z 0 k &lt; ffi.
References-found: 10

