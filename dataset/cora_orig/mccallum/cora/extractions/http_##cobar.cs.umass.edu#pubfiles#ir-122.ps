URL: http://cobar.cs.umass.edu/pubfiles/ir-122.ps
Refering-URL: http://cobar.cs.umass.edu/pubfiles/
Root-URL: 
Title: A Theory of Term Weighting Based on Exploratory Data Analysis  
Author: Warren R. Greiff 
Web: www.cs.umass.edu/~greiff/  
Address: Amherst  
Affiliation: Computer Science Department University of Massachusetts,  
Abstract: Techniques of exploratory data analysis are used to study the weight of evidence that the occurrence of a query term provides in support of the hypothesis that a document is relevant to an information need. In particular, the relationship between the document frequency and the weight of evidence is investigated. A correlation between document frequency normalized by collection size and the mutual information between relevance and term occurrence is uncovered. This correlation is found to be robust across a variety of query sets and document collections. Based on this relationship, a theoretical explanation of the efficacy of inverse document frequency for term weighting is developed which differs in both style and content from theories previously put forth. The theory predicts that a "flattening" of idf at both low and high frequency should result in improved retrieval performance. This altered idf formulation is tested on all TREC query sets. Retrieval results corroborate the prediction of improved retrieval performance. In conclusion, we argue that exploratory data analysis can be a valuable tool for research whose goal is the development of an explanatory theory of information retrieval. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. P. Callan, W. B. Croft, and S. M. Harding. </author> <title> The in-query retrieval system. </title> <booktitle> In Proceedings of the 3rd In ternational Conference on Database and Expert Sys--tems Applications, </booktitle> <pages> pages 78-83, </pages> <year> 1992. </year>
Reference-contexts: The group at Berke-ley has conducted extensive research into the use of logistic regression [10, 4]. Logistic regression is generally considered a natural approach for estimating a probability. The <ref> [0; 1] </ref> range that can be assumed by a probability does not correspond to other regression models, but is accounted for in logistic regression. Also, normality assumptions which are often behind the statistical inference techniques used in standard regression analysis are inappropriate for a dichotomous response variable (such as relevance). <p> To test this prediction, we compared retrieval performance of two versions of the INQUERY IR system <ref> [1] </ref> on each of the ad-hoc tasks for TREC 1 through TREC 6 [14]. Queries were formed by taking all words from both the title and description. All stopwords were removed, as were all duplicates.
Reference: [2] <author> Kenneth Church, William Gale, Patrick Hanks, and Donald Hindle. </author> <title> Using statistics in lexical analysis. In Uri Zernik, editor, Lexical Acquisition: Exploiting On-Line Resources to Build a Lexicon, </title> <address> pages 115-164, Hillsdale, NJ, 1991. </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: Often referred to as mutual information, it has been used as a measure of variable dependence in both information retrieval [25, 6] and computational linguistics <ref> [2] </ref>. In a very important sense, it can be taken as a measure of the information about one event provided by the occurrence of another [7]. In our context, it can be taken as a measure the information about relevance provided by the occurrence of a query term.
Reference: [3] <author> W. S. Cooper, D. Dabney, and F. Gey. </author> <title> Probabilistic retrieval based on staged logistic regression. </title> <editor> In Nicholas Belkin, Peter Ingwersen, and An-nelise Mark Mejtersen, editors, </editor> <booktitle> Proceedings of the 15th Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 198-210, </pages> <address> Copenhagen, Denmark, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: In 1983, Fox used multiple regression analysis to derive an equation for predicting the probability that a document will be judged relevant to a query <ref> [3] </ref>. In [28], Yu and Mizuno use linear regression to determine parameter settings for both a binary and non-binary model. Fuhr and Buckley [9, 8] have used a least-square error criterion to determine coefficients for a polynomial weighting function of term-document pair descriptor variables.
Reference: [4] <author> Wm. S. Cooper, Aitao Chen, and Fredric C. Gey. </author> <title> Full text retrieval based on probabilistic equations with coefficients fitted by logistic regression. </title> <editor> In D. K. Harman, editor, </editor> <booktitle> The Second Text REtreival Conference (TREC-2), </booktitle> <pages> pages 57-66, </pages> <address> Gaithersburg, Md., </address> <month> March </month> <year> 1994. </year> <note> NIST Special Publication 500-215. </note>
Reference-contexts: Fuhr and Buckley [9, 8] have used a least-square error criterion to determine coefficients for a polynomial weighting function of term-document pair descriptor variables. The group at Berke-ley has conducted extensive research into the use of logistic regression <ref> [10, 4] </ref>. Logistic regression is generally considered a natural approach for estimating a probability. The [0; 1] range that can be assumed by a probability does not correspond to other regression models, but is accounted for in logistic regression.
Reference: [5] <author> W. B. Croft and D. J. Harper. </author> <title> Using probabilistic models of document retrieval without relevance information. </title> <journal> Journal of Documentation, </journal> <volume> 35(4) </volume> <pages> 285-295, </pages> <month> December </month> <year> 1979. </year>
Reference-contexts: Use of the model depends on the availability of relevance feedback information, on which estimates of the two conditional probabilities can be based. Applying the probabilistic approach of Robertson and Sparck Jones, Croft and Harper <ref> [5] </ref> work with an equivalent formulation of w rsj : w rsj = log p (occjrel) 1 p (occjrel) log p (occjrel) 1 p (occjrel) (5) Their goal is the development of a probabilistically justified weighting formula that can be used in a retrieval setting in the absence of, or prior <p> They make two assumptions: 1) there "is no information about the relevant documents and we could therefore assume that all the query terms had equal probabilities of occurring in the relevant documents" <ref> [5, p. 287] </ref>; and 2) the probability, p (occjrel), of a term occurring in a non-relevant document can be estimated by n N , the proportion of documents that contain the term in the entire collection. <p> This plot of p (occjrel) vs. p (occ) gives us reason to question the advisability of the assumption of equal probability of term occurrence in the relevant documents, used in <ref> [5] </ref> as the basis of eq. 6. This also puts into question the assumptions made in [19]. Both the assumption that p (occjrel) increases from a non-zero starting point and that the increase is linear with increasing p (occ) contradict the evidence provided by figure 4. <p> Even when such a rare query term appears, its scarcity in the collection as a whole implies that its precise term weight is unlikely to have a major impact on overall retrieval performance. The derivation of the combination match weighting formula (equation 6) in <ref> [5] </ref> also depends on p (occjrel) being well approximated by p (occ). We emphasize, however, that for low frequency query terms, the argument given here depends heavily on the value of p (occjrel) p (occ) relative to p (rel).
Reference: [6] <author> W. B. Croft and Jinxi Xu. </author> <title> Corpus-specific stemming using word form co-occurence. </title> <booktitle> In Proceedings for the Fourth Annual Symposium on Document Analysis and Information Retrieval, </booktitle> <pages> pages 147-159, </pages> <address> Las Vegas, Nevada, </address> <month> April </month> <year> 1995. </year>
Reference-contexts: Finally, we note that the quantity: log p (occjrel) p (occ) p (occ ^ rel) = log p (reljocc) p (rel) has connections to information theory. Often referred to as mutual information, it has been used as a measure of variable dependence in both information retrieval <ref> [25, 6] </ref> and computational linguistics [2]. In a very important sense, it can be taken as a measure of the information about one event provided by the occurrence of another [7].
Reference: [7] <author> Robert M. Fano. </author> <title> Transmission of Information; a Statistical Theory of Communications. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1961. </year>
Reference-contexts: In a very important sense, it can be taken as a measure of the information about one event provided by the occurrence of another <ref> [7] </ref>. In our context, it can be taken as a measure the information about relevance provided by the occurrence of a query term.
Reference: [8] <author> N. Fuhr. </author> <title> Optimum polynomial retrieval functions based on the probability ranking principle. </title> <journal> ACM Transactions on Information Systems, </journal> <volume> 7(3) </volume> <pages> 183-204, </pages> <year> 1989. </year>
Reference-contexts: In 1983, Fox used multiple regression analysis to derive an equation for predicting the probability that a document will be judged relevant to a query [3]. In [28], Yu and Mizuno use linear regression to determine parameter settings for both a binary and non-binary model. Fuhr and Buckley <ref> [9, 8] </ref> have used a least-square error criterion to determine coefficients for a polynomial weighting function of term-document pair descriptor variables. The group at Berke-ley has conducted extensive research into the use of logistic regression [10, 4]. Logistic regression is generally considered a natural approach for estimating a probability.
Reference: [9] <author> Norbert Fuhr and Chris Buckley. </author> <title> Probabilistic document indexing from relevance feedback data. </title> <journal> ACM Transactions on Information Systems, </journal> <volume> 9(2) </volume> <pages> 45-61, </pages> <year> 1991. </year>
Reference-contexts: In 1983, Fox used multiple regression analysis to derive an equation for predicting the probability that a document will be judged relevant to a query [3]. In [28], Yu and Mizuno use linear regression to determine parameter settings for both a binary and non-binary model. Fuhr and Buckley <ref> [9, 8] </ref> have used a least-square error criterion to determine coefficients for a polynomial weighting function of term-document pair descriptor variables. The group at Berke-ley has conducted extensive research into the use of logistic regression [10, 4]. Logistic regression is generally considered a natural approach for estimating a probability.
Reference: [10] <author> Fredric C. Gey. </author> <title> Inferring probability of relevance using the method of logistic regression. </title> <editor> In W. Bruce Croft and C. J. van Rijsbergen, editors, </editor> <booktitle> Proceedings of the 17th Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 222-231, </pages> <address> Dublin, Ireland, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: Fuhr and Buckley [9, 8] have used a least-square error criterion to determine coefficients for a polynomial weighting function of term-document pair descriptor variables. The group at Berke-ley has conducted extensive research into the use of logistic regression <ref> [10, 4] </ref>. Logistic regression is generally considered a natural approach for estimating a probability. The [0; 1] range that can be assumed by a probability does not correspond to other regression models, but is accounted for in logistic regression. <p> If the terms are correlated, woe (rel : occ 2 ) will be greater than woe (rel : occ 2 j occ 1 ). It is generally accepted that interdependence of query terms has a noticeable impact on the effectiveness of term weighting <ref> [15, 25, 10] </ref>. Since, to date, we have made no attempt to model the influence of term dependence, determination of a precise function for estimation of woe (rel : occ) is not indicated.
Reference: [11] <author> I. J. </author> <title> Good. Probability and the Weighing of Evidence. </title> <address> Charles Griffin, London, </address> <year> 1950. </year>
Reference-contexts: In conclusion we return to the research summarized in Section 3, and compare and contrast it with the work presented here. 2 Weight of Evidence & EDA Weight of Evidence I. J. Good formally defines the weight in favor of a hypothesis, h, provided by evidence, e, as <ref> [12, 11] </ref>: O (hje) (1) which he thinks is a concept "almost as important as that of probability itself" [11, p. 249]. <p> J. Good formally defines the weight in favor of a hypothesis, h, provided by evidence, e, as [12, 11]: O (hje) (1) which he thinks is a concept "almost as important as that of probability itself" <ref> [11, p. 249] </ref>.
Reference: [12] <author> I. J. </author> <title> Good. Weight of evidence: A brief survey. </title> <editor> In J. M. Bernardo, M. H. DeGroot, D. V. Lindley, and A. F. M. Smith, editors, </editor> <booktitle> Bayesian Statistics 2, </booktitle> <pages> pages 249-269. </pages> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1983. </year>
Reference-contexts: In conclusion we return to the research summarized in Section 3, and compare and contrast it with the work presented here. 2 Weight of Evidence & EDA Weight of Evidence I. J. Good formally defines the weight in favor of a hypothesis, h, provided by evidence, e, as <ref> [12, 11] </ref>: O (hje) (1) which he thinks is a concept "almost as important as that of probability itself" [11, p. 249].
Reference: [13] <author> Donna Harman. </author> <title> Overview of the first Text REtrieval Conference (TREC-1). </title> <editor> In D. K. Harman, editor, </editor> <booktitle> The First Text REtrieval Conference (TREC1), </booktitle> <pages> pages 1-20, </pages> <address> Gaithersburg, Md., </address> <month> February </month> <year> 1993. </year> <note> NIST Special Publication 500-207. </note>
Reference-contexts: The study involved data from queries 051-100 from the first Text REtrieval Conference (TREC) and the Associated Press (AP) documents from TREC volume 1 <ref> [13] </ref>. Each data point corresponds to one query term. The query terms were taken from the concepts field of the TREC 1 topics.
Reference: [14] <author> Donna Harman. </author> <title> Overview of the fifth Text REtrieval Conference (TREC-5). </title> <editor> In E. M. Voorhees and D. K. Harman, editors, </editor> <booktitle> The Fifth Text RE-treival Conference (TREC-5), </booktitle> <pages> pages 1-28, </pages> <address> Gaithers-burg, Md. 500-238, </address> <month> November </month> <year> 1997. </year> <note> NIST Special Publication 500-238. </note>
Reference-contexts: To test this prediction, we compared retrieval performance of two versions of the INQUERY IR system [1] on each of the ad-hoc tasks for TREC 1 through TREC 6 <ref> [14] </ref>. Queries were formed by taking all words from both the title and description. All stopwords were removed, as were all duplicates. The baseline system used pure idf term weighting with idf = log O (occ) 4 . The test system used a flattened version of idf.
Reference: [15] <author> D. J. Harper and C. J. van Rijsbergen. </author> <title> An evaluation of feedback in document retrieval using co-occurrence data. </title> <journal> Journal of Documentation, </journal> <volume> 34(3) </volume> <pages> 189-216, </pages> <month> September </month> <year> 1978. </year>
Reference-contexts: If the terms are correlated, woe (rel : occ 2 ) will be greater than woe (rel : occ 2 j occ 1 ). It is generally accepted that interdependence of query terms has a noticeable impact on the effectiveness of term weighting <ref> [15, 25, 10] </ref>. Since, to date, we have made no attempt to model the influence of term dependence, determination of a precise function for estimation of woe (rel : occ) is not indicated.
Reference: [16] <author> Frederick Hartwig and Brian E. Dearing. </author> <title> Exploratory Data Analysis. </title> <publisher> Sage Publications, </publisher> <year> 1979. </year>
Reference-contexts: Exploratory Data Analysis Hartwig and Dearing define exploratory data analysis as "a state of mind, a way of thinking about data analysis and also a way of doing it" <ref> [16, p. 9] </ref>. They advance adherence to two principles. First, that one should be skeptical of data summaries which may disguise the most enlightening characteristics of the phenomenon being investigated.
Reference: [17] <author> S. E. Robertson. </author> <title> Term specificity. </title> <journal> Journal of Documentation, </journal> <volume> 28(2) </volume> <pages> 164-165, </pages> <year> 1972. </year> <title> Letter to the editor, with response by K. </title> <publisher> Sparck Jones. </publisher>
Reference-contexts: the number of documents in the entire collection. 3.1 Probabilistic Explanations In a letter to the Journal of Documentation, Robertson pointed out that, viewed as a function of the probability of term occurrence, the sum of weights could be interpreted as the probability of mutual occurrence of multiple query terms <ref> [17] </ref>; thus providing theoretical arguments for the use of w sj .
Reference: [18] <author> S. E. Robertson and K. Sparck Jones. </author> <title> Relevance weighting of search terms. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 27 </volume> <pages> 129-146, </pages> <year> 1977. </year>
Reference-contexts: Together, in 1976, Robertson and Sparck Jones presented the Binary Independence Model <ref> [18] </ref>, in which terms are weighted by: w rsj = log p (occjrel) (1 p (occjrel)) (1 p (occjrel)) p (occjrel) (4) where p (occjrel) is the probability of the term occurring in relevant documents 1 , and p (occjrel) is the corresponding probability for non-relevant documents. <p> For variables that are functions of log odds, a zero count translates to a (positive or negative) infinite value. One way around the problem is to add a small value to each of the counts of interest, as done for instance in <ref> [18] </ref>, where for the purpose of estimating w rsj , 0.5 is added to each count. The choice of constant, however, is to a large degree arbitrary.
Reference: [19] <author> S. E. Robertson and S. Walker. </author> <title> On relevance weights with little relevance information. </title> <editor> In Nicholas J. Belkin, A. Desai Narasimhalu, and Peter Willett, editors, </editor> <booktitle> Proceedings of the 20th Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 16-24, </pages> <address> Philadelphia, Pennsylvania, </address> <month> July </month> <year> 1997. </year>
Reference-contexts: In this formula, k is an experimentally determined constant, corresponding to the log-odds of a term occurring in a relevant document. The second component is essentially equivalent to ( 3) for all but very high frequency terms. Robertson and Walker <ref> [19] </ref> have recently looked anew at the combination match weight, w ch . They point out two "anomalies" of the Croft/Harper weights. <p> These anomalies cause them to modify the assumption of equal probability of occurrence in relative documents, in favor of an assumption that this probability "increases from a non-zero starting point to reach unity" <ref> [19, p. 19] </ref> for a term that appears in all documents. <p> This plot of p (occjrel) vs. p (occ) gives us reason to question the advisability of the assumption of equal probability of term occurrence in the relevant documents, used in [5] as the basis of eq. 6. This also puts into question the assumptions made in <ref> [19] </ref>. Both the assumption that p (occjrel) increases from a non-zero starting point and that the increase is linear with increasing p (occ) contradict the evidence provided by figure 4. We return to discuss these points further in Section 7.
Reference: [20] <author> G. Salton, A. Wong, and C. T. Yu. </author> <title> Automatic indexing using term discrimination and term precision measurements. </title> <booktitle> Information Processing & Management, </booktitle> <volume> 12 </volume> <pages> 43-51, </pages> <year> 1976. </year>
Reference-contexts: In earlier papers, term precision was defined as <ref> [20] </ref>: p (occjrel) 1 p (occjrel) = p (occjrel) 1 p (occjrel) Later, term precision was defined as the log of this quantity [21, 27], yielding the same weight as given by Robert-son and Sparck Jones (eq. 4). <p> Prior to the term precision model, Salton and others experimented with the discrimination value of a term <ref> [20] </ref>. This is a measure of how important a term is in distinguishing documents of the collection from each other. Information theoretic considerations have also been used. In early work, information theory was used to derive a weight based on signal-noise ratio [22].
Reference: [21] <author> G. Salton, H. Wu, and C. Y. Yu. </author> <title> The measurement of term importance in automatic indexing. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 32 </volume> <pages> 175-186, </pages> <year> 1981. </year>
Reference-contexts: In earlier papers, term precision was defined as [20]: p (occjrel) 1 p (occjrel) = p (occjrel) 1 p (occjrel) Later, term precision was defined as the log of this quantity <ref> [21, 27] </ref>, yielding the same weight as given by Robert-son and Sparck Jones (eq. 4). The form they adopt for what amounts to p (occjrel) differs from that of both Croft/Harper and Robertson/Walker.
Reference: [22] <author> Gerard Salton and Michael J. McGill. </author> <title> Introduction to Modern Information Retrieval. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: This is a measure of how important a term is in distinguishing documents of the collection from each other. Information theoretic considerations have also been used. In early work, information theory was used to derive a weight based on signal-noise ratio <ref> [22] </ref>. In [26], Wong and Yao develop a term weighting theory based on the entropy of a term's distribution in the collection. They show that idf weighting is easily derived as a special case of their more general weighting scheme.
Reference: [23] <author> K. Sparck-Jones. </author> <title> A statistical interpretation of term specificity and its application in retrieval. </title> <journal> Journal of Documentation, </journal> <volume> 28 </volume> <pages> 11-21, </pages> <year> 1972. </year>
Reference-contexts: 1 Introduction In 1972, Spark Jones demonstrated that document frequency can be used effectively for the weighting of query terms <ref> [23] </ref>. Ever since, formulations of inverse document frequency have played a key role in information retrieval research. In this paper a theory of why inverse document frequency has been so effective is developed. Both the approach taken and the conclusions drawn differ from theories previously put forth. <p> Tukey [24]. For example, techniques for data smoothing and re-expression of variables have been used in the study presented in this article. 3 Related Work In 1972, Sparck Jones, convincingly demonstrated that the weighting of query terms can significantly improve retrieval performance compared to unweighted coordination match ranking <ref> [23] </ref>.
Reference: [24] <author> John W. Tukey. </author> <title> Exploratory Data Analysis. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, MA, </address> <year> 1977. </year>
Reference-contexts: Depending solely on the reduction of large quantities of data to a few summary statistics erases most of the message the data have for us. EDA embodies a set of useful methods and strategies, fomented primarily by John W. Tukey <ref> [24] </ref>. For example, techniques for data smoothing and re-expression of variables have been used in the study presented in this article. 3 Related Work In 1972, Sparck Jones, convincingly demonstrated that the weighting of query terms can significantly improve retrieval performance compared to unweighted coordination match ranking [23]. <p> A glance at this graph suggests that a re-expression of variables may be indicated. The histogram shown at the left in figure 3 confirms that the distribution of document frequencies is highly skewed. With this type of skew, a logarithmic 2 transformation is often found to be beneficial <ref> [24] </ref>. In this paper, we go one step further and re-express the variable as log O (occ) = log p (occ) practical purposes, given typical document frequencies for query terms, the difference between log p (occ) and log O (occ) is negligible.
Reference: [25] <editor> C. J. van Rijsbergen. </editor> <booktitle> Information Retrieval. </booktitle> <address> But-terworths, London, 2 edition, </address> <year> 1979. </year>
Reference-contexts: Finally, we note that the quantity: log p (occjrel) p (occ) p (occ ^ rel) = log p (reljocc) p (rel) has connections to information theory. Often referred to as mutual information, it has been used as a measure of variable dependence in both information retrieval <ref> [25, 6] </ref> and computational linguistics [2]. In a very important sense, it can be taken as a measure of the information about one event provided by the occurrence of another [7]. <p> If the terms are correlated, woe (rel : occ 2 ) will be greater than woe (rel : occ 2 j occ 1 ). It is generally accepted that interdependence of query terms has a noticeable impact on the effectiveness of term weighting <ref> [15, 25, 10] </ref>. Since, to date, we have made no attempt to model the influence of term dependence, determination of a precise function for estimation of woe (rel : occ) is not indicated.
Reference: [26] <author> S. K. M. Wong and Y. Y. Yao. </author> <title> An information-theoretic measure of term specificity. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 43(1) </volume> <pages> 54-61, </pages> <year> 1992. </year>
Reference-contexts: This is a measure of how important a term is in distinguishing documents of the collection from each other. Information theoretic considerations have also been used. In early work, information theory was used to derive a weight based on signal-noise ratio [22]. In <ref> [26] </ref>, Wong and Yao develop a term weighting theory based on the entropy of a term's distribution in the collection. They show that idf weighting is easily derived as a special case of their more general weighting scheme.
Reference: [27] <author> C. T. Yu, K. Lam, and G. Salton. </author> <title> Term weighting in information retrieval using the term precision model. </title> <journal> Journal of the ACM, </journal> <volume> 29(1) </volume> <pages> 152-170, </pages> <month> January </month> <year> 1982. </year>
Reference-contexts: In earlier papers, term precision was defined as [20]: p (occjrel) 1 p (occjrel) = p (occjrel) 1 p (occjrel) Later, term precision was defined as the log of this quantity <ref> [21, 27] </ref>, yielding the same weight as given by Robert-son and Sparck Jones (eq. 4). The form they adopt for what amounts to p (occjrel) differs from that of both Croft/Harper and Robertson/Walker. <p> This function is chosen based on the assumption that "the user will pick terms with properties somewhere between those obtaining for the random and perfect terms" <ref> [27, p. 159] </ref>, sustained by solid theoretical arguments as to what the probability of occurrence conditioned on relevance must be for both perfect and random terms as a function of document frequency. 3.3 Regression Regression strategies (explicitly or implicitly) assume a parameterized model and apply statistical techniques to fit the model
Reference: [28] <author> Clement T. Yu and Ilirotaka Mizuno. </author> <title> Two learning schemes in information retrieval. </title> <editor> In Yves Chiaramella, editor, </editor> <booktitle> Proceedings of the 11th International Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 201-215, </pages> <address> Grenoble, France, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: In 1983, Fox used multiple regression analysis to derive an equation for predicting the probability that a document will be judged relevant to a query [3]. In <ref> [28] </ref>, Yu and Mizuno use linear regression to determine parameter settings for both a binary and non-binary model. Fuhr and Buckley [9, 8] have used a least-square error criterion to determine coefficients for a polynomial weighting function of term-document pair descriptor variables.
References-found: 28

