URL: http://www.cs.cmu.edu/~knigam/papers/emcat-aaai98.ps.gz
Refering-URL: http://www.cs.cmu.edu/~knigam/resume.html
Root-URL: 
Email: knigam@cs.cmu.edu  mccallum@cs.cmu.edu  thrun@cs.cmu.edu  mitchell+@cs.cmu.edu  
Title: Learning to Classify Text from Labeled and Unlabeled Documents  
Author: Kamal Nigam Andrew McCallum zy Sebastian Thrun Tom Mitchell 
Address: Pittsburgh, PA 15213  4616 Henry Street Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  Just Research  
Abstract: In many important text classification problems, acquiring class labels for training documents is costly, while gathering large quantities of unlabeled data is cheap. This paper shows that the accuracy of text classifiers trained with a small number of labeled documents can be improved by augmenting this small training set with a large pool of unlabeled documents. We present a theoretical argument showing that, under common assumptions, unlabeled data contain information about the target function. We then introduce an algorithm for learning from labeled and unlabeled text based on the combination of Expectation-Maximization with a naive Bayes classifier. The algorithm first trains a classifier using the available labeled documents, and probabilistically labels the unlabeled documents; it then trains a new classifier using the labels for all the documents, and iterates to convergence. Experimental results, obtained using text from three different real-world tasks, show that the use of unlabeled data reduces classification error by up to 33%. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Castelli, V., and Cover, T. </author> <year> 1995. </year> <title> On the exponential value of labeled samples. </title> <journal> Pattern Recognition Letters 16 </journal> <pages> 105-111. </pages>
Reference-contexts: But, with infinite unlabeled data and finite labeled data, there is classification improvement. With infinite unlabeled data, the classification error approaches the Bayes optimal solution at an exponential rate in the number of labeled examples given <ref> (Castelli & Cover 1995) </ref>. Thus, infinite amounts of unlabeled data are shown to help classification when there is some, but not infinite, amounts of labeled data. Unfortunately, little is known for the case in which there are finite amounts of each.
Reference: <author> Cheeseman, P., and Stutz, J. </author> <year> 1996. </year> <title> Bayesian classification (AutoClass): Theory and results. </title> <editor> In Fayyad, U.; Piatetski-Shapiro, G.; Smyth, P.; and Uthurusamy, R., eds., </editor> <booktitle> Advances in Knowledge Discovery and Data Mining. </booktitle> <address> Cam-bridge: </address> <publisher> MIT Press. </publisher>
Reference-contexts: These results indicate that correct model selection is crucial for EM when there is not a simple one-to-one correspondence between generative components and classes. When the data is accurately modeled, EM provides significant gains in performance. One obvious question is how to select the best model. AutoClass <ref> (Cheeseman & Stutz 1996) </ref> does this for unsupervised clustering tasks by selecting the most probable model given the data and a prior that prefers smaller models. For classification tasks, it may be more beneficial to use classification accuracy with leave-one-out cross-validation, as was successful for ff-tuning. <p> Ghahramani and Jordan (1994) use EM with mixture models to fill in missing values. The emphasis of their work is on missing feature values, where we focus on augmenting a very small but complete set of labeled data. The AutoClass project <ref> (Cheeseman & Stutz 1996) </ref> investigates the combination of the EM algorithm with an underlying model of a naive Bayes classifier. The emphasis of their research is the discovery of novel clus-terings for unsupervised learning over unlabeled data. AutoClass has not been applied to text or classification.
Reference: <author> Cohen, W., and Singer, Y. </author> <year> 1997. </year> <title> Context-sensitive learning methods for text categorization. </title> <booktitle> In Proceedings of ACM SIGIR Conference. </booktitle>
Reference: <author> Craven, M.; DiPasquo, D.; Freitag, D.; McCallum, A.; Mitchell, T.; Nigam, K.; and Slattery, S. </author> <year> 1998. </year> <title> Learning to extract symbolic knowledge from the World Wide Web. </title> <booktitle> In Proceedings of AAAI-98. </booktitle>
Reference-contexts: There are statistical text learning algorithms that can be trained to approximately classify documents, given a sufficient set of labeled training examples. These text classification algorithms have been used to automatically catalog news articles (Lewis & Ringuette 1994; Joachims 1998) and web pages <ref> (Craven et al. 1998) </ref>, automatically learn the reading interests of users (Paz-zani, Muramatsu, & Billsus 1996; Lang 1995), and automatically sort electronic mail (Lewis & Knowles 1997). <p> Best performance was obtained with no feature selection, and by normalizing word counts by document length. Accuracy results are reported as averages of ten test/train splits, with 20% of the documents randomly selected for placement in the test set. The WebKB data set <ref> (Craven et al. 1998) </ref> contains web pages gathered from university computer science departments. In this paper, we use the four most populous entity-representing categories: student, faculty, course and project, all together containing 4199 pages. <p> We did not use stemming or a stoplist; we found that using a stoplist actually hurt performance because, for example, "my" is the fourth-ranked word by information gain, and is an excellent indicator of a student homepage. As done previously <ref> (Craven et al. 1998) </ref>, we use only the 2000 most informative words, as measured by average mutual information with the class variable (Yang & Pederson 1997; Joachims 1997). Accuracy results presented below are an average of twenty test/train splits, again randomly holding out 20% of the documents for testing.
Reference: <author> Dempster, A. P.; Laird, N. M.; and Rubin, D. B. </author> <year> 1977. </year> <title> Maximum likelihood from incomplete data via the EM. algorithm. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> Series B 39 </volume> <pages> 1-38. </pages>
Reference-contexts: The specific approach we describe here is based on a combination of two well-known learning algorithms: the naive Bayes classifier (Lewis & Ringuette 1994; McCallum & Nigam 1998) and the Expectation-Maximization (EM) algorithm <ref> (Dempster, Laird, & Ru-bin 1977) </ref>. The naive Bayes algorithm is one of a class of statistical text classifiers that uses word frequencies as features. Other examples include TFIDF/Rocchio (Salton 1991), Support Vector Machines (Joachims 1998), k-nearest-neighbor (Yang & Pederson 1997), exponentiated-gradient and regression models (Lewis et al. 1996).
Reference: <author> Domingos, P., and Pazzani, M. </author> <year> 1997. </year> <title> Beyond independence: Conditions for the optimality of the simple Bayesian classifier. </title> <booktitle> Machine Learning 29 </booktitle> <pages> 103-130. </pages>
Reference: <author> Friedman, J. H. </author> <year> 1997. </year> <title> On bias, variance, 0/1 loss, </title> <booktitle> and the curse-of-dimensionality. Data Mining and Knowledge Discovery 1 </booktitle> <pages> 55-77. </pages>
Reference: <author> Ghahramani, Z., and Jordan, M. </author> <year> 1994. </year> <title> Supervised learning from incomplete data via an EM approach. </title> <booktitle> In Advances in Neural Information Processing Systems (NIPS 6). </booktitle>
Reference: <author> Joachims, T. </author> <year> 1997. </year> <title> A probabilistic analysis of the Rocchio algorithm with TFIDF for text categorization. </title> <booktitle> In Intl. Conference on Machine Learning (ICML-97). </booktitle>
Reference-contexts: Experimental Results In this section, we give empirical evidence that using the algorithm in Table 1 outperforms traditional naive Bayes. We present experimental results with three different text corpora. 1 Datasets and Protocol The 20 Newsgroups data set <ref> (Joachims 1997) </ref>, collected by Ken Lang, consists of 20,017 articles divided almost evenly among 20 different UseNet discussion groups. We remove words from a stoplist of common short words and words that occur only once.
Reference: <author> Joachims, T. </author> <year> 1998. </year> <title> Text categorization with Support Vector Machines: Learning with many relevant features. </title> <booktitle> In European Conference on Machine Learning (ECML-98). </booktitle>
Reference-contexts: The naive Bayes algorithm is one of a class of statistical text classifiers that uses word frequencies as features. Other examples include TFIDF/Rocchio (Salton 1991), Support Vector Machines <ref> (Joachims 1998) </ref>, k-nearest-neighbor (Yang & Pederson 1997), exponentiated-gradient and regression models (Lewis et al. 1996). EM is a class of iterative algorithms for maximum likelihood estimation in problems with incomplete data. <p> Accuracy results presented below are an average of twenty test/train splits, again randomly holding out 20% of the documents for testing. The `ModApte' test/train split of the Reuters 21578 Distribution 1.0 data set consists of 12902 articles and 135 topic categories from the Reuters newswire. Following other studies <ref> (Joachims 1998) </ref> we present results on the 10 most populous classes, building binary classifiers for each class that include all 134 other classes in the negative category. We use a stoplist, but do not stem. Vocabulary selection, when used, is again performed with average mutual information with the class variable.
Reference: <author> Lang, K. </author> <year> 1995. </year> <title> Newsweeder: Learning to filter netnews. </title> <booktitle> In Intl. Conference on Machine Learning (ICML-95). </booktitle>
Reference: <author> Lewis, D. D., and Knowles, K. A. </author> <year> 1997. </year> <title> Threading electronic mail: A preliminary study. </title> <booktitle> Information Processing and Management 33(2) </booktitle> <pages> 209-217. </pages>
Reference-contexts: These text classification algorithms have been used to automatically catalog news articles (Lewis & Ringuette 1994; Joachims 1998) and web pages (Craven et al. 1998), automatically learn the reading interests of users (Paz-zani, Muramatsu, & Billsus 1996; Lang 1995), and automatically sort electronic mail <ref> (Lewis & Knowles 1997) </ref>. One key difficulty with these current algorithms, and the issue addressed by this paper, is that they require a large, often prohibitive, number of labeled training examples to learn accurately.
Reference: <author> Lewis, D., and Ringuette, M. </author> <year> 1994. </year> <title> A comparison of two learning algorithms for text categorization. </title> <booktitle> In Third Annual Symposium on Document Analysis and IR. </booktitle>
Reference: <author> Lewis, D.; Schapire, R.; Callan, J.; and Papka, R. </author> <year> 1996. </year> <title> Training algorithms for linear text classifiers. </title> <booktitle> In Proceedings of ACM SIGIR Conference. </booktitle>
Reference-contexts: The naive Bayes algorithm is one of a class of statistical text classifiers that uses word frequencies as features. Other examples include TFIDF/Rocchio (Salton 1991), Support Vector Machines (Joachims 1998), k-nearest-neighbor (Yang & Pederson 1997), exponentiated-gradient and regression models <ref> (Lewis et al. 1996) </ref>. EM is a class of iterative algorithms for maximum likelihood estimation in problems with incomplete data.
Reference: <author> McCallum, A., and Nigam, K. </author> <year> 1998. </year> <title> A comparison of event models for naive Bayes text classification. </title> <booktitle> In AAAI-98 Workshop on Learning for Text Categorization. </booktitle> <address> http://www.cs.cmu.edu/~mccallum. </address>
Reference-contexts: Also note that our formulation of naive Bayes assumes a multinomial event model for documents; this generally produces better text classification accuracy than another formulation that assumes a multi-variate Bernoulli <ref> (McCallum & Nigam 1998) </ref>. Incorporating Unlabeled Data with EM When naive Bayes is given just a small set of labeled training data, classification accuracy will suffer because variance in the parameter estimates of the generative model will be high.
Reference: <author> McLachlan, G., and Basford, K. </author> <year> 1988. </year> <title> Mixture Models. </title> <address> New York: </address> <publisher> Marcel Dekker. </publisher>
Reference-contexts: However, this does not show that unlabeled data aids the reduction of classification error. For example, unlabeled data does not help if there is already an infinite amount of labeled data; all parameters can be recovered from just the labeled data and the resulting classifier is Bayes-optimal <ref> (McLachlan & Basford 1988) </ref>. With an infinite amount of unlabeled data and no labeled data, the parameters can be estimated except classes cannot be matched with components, so classification error remains unimproved. But, with infinite unlabeled data and finite labeled data, there is classification improvement.
Reference: <author> Miller, D. J., and Uyar, H. S. </author> <year> 1997. </year> <title> A mixture of experts classifier with learning based on both labelled and unla-belled data. </title> <booktitle> In Advances in Neural Information Processing Systems (NIPS 9). </booktitle>
Reference: <author> Nigam, K.; McCallum, A.; Thrun, S.; and Mitchell, T. </author> <year> 1998. </year> <title> Learning to classify text from labeled and unlabeled documents. </title> <type> Technical Report CMU-CS-98-120, </type> <institution> Carnegie Mellon University. </institution>
Reference-contexts: We also introduce a method for balancing the contributions of the labeled and unlabeled data that avoids degradation in classification accuracy when these conditions are violated. A more detailed version of this paper is available <ref> (Nigam et al. 1998) </ref>. The Probabilistic Framework To ground the theoretical aspects of our work, and to provide a setting for our algorithm, this section presents a probabilistic framework for characterizing the nature of documents and classifiers. <p> Also note that our formulation of naive Bayes assumes a multinomial event model for documents; this generally produces better text classification accuracy than another formulation that assumes a multi-variate Bernoulli <ref> (McCallum & Nigam 1998) </ref>. Incorporating Unlabeled Data with EM When naive Bayes is given just a small set of labeled training data, classification accuracy will suffer because variance in the parameter estimates of the generative model will be high.
Reference: <author> Pazzani, M. J.; Muramatsu, J.; and Billsus, D. </author> <year> 1996. </year> <title> Syskill & Webert: Identifying interesting Web sites. </title> <booktitle> In Proceedings of AAAI-96. </booktitle>
Reference: <author> Salton, G. </author> <year> 1991. </year> <title> Developments in automatic text retrieval. </title> <booktitle> Science 253 </booktitle> <pages> 974-979. </pages>
Reference-contexts: The naive Bayes algorithm is one of a class of statistical text classifiers that uses word frequencies as features. Other examples include TFIDF/Rocchio <ref> (Salton 1991) </ref>, Support Vector Machines (Joachims 1998), k-nearest-neighbor (Yang & Pederson 1997), exponentiated-gradient and regression models (Lewis et al. 1996). EM is a class of iterative algorithms for maximum likelihood estimation in problems with incomplete data.
Reference: <author> Shahshahani, B., and Landgrebe, D. </author> <year> 1994. </year> <title> The effect of unlabeled samples in reducing the small sample size problem and mitigating the Hughes phenomenon. </title> <journal> IEEE Trans. on Geoscience and Remote Sensing 32(5) </journal> <pages> 1087-1095. </pages>
Reference: <author> Vapnik, V. </author> <year> 1982. </year> <title> Estimation of dependences based on empirical data. </title> <publisher> Springer. </publisher>
Reference-contexts: With labeled training documents, D = fd 1 ; : : : ; d jDj g, we can calculate Bayes-optimal estimates for the parameters of the model that generated these documents <ref> (Vapnik 1982) </ref>. To calculate the probability of a word given a class, w t jc j , simply count the fraction of times the word occurs in the data for that class, augmented with a Laplacean prior. This smoothing prevents zero probabilities for infrequently occurring words.
Reference: <author> Yang, Y., and Pederson, J. </author> <year> 1997. </year> <title> Feature selection in statistical learning of text categorization. </title> <booktitle> In Intl Conference on Machine Learning (ICML-97), </booktitle> <pages> 412-420. </pages>
Reference-contexts: The naive Bayes algorithm is one of a class of statistical text classifiers that uses word frequencies as features. Other examples include TFIDF/Rocchio (Salton 1991), Support Vector Machines (Joachims 1998), k-nearest-neighbor <ref> (Yang & Pederson 1997) </ref>, exponentiated-gradient and regression models (Lewis et al. 1996). EM is a class of iterative algorithms for maximum likelihood estimation in problems with incomplete data.
References-found: 23

