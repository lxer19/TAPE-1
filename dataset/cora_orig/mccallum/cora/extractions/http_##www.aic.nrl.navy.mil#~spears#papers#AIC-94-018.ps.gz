URL: http://www.aic.nrl.navy.mil/~spears/papers/AIC-94-018.ps.gz
Refering-URL: http://www.aic.nrl.navy.mil/~spears/pubs.html
Root-URL: 
Email: spears@aic.nrl.navy.mil gordon@aic.nrl.navy.mil  
Phone: (202) 767-9006  
Title: A Simpler Look at Consistency  
Author: William M. Spears Diana F. Gordon 
Keyword: Consistency bias, simplicity bias, supervised concept learning  
Note: Submitted as a technical note to the Special Issue of Machine Learning Journal on "Bias Evaluation and Selection"  
Address: Washington, D.C. 20375 USA  
Affiliation: Naval Research Laboratory  
Abstract: One of the major goals of most early concept learners was to find hypotheses that were perfectly consistent with the training data. It was believed that this goal would indirectly achieve a high degree of predictive accuracy on a set of test data. Later research has partially disproved this belief. However, the issue of consistency has not yet been resolved completely. We examine the issue of consistency from a new perspective. To avoid overfitting the training data, a considerable number of current systems have sacrificed the goal of learning hypotheses that are perfectly consistent with the training instances by setting a goal of hypothesis simplicity (Occam's razor). Instead of using simplicity as a goal, we have developed a novel approach that addresses consistency directly. In other words, our concept learner has the explicit goal of selecting the most appropriate degree of consistency with the training data. We begin this paper by exploring concept learning with less than perfect consistency. Next, we describe a system that can adapt its degree of consistency in response to feedback about predictive accuracy on test data. Finally, we present the results of initial experiments that begin to address the question of how tightly hypotheses should fit the training data for different problems. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Angluin, D. & Laird, P. </author> <year> (1988). </year> <title> Learning from Noisy Examples. </title> <journal> Machine Learning, </journal> <volume> 2. </volume>
Reference-contexts: Our fourth direction for future research relates to the results in computational learning theory. Valiant (1984) has introduced the criterion of Probably Approximately Correct (PAC) identification of a target concept. Recently, a number of researchers have considered the computational feasibility of PAC identification in the context of noisy examples <ref> (e.g., Angluin & Laird, 1988) </ref>. However, they assume the strategy is to maximize consistency with the training sample. It would be interesting to also explore the computational feasibility of PAC identification assuming a strategy of lower consistency.
Reference: <author> Breiman, L., Friedman, J., Olshen, R., & Stone, C. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <type> Belmont: Wadsworth. 23 Buntine, </type> <institution> W. </institution> <year> (1991). </year> <title> Classifiers: A theoretical and empirical study. </title> <booktitle> In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence. </booktitle>
Reference: <author> De Jong, K., Spears, W., & Gordon, D. </author> <year> (1993). </year> <title> Using genetic algorithms for concept learning. </title> <journal> Machine Learning, </journal> <volume> 14. </volume>
Reference-contexts: In this paper, we examine one learning algorithm and consider the effects of varying the consistency level. Section 2 describes our concept learner, called the Genetic Algorithm Batch-Incremental Learner (GABIL), that we use in all experiments <ref> (De Jong et al. 1993) </ref>. Section 3 describes a modified version of GABIL that can learn concepts with different levels of consistency. This section also presents experimental results that compare predictive accuracy with different consistency levels on both clean and noisy data and a variety of target concepts.
Reference: <author> Fayyad, U. </author> <year> (1993). </year> <title> SKICAT: A machine learning system for automated cataloging of large scale sky surveys. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning. </booktitle>
Reference: <author> Fisher, D. & Schlimmer, D. </author> <year> (1988). </year> <title> Concept Simplification and Prediction Accuracy. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning. </booktitle>
Reference: <author> Michalski, R. </author> <year> (1983). </year> <title> A theory and methodology of inductive learning. </title> <editor> In R. Michalski, </editor> <publisher> J. </publisher>
Reference-contexts: 1. Introduction Early studies in supervised concept learning made the implicit assumption that the best method for obtaining high predictive accuracy on a test set is to find hypotheses that are perfectly consistent with respect to all examples in a training set <ref> (e.g., Michalski, 1983) </ref>. A positive hypothesis (i.e., a hypothesis intended to cover the positive examples) is 100% consistent with respect to a set of examples if it covers all positive examples and no negative examples in the set.
Reference: <editor> Carbonell, & T. Mitchell (Eds.), </editor> <booktitle> Machine learning: An Artificial Intelligence Approach (Vol. 1). </booktitle> <address> Palo Alto: </address> <publisher> Tioga. </publisher>
Reference: <author> Michalski, R. </author> <year> (1990). </year> <title> Learning flexible concepts: Fundamental ideas and a method based on two-tiered representation. </title> <editor> In Y. Kodratoff, R. Michalski (Eds.), </editor> <booktitle> Machine learning: An Artificial Intelligence Approach (Vol. </booktitle> <address> 3) San Mateo: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: CART, a system developed by Breiman et al. (1984), dynamically varies the amount of pruning by using cross-validation. In this respect CART is similar to AGABIL; however CART differs from AGABIL because it varies simplicity rather than con sistency. 6 The removal of hypothesis disjuncts <ref> (e.g., Michalski, 1990) </ref> is an effective method to increase the simplicity of DNF hypotheses that are in the form of classification rules. This method may sacri fice 100% consistency because the removed disjuncts may uniquely cover some of the training examples.
Reference: <author> Mitchell, T. </author> <year> (1978). </year> <title> Version spaces: An approach to concept learning. </title> <type> Ph.D. thesis, </type> <institution> Stan-ford University, Stanford, </institution> <address> CA. </address>
Reference-contexts: Related work There have been many methods for handling noisy data, such as weighted hypotheses (Schlimmer & Granger, 1986), Bayesian approaches (Buntine, 1991), multiple version spaces <ref> (Mitchell, 1978) </ref>, and tree pruning (Quinlan, 1987; Breiman et al., 1984). The goal of our research is to vary the consistency level to improve predictive accuracy. No previous research has had precisely the same goal. The most closely related research investigates the effectiveness of a simplicity bias.
Reference: <author> Pazzani, P. & Sarrett, W. </author> <year> (1992). </year> <title> A framework for average case analysis of conjunctive learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 9. </volume>
Reference-contexts: We would also like to gain a theoretical understanding of why GABIL improves its predictive accuracy when using lower levels of consistency on more complex target concepts and noisier data. Perhaps an average 22 case analysis <ref> (Pazzani & Sarrett, 1992) </ref> would be appropriate for this task. Finally, we would like to mention that although our experimental results are preliminary, we hope that they will inspire others to further investigate the regions of appropriateness of varied levels of consistency.
Reference: <author> Quinlan, J. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1. </volume>
Reference: <author> Quinlan, J. </author> <year> (1987). </year> <title> Simplifying decision trees. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 27. 24 Quinlan, </volume> <editor> J. & Rivest, R. </editor> <year> (1989). </year> <title> Inferring decision trees using the minimum description length principle. </title> <journal> Information and Computation, </journal> <volume> 80. </volume>
Reference-contexts: There exists a variety of criteria, such as minimum description-length and reduced entropy, that can be used to achieve hypothesis simplicity (Quinlan & Rivest, 1989; Quinlan, 1986). Simplicity biases have been implemented with two of the most widely used hypothesis representations: decision trees and classification rules. Pruning <ref> (e.g., Quinlan, 1987) </ref> is a popular method for increasing the simplicity of decision trees. Pruning can reduce the consistency level because each decision tree branch that is pruned away may contain information for distinguishing the classifications of the training instances. After pruning, this information is lost.
Reference: <author> Schaffer, C. </author> <year> (1993). </year> <title> Overfitting avoidance as bias. </title> <journal> Machine Learning, </journal> <volume> 10. </volume>
Reference: <author> Schlimmer, J. & Granger, R. </author> <year> (1986). </year> <title> Incremental learning from noisy data. </title> <journal> Machine Learning, </journal> <volume> 1. </volume>
Reference-contexts: Examples N% 90 0 100 200 Examples N% 90 0 100 200 18 Examples N% 90 0 100 200 Examples N% 90 0 100 200 5. Related work There have been many methods for handling noisy data, such as weighted hypotheses <ref> (Schlimmer & Granger, 1986) </ref>, Bayesian approaches (Buntine, 1991), multiple version spaces (Mitchell, 1978), and tree pruning (Quinlan, 1987; Breiman et al., 1984). The goal of our research is to vary the consistency level to improve predictive accuracy. No previous research has had precisely the same goal.
Reference: <author> Valiant, L. </author> <year> (1984). </year> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27. </volume> <pages> 25 </pages>
References-found: 15

