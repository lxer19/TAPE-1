URL: http://iacoma.cs.uiuc.edu/iacoma-papers/adva.ps
Refering-URL: http://iacoma.cs.uiuc.edu/papers.html
Root-URL: http://www.cs.uiuc.edu
Title: Architectural Advances in DSMs: A Possible Road Ahead  
Author: Josep Torrellas 
Date: August 1998  
Web: http://iacoma.cs.uiuc.edu  
Address: USA  
Affiliation: Computer Science Department University of Illinois at Urbana-Champaign,  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Semiconductor Industry Association. The National Technology Roadmap for Semiconductors. SIA, </institution> <year> 1994. </year>
Reference-contexts: Unfortunately, it is less clear that the considerable cost of application tuning will be reduced significantly. The major drive behind these new architectural technologies is the increasing integration of transistors on a chip, as documented by the Semiconductor Industry Association <ref> [1] </ref>. This trend will not ensure ever faster processor frequencies, as technological constraints are believed to bound frequencies of traditional-technology processors to a few GHz. Instead, the higher performance will come from the fact that high transistor density will enable both more system integration and more sophisticated designs.
Reference: [2] <author> S. Basu and J. Torrellas. </author> <title> Enhancing Memory Use in Simple Coma: Multiplexed Simple Coma. </title> <booktitle> In Proceedings of the 4th International Symposium on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1998. </year>
Reference-contexts: Remote cache organization can be optimized [8] and possibly combined with another cache organization called page cache [4], to minimize the time wasted in remote accesses. An interesting approach is Simple COMA, where pages of data are automatically migrated and replicated in the memories <ref> [2, 13] </ref>. In this organization, while space in memory is allocated at page granularity, the unit of coherence is still a memory line. This approach is attractive because it eliminates remote memory accesses transparently at a moderate cost.
Reference: [3] <author> K. Ekanadham, B.-H. Lim, P. Pattnaik, and M. Snir. </author> <title> PRISM: An Integrated Architecture for Scalable Shared Memory. </title> <booktitle> In Proceedings of the 4th International Symposium on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1998. </year>
Reference-contexts: This approach is attractive because it eliminates remote memory accesses transparently at a moderate cost. Best results can be obtained by some selective combination of COMA and traditional CC-NUMA <ref> [3, 4] </ref>. Alternatively, the operating system can also move pages transparently [17].
Reference: [4] <author> B. Falsafi and D. Wood. </author> <title> Reactive NUMA: A Design for Unifying S-COMA and CC-NUMA. </title> <booktitle> In Proceedings of the 24th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 229-240, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: Larger remote caches reduce the number of long-latency remote accesses, although they also tend to slow down the remaining remote accesses. Remote cache organization can be optimized [8] and possibly combined with another cache organization called page cache <ref> [4] </ref>, to minimize the time wasted in remote accesses. An interesting approach is Simple COMA, where pages of data are automatically migrated and replicated in the memories [2, 13]. In this organization, while space in memory is allocated at page granularity, the unit of coherence is still a memory line. <p> This approach is attractive because it eliminates remote memory accesses transparently at a moderate cost. Best results can be obtained by some selective combination of COMA and traditional CC-NUMA <ref> [3, 4] </ref>. Alternatively, the operating system can also move pages transparently [17].
Reference: [5] <institution> IBM Corporation. </institution> <address> http://www.chips.ibm.com/ services/foundry/offerings/cmos/7ld. </address> <year> 1998. </year>
Reference-contexts: Signals from the processor do not need to leave the package to access memory, which reduces the delays considerably. A more aggressive and possibly a bit more far-off technology is to integrate both the processor and a large amount of DRAM on the same chip <ref> [5] </ref>. The resulting architecture is called processor-in-memory (PIM) or intelligent memory [6, 10, 12]. With this technology, we can put many megabytes of memory within the reach of an on-chip processor access (20-40 ns).
Reference: [6] <author> P. Kogge, S. Bass, J. Brockman, D. Chen, and E. Sha. Pursuing a Petaflop: </author> <title> Point Designs for 100 TF Computers Using PIM Technologies. </title> <booktitle> In Proceedings of the 1996 Frontiers of Massively Parallel Computation Symposium, </booktitle> <pages> pages 88-97, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: A more aggressive and possibly a bit more far-off technology is to integrate both the processor and a large amount of DRAM on the same chip [5]. The resulting architecture is called processor-in-memory (PIM) or intelligent memory <ref> [6, 10, 12] </ref>. With this technology, we can put many megabytes of memory within the reach of an on-chip processor access (20-40 ns). At current integration levels, we can now include in a PIM chip the memory that comes with the average PC.
Reference: [7] <author> V. Krishnan and J. Torrellas. </author> <title> Hardware and Software Support for Speculative Execution of Sequential Binaries on a Chip-Multiprocessor. </title> <booktitle> In Proceedings of the 1998 International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1998. </year>
Reference-contexts: Alternatively, instead of integrating processor and memory, a second trend is to integrate multiple compute engines on the chip. Several designs have been proposed <ref> [7, 9, 14, 16] </ref>. One such chip can include, for example, several plain superscalar processors with their private L1 caches, an interconnect, and a shared L2 cache. Alternatively, the compute engines can be one or several multithreaded processors, with different cache and functional unit sharing arrangements.
Reference: [8] <author> A. Moga and M. Dubois. </author> <title> The Effectiveness of SRAM Network Caches in Clustered DSMs. </title> <booktitle> In Proceedings of the 4th International Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 103-112, </pages> <month> February </month> <year> 1998. </year>
Reference-contexts: Its purpose is to hold the fraction of the working set data whose home is remote [19]. Larger remote caches reduce the number of long-latency remote accesses, although they also tend to slow down the remaining remote accesses. Remote cache organization can be optimized <ref> [8] </ref> and possibly combined with another cache organization called page cache [4], to minimize the time wasted in remote accesses. An interesting approach is Simple COMA, where pages of data are automatically migrated and replicated in the memories [2, 13].
Reference: [9] <author> K. Olukotun, B. Nayfeh, L. Hammond, K. Wilson, and K. Chang. </author> <title> The Case for a Single-Chip Multiprocessor. </title> <booktitle> In 7th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 2-11, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: Alternatively, instead of integrating processor and memory, a second trend is to integrate multiple compute engines on the chip. Several designs have been proposed <ref> [7, 9, 14, 16] </ref>. One such chip can include, for example, several plain superscalar processors with their private L1 caches, an interconnect, and a shared L2 cache. Alternatively, the compute engines can be one or several multithreaded processors, with different cache and functional unit sharing arrangements.
Reference: [10] <author> D. Patterson, T. Anderson, N. Cardwell, R. Fromm, K. Keeton, C. Kozyrakis, R. Tomas, and K. </author> <note> Yelick. </note>
Reference-contexts: A more aggressive and possibly a bit more far-off technology is to integrate both the processor and a large amount of DRAM on the same chip [5]. The resulting architecture is called processor-in-memory (PIM) or intelligent memory <ref> [6, 10, 12] </ref>. With this technology, we can put many megabytes of memory within the reach of an on-chip processor access (20-40 ns). At current integration levels, we can now include in a PIM chip the memory that comes with the average PC.
References-found: 10

