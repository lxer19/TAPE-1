URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/MP-TR-96-05/MP-TR-96-05.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/MP-TR-96-05/
Root-URL: http://www.cs.wisc.edu
Title: Mathematical Programming in Data Mining  
Author: O. L. Mangasarian 
Abstract: Mathematical programming approaches to three fundamental problems will be described: feature selection, clustering and robust representation. The feature selection problem considered is that of discriminating between two sets while recognizing irrelevant and redundant features and suppressing them. This creates a lean model that often generalizes better to new unseen data. Computational results on real data confirm improved generalization of leaner models. Clustering is exemplified by the unsupervised learning of patterns and clusters that may exist in a given database and is a useful tool for knowledge discovery in databases (KDD). A mathematical programming formulation of this problem is proposed that is theoretically justifiable and computationally implementable in a finite number of steps. A resulting k-Median Algorithm is utilized to discover very useful survival curves for breast cancer patients from a medical database. Robust representation is concerned with minimizing trained model degradation when applied to new problems. A novel approach is proposed that purposely tolerates a small error in the training process in order to avoid overfitting data that may contain errors. Examples of applications of these concepts are given.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Al-Sultan. </author> <title> A Tabu search approach to the clustering problem. </title> <journal> Pattern Recognition, </journal> <volume> 28(9) </volume> <pages> 1443-1451, </pages> <year> 1995. </year>
Reference-contexts: If some of the features are discrete and can be represented as integers, then the techniques of integer programming [24, 55, 14] can be employed. Integer programming approaches have been applied for example to clustering problems <ref> [64, 1] </ref>, but will not be described here, principally because the combinatorial approach is fundamentally different than the analytical approach of optimization with real variables. Stochastic optimization methods based on simulated annealing have also been used in problems of inductive concept learning [50]. <p> There are many approaches to this problem, including statistical [32], machine learning [20], integer and mathematical programming approaches <ref> [64, 1, 58, 11] </ref>. We shall describe here the recent approach of [11] that utilizes a fast bilinear programming approach: minimizing the product of two linear functions on a set defined by linear inequalities.
Reference: [2] <author> K. P. Bennett and E. J. Bredensteiner. </author> <title> Geometry in learning. </title> <editor> In C. Gorini, E. Hart, W. Meyer, and T. Phillips, editors, </editor> <booktitle> Geometry at Work, </booktitle> <address> Washington, D.C., </address> <year> 1997. </year> <journal> Mathematical Association of America. </journal> <note> To appear, www.rpi.math.edu/ bennek/geometry2.ps. </note>
Reference-contexts: In a somewhat related approach Vapnik has proposed a quadratic program for constructing a plane to obtain a smallest probability of error for separating two point sets [70, Section 5.4]. Bennett and Bredensteiner <ref> [2] </ref> have formulated a similar problem using linear programming. Vapnik [70, Section 5.9] also makes use of Huber's robust regression ideas [30] and extends the latter's robust regression loss function [70, p 152] by adding an *- insensitive zone to it.
Reference: [3] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Neural network training via linear programming. </title> <editor> In P. M. Pardalos, editor, </editor> <booktitle> Advances in Optimization and Parallel Computing, </booktitle> <pages> pages 56-67, </pages> <address> Amsterdam, 1992. </address> <publisher> North Holland. </publisher>
Reference-contexts: programming, that is optimization subject to constraints, is a broad discipline that has been applied to a great variety of theoretical and applied problems such as operations research [29, 54], network problems [60, 53], game theory and economics [71, 35], engineering mechanics [57, 37] and more recently to machine learning <ref> [3, 61, 23, 48, 46] </ref>. In this paper we describe three recent mathematical-programming-based developments that are relevant to data mining: feature selection [45, 10], clustering [11] and robust representation [67].
Reference: [4] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Robust linear programming discrimination of two linearly inseparable sets. </title> <journal> Optimization Methods and Software, </journal> <volume> 1 </volume> <pages> 23-34, </pages> <year> 1992. </year>
Reference-contexts: This leads to the following Robust Linear Programming formulation <ref> [4] </ref>: RLP min 8 &gt; : m e T z fi fi fi fi Bw efl + e z; 9 &gt; ; Robustness here refers to the fact that the useless null vector (w = 0) is naturally excluded as a solution of (7), which is not the case in other <p> The linear programming formulation (7) which has a number of natural theoretical properties including robustness is also very effective computationally <ref> [4, 49] </ref>. However it does not address the problem of suppressing irrelevant features.
Reference: [5] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Bilinear separation of two sets in n-space. </title> <journal> Computational Optimization & Applications, </journal> <volume> 2 </volume> <pages> 207-227, </pages> <year> 1993. </year>
Reference-contexts: Note that the constraints of (12) are uncoupled in the variables (C; D) and the variable T . Hence the Uncoupled Bilinear Program Algorithm UBPA <ref> [5, Algorithm 2.1] </ref> is applicable. Simply stated, this algorithm alternates between solving a linear program in the variable T and a linear program in the variables (C; D). <p> Simply stated, this algorithm alternates between solving a linear program in the variable T and a linear program in the variables (C; D). The algorithm terminates in a finite number of iterations at a stationary point satisfying the minimum principle necessary optimality condition for problem (12) <ref> [5, Theorem 2.1] </ref>. We note however, because of the simple structure the bilinear program (12), the two linear programs can be solved explicitly in closed form. This leads to the following algorithmic implementation.
Reference: [6] <author> D. P. Bertsekas. </author> <title> Nonlinear Programming. </title> <publisher> Athena Scientific, </publisher> <address> Belmont, MA, </address> <year> 1995. </year>
Reference-contexts: We will, however, point out other approaches that are mostly not based on mathematical programming. The fundamental nonlinear programming problem <ref> [6, 44] </ref> consists of minimizing an objective function subject to inequality and equality constraints and is typically written as follows min f (x) subject to g (x) 0; h (x) = 0; (1) where x is an n-dimensional vector of real variables, f is a real-valued function of x, g and <p> However in some inherently nonlinear problems where for example the parameters of a separating surface appear nonlinearly, one may have to resort to nonlinear models and the theory and algorithms of nonlinear programming <ref> [42, 6] </ref>. This would again be a promising problem to pursue. We conclude with the hope that the problems solved demonstrate the theoretical and computational potential of mathematical programming as a versatile and effective tool for solving important problems in data mining and knowledge discovery in databases. 14
Reference: [7] <author> D. P. Bertsekas and J. N. Tsitsiklis. </author> <title> Parallel and Distributed Computation. </title> <publisher> Prentice-Hall, Inc, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1989. </year>
Reference-contexts: As mentioned earlier, since linear programs with millions of variables can be solved with present day state-of-the-art methods, large-scale databases are amenable to the proposed linear-programming-based methods. In addition there is a large body of literature on the parallel solution and decomposition of large scale mathematical programs <ref> [7, 16, 17, 19] </ref> where for many of the algorithms only part of the data is loaded into memory at a time. Such parallel and decomposition algorithms extend further the applicability of the proposed methods to very large scale databases.
Reference: [8] <author> R. E. Bixby, J. W. Gregory, I. J. Lustig, R. E. Marsten, and D. F. Shanno. </author> <title> Very large-scale linear programming: a case study combining interior point and simplex methods. </title> <journal> Operations Research, </journal> <volume> 40 </volume> <pages> 885-897, </pages> <year> 1992. </year>
Reference-contexts: Extremely fast linear and quadratic programming codes [14] that are capable of solving linear programs with millions of variables <ref> [8, 40] </ref> and very large quadratic programs, make the proposed algorithms easily scalable and effective for solving a wide range of problems. One limitation however is that the problem features must be real numbers or easily mapped into real numbers.
Reference: [9] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. </author> <title> Occam's razor. </title> <journal> Information Processing Letters, </journal> <volume> 24 </volume> <pages> 377-380, </pages> <year> 1987. </year>
Reference-contexts: Hence it can be considered as an application of Occam's "law of parsimony", also known as Occam's Razor <ref> [65, 9] </ref>, which states: "What can be done with fewer [assumptions] is done in vain with more". There are statistical [22], machine learning [39, 33] as well as mathematical programming [12, 45, 10] approaches to the feature selection problem.
Reference: [10] <author> P. S. Bradley, O. L. Mangasarian, and W. N. </author> <title> Street. Feature selection via mathematical programming. </title> <type> Technical Report 95-21, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin, </institution> <month> December </month> <year> 1995. </year> <note> INFORMS Journal on Computing, submitted. Available by ftp://ftp.cs.wisc.edu/math-prog/tech-reports/95-21.ps.Z. </note>
Reference-contexts: In this paper we describe three recent mathematical-programming-based developments that are relevant to data mining: feature selection <ref> [45, 10] </ref>, clustering [11] and robust representation [67]. We note at the outset that we do not plan to survey either the fields of data mining or mathematical programming, but rather highlight some recent and highly effective applications of the latter to the former. <p> Feature Selection The feature selection problem treated is that of discriminating between two finite point sets in n-dimensional feature space by a separating plane that utilizes as few of the features as possible. The problem is formulated as a mathematical program with a parametric objective function and linear constraints <ref> [10] </ref>. A step function that appears in the objective function is approximated by a concave exponential on the nonnegative real line instead of the conventional sigmoid function of neural networks [28]. <p> Hence it can be considered as an application of Occam's "law of parsimony", also known as Occam's Razor [65, 9], which states: "What can be done with fewer [assumptions] is done in vain with more". There are statistical [22], machine learning [39, 33] as well as mathematical programming <ref> [12, 45, 10] </ref> approaches to the feature selection problem. In this work we shall deal principally with the latter because of the novelty of the approach and it effectiveness. <p> For the sake of simplicity and because it is not needed computationally, we shall forgo this transformation here.) A fast, finitely-terminating successive linear programming algorithm has been proposed for solving this problem <ref> [10] </ref> as follows. Algorithm 2.1 Successive Linearization Algorithm (SLA) for FSV (8). Choose 2 [0; 1). Start with a random (w 0 ; fl 0 ; y 0 ; z 0 ; v 0 ). <p> Thus in the terminology of our formulation (8), n = 32; m = 28 and k = 118. For this problem the separating plane obtained by the Successive Linearization Algorithm 2.1 with = 0:05 used only 4 features out of 32, while increasing tenfold cross-validation correctness by 35.4% <ref> [10] </ref>. If other values of the parameter are used in the Successive Linearization Algorithm 2.1, then the number of features that determine the separating plane will vary between 1 and 32.
Reference: [11] <author> P. S. Bradley, O. L. Mangasarian, and W. N. </author> <title> Street. Clustering via concave minimization. </title> <type> Technical Report 96-03, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin, </institution> <month> May </month> <year> 1996. </year> <booktitle> Advances in Neural Information Processing Systems 9, </booktitle> <publisher> MIT Press, </publisher> <address> Cam-bridge, MA 1997, </address> <note> to appear. Available by ftp://ftp.cs.wisc.edu/math-prog/tech-reports/96-03.ps.Z. </note>
Reference-contexts: In this paper we describe three recent mathematical-programming-based developments that are relevant to data mining: feature selection [45, 10], clustering <ref> [11] </ref> and robust representation [67]. We note at the outset that we do not plan to survey either the fields of data mining or mathematical programming, but rather highlight some recent and highly effective applications of the latter to the former. <p> the 1-norm distance) is used, the problem can be formulated as that of minimizing a piecewise-linear concave function on a polyhedral set which is shown to be equivalent to a bilinear program: minimizing the product of two linear functions on a set determined by satisfying a system of linear inequalities <ref> [11] </ref>. Although a bilinear program is a nonconvex optimization problem (i.e. minimizing a function that is not valley-like), a fast finite k-Median Algorithm consisting of solving few linear programs in closed form leads to a stationary point. <p> There are many approaches to this problem, including statistical [32], machine learning [20], integer and mathematical programming approaches <ref> [64, 1, 58, 11] </ref>. We shall describe here the recent approach of [11] that utilizes a fast bilinear programming approach: minimizing the product of two linear functions on a set defined by linear inequalities. <p> There are many approaches to this problem, including statistical [32], machine learning [20], integer and mathematical programming approaches [64, 1, 58, 11]. We shall describe here the recent approach of <ref> [11] </ref> that utilizes a fast bilinear programming approach: minimizing the product of two linear functions on a set defined by linear inequalities. <p> Clustering as a Bilinear Program The clustering problem (11) is equivalent to the following bilinear program: minimize C ` 2R n ;D i` 2R n ;T i` 2R n i=1 `=1 e T D i` T i` subject to D i` A T P k (12) This essentially obvious result <ref> [11, Proposition 2.2] </ref> can be seen from the fact that, for a fixed i, setting all the components of T i` ; ` = 1; : : : ; k equal to zero except one corresponding to a smallest e T D i` , with respect to `, equal to 1,
Reference: [12] <author> E. J. Bredensteiner and K. P. Bennett. </author> <title> Feature minimization within decision trees. </title> <note> Department of Mathematical Sciences Math Report No. 218, </note> <institution> Rensselaer Polytechnic Institute, </institution> <address> Troy, NY 12180, </address> <year> 1995. </year>
Reference-contexts: Hence it can be considered as an application of Occam's "law of parsimony", also known as Occam's Razor [65, 9], which states: "What can be done with fewer [assumptions] is done in vain with more". There are statistical [22], machine learning [39, 33] as well as mathematical programming <ref> [12, 45, 10] </ref> approaches to the feature selection problem. In this work we shall deal principally with the latter because of the novelty of the approach and it effectiveness. <p> The model that we shall adopt will be that of a perceptron or linear threshold unit (LTU) that employs as few of the features of the given problem as possible. Extensions to more complex models leading to compact and accurate decision trees have also been made <ref> [12] </ref>.
Reference: [13] <author> F. Cordellier and J. Ch. Fiorot. </author> <title> On the Fermat-Weber problem with convex cost functionals. </title> <journal> Mathematical Programming, </journal> <volume> 14 </volume> <pages> 295-311, </pages> <year> 1978. </year>
Reference-contexts: Without this squared distance term, the subproblem of the k-Mean Algorithm becomes the considerably harder Weber problem <ref> [56, 13] </ref> which locates a center in R n closest in sum of Euclidean distances (not their squares!) to a finite set of given points. The Weber problem has no closed form solution.
Reference: [14] <author> CPLEX Optimization Inc., </author> <title> Incline Village, Nevada. Using the CPLEX(TM) Linear Optimizer and CPLEX(TM) Mixed Integer Optimizer (Version 2.0), </title> <booktitle> 1992. </booktitle> <pages> 15 </pages>
Reference-contexts: From the point of view of applicability to large-scale data mining problems, the proposed algorithms employ either linear programming (Sections 2 and 3) which is polynomial-time-solvable [36, 69], or convex quadratic programming (Section 4) which is also polynomial-time-solvable [69]. Extremely fast linear and quadratic programming codes <ref> [14] </ref> that are capable of solving linear programs with millions of variables [8, 40] and very large quadratic programs, make the proposed algorithms easily scalable and effective for solving a wide range of problems. <p> One limitation however is that the problem features must be real numbers or easily mapped into real numbers. If some of the features are discrete and can be represented as integers, then the techniques of integer programming <ref> [24, 55, 14] </ref> can be employed. Integer programming approaches have been applied for example to clustering problems [64, 1], but will not be described here, principally because the combinatorial approach is fundamentally different than the analytical approach of optimization with real variables. <p> The well-understood polynomial-time finite termination of linear and quadratic programming interior methods has led to very powerful and reliable commercial software such as CPLEX <ref> [14] </ref> that can easily and reliably implement all the proposed algorithms.
Reference: [15] <author> G. B. Dantzig. </author> <title> Linear Programming and Extensions. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey, </address> <year> 1963. </year>
Reference-contexts: If all the functions f , g and h are linear then the problem simplifies to a linear program <ref> [15, 52, 69] </ref> which is the classical problem of mathematical fl Mathematical Programming Technical Report 96-05, August 1996 Revised November 1996 & March 1997.
Reference: [16] <author> R. De Leone and O. L. Mangasarian. </author> <title> Serial and parallel solution of large scale linear programs by augmented Lagrangian successive overrelaxation. </title> <editor> In A. Kurzhanski, K. Neumann, and D. Pallaschke, editors, </editor> <booktitle> Optimization, Parallel Processing and Applications, </booktitle> <pages> pages 103-124, </pages> <address> Berlin, </address> <year> 1988. </year> <note> Springer-Verlag. Lecture Notes in Economics and Mathematical Systems 304. </note>
Reference-contexts: As mentioned earlier, since linear programs with millions of variables can be solved with present day state-of-the-art methods, large-scale databases are amenable to the proposed linear-programming-based methods. In addition there is a large body of literature on the parallel solution and decomposition of large scale mathematical programs <ref> [7, 16, 17, 19] </ref> where for many of the algorithms only part of the data is loaded into memory at a time. Such parallel and decomposition algorithms extend further the applicability of the proposed methods to very large scale databases.
Reference: [17] <author> R. De Leone and M. A. Tork Roth. </author> <title> Massively parallel solution of quadratic programs via successive overrelaxation. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 5 </volume> <pages> 623-634, </pages> <year> 1993. </year>
Reference-contexts: As mentioned earlier, since linear programs with millions of variables can be solved with present day state-of-the-art methods, large-scale databases are amenable to the proposed linear-programming-based methods. In addition there is a large body of literature on the parallel solution and decomposition of large scale mathematical programs <ref> [7, 16, 17, 19] </ref> where for many of the algorithms only part of the data is loaded into memory at a time. Such parallel and decomposition algorithms extend further the applicability of the proposed methods to very large scale databases.
Reference: [18] <author> U. Fayyad, G. Piatetsky-Shapiro, and P. Smyth. </author> <title> The KDD process for extracting useful knowledge from volumes of data. </title> <journal> Communications of the ACM, </journal> <volume> 39 </volume> <pages> 27-34, </pages> <year> 1996. </year>
Reference-contexts: Although a bilinear program is a nonconvex optimization problem (i.e. minimizing a function that is not valley-like), a fast finite k-Median Algorithm consisting of solving few linear programs in closed form leads to a stationary point. Computational testing of this algorithm as a KDD tool <ref> [18] </ref> has been quite encouraging. <p> The latter heuristic was used in our computational results. To test the effectiveness of the k-Median Algorithm it was used as a KDD tool <ref> [18] </ref> to mine the Wisconsin Prognostic Breast Cancer Database (WPBC) in order to discover medical knowledge. For such medical databases, extracting well-separated survival curves provides an essential prognostic tool. Survival curves [34, 38] give expected percent of surviving patients as a function of time.
Reference: [19] <author> M. C. Ferris and O. L. Mangasarian. </author> <title> Parallel variable distribution. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 4(4) </volume> <pages> 815-832, </pages> <year> 1994. </year>
Reference-contexts: As mentioned earlier, since linear programs with millions of variables can be solved with present day state-of-the-art methods, large-scale databases are amenable to the proposed linear-programming-based methods. In addition there is a large body of literature on the parallel solution and decomposition of large scale mathematical programs <ref> [7, 16, 17, 19] </ref> where for many of the algorithms only part of the data is loaded into memory at a time. Such parallel and decomposition algorithms extend further the applicability of the proposed methods to very large scale databases.
Reference: [20] <author> D. Fisher. </author> <title> Knowledge acquisition via incremental conceptual clustering. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 139-172, </pages> <year> 1987. </year>
Reference-contexts: There are many approaches to this problem, including statistical [32], machine learning <ref> [20] </ref>, integer and mathematical programming approaches [64, 1, 58, 11]. We shall describe here the recent approach of [11] that utilizes a fast bilinear programming approach: minimizing the product of two linear functions on a set defined by linear inequalities.
Reference: [21] <author> Christodoulos A. </author> <title> Floudas. Nonlinear and Mixed-Integer Optimization : Fundamentals and Applications (Topics in Chemical Engineering. </title> <publisher> Oxford Univ Press, </publisher> <address> New York, </address> <year> 1995. </year>
Reference-contexts: Nevertheless the proposed methods can be applied to problems with discrete variables if one is willing to use the techniques of integer and mixed integer programming <ref> [55, 21] </ref> which are more difficult. In fact one of the proposed algorithms, the k-Median Algorithm, whose finite termination is established for problems with real variables, is directly applicable with no change to problems with ordered discrete variables such as integers.
Reference: [22] <author> K. Fukunaga. </author> <title> Statistical Pattern Recognition. </title> <publisher> Academic Press, </publisher> <address> NY, </address> <year> 1990. </year>
Reference-contexts: Hence it can be considered as an application of Occam's "law of parsimony", also known as Occam's Razor [65, 9], which states: "What can be done with fewer [assumptions] is done in vain with more". There are statistical <ref> [22] </ref>, machine learning [39, 33] as well as mathematical programming [12, 45, 10] approaches to the feature selection problem. In this work we shall deal principally with the latter because of the novelty of the approach and it effectiveness. <p> Stop when C j+1 j ` . Assign each point to a cluster whose center is closest in the 1-norm to the point. Although the k-Median Algorithm is similar to the k-Mean Algorithm wherein the 2-norm distance is used <ref> [64, 22] </ref>, it differs from it computationally, and theoretically.
Reference: [23] <author> A. A. Gaivoronski. </author> <title> Convergence properties of backpropagation for neural nets via theory of stochastic gradient methods. part i. </title> <journal> Optimization Methods and Software, </journal> <volume> 4(2), </volume> <year> 1994. </year>
Reference-contexts: programming, that is optimization subject to constraints, is a broad discipline that has been applied to a great variety of theoretical and applied problems such as operations research [29, 54], network problems [60, 53], game theory and economics [71, 35], engineering mechanics [57, 37] and more recently to machine learning <ref> [3, 61, 23, 48, 46] </ref>. In this paper we describe three recent mathematical-programming-based developments that are relevant to data mining: feature selection [45, 10], clustering [11] and robust representation [67].
Reference: [24] <author> R. S. Garfinkel and G. L. Nemhauser. </author> <title> Integer Programming. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1972. </year>
Reference-contexts: One limitation however is that the problem features must be real numbers or easily mapped into real numbers. If some of the features are discrete and can be represented as integers, then the techniques of integer programming <ref> [24, 55, 14] </ref> can be employed. Integer programming approaches have been applied for example to clustering problems [64, 1], but will not be described here, principally because the combinatorial approach is fundamentally different than the analytical approach of optimization with real variables.
Reference: [25] <author> F. Glover. </author> <title> Improved linear programming models for discriminant analysis. </title> <journal> Decision Sciences, </journal> <volume> 21 </volume> <pages> 771-785, </pages> <year> 1990. </year>
Reference-contexts: e T z fi fi fi fi Bw efl + e z; 9 &gt; ; Robustness here refers to the fact that the useless null vector (w = 0) is naturally excluded as a solution of (7), which is not the case in other linear programming formulations of this problem <ref> [41, 66, 26, 25] </ref>.
Reference: [26] <author> R. C. Grinold. </author> <title> Mathematical methods for pattern classification. </title> <journal> Management Science, </journal> <volume> 19 </volume> <pages> 272-289, </pages> <year> 1972. </year>
Reference-contexts: e T z fi fi fi fi Bw efl + e z; 9 &gt; ; Robustness here refers to the fact that the useless null vector (w = 0) is naturally excluded as a solution of (7), which is not the case in other linear programming formulations of this problem <ref> [41, 66, 26, 25] </ref>.
Reference: [27] <author> M. H. Hassoun. </author> <title> Fundamentals of Artificial Neural Networks. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1995. </year>
Reference: [28] <author> J. Hertz, A. Krogh, and R. G. Palmer. </author> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison-Wesley, </publisher> <address> Redwood City, California, </address> <year> 1991. </year>
Reference-contexts: The problem is formulated as a mathematical program with a parametric objective function and linear constraints [10]. A step function that appears in the objective function is approximated by a concave exponential on the nonnegative real line instead of the conventional sigmoid function of neural networks <ref> [28] </ref>. This leads to a very fast iterative linear-programming-based algorithm for solving the problem that terminates in a finite number of steps.
Reference: [29] <author> Frederick S. Hillier and Gerald J. Lieberman. </author> <title> Introduction to Operations Research. </title> <publisher> McGraw-Hill, </publisher> <address> New York, sixth edition, </address> <year> 1995. </year>
Reference-contexts: 1 Introduction Mathematical programming, that is optimization subject to constraints, is a broad discipline that has been applied to a great variety of theoretical and applied problems such as operations research <ref> [29, 54] </ref>, network problems [60, 53], game theory and economics [71, 35], engineering mechanics [57, 37] and more recently to machine learning [3, 61, 23, 48, 46].
Reference: [30] <author> P. J. Huber. </author> <title> Robust estimation of location parameter. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 35 </volume> <pages> 73-101, </pages> <year> 1964. </year>
Reference-contexts: Bennett and Bredensteiner [2] have formulated a similar problem using linear programming. Vapnik [70, Section 5.9] also makes use of Huber's robust regression ideas <ref> [30] </ref> and extends the latter's robust regression loss function [70, p 152] by adding an *- insensitive zone to it. This *-insensitive zone is similar to our t -tolerance zone (see (17) and (18) below) wherein errors are disregarded if they fall within the band [t; t ].
Reference: [31] <author> P. J. Huber. </author> <title> Robust Statistics. </title> <publisher> John Wiley, </publisher> <address> New York, </address> <year> 1981. </year>
Reference: [32] <author> A. K. Jain and R. C. Dubes. </author> <title> Algorithms for Clustering Data. </title> <publisher> Prentice-Hall, Inc, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year> <month> 16 </month>
Reference-contexts: Computational testing of this algorithm as a KDD tool [18] has been quite encouraging. On the Wisconsin Prognosis Breast Cancer Database (WPBC), distinct and clinically important 2 survival curves were discovered from the database by the k-Median Algorithm, whereas the traditional k-Mean Algorithm <ref> [32, 64] </ref>, which uses the square of the 2-norm distance, thus emphasizing outliers, failed to obtain such distinct survival curves for the same database. On four other publicly available databases each of the k-Median and k-Mean Algorithms did best on two of the databases. 3. <p> There are many approaches to this problem, including statistical <ref> [32] </ref>, machine learning [20], integer and mathematical programming approaches [64, 1, 58, 11]. We shall describe here the recent approach of [11] that utilizes a fast bilinear programming approach: minimizing the product of two linear functions on a set defined by linear inequalities. <p> This simple formulation can be restated as a bilinear program (12) that leads to a fast k-Median Algorithm 3.1. A reformulation of (11) using the square of the 2-norm instead of the 1-norm, leads to the k-Mean Algorithm <ref> [32, 64] </ref>. Although it is not the intention here to carry out a detailed comparative study of these two clustering algorithms, the k-Median Algorithm, as described below, does give well separated survival curves for breast cancer patients whereas the k-Mean algorithm does not.
Reference: [33] <author> G. H. John, R. Kohavi, and K. Pfleger. </author> <title> Irrelevant features and the subset selection problem. </title> <booktitle> In Proceedings of the 11th International Conference on Machine Learning, </booktitle> <address> San Mateo, CA, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Hence it can be considered as an application of Occam's "law of parsimony", also known as Occam's Razor [65, 9], which states: "What can be done with fewer [assumptions] is done in vain with more". There are statistical [22], machine learning <ref> [39, 33] </ref> as well as mathematical programming [12, 45, 10] approaches to the feature selection problem. In this work we shall deal principally with the latter because of the novelty of the approach and it effectiveness.
Reference: [34] <author> E. L. Kaplan and P. Meier. </author> <title> Nonparametric estimation from incomplete observations. </title> <journal> J. Am. Stat. Assoc., </journal> <volume> 53 </volume> <pages> 457-481, </pages> <year> 1958. </year>
Reference-contexts: To test the effectiveness of the k-Median Algorithm it was used as a KDD tool [18] to mine the Wisconsin Prognostic Breast Cancer Database (WPBC) in order to discover medical knowledge. For such medical databases, extracting well-separated survival curves provides an essential prognostic tool. Survival curves <ref> [34, 38] </ref> give expected percent of surviving patients as a function of time. The k-Median Algorithm was applied to WPBC to extract such curves. Survival curves were constructed for 194 patients using two clinically available features for each patient: tumor size and number of cancerous lymph nodes excised.
Reference: [35] <author> Samuel Karlin. </author> <title> Mathematical Methods and Theory in Games, Programming, and Economics, Volumes 1 and 2. </title> <publisher> Dover Publications, </publisher> <address> Mineola, New York, </address> <year> 1992. </year>
Reference-contexts: 1 Introduction Mathematical programming, that is optimization subject to constraints, is a broad discipline that has been applied to a great variety of theoretical and applied problems such as operations research [29, 54], network problems [60, 53], game theory and economics <ref> [71, 35] </ref>, engineering mechanics [57, 37] and more recently to machine learning [3, 61, 23, 48, 46]. In this paper we describe three recent mathematical-programming-based developments that are relevant to data mining: feature selection [45, 10], clustering [11] and robust representation [67].
Reference: [36] <author> N. Karmarkar. </author> <title> A new polynomial time algorithm for linear programming. </title> <journal> Combinatorica, </journal> <volume> 4 </volume> <pages> 373-395, </pages> <year> 1984. </year>
Reference-contexts: From the point of view of applicability to large-scale data mining problems, the proposed algorithms employ either linear programming (Sections 2 and 3) which is polynomial-time-solvable <ref> [36, 69] </ref>, or convex quadratic programming (Section 4) which is also polynomial-time-solvable [69].
Reference: [37] <author> A. Klarbring. </author> <title> Mathematical programming in contact problems. </title> <editor> In M. H. Aliabadi and C. A. Brebbia, editors, </editor> <booktitle> Computational Methods in Contact Mechanics, </booktitle> <pages> pages 233-263. </pages> <publisher> Computational Mechanics Publications, </publisher> <address> Southampton, England, </address> <year> 1993. </year>
Reference-contexts: 1 Introduction Mathematical programming, that is optimization subject to constraints, is a broad discipline that has been applied to a great variety of theoretical and applied problems such as operations research [29, 54], network problems [60, 53], game theory and economics [71, 35], engineering mechanics <ref> [57, 37] </ref> and more recently to machine learning [3, 61, 23, 48, 46]. In this paper we describe three recent mathematical-programming-based developments that are relevant to data mining: feature selection [45, 10], clustering [11] and robust representation [67].
Reference: [38] <author> David G. Kleinbaum. </author> <title> Survival Analysis. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1996. </year>
Reference-contexts: To test the effectiveness of the k-Median Algorithm it was used as a KDD tool [18] to mine the Wisconsin Prognostic Breast Cancer Database (WPBC) in order to discover medical knowledge. For such medical databases, extracting well-separated survival curves provides an essential prognostic tool. Survival curves <ref> [34, 38] </ref> give expected percent of surviving patients as a function of time. The k-Median Algorithm was applied to WPBC to extract such curves. Survival curves were constructed for 194 patients using two clinically available features for each patient: tumor size and number of cancerous lymph nodes excised.
Reference: [39] <author> Y. le Cun, J. S. Denker, and S. A. Solla. </author> <title> Optimal brain damage. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems II (Denver 1989), </booktitle> <pages> pages 598-605, </pages> <address> San Mateo, California, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This problem is closely related to the generalization problem of machine learning of how to train a system on a given training set so as to improve generalization on a new unseen testing set <ref> [39, 63, 73] </ref>. <p> Hence it can be considered as an application of Occam's "law of parsimony", also known as Occam's Razor [65, 9], which states: "What can be done with fewer [assumptions] is done in vain with more". There are statistical [22], machine learning <ref> [39, 33] </ref> as well as mathematical programming [12, 45, 10] approaches to the feature selection problem. In this work we shall deal principally with the latter because of the novelty of the approach and it effectiveness. <p> This problem is closely related to the generalization problem of machine learning of how to train a system on a given training set so as to improve generalization on a new unseen testing set <ref> [39, 63, 73] </ref>. We shall concentrate on some recent results [67] obtained for a simple linear model and which make essential use of mathematical programming ideas. These ideas, although rigorously established for a simple linear model here, are likely to extend to more complex systems.
Reference: [40] <author> I. J. Lustig, R. E. Marsten, and D. F. Shanno. </author> <title> Interior-point methods for linear programming: Computational state of the art. </title> <journal> ORSA Journal on Computing, </journal> <volume> 6(1) </volume> <pages> 1-38, </pages> <year> 1994. </year>
Reference-contexts: Extremely fast linear and quadratic programming codes [14] that are capable of solving linear programs with millions of variables <ref> [8, 40] </ref> and very large quadratic programs, make the proposed algorithms easily scalable and effective for solving a wide range of problems. One limitation however is that the problem features must be real numbers or easily mapped into real numbers.
Reference: [41] <author> O. L. Mangasarian. </author> <title> Linear and nonlinear separation of patterns by linear programming. </title> <journal> Operations Research, </journal> <volume> 13 </volume> <pages> 444-452, </pages> <year> 1965. </year>
Reference-contexts: e T z fi fi fi fi Bw efl + e z; 9 &gt; ; Robustness here refers to the fact that the useless null vector (w = 0) is naturally excluded as a solution of (7), which is not the case in other linear programming formulations of this problem <ref> [41, 66, 26, 25] </ref>. <p> Finally we point out that although a linear model was used for both the feature selection and robust representation models, nonlinear models that are linear in their parameters, e.g. quadratic surfaces, can be easily transformed into a linear system, as was done for example in <ref> [41] </ref> where quadratic separation was achieved by linear programming. However in some inherently nonlinear problems where for example the parameters of a separating surface appear nonlinearly, one may have to resort to nonlinear models and the theory and algorithms of nonlinear programming [42, 6].
Reference: [42] <author> O. L. Mangasarian. </author> <title> Nonlinear Programming. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1969. </year> <note> Reprint: SIAM Classic in Applied Mathematics 10, 1994, Philadelphia. </note>
Reference-contexts: However in some inherently nonlinear problems where for example the parameters of a separating surface appear nonlinearly, one may have to resort to nonlinear models and the theory and algorithms of nonlinear programming <ref> [42, 6] </ref>. This would again be a promising problem to pursue. We conclude with the hope that the problems solved demonstrate the theoretical and computational potential of mathematical programming as a versatile and effective tool for solving important problems in data mining and knowledge discovery in databases. 14
Reference: [43] <author> O. L. Mangasarian. </author> <title> Mathematical programming in neural networks. </title> <journal> ORSA Journal on Computing, </journal> <volume> 5(4) </volume> <pages> 349-360, </pages> <year> 1993. </year>
Reference-contexts: with respect to two given point sets A and B in R n , is a plane that attempts to separate R n into two halfspaces such that each open halfspace contains points mostly of A or B: * Alternatively, a separating plane can be interpreted as a classical perceptron <ref> [62, 43] </ref> with a threshold determined by the distance of the plane to the origin, and the incoming arc weights of the perceptron determined by the components of the normal vector to the plane. 2 Feature Selection Feature selection (or extraction) attempts to use the simplest model to describe the essence
Reference: [44] <author> O. L. Mangasarian. </author> <title> Nonlinear Programming. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1994. </year>
Reference-contexts: We will, however, point out other approaches that are mostly not based on mathematical programming. The fundamental nonlinear programming problem <ref> [6, 44] </ref> consists of minimizing an objective function subject to inequality and equality constraints and is typically written as follows min f (x) subject to g (x) 0; h (x) = 0; (1) where x is an n-dimensional vector of real variables, f is a real-valued function of x, g and <p> We also note that since the objective function of (11) is the minimum of k linear (and hence concave) functions, it is a piecewise-linear concave function <ref> [44, Corollary 4.1.14] </ref>. This is not the case for the 2-norm or p-norm, p 6= 1.
Reference: [45] <author> O. L. Mangasarian. </author> <title> Machine learning via polyhedral concave minimization. </title> <editor> In H. Fischer, B. Riedmueller, and S. Schae*er, editors, </editor> <booktitle> Applied Mathematics and Parallel Computing - Festschrift for Klaus Ritter, </booktitle> <pages> pages 175-188. </pages> <publisher> Physica-Verlag A Springer-Verlag Company, </publisher> <address> Hei-delberg, </address> <year> 1996. </year> <note> Available by ftp://ftp.cs.wisc.edu/math-prog/tech-reports/95-20.ps.Z. </note>
Reference-contexts: In this paper we describe three recent mathematical-programming-based developments that are relevant to data mining: feature selection <ref> [45, 10] </ref>, clustering [11] and robust representation [67]. We note at the outset that we do not plan to survey either the fields of data mining or mathematical programming, but rather highlight some recent and highly effective applications of the latter to the former. <p> Hence it can be considered as an application of Occam's "law of parsimony", also known as Occam's Razor [65, 9], which states: "What can be done with fewer [assumptions] is done in vain with more". There are statistical [22], machine learning [39, 33] as well as mathematical programming <ref> [12, 45, 10] </ref> approaches to the feature selection problem. In this work we shall deal principally with the latter because of the novelty of the approach and it effectiveness. <p> The parameter was chosen in the set f0, 0.05, 0.10, : : :, 1.00g, with the desired being the one achieving the best cross-validated separation. It has been shown <ref> [45, Theorem 4.2] </ref> that this algorithm terminates in a finite number of steps, typically five or six, at a global solution or a stationary point satisfying a necessary optimality condition.
Reference: [46] <author> O. L. Mangasarian. </author> <title> Mathematical programming in machine learning. </title> <editor> In G. Di Pillo and F. Giannessi, editors, </editor> <booktitle> Nonlinear Optimization and Applications, </booktitle> <pages> pages 283-295, </pages> <address> New York, 1996. </address> <publisher> Plenum Publishing. </publisher>
Reference-contexts: programming, that is optimization subject to constraints, is a broad discipline that has been applied to a great variety of theoretical and applied problems such as operations research [29, 54], network problems [60, 53], game theory and economics [71, 35], engineering mechanics [57, 37] and more recently to machine learning <ref> [3, 61, 23, 48, 46] </ref>. In this paper we describe three recent mathematical-programming-based developments that are relevant to data mining: feature selection [45, 10], clustering [11] and robust representation [67].
Reference: [47] <author> O. L. Mangasarian and R. R. Meyer. </author> <title> Nonlinear perturbation of linear programs. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 17(6) </volume> <pages> 745-752, </pages> <month> November </month> <year> 1979. </year>
Reference-contexts: As described below in Algorithm 2.1 the parameter is chosen to give the best cross-validated error. For small values of it can be shown theoretically <ref> [47] </ref> that the minimization problem (8) picks that solution of the Robust Linear Program (7) that minimizes the exponential term of (8), and hence solves the RLP (7) while suppressing redundant components of w.
Reference: [48] <author> O. L. Mangasarian and M. V. Solodov. </author> <title> Serial and parallel backpropagation convergence via nonmonotone perturbed minimization. </title> <journal> Optimization Methods and Software, </journal> <volume> 4(2) </volume> <pages> 103-116, </pages> <year> 1994. </year>
Reference-contexts: programming, that is optimization subject to constraints, is a broad discipline that has been applied to a great variety of theoretical and applied problems such as operations research [29, 54], network problems [60, 53], game theory and economics [71, 35], engineering mechanics [57, 37] and more recently to machine learning <ref> [3, 61, 23, 48, 46] </ref>. In this paper we describe three recent mathematical-programming-based developments that are relevant to data mining: feature selection [45, 10], clustering [11] and robust representation [67].
Reference: [49] <author> O. L. Mangasarian, W. Nick Street, and W. H. Wolberg. </author> <title> Breast cancer diagnosis and prognosis via linear programming. </title> <journal> Operations Research, </journal> <volume> 43(4) </volume> <pages> 570-577, </pages> <month> July-August </month> <year> 1995. </year>
Reference-contexts: The linear programming formulation (7) which has a number of natural theoretical properties including robustness is also very effective computationally <ref> [4, 49] </ref>. However it does not address the problem of suppressing irrelevant features.
Reference: [50] <author> D. Mladenic. </author> <title> Combinatorial optimization in inductive concept learning. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 205-211, </pages> <address> San MAteo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Stochastic optimization methods based on simulated annealing have also been used in problems of inductive concept learning <ref> [50] </ref>. The problems considered in this paper are: 1. Feature Selection The feature selection problem treated is that of discriminating between two finite point sets in n-dimensional feature space by a separating plane that utilizes as few of the features as possible.
Reference: [51] <author> P. M. Murphy and D. W. Aha. </author> <title> UCI repository of machine learning databases. </title> <institution> Department of Information and Computer Science, University of California, </institution> <address> Irvine, www.ics.uci.edu/AI/ML/MLDBRepository.html, 1992. </address>
Reference-contexts: This leads to a very fast iterative linear-programming-based algorithm for solving the problem that terminates in a finite number of steps. On the Wisconsin Prognosis Breast Cancer (WPBC) <ref> [72, 51] </ref> database the proposed algorithm reduced cross-validation error on a cancer prognosis database by 35.4% while reducing problem features from 32 to 4. 2. Clustering The clustering problem considered in this paper is that of assigning m points in the n-dimensional real space R n to k clusters. <p> It has been shown [45, Theorem 4.2] that this algorithm terminates in a finite number of steps, typically five or six, at a global solution or a stationary point satisfying a necessary optimality condition. This algorithm was tested on the 32-feature Wisconsin Prognostic Breast Cancer (WPBC) database <ref> [72, 51] </ref> which was collected from 28 patients for which cancer recurred within two years, and 119 patients for which cancer did not recur within two years. Thus in the terminology of our formulation (8), n = 32; m = 28 and k = 118. <p> Correctness was measured by the ratio of the sum of the number of majority points in each cluster to the total number of points m in the data set. Table 1 shows results averaged over ten random starts for four databases from the Irvine repository of databases <ref> [51] </ref>.
Reference: [52] <author> K. G. Murty. </author> <title> Linear Programming. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: If all the functions f , g and h are linear then the problem simplifies to a linear program <ref> [15, 52, 69] </ref> which is the classical problem of mathematical fl Mathematical Programming Technical Report 96-05, August 1996 Revised November 1996 & March 1997.
Reference: [53] <author> K. G. Murty. </author> <title> Network Programming. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1992. </year>
Reference-contexts: 1 Introduction Mathematical programming, that is optimization subject to constraints, is a broad discipline that has been applied to a great variety of theoretical and applied problems such as operations research [29, 54], network problems <ref> [60, 53] </ref>, game theory and economics [71, 35], engineering mechanics [57, 37] and more recently to machine learning [3, 61, 23, 48, 46]. In this paper we describe three recent mathematical-programming-based developments that are relevant to data mining: feature selection [45, 10], clustering [11] and robust representation [67].
Reference: [54] <author> K. G. Murty. </author> <title> Operations Research. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1995. </year>
Reference-contexts: 1 Introduction Mathematical programming, that is optimization subject to constraints, is a broad discipline that has been applied to a great variety of theoretical and applied problems such as operations research <ref> [29, 54] </ref>, network problems [60, 53], game theory and economics [71, 35], engineering mechanics [57, 37] and more recently to machine learning [3, 61, 23, 48, 46].
Reference: [55] <author> G. L. Nemhauser and L. A. Wolsey. </author> <title> Integer and Combinatorial Optimization. </title> <publisher> John Wiley, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: One limitation however is that the problem features must be real numbers or easily mapped into real numbers. If some of the features are discrete and can be represented as integers, then the techniques of integer programming <ref> [24, 55, 14] </ref> can be employed. Integer programming approaches have been applied for example to clustering problems [64, 1], but will not be described here, principally because the combinatorial approach is fundamentally different than the analytical approach of optimization with real variables. <p> Nevertheless the proposed methods can be applied to problems with discrete variables if one is willing to use the techniques of integer and mixed integer programming <ref> [55, 21] </ref> which are more difficult. In fact one of the proposed algorithms, the k-Median Algorithm, whose finite termination is established for problems with real variables, is directly applicable with no change to problems with ordered discrete variables such as integers.
Reference: [56] <author> M. L. Overton. </author> <title> A quadratically convergent method for minimizing a sum of euclidean norms. </title> <journal> Mathematical Programming, </journal> <volume> 27 </volume> <pages> 34-63, </pages> <year> 1983. </year>
Reference-contexts: Without this squared distance term, the subproblem of the k-Mean Algorithm becomes the considerably harder Weber problem <ref> [56, 13] </ref> which locates a center in R n closest in sum of Euclidean distances (not their squares!) to a finite set of given points. The Weber problem has no closed form solution.
Reference: [57] <author> P. D. Panagiotopoulos. </author> <title> Inequality Problems in Mechanics and Applications. </title> <publisher> Birkhauser, </publisher> <address> Boston, </address> <year> 1985. </year>
Reference-contexts: 1 Introduction Mathematical programming, that is optimization subject to constraints, is a broad discipline that has been applied to a great variety of theoretical and applied problems such as operations research [29, 54], network problems [60, 53], game theory and economics [71, 35], engineering mechanics <ref> [57, 37] </ref> and more recently to machine learning [3, 61, 23, 48, 46]. In this paper we describe three recent mathematical-programming-based developments that are relevant to data mining: feature selection [45, 10], clustering [11] and robust representation [67].
Reference: [58] <author> M. R. Rao. </author> <title> Cluster analysis and mathematical programming. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 66 </volume> <pages> 622-626, </pages> <year> 1971. </year>
Reference-contexts: There are many approaches to this problem, including statistical [32], machine learning [20], integer and mathematical programming approaches <ref> [64, 1, 58, 11] </ref>. We shall describe here the recent approach of [11] that utilizes a fast bilinear programming approach: minimizing the product of two linear functions on a set defined by linear inequalities.
Reference: [59] <author> R. T. Rockafellar. </author> <title> Convex Analysis. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey, </address> <year> 1970. </year>
Reference-contexts: Because the objective function of (8) is a concave function bounded below by zero on the nonempty polyhedral set of (8), it follows that it has a vertex solution if the feasible region does not contain lines extending to infinity in both directions <ref> [59, Corollaries 32.3.3, 32.3.4] </ref>. (Excluding such lines can be readily accomplished by a simple transformation of the variables (w; fl) into the nonnegative variables (w 1 ; fl 1 ; 1 ) using the standard transformation w = w 1 e 1 ; fl = fl 1 1 .
Reference: [60] <author> R. T. Rockafellar. </author> <title> Network Flows and Monotropic Optimization. </title> <publisher> Wiley-Interscience, </publisher> <address> New York, </address> <year> 1984. </year>
Reference-contexts: 1 Introduction Mathematical programming, that is optimization subject to constraints, is a broad discipline that has been applied to a great variety of theoretical and applied problems such as operations research [29, 54], network problems <ref> [60, 53] </ref>, game theory and economics [71, 35], engineering mechanics [57, 37] and more recently to machine learning [3, 61, 23, 48, 46]. In this paper we describe three recent mathematical-programming-based developments that are relevant to data mining: feature selection [45, 10], clustering [11] and robust representation [67].
Reference: [61] <author> A. Roy, L. S. Kim, and S. Mukhopadhyay. </author> <title> A polynomial time algorithm for the construction and training of a class of multilayer perceptrons. </title> <booktitle> Neural Networks, </booktitle> <volume> 6 </volume> <pages> 535-545, </pages> <year> 1993. </year>
Reference-contexts: programming, that is optimization subject to constraints, is a broad discipline that has been applied to a great variety of theoretical and applied problems such as operations research [29, 54], network problems [60, 53], game theory and economics [71, 35], engineering mechanics [57, 37] and more recently to machine learning <ref> [3, 61, 23, 48, 46] </ref>. In this paper we describe three recent mathematical-programming-based developments that are relevant to data mining: feature selection [45, 10], clustering [11] and robust representation [67].
Reference: [62] <author> D. E. Rumelhart and J. L. McClelland. </author> <title> Parallel Distributed Processing. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1986. </year>
Reference-contexts: with respect to two given point sets A and B in R n , is a plane that attempts to separate R n into two halfspaces such that each open halfspace contains points mostly of A or B: * Alternatively, a separating plane can be interpreted as a classical perceptron <ref> [62, 43] </ref> with a threshold determined by the distance of the plane to the origin, and the incoming arc weights of the perceptron determined by the components of the normal vector to the plane. 2 Feature Selection Feature selection (or extraction) attempts to use the simplest model to describe the essence
Reference: [63] <author> C. Schaffer. </author> <title> Overfitting avoidance as bias. </title> <journal> Machine Learning, </journal> <volume> 10 </volume> <pages> 153-178, </pages> <year> 1993. </year>
Reference-contexts: This problem is closely related to the generalization problem of machine learning of how to train a system on a given training set so as to improve generalization on a new unseen testing set <ref> [39, 63, 73] </ref>. <p> This problem is closely related to the generalization problem of machine learning of how to train a system on a given training set so as to improve generalization on a new unseen testing set <ref> [39, 63, 73] </ref>. We shall concentrate on some recent results [67] obtained for a simple linear model and which make essential use of mathematical programming ideas. These ideas, although rigorously established for a simple linear model here, are likely to extend to more complex systems.
Reference: [64] <author> S. Z. Selim and M. A. Ismail. </author> <title> K-Means-Type algorithms: a generalized convergence theorem and characterization of local optimality. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-6:81-87, </volume> <year> 1984. </year>
Reference-contexts: If some of the features are discrete and can be represented as integers, then the techniques of integer programming [24, 55, 14] can be employed. Integer programming approaches have been applied for example to clustering problems <ref> [64, 1] </ref>, but will not be described here, principally because the combinatorial approach is fundamentally different than the analytical approach of optimization with real variables. Stochastic optimization methods based on simulated annealing have also been used in problems of inductive concept learning [50]. <p> Computational testing of this algorithm as a KDD tool [18] has been quite encouraging. On the Wisconsin Prognosis Breast Cancer Database (WPBC), distinct and clinically important 2 survival curves were discovered from the database by the k-Median Algorithm, whereas the traditional k-Mean Algorithm <ref> [32, 64] </ref>, which uses the square of the 2-norm distance, thus emphasizing outliers, failed to obtain such distinct survival curves for the same database. On four other publicly available databases each of the k-Median and k-Mean Algorithms did best on two of the databases. 3. <p> There are many approaches to this problem, including statistical [32], machine learning [20], integer and mathematical programming approaches <ref> [64, 1, 58, 11] </ref>. We shall describe here the recent approach of [11] that utilizes a fast bilinear programming approach: minimizing the product of two linear functions on a set defined by linear inequalities. <p> This simple formulation can be restated as a bilinear program (12) that leads to a fast k-Median Algorithm 3.1. A reformulation of (11) using the square of the 2-norm instead of the 1-norm, leads to the k-Mean Algorithm <ref> [32, 64] </ref>. Although it is not the intention here to carry out a detailed comparative study of these two clustering algorithms, the k-Median Algorithm, as described below, does give well separated survival curves for breast cancer patients whereas the k-Mean algorithm does not. <p> Stop when C j+1 j ` . Assign each point to a cluster whose center is closest in the 1-norm to the point. Although the k-Median Algorithm is similar to the k-Mean Algorithm wherein the 2-norm distance is used <ref> [64, 22] </ref>, it differs from it computationally, and theoretically.
Reference: [65] <author> J. W. Shavlik and T. G. Dietterich (editors). </author> <booktitle> Readings in Machine Learning. </booktitle> <publisher> Morgan Kaufman, </publisher> <address> San Mateo, California, </address> <year> 1990. </year>
Reference-contexts: Hence it can be considered as an application of Occam's "law of parsimony", also known as Occam's Razor <ref> [65, 9] </ref>, which states: "What can be done with fewer [assumptions] is done in vain with more". There are statistical [22], machine learning [39, 33] as well as mathematical programming [12, 45, 10] approaches to the feature selection problem.
Reference: [66] <author> F. W. Smith. </author> <title> Pattern classifier design by linear programming. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-17:367-372, </volume> <year> 1968. </year> <month> 18 </month>
Reference-contexts: e T z fi fi fi fi Bw efl + e z; 9 &gt; ; Robustness here refers to the fact that the useless null vector (w = 0) is naturally excluded as a solution of (7), which is not the case in other linear programming formulations of this problem <ref> [41, 66, 26, 25] </ref>.
Reference: [67] <author> W. Nick Street and O. L. Mangasarian. </author> <title> Improved generalization via tolerant training. </title> <type> Technical Report 95-11, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin, </institution> <month> July </month> <year> 1995. </year> <note> Machine Learning, submitted. Available by ftp://ftp.cs.wisc.edu/math-prog/tech-reports/95-11.ps.Z. </note>
Reference-contexts: In this paper we describe three recent mathematical-programming-based developments that are relevant to data mining: feature selection [45, 10], clustering [11] and robust representation <ref> [67] </ref>. We note at the outset that we do not plan to survey either the fields of data mining or mathematical programming, but rather highlight some recent and highly effective applications of the latter to the former. <p> This problem is closely related to the generalization problem of machine learning of how to train a system on a given training set so as to improve generalization on a new unseen testing set [39, 63, 73]. We use here a simple linear model <ref> [67] </ref> and will show that if a sufficiently small error t is purposely tolerated in constructing the model, then for a broad class of perturbations the model will be a more accurate representation than one obtained by a conventional zero error tolerance. <p> This problem is closely related to the generalization problem of machine learning of how to train a system on a given training set so as to improve generalization on a new unseen testing set [39, 63, 73]. We shall concentrate on some recent results <ref> [67] </ref> obtained for a simple linear model and which make essential use of mathematical programming ideas. These ideas, although rigorously established for a simple linear model here, are likely to extend to more complex systems. <p> 2 The key question to ask here, is this: Under what conditions does a solution x (t ) of (18), for some t &gt; 0 give a smaller error than x (0) on a testing set? We are able to give an answer to this question and corroborate it computationally <ref> [67] </ref>, by considering a general testing set (C; c) 2 R kfin fi R k for the problem (15) as well as a simpler testing set, where only the right side of (14) is perturbed. <p> Proposition 4.1 Robust Representation of Ax = a + p via (18) <ref> [67] </ref>. <p> The top curve demonstrates an increasing error effect of violating (23) for sufficiently small *. Computational results carried out in <ref> [67] </ref> have corroborated the improved generalization results of Proposition 4.1 above. <p> We thus have the error g (t ) := 2 2 (25) For this very general formulation we are able to give the following result that tells us when tolerant training does lead to improved generalization. Proposition 4.2 Improved generalization with positive tolerance for testing model Cx = c <ref> [67] </ref>. Let x (t ) be defined by the tolerant training of Ax = a by the regularized quadratic program (18) with tolerance t 0. Let g (t ) denote the error generated by x (t ) in the testing model 13 Cx = c, defined by (25).
Reference: [68] <author> Y. Z. Tsypkin. </author> <title> Foundations of the Theory of Learning Systems. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: A great variety of problems from many fields can be formulated and effectively solved as mathematical programs. According to the great eighteenth century mathematician Leonhard Euler: "Nothing happens in the universe that does not have a sense of either certain maximum or minimum" <ref> [68, p 1] </ref>. From the point of view of applicability to large-scale data mining problems, the proposed algorithms employ either linear programming (Sections 2 and 3) which is polynomial-time-solvable [36, 69], or convex quadratic programming (Section 4) which is also polynomial-time-solvable [69].
Reference: [69] <author> Robert J. Vanderbei. </author> <title> Linear Programming: Foundations and Extensions. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Hingham, MA, </address> <year> 1997. </year>
Reference-contexts: If all the functions f , g and h are linear then the problem simplifies to a linear program <ref> [15, 52, 69] </ref> which is the classical problem of mathematical fl Mathematical Programming Technical Report 96-05, August 1996 Revised November 1996 & March 1997. <p> From the point of view of applicability to large-scale data mining problems, the proposed algorithms employ either linear programming (Sections 2 and 3) which is polynomial-time-solvable <ref> [36, 69] </ref>, or convex quadratic programming (Section 4) which is also polynomial-time-solvable [69]. <p> From the point of view of applicability to large-scale data mining problems, the proposed algorithms employ either linear programming (Sections 2 and 3) which is polynomial-time-solvable [36, 69], or convex quadratic programming (Section 4) which is also polynomial-time-solvable <ref> [69] </ref>. Extremely fast linear and quadratic programming codes [14] that are capable of solving linear programs with millions of variables [8, 40] and very large quadratic programs, make the proposed algorithms easily scalable and effective for solving a wide range of problems.
Reference: [70] <author> V. N. Vapnik. </author> <title> The Nature of Statistical Learning Theory. </title> <publisher> Springer, </publisher> <address> New York, </address> <year> 1995. </year>
Reference-contexts: These ideas, although rigorously established for a simple linear model here, are likely to extend to more complex systems. In a somewhat related approach Vapnik has proposed a quadratic program for constructing a plane to obtain a smallest probability of error for separating two point sets <ref> [70, Section 5.4] </ref>. Bennett and Bredensteiner [2] have formulated a similar problem using linear programming. Vapnik [70, Section 5.9] also makes use of Huber's robust regression ideas [30] and extends the latter's robust regression loss function [70, p 152] by adding an *- insensitive zone to it. <p> In a somewhat related approach Vapnik has proposed a quadratic program for constructing a plane to obtain a smallest probability of error for separating two point sets [70, Section 5.4]. Bennett and Bredensteiner [2] have formulated a similar problem using linear programming. Vapnik <ref> [70, Section 5.9] </ref> also makes use of Huber's robust regression ideas [30] and extends the latter's robust regression loss function [70, p 152] by adding an *- insensitive zone to it. <p> Bennett and Bredensteiner [2] have formulated a similar problem using linear programming. Vapnik [70, Section 5.9] also makes use of Huber's robust regression ideas [30] and extends the latter's robust regression loss function <ref> [70, p 152] </ref> by adding an *- insensitive zone to it. This *-insensitive zone is similar to our t -tolerance zone (see (17) and (18) below) wherein errors are disregarded if they fall within the band [t; t ]. <p> Another similarity with Vapnik's work is the presence of a regularization term * 2 kxk 2 in our minimization (17) problem (introduced here in order to make the solution unique and to rigorously derive our generalization theorems) and Vapnik's bounding his weight vector in order to control the VC dimension <ref> [70, p 128, Theorem 5.1] </ref>. However, what our approach provides here that is novel, are precise deterministic conditions (Propositions 4.1 and 4.2 below) under which tolerating a t -insensitivity zone will give better generalization results than the conventional zero-tolerance that is used in an ordinary least squares approach.
Reference: [71] <author> J. von Neumann and O. Morgenstern. </author> <title> Theory of Games and Economic Behavior. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey, </address> <year> 1944. </year>
Reference-contexts: 1 Introduction Mathematical programming, that is optimization subject to constraints, is a broad discipline that has been applied to a great variety of theoretical and applied problems such as operations research [29, 54], network problems [60, 53], game theory and economics <ref> [71, 35] </ref>, engineering mechanics [57, 37] and more recently to machine learning [3, 61, 23, 48, 46]. In this paper we describe three recent mathematical-programming-based developments that are relevant to data mining: feature selection [45, 10], clustering [11] and robust representation [67].
Reference: [72] <author> W. H. Wolberg, W. N. Street, and O. L. </author> <type> Mangasarian. </type> <institution> WPBC: Wisconsin Prognostic Breast Cancer Database. Computer Sciences Department, University of Wisconsin, Madison, ftp://ftp.cs.wisc.edu/math-prog/cpo-dataset/machine-learn/WPBC/, </institution> <year> 1995. </year>
Reference-contexts: This leads to a very fast iterative linear-programming-based algorithm for solving the problem that terminates in a finite number of steps. On the Wisconsin Prognosis Breast Cancer (WPBC) <ref> [72, 51] </ref> database the proposed algorithm reduced cross-validation error on a cancer prognosis database by 35.4% while reducing problem features from 32 to 4. 2. Clustering The clustering problem considered in this paper is that of assigning m points in the n-dimensional real space R n to k clusters. <p> It has been shown [45, Theorem 4.2] that this algorithm terminates in a finite number of steps, typically five or six, at a global solution or a stationary point satisfying a necessary optimality condition. This algorithm was tested on the 32-feature Wisconsin Prognostic Breast Cancer (WPBC) database <ref> [72, 51] </ref> which was collected from 28 patients for which cancer recurred within two years, and 119 patients for which cancer did not recur within two years. Thus in the terminology of our formulation (8), n = 32; m = 28 and k = 118.
Reference: [73] <author> D. H. Wolpert, </author> <title> editor. The Mathematics of Generalization, </title> <address> Reading, MA, 1995. </address> <publisher> Addison-Wesley. </publisher> <pages> 19 </pages>
Reference-contexts: This problem is closely related to the generalization problem of machine learning of how to train a system on a given training set so as to improve generalization on a new unseen testing set <ref> [39, 63, 73] </ref>. <p> This problem is closely related to the generalization problem of machine learning of how to train a system on a given training set so as to improve generalization on a new unseen testing set <ref> [39, 63, 73] </ref>. We shall concentrate on some recent results [67] obtained for a simple linear model and which make essential use of mathematical programming ideas. These ideas, although rigorously established for a simple linear model here, are likely to extend to more complex systems.
References-found: 73

