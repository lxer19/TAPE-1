URL: http://www.stern.nyu.edu/~aweigend/Research/Papers/BEFORENYU/WeigendMangeasSrivastava_IJNS95.ps.Z
Refering-URL: http://www.stern.nyu.edu/~aweigend/Research/Papers/BEFORENYU/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: andreas@cs.colorado.edu  morgan.mangeas@der.edf.fr  srivasan@cs.colorado.edu  
Title: Nonlinear gated experts for time series: discovering regimes and avoiding overfitting  
Author: Andreas S. Weigend Morgan Mangeas Ashok N. Srivastava 
Note: http://www.cs.colorado.edu/~andreas/Home.html  
Address: Boulder, CO 80309-0430  1, av. du general de Gaulle, 92141 Clamart, France, and  Boulder, CO 80309-0430  Boulder, CO 80309-0529  
Affiliation: Department of Computer Science and Institute of Cognitive Science University of Colorado  Electricite de France, Direction des Etudes et Recherches  Department of Computer Science University of Colorado,  Department of Electrical and Computer Engineering and Center for Space Construction University of Colorado  
Abstract: In: International Journal of Neural Systems 6 (1995) p. 373 - 399. URL of this paper: ftp://ftp.cs.colorado.edu/pub/Time-Series/MyPapers/experts.ps.Z, or http://www.cs.colorado.edu/~andreas/Time-Series/MyPapers/experts.ps.Z University of Colorado Computer Science Technical Report CU-CS-798-95. In the analysis and prediction of real-world systems, two of the key problems are nonstationarity(often in the form of switching between regimes), and overfitting (particularly serious for noisy processes). This article addresses these problems using gated experts, consisting of a (nonlinear) gating network, and several (also nonlinear) competing experts. Each expert learns to predict the conditional mean, and each expert adapts its width to match the noise level in its regime. The gating network learns to predict the probability of each expert, given the input. This article focuses on the case where the gating network bases its decision on information from the inputs. This can be contrasted to hidden Markov models where the decision is based on the previous state(s) (i.e., on the output of the gating network at the previous time step), as well as to averaging over several predictors. In contrast, gated experts soft-partition the input space. This article discusses the underlying statistical assumptions, derives the weight update rules, and compares the performance of gated experts to standard methods on three time series: (1) a computer-generated series, obtained by randomly switching between two nonlinear processes, (2) a time series from the Santa Fe Time Series Competition (the light intensity of a laser in chaotic state), and (3) the daily electricity demand of France, a real-world multivariate problem with structure on several time scales. The main results are (1) the gating network correctly discovers the different regimes of the process, (2) the widths associated with each expert are important for the segmentation task (and they can be used to characterize the sub-processes), and (3) there is less overfitting compared to single networks (homogeneous multi-layer perceptrons), since the experts learn to match their variances to the (local) noise levels. This can be viewed as matching the local complexity of the model to the local complexity of the data. 
Abstract-found: 1
Intro-found: 1
Reference: [Baldi and Chauvin, 1994] <author> Baldi, P. and Chauvin, Y. </author> <year> (1994). </year> <title> Smooth online learning algorithms for hidden Markov models. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 307-318. </pages>
Reference: [Bengio and Frasconi, 1995] <author> Bengio, Y. and Frasconi, P. </author> <year> (1995). </year> <title> An input output HMM architecture. </title> <editor> In Tesauro, G., Touretzky, D. S., and Leen, T. K., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7 (NIPS*94), </booktitle> <pages> pages 427-434. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Recently, <ref> [Bengio and Frasconi, 1995] </ref> and [Predoviciu and Jordan, 1995] presented a framework for an input-output hidden Markov model that allows the gate to access both external information (from the inputs) and its own past outputs. the gating network have access to the inputs: they can either share the same inputs, or
Reference: [Bishop, 1994] <author> Bishop, C. M. </author> <year> (1994). </year> <title> Mixture density networks. </title> <type> Technical report, </type> <institution> Aston University. </institution>
Reference-contexts: In that case, it can still be interesting to read off the entire conditional density and compare it to a simple density estimation with Gaussians <ref> [Bishop, 1994] </ref>, or to nonparametric density estimation using fractional binning [Weigend and Srivastava, 1995]. In order to evaluate the likelihood of the data given the modelhow well the model predicts the observed datawe need to assume a specific distribution for the measurement errors.
Reference: [Bollerslev, 1986] <author> Bollerslev, T. </author> <year> (1986). </year> <title> Generalized autoregressive conditional heteroskedasticity. </title> <journal> Journal of Econometrics, </journal> <volume> 21 </volume> <pages> 307-328. </pages>
Reference-contexts: TAR models tend to be successful when there are relatively few data points available, O (100), too few to actually learn the splits. Splits are often made in an exogenous variable, such as the volatility <ref> [Engle, 1982, Bollerslev, 1986, Bollerslev et al., 1990] </ref>. Multivariate adaptive regression splines (MARS) [Friedman, 1991] is a more flexible model that has recently been applied to forecasting financial data [Lewis et al., 1994]. Radial basis functions, TAR, and MARS models all define the state by proximity in the observed variables.
Reference: [Bollerslev et al., 1990] <author> Bollerslev, T., Chou, R. Y., Jayaraman, N., and Kroner, K. F. </author> <year> (1990). </year> <title> ARCH modeling in finance: A review of the theory and empirical evidence. </title> <journal> Journal of Econometrics, </journal> <volume> 52(1) </volume> <month> 5-60. </month> <title> 15 If segmentation is the sole goal, a variation of the present architecture uses a two-sided, acausal filter, by presenting data from both before and after the present point as input. </title> <type> 25 </type>
Reference-contexts: TAR models tend to be successful when there are relatively few data points available, O (100), too few to actually learn the splits. Splits are often made in an exogenous variable, such as the volatility <ref> [Engle, 1982, Bollerslev, 1986, Bollerslev et al., 1990] </ref>. Multivariate adaptive regression splines (MARS) [Friedman, 1991] is a more flexible model that has recently been applied to forecasting financial data [Lewis et al., 1994]. Radial basis functions, TAR, and MARS models all define the state by proximity in the observed variables.
Reference: [Bridle, 1989] <author> Bridle, J. S. </author> <year> (1989). </year> <title> Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition. </title> <editor> In Fogelman-Soulie, F. and Herault, J., editors, Neuro-computing: </editor> <booktitle> Algorithms, Architectures and Applicatations, </booktitle> <pages> pages 227-236. </pages> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Its goal is to estimate the probability that a given input x was generated by expert j. The hidden units of the gating network have tanh activation functions as building blocks for the nonlinearities (Eq. 1). The outputs of the gating network are normalized exponentials (also called softmax-units) <ref> [McCullagh and Nelder, 1989, Bridle, 1989] </ref>. This choice incorporates into the architecture the constraints that the outputs should be positive and sum to unity.
Reference: [Broomhead and Lowe, 1988] <author> Broomhead, D. S. and Lowe, D. </author> <year> (1988). </year> <title> Multivariable functional interpolation and adaptive networks. </title> <journal> Complex Systems, </journal> <volume> 2 </volume> <pages> 321-355. </pages>
Reference-contexts: In this architecture, the single network has the difficult task of learning two potentially quite different mappings across the same set of units. Gated experts can also be compared and contrasted to connectionist architectures with local basis functions: whereas the architecture of radial basis functions <ref> [Broomhead and Lowe, 1988, Casdagli, 1989, Poggio and Girosi, 1990] </ref> does split up the input space into local regions (as opposed to global sigmoids), there is no incentive in the learning algorithm to find regions defined by similar structure, noise level, or dynamics.
Reference: [Buntine and Weigend, 1991] <author> Buntine, W. L. and Weigend, A. S. </author> <year> (1991). </year> <title> Bayesian back-propagation. </title> <journal> Complex Systems, </journal> <volume> 5 </volume> <pages> 603-643. </pages>
Reference: [Cacciatore and Nowlan, 1994] <author> Cacciatore, T. W. and Nowlan, S. J. </author> <year> (1994). </year> <title> Mixtures of controllers for jump linear and nonlinear plants. </title> <editor> In Cowen, J. D., Tesauro, G., and Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6 (NIPS*93), </booktitle> <pages> pages 719-726, </pages> <address> San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Note, however, that HMMs split the input space only indirectly, whereas the gate here is directly connected to the inputs. This direct connection can facilitate the splitting. The information from external inputs and from the previous internal state of the gating network can also be combined: <ref> [Cacciatore and Nowlan, 1994] </ref>, in discussing control problems, feed the output of the gating network back to the input of the gating network at the next time step, providing the gating network with the information of the last state.
Reference: [Casdagli, 1989] <author> Casdagli, M. </author> <year> (1989). </year> <title> Nonlinear prediction of chaotic time series. </title> <journal> Physica D, </journal> <volume> 35 </volume> <pages> 335-356. </pages>
Reference-contexts: In this architecture, the single network has the difficult task of learning two potentially quite different mappings across the same set of units. Gated experts can also be compared and contrasted to connectionist architectures with local basis functions: whereas the architecture of radial basis functions <ref> [Broomhead and Lowe, 1988, Casdagli, 1989, Poggio and Girosi, 1990] </ref> does split up the input space into local regions (as opposed to global sigmoids), there is no incentive in the learning algorithm to find regions defined by similar structure, noise level, or dynamics.
Reference: [Cottrell et al., 1995] <author> Cottrell, M., Girard, B., Girard, Y., Mangeas, M., and Muller, C. </author> <year> (1995). </year> <title> Neural modeling for time series: a statistical stepwise method for weight elimination. </title> <journal> IEEE Transaction on Neural Networks, </journal> <volume> 6 </volume> <pages> 1355-1364. </pages>
Reference-contexts: exogenous variables (e.g., temperature, cloud coverage, weather, etc.); * multi-scale: there is structure on several time scales (e.g., daily patterns, weekly patterns, yearly patterns); * multi-stationary: there are different regimes (e.g., holidays vs. workdays, summer vs. winter, etc.). 19 The data, task and performance are described in more detail in <ref> [Cottrell et al., 1995] </ref>; the gated experts approach is presented in [Mangeas et al., 1995]. <p> For the final performance, we used standard pruning <ref> [Finnoff et al., 1993, Cottrell et al., 1995] </ref> to remove irrelevant weights. The basic idea is to compare the size of the weight with the size of its fluctuations (i.e., the standard deviation of the weight changes in response to the input patterns). <p> on the test set, indicating that there was essentially no overfitting after pruning. 6.3 Performance and analysis We compare the performance of the combined stage 1 + stage 2 model to two benchmarks: a linear autoregressive model using exogenous variables (ARX), and a single neural network with two hidden layers <ref> [Cottrell et al., 1995] </ref>. In terms of squared error, the two-stage model gives an 11% improvement over the ARX model presently used. The performance of the single neural network falls between the two; it gives a 7% improvement over the ARX model.
Reference: [Dempster et al., 1977] <author> Dempster, A. P., Laird, N. M., and Rubin, D. B. </author> <year> (1977). </year> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> J. Roy. Stat. Soc. B, </journal> <volume> 39 </volume> <pages> 1-38. </pages>
Reference: [Diebold and Rudebusch, 1996] <author> Diebold, F. X. and Rudebusch, G. D. </author> <year> (1996). </year> <title> Measuring business cycles: A modern perspective. </title> <journal> Review of Economics and Statistics. </journal>
Reference-contexts: For example, regimes of electricity demand depend on the seasons, and regimes of financial forecasts depend on the economy (e.g., expansion and contraction, also called growth and recession) <ref> [Granger, 1994, Diebold and Rudebusch, 1996] </ref>. Although a single global model can in principle emulate any function, including regime switching, it is often very hard to extract such an unstructured, global model from the data.
Reference: [Doutriaux and Zipser, 1990] <author> Doutriaux, A. and Zipser, D. </author> <year> (1990). </year> <title> Unsupervised discovery of speech segments using recurrent networks. </title> <editor> In Touretzky, D. S., Elman, J. L., Sejnowski, T. J., and Hinton, G. E., editors, </editor> <booktitle> Proceedings of the 1990 Connectionist Models Summer School, </booktitle> <pages> pages 303-309, </pages> <address> San Fransisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This is a new approach to the problem of overfitting. 1.2 Related Work Gated experts have a solid statistical basis. This can be compared to prior connectionist work addressing segmentation of temporal sequences. [Elman, 1990] uses the size of the errors, and <ref> [Doutriaux and Zipser, 1990] </ref> use large changes in the activations of the hidden units to indicate segmentation. [Levin, 1991] adds a set of auxiliary input units which 2 encode a (discrete) state, set to fixed values in training (supervised) and estimated in testing.
Reference: [Elman, 1990] <author> Elman, J. L. </author> <year> (1990). </year> <title> Finding structure in time. </title> <journal> Cognitive Science, </journal> <volume> 14 </volume> <pages> 179-211. </pages>
Reference-contexts: This is a new approach to the problem of overfitting. 1.2 Related Work Gated experts have a solid statistical basis. This can be compared to prior connectionist work addressing segmentation of temporal sequences. <ref> [Elman, 1990] </ref> uses the size of the errors, and [Doutriaux and Zipser, 1990] use large changes in the activations of the hidden units to indicate segmentation. [Levin, 1991] adds a set of auxiliary input units which 2 encode a (discrete) state, set to fixed values in training (supervised) and estimated in
Reference: [Engle, 1982] <author> Engle, R. F. </author> <year> (1982). </year> <title> Autoregressive conditional heteroskedasticity with estimates of the variance of united kingdom inflation. </title> <journal> Econometrica, </journal> <volume> 50 </volume> <pages> 987-1007. </pages>
Reference-contexts: TAR models tend to be successful when there are relatively few data points available, O (100), too few to actually learn the splits. Splits are often made in an exogenous variable, such as the volatility <ref> [Engle, 1982, Bollerslev, 1986, Bollerslev et al., 1990] </ref>. Multivariate adaptive regression splines (MARS) [Friedman, 1991] is a more flexible model that has recently been applied to forecasting financial data [Lewis et al., 1994]. Radial basis functions, TAR, and MARS models all define the state by proximity in the observed variables.
Reference: [Finnoff et al., 1993] <author> Finnoff, W., Hergert, F., and Zimmermann, H. G. </author> <year> (1993). </year> <title> Improving generalization performance by nonconvergent model selection methods. </title> <booktitle> Neural Networks, </booktitle> <volume> 6 </volume> <pages> 771-783. </pages>
Reference-contexts: For the final performance, we used standard pruning <ref> [Finnoff et al., 1993, Cottrell et al., 1995] </ref> to remove irrelevant weights. The basic idea is to compare the size of the weight with the size of its fluctuations (i.e., the standard deviation of the weight changes in response to the input patterns).
Reference: [Fraser and Dimitriadis, 1994] <author> Fraser, A. M. and Dimitriadis, A. </author> <year> (1994). </year> <title> Forecasting probability densities by using hidden Markov models. </title> <editor> In Weigend, A. S. and Gershenfeld, N. A., editors, </editor> <title> Time Series Prediction: </title> <booktitle> Forecasting the Future and Understanding the Past, </booktitle> <pages> pages 265-282, </pages> <address> Reading, MA. </address> <publisher> Addison-Wesley. </publisher>
Reference: [Friedman, 1991] <author> Friedman, J. H. </author> <year> (1991). </year> <title> Multivariate adaptive regression splines. </title> <journal> Annals of Statistics, </journal> <volume> 19 </volume> <pages> 1-142. </pages>
Reference-contexts: TAR models tend to be successful when there are relatively few data points available, O (100), too few to actually learn the splits. Splits are often made in an exogenous variable, such as the volatility [Engle, 1982, Bollerslev, 1986, Bollerslev et al., 1990]. Multivariate adaptive regression splines (MARS) <ref> [Friedman, 1991] </ref> is a more flexible model that has recently been applied to forecasting financial data [Lewis et al., 1994]. Radial basis functions, TAR, and MARS models all define the state by proximity in the observed variables.
Reference: [Granger, 1994] <author> Granger, C. W. J. </author> <year> (1994). </year> <title> Forecasting in economics. </title> <editor> In Weigend, A. S. and Gershenfeld, N. A., editors, </editor> <title> Time Series Prediction: </title> <booktitle> Forecasting the Future and Understanding the Past, </booktitle> <pages> pages 529-538, </pages> <address> Reading, MA. </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: For example, regimes of electricity demand depend on the seasons, and regimes of financial forecasts depend on the economy (e.g., expansion and contraction, also called growth and recession) <ref> [Granger, 1994, Diebold and Rudebusch, 1996] </ref>. Although a single global model can in principle emulate any function, including regime switching, it is often very hard to extract such an unstructured, global model from the data.
Reference: [Granger and Terasvirta, 1993] <author> Granger, C. W. J. and Terasvirta, T. </author> <year> (1993). </year> <title> Modelling Nonlinear Economic Relationships. </title> <publisher> Oxford University Press, Oxford, </publisher> <address> UK. </address>
Reference-contexts: There are different sets of coefficients associated with each regime, but the functions are linear, and the Markov transition probabilities constant. To our knowledge, neither the double-nonlinear gated experts presented here, nor the flexible individual noise levels for the different regimes have been used in economics or econometrics <ref> [Granger and Terasvirta, 1993, Hamilton, 1994] </ref>. 1 However, the rigorous probabilistic interpretation of the linear regime switching models fully generalizes to the gated experts discussed here.
Reference: [Hamilton, 1989] <author> Hamilton, J. D. </author> <year> (1989). </year> <title> A new approach to the economic analysis of nonstationary time series and the business cycle. </title> <journal> Econometrica, </journal> <volume> 57 </volume> <pages> 357-384. </pages>
Reference-contexts: Radial basis functions, TAR, and MARS models all define the state by proximity in the observed variables. What if the state is not directly observable? Such latent or hidden states were popularized in the econometrics community some five years ago <ref> [Hamilton, 1989, Hamilton, 1990] </ref>; they can be traced back to [Quandt, 1958]. However, expressed in connectionist language, these models do not have any hidden units: both the gate and the experts are linear.
Reference: [Hamilton, 1990] <author> Hamilton, J. D. </author> <year> (1990). </year> <title> Analysis of time series subject to changes in regime. </title> <journal> Journal of Econometrics, </journal> <volume> 45 </volume> <pages> 39-70. </pages>
Reference-contexts: Radial basis functions, TAR, and MARS models all define the state by proximity in the observed variables. What if the state is not directly observable? Such latent or hidden states were popularized in the econometrics community some five years ago <ref> [Hamilton, 1989, Hamilton, 1990] </ref>; they can be traced back to [Quandt, 1958]. However, expressed in connectionist language, these models do not have any hidden units: both the gate and the experts are linear.
Reference: [Hamilton, 1994] <author> Hamilton, J. D. </author> <year> (1994). </year> <title> Time Series Analysis. </title> <publisher> Princeton University Press, Princeton. </publisher>
Reference-contexts: Summarizing, the key elements of gated experts are: * nonlinear gate and experts, * soft-partitioning the input space, * adaptive noise levels (variances) of the experts. In contrast to related work (e.g., <ref> [Hamilton, 1994, Jordan and Jacobs, 1994] </ref>) we allow the noise-level parameter associated with each individual expert to adapt separately to the data. <p> There are different sets of coefficients associated with each regime, but the functions are linear, and the Markov transition probabilities constant. To our knowledge, neither the double-nonlinear gated experts presented here, nor the flexible individual noise levels for the different regimes have been used in economics or econometrics <ref> [Granger and Terasvirta, 1993, Hamilton, 1994] </ref>. 1 However, the rigorous probabilistic interpretation of the linear regime switching models fully generalizes to the gated experts discussed here.
Reference: [Ivanova et al., 1994] <author> Ivanova, T. O., Mottle, V. V., and Muchnik, I. B. </author> <year> (1994). </year> <title> Estimation of the parameters of hidden Markov models of noise-like signals with abruptly changing probabilistic properties. </title> <journal> Automation and Remote Control, </journal> <volume> 55 </volume> <month> 1299-1315,1428-1445. </month>
Reference: [Jacobs, 1995] <author> Jacobs, R. A. </author> <year> (1995). </year> <title> Methods for combining experts' probability assessments. </title> <journal> Neural Computation, </journal> <volume> 7 </volume> <pages> 867-888. </pages>
Reference-contexts: This fact drives the specialization of the experts (or the carving up of the input space). Note that this is different from simple averaging of different predictors, where we can only hope for reduction of statistical error to the degree that the individual predictors are uncorrelated <ref> [Perrone, 1994, Jacobs, 1995] </ref>. The cost function Eq. 21 is central for the gated experts model. The M-step minimizes this cost function by adjusting the parameters. The parameters under consideration are the variances of the experts, as well as the weights of all the experts and the gating network.
Reference: [Jacobs et al., 1991] <author> Jacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E. </author> <year> (1991). </year> <title> Adaptive mixtures of local experts. </title> <journal> Neural Computation, </journal> <volume> 3 </volume> <pages> 79-87. </pages>
Reference-contexts: This turns out to be particularly advantageous in modeling multivariate problems where different variables are important in different regimes. Addressing these problems, we present a class of models for time series prediction that we call gated experts. They were introduced into the connectionist community as the mixture of experts <ref> [Jacobs et al., 1991] </ref>, and are also called society of experts [Rumelhart et al., 1995]. We use the term gated experts for nonlinearly gated nonlinear experts.
Reference: [Johnson and Kotz, 1972] <author> Johnson, N. L. and Kotz, S. </author> <year> (1972). </year> <title> Distributions in Statistics. Continuous Multivariate Distributions. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference-contexts: not changed; this prevents them from growing too large. (Too large values for s j are assumed to indicate overfitting: The gate should not be that sure about one expert being responsible for any pattern.) A soft version assumes that the g j 's are drawn from a Dirichlet distribution <ref> [Johnson and Kotz, 1972] </ref>. Expressed as a prior on s, this implies that exp (s) is Gamma distributed. For two experts, the Dirichlet distribution for g reduces to a Beta distribution (see Footnote 7).
Reference: [Jordan and Jacobs, 1994] <author> Jordan, M. I. and Jacobs, R. A. </author> <year> (1994). </year> <title> Hierarchical mixtures of experts and the EM algorithm. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 181-214. </pages>
Reference-contexts: Summarizing, the key elements of gated experts are: * nonlinear gate and experts, * soft-partitioning the input space, * adaptive noise levels (variances) of the experts. In contrast to related work (e.g., <ref> [Hamilton, 1994, Jordan and Jacobs, 1994] </ref>) we allow the noise-level parameter associated with each individual expert to adapt separately to the data. <p> An important inspiration for our work has been the introduction of mixture models into the connectionist community by Jacobs, Jordan, Nowlan and Hinton (1991). 2 Subsequently, <ref> [Jordan and Jacobs, 1994] </ref> introduced the architecture of a hierarchical mixture of linear experts (with fixed widths).
Reference: [Jordan and Xu, 1995] <author> Jordan, M. I. and Xu, L. </author> <year> (1995). </year> <title> Convergence results for the EM approach to mixtures of experts architectures. </title> <booktitle> Neural Networks, </booktitle> <volume> 8 </volume> <pages> 1409-1431. </pages>
Reference-contexts: An important inspiration for our work has been the introduction of mixture models into the connectionist community by Jacobs, Jordan, Nowlan and Hinton (1991). 2 Subsequently, [Jordan and Jacobs, 1994] introduced the architecture of a hierarchical mixture of linear experts (with fixed widths). Recently, <ref> [Jordan and Xu, 1995] </ref> proved the convergence, and [Waterhouse and Robinson, 1995] applied the hierarchical mixture of linear experts to time series prediction of the sunspots [Weigend et al., 1990, Nowlan and Hinton, 1992] and to nonlinear regression on an example of noise heterogeneity [Weigend and Nix, 1994, Nix and Weigend,
Reference: [Lapedes and Farber, 1987] <author> Lapedes, A. and Farber, R. </author> <year> (1987). </year> <title> Nonlinear signal processing using neural networks. </title> <type> Technical Report LA-UR-87-2662, </type> <institution> Los Alamos National Laboratory, </institution> <address> Los Alamos, NM. </address>
Reference-contexts: They can be linear, assuming that the next value is a linear superposition of preceding values [Yule, 1927], or they can be nonlinear, conveniently described in the quite general language of neural networks with hidden units <ref> [Rumelhart et al., 1986, Lapedes and Farber, 1987] </ref>. Such single, global, and traditionally univariate models are well suited to problems with stationary dynamics. However, the assumption of stationarity is violated in many real-world time series.
Reference: [Levin, 1991] <author> Levin, E. </author> <year> (1991). </year> <title> Modeling time varying systems using hidden control neural architecture. </title> <editor> In Lippmann, R. P., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3 (NIPS*90), </booktitle> <pages> pages 147-154. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This can be compared to prior connectionist work addressing segmentation of temporal sequences. [Elman, 1990] uses the size of the errors, and [Doutriaux and Zipser, 1990] use large changes in the activations of the hidden units to indicate segmentation. <ref> [Levin, 1991] </ref> adds a set of auxiliary input units which 2 encode a (discrete) state, set to fixed values in training (supervised) and estimated in testing. In this architecture, the single network has the difficult task of learning two potentially quite different mappings across the same set of units.
Reference: [Lewis et al., 1994] <author> Lewis, P. A. W., Ray, B. K., and Stevens, J. G. </author> <year> (1994). </year> <title> Modeling time series using multivariate adaptive regression splines (MARS). </title> <editor> In Weigend, A. S. and Gershenfeld, N. A., editors, </editor> <title> Time Series Prediction: </title> <booktitle> Forecasting the Future and Understanding the Past, </booktitle> <pages> pages 296-318, </pages> <address> Reading, MA. </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: Splits are often made in an exogenous variable, such as the volatility [Engle, 1982, Bollerslev, 1986, Bollerslev et al., 1990]. Multivariate adaptive regression splines (MARS) [Friedman, 1991] is a more flexible model that has recently been applied to forecasting financial data <ref> [Lewis et al., 1994] </ref>. Radial basis functions, TAR, and MARS models all define the state by proximity in the observed variables.
Reference: [Mangeas et al., 1995] <author> Mangeas, M., Muller, C., and Weigend, A. S. </author> <year> (1995). </year> <title> Forecasting electricity demand using a mixture of nonlinear experts. </title> <booktitle> In World Congress on Neural Networks (WCNN'95), </booktitle> <pages> pages II-48-53. </pages>
Reference-contexts: is structure on several time scales (e.g., daily patterns, weekly patterns, yearly patterns); * multi-stationary: there are different regimes (e.g., holidays vs. workdays, summer vs. winter, etc.). 19 The data, task and performance are described in more detail in [Cottrell et al., 1995]; the gated experts approach is presented in <ref> [Mangeas et al., 1995] </ref>.
Reference: [McCullagh and Nelder, 1989] <author> McCullagh, P. and Nelder, J. A. </author> <year> (1989). </year> <title> Generalized Linear Models. </title> <publisher> Chapman and Hall, London. </publisher>
Reference-contexts: Its goal is to estimate the probability that a given input x was generated by expert j. The hidden units of the gating network have tanh activation functions as building blocks for the nonlinearities (Eq. 1). The outputs of the gating network are normalized exponentials (also called softmax-units) <ref> [McCullagh and Nelder, 1989, Bridle, 1989] </ref>. This choice incorporates into the architecture the constraints that the outputs should be positive and sum to unity. <p> Since the indicator variables are binary, they filter out all but the true term. Note that this assumption of one expert being responsible for each pattern is identical to the assumption needed for Eq. 5. 7 For classification problems, the usual choice is a binomial distribution <ref> [McCullagh and Nelder, 1989, Rumelhart et al., 1995] </ref>. A binomial distribution is described by a single parameter because the mean and the variance are related. <p> As usual, the activation function is chosen such that the update rules become simple <ref> [McCullagh and Nelder, 1989] </ref>): @C M = h j g j (x ; g ) (25) 8 An alternative that also worked for the laser data is to introduce a lower bound for 2 , set to the experimental resolution of the analog-to-digital converter or of 1 bit out of 8
Reference: [Nadas and Mercer, 1996] <author> Nadas, A. and Mercer, R. L. </author> <year> (1996). </year> <title> Hidden Markov models and some connections with artificial neural networks. </title> <editor> In Smolensky, P., Mozer, M. C., and Rumelhart, D. E., editors, </editor> <booktitle> Mathematical Perspectives on Neural Networks. </booktitle> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ. </address>
Reference: [Nix and Weigend, 1995] <author> Nix, D. A. and Weigend, A. S. </author> <year> (1995). </year> <title> Learning local error bars for nonlinear regression. </title> <editor> In Tesauro, G., Touretzky, D. S., and Leen, T. K., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7 (NIPS*94), </booktitle> <pages> pages 488-496. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Recently, [Jordan and Xu, 1995] proved the convergence, and [Waterhouse and Robinson, 1995] applied the hierarchical mixture of linear experts to time series prediction of the sunspots [Weigend et al., 1990, Nowlan and Hinton, 1992] and to nonlinear regression on an example of noise heterogeneity <ref> [Weigend and Nix, 1994, Nix and Weigend, 1995] </ref>. [Xu, 1994] applied this architecture to two linear AR (2) processes (well suited for linear experts). <p> Section 7 analyzes why gated experts help avoid overfitting and compares them with a method developed to predict local error bars <ref> [Weigend and Nix, 1994, Nix and Weigend, 1995] </ref>. <p> If we allow the width of the Gaussian to become a function of the inputs (e.g., by adding a second output unit to the network to predict the local error bar), we obtain a model for estimating the local noise level <ref> [Nix and Weigend, 1995] </ref>. 5 In this article, we assume the output to be a scalar. <p> The denominator normalizes the weightings for that expert. Consider the case that a certain expert j happens to only win a few patterns, and that it fits those with small error (e.g., if the expert is too flexible and overfits the training patterns and consequently underestimates the noise level). <ref> [Nix and Weigend, 1995] </ref> use a cross-validation scheme where a subset of the data is used for the estimation of 2 (x) different from the subset used for the estimation of the network parameters. <p> The expression in square brackets, weighting expert j by its relevance h (t) j for pattern t, is identical to the cost function derived and discussed in <ref> [Nix and Weigend, 1995] </ref> for the case of predicting local error bars with a single network with two output units, one for the conditional mean, the other one for the conditional variance: C LEB = 2 d y (x) 2 (x) # where LEB stands for local error bars. <p> This article considers two alternatives to a standard network trained with global least mean squared errors. Section 7.2 reviews a technique, introduced in <ref> [Weigend and Nix, 1994, Nix and Weigend, 1995] </ref>, originally developed to obtain 22 local error bars for regression and prediction problems. <p> Section 7.3 compares the dynamics of overfitting on the three architectures: gated experts, a network with local error bars performing weighted regression, and a standard single network. 7.2 Estimating Local Error Bars and Weighted Regression The goal of the technique presented in <ref> [Nix and Weigend, 1995] </ref> is to estimate explicitly the local noise level (in addition to the value of the prediction itself). Local means that, just like the prediction, the noise level is a function of the inputs. the y-unit's hidden layer and the input pattern itself.
Reference: [Nowlan, 1991] <author> Nowlan, S. J. </author> <year> (1991). </year> <title> Soft Competitive Adaptation: Neural Network Learning Algorithms based on Fitting Statistical Mixtures. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University. </institution> <note> Technical Report CMU-CS-91-126. </note>
Reference: [Nowlan and Hinton, 1992] <author> Nowlan, S. J. and Hinton, G. E. </author> <year> (1992). </year> <title> Simplifying neural networks by soft weight-sharing. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 473-493. </pages>
Reference-contexts: Recently, [Jordan and Xu, 1995] proved the convergence, and [Waterhouse and Robinson, 1995] applied the hierarchical mixture of linear experts to time series prediction of the sunspots <ref> [Weigend et al., 1990, Nowlan and Hinton, 1992] </ref> and to nonlinear regression on an example of noise heterogeneity [Weigend and Nix, 1994, Nix and Weigend, 1995]. [Xu, 1994] applied this architecture to two linear AR (2) processes (well suited for linear experts).
Reference: [Pawelzik et al., 1996] <author> Pawelzik, K., Kohlmorgen, J., and Muller, K.-R. </author> <year> (1996). </year> <title> Annealed competition of experts for a segmentation and classification of switching dynamics. </title> <journal> Neural Computation, </journal> <volume> 8 </volume> <pages> 340-356. </pages>
Reference-contexts: Furthermore, gated experts can be compared to the approach developed independently by <ref> [Pawelzik et al., 1996] </ref> as follows: (1) Gated experts automatically adjust the local noise levels of the experts in each regimes, whereas Pawelzik et al. externally adjust the global granularity of the approximation, and anneal it during learning. (2) Our gate is connected to the inputs, whereas their gate has no
Reference: [Pearson, 1894] <author> Pearson, K. </author> <title> (1894). Contributions to the mathematical theory of evolution. </title> <journal> Phil. Trans. Royal Soc., </journal> <volume> 185 </volume> <pages> 71-110. </pages> <note> See also V. 185A, p. 195. </note>
Reference-contexts: Finally, in the statistics literature, the use of mixture models goes back for more than a century. Pearson <ref> [Pearson, 1894] </ref> modeled the forehead size of crabs with a mixture of two Gaussians; he used the method of moments to estimate the parameters.
Reference: [Perrone, 1994] <author> Perrone, M. P. </author> <year> (1994). </year> <title> General averaging results for complex optimization. </title> <editor> In Mozer, M. C., Smolensky, P., Touretzky, D. S., Elman, J. L., and Weigend, A. S., editors, </editor> <booktitle> Proceedings of the 1993 Connectionist Models Summer School, </booktitle> <pages> pages 364-371, </pages> <address> Hillsdale, NJ. </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: This fact drives the specialization of the experts (or the carving up of the input space). Note that this is different from simple averaging of different predictors, where we can only hope for reduction of statistical error to the degree that the individual predictors are uncorrelated <ref> [Perrone, 1994, Jacobs, 1995] </ref>. The cost function Eq. 21 is central for the gated experts model. The M-step minimizes this cost function by adjusting the parameters. The parameters under consideration are the variances of the experts, as well as the weights of all the experts and the gating network.
Reference: [Poggio and Girosi, 1990] <author> Poggio, T. and Girosi, F. </author> <year> (1990). </year> <title> Networks for Approximation and Learning. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 78(9) </volume> <pages> 1481-1497. </pages>
Reference-contexts: In this architecture, the single network has the difficult task of learning two potentially quite different mappings across the same set of units. Gated experts can also be compared and contrasted to connectionist architectures with local basis functions: whereas the architecture of radial basis functions <ref> [Broomhead and Lowe, 1988, Casdagli, 1989, Poggio and Girosi, 1990] </ref> does split up the input space into local regions (as opposed to global sigmoids), there is no incentive in the learning algorithm to find regions defined by similar structure, noise level, or dynamics.
Reference: [Poritz, 1988] <author> Poritz, A. B. </author> <year> (1988). </year> <title> Hidden Markov models: A guided tour. </title> <booktitle> In IEEE International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 7-13. </pages>
Reference: [Predoviciu and Jordan, 1995] <author> Predoviciu, M. M. and Jordan, M. I. </author> <year> (1995). </year> <title> Learning the parameters of HMMs with auxilliary input. </title> <type> Technical report, </type> <institution> MIT. </institution>
Reference-contexts: Recently, [Bengio and Frasconi, 1995] and <ref> [Predoviciu and Jordan, 1995] </ref> presented a framework for an input-output hidden Markov model that allows the gate to access both external information (from the inputs) and its own past outputs. the gating network have access to the inputs: they can either share the same inputs, or be given different sets of
Reference: [Press et al., 1992] <author> Press, W. H., Flannery, B. P., Teukolsky, S. A., and Vetterling, W. T. </author> <year> (1992). </year> <title> Numerical Recipes in C: </title> <booktitle> The Art of Scientific Computing. </booktitle> <publisher> Cambridge University Press, </publisher> <address> Cambridge. </address> <month> 27 </month>
Reference-contexts: In the simulations reported here, we use a second order method, the Broyden Fletcher-Goldfarb-Shanno algorithm (BFGS) as described in <ref> [Press et al., 1992] </ref>. It is a batch method: Once the entire learning set has been presented, it computes a descent direction as function of the first and second derivatives, and chooses the best step in this direction.
Reference: [Quandt, 1958] <author> Quandt, R. E. </author> <year> (1958). </year> <title> The estimation of parameters of linear regression system obeying two separate regimes. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 55 </volume> <pages> 873-880. </pages>
Reference-contexts: What if the state is not directly observable? Such latent or hidden states were popularized in the econometrics community some five years ago [Hamilton, 1989, Hamilton, 1990]; they can be traced back to <ref> [Quandt, 1958] </ref>. However, expressed in connectionist language, these models do not have any hidden units: both the gate and the experts are linear. There are different sets of coefficients associated with each regime, but the functions are linear, and the Markov transition probabilities constant.
Reference: [Rabiner, 1989] <author> Rabiner, L. R. </author> <year> (1989). </year> <title> A tutorial on hidden Markov models and selected applications in speech recognition. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 77(2) </volume> <pages> 257-286. </pages>
Reference: [Redner and Walker, 1984] <author> Redner, R. A. and Walker, H. F. </author> <year> (1984). </year> <title> Mixture densities, maximum likelihood and the EM algorithm. </title> <journal> SIAM Review, </journal> <volume> 26 </volume> <pages> 195-237. </pages>
Reference-contexts: The arrival of computers in the last half-century allowed the estimation of mixture models in a maximum likelihood framework. [Titterington et al., 1985] cite more than 130 applications of mixture models on real problems. <ref> [Redner and Walker, 1984] </ref> review mixture density estimation as it was known a decade ago in the statistics community. 1 The problem of estimating local noise levels is known in the statistics literature as noise heterogeneity [Seber and Wild, 1989].
Reference: [Rose et al., 1990] <author> Rose, K., Gurewitz, E., and Fox, G. C. </author> <year> (1990). </year> <title> Statistical mechanics and phase transitions in clustering. </title> <journal> Physical Review Letters, </journal> <volume> 65 </volume> <pages> 945-948. </pages>
Reference-contexts: It reflects a level of granularity for the segmentation, similarly to a level of granularity in clustering <ref> [Rose et al., 1990] </ref>. The annealing of fi is discussed in [Srivastava and Weigend, 1995].
Reference: [Rumelhart et al., 1995] <author> Rumelhart, D. E., Durbin, R., Golden, R., and Chauvin, Y. </author> <year> (1995). </year> <title> Backpropagation: The basic theory. </title> <editor> In Chauvin, Y. and Rumelhart, D. E., editors, Backpropagation: </editor> <booktitle> Theory, Architectures, and Applications, </booktitle> <pages> pages 1-34, </pages> <address> Hillsdale, NJ. </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: Addressing these problems, we present a class of models for time series prediction that we call gated experts. They were introduced into the connectionist community as the mixture of experts [Jacobs et al., 1991], and are also called society of experts <ref> [Rumelhart et al., 1995] </ref>. We use the term gated experts for nonlinearly gated nonlinear experts. The input space can be split nonlinearly through the hidden units of the gating network, and the sub-processes can be nonlinear through the hidden units of the expert networks. <p> value d 4 If there is only a single expert and we assume a Gaussian error model with constant noise level (variance), then this is equivalent to minimizing the squared error between the output and the target value (as can be seen by taking the negative logarithm of the Gaussian) <ref> [Rumelhart et al., 1995] </ref>. <p> Since the indicator variables are binary, they filter out all but the true term. Note that this assumption of one expert being responsible for each pattern is identical to the assumption needed for Eq. 5. 7 For classification problems, the usual choice is a binomial distribution <ref> [McCullagh and Nelder, 1989, Rumelhart et al., 1995] </ref>. A binomial distribution is described by a single parameter because the mean and the variance are related. <p> It can be interpreted from two angles. In a 1-of-K classification framework, it is the standard cost function <ref> [Rumelhart et al., 1995] </ref>. However, if we assume that the g learns to approximate h, it can also be viewed as ( P j p j ln p j ) (with p = g or h), i.e., as the entropy of distributing a pattern across the experts.
Reference: [Rumelhart et al., 1986] <author> Rumelhart, D. E., McClelland, J. L., and group, P. R. </author> <year> (1986). </year> <title> Parallel Distributed Processing: Exploration in the Microstructure of Cognition. Volume 1: Cognition. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: They can be linear, assuming that the next value is a linear superposition of preceding values [Yule, 1927], or they can be nonlinear, conveniently described in the quite general language of neural networks with hidden units <ref> [Rumelhart et al., 1986, Lapedes and Farber, 1987] </ref>. Such single, global, and traditionally univariate models are well suited to problems with stationary dynamics. However, the assumption of stationarity is violated in many real-world time series.
Reference: [Seber and Wild, 1989] <author> Seber, G. A. F. and Wild, C. J. </author> <year> (1989). </year> <title> Nonlinear Regression. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference-contexts: et al., 1985] cite more than 130 applications of mixture models on real problems. [Redner and Walker, 1984] review mixture density estimation as it was known a decade ago in the statistics community. 1 The problem of estimating local noise levels is known in the statistics literature as noise heterogeneity <ref> [Seber and Wild, 1989] </ref>.
Reference: [Srivastava and Weigend, 1995] <author> Srivastava, A. N. and Weigend, A. S. </author> <year> (1995). </year> <title> Improving time series segmentation with gated experts through annealing. </title> <type> Technical Report CU-CS-795-95, </type> <institution> University of Colorado at Boulder, Computer Science Department. </institution>
Reference-contexts: It reflects a level of granularity for the segmentation, similarly to a level of granularity in clustering [Rose et al., 1990]. The annealing of fi is discussed in <ref> [Srivastava and Weigend, 1995] </ref>. <p> The gated experts described in this paper serve as a starting point into several directions. The idea of annealing to achieve stable segmentation, briefly mentioned in Section 2.3.4, is discussed in detail in <ref> [Srivastava and Weigend, 1995] </ref>. The present article assumes that the switching occurs at random. To capture the dynamics of the switching process, we introduce recurrence into the gating network.
Reference: [Tanner, 1993] <author> Tanner, M. A. </author> <year> (1993). </year> <title> Tools for Statistical Inference. </title> <publisher> Springer-Verlag, 2nd edition. </publisher>
Reference: [Titterington et al., 1985] <author> Titterington, D., Smith, A. F. M., and Makov, U. E. </author> <year> (1985). </year> <title> Statistical Analysis of Finite Mixture Distributions. </title> <publisher> John Wiley, </publisher> <address> New York. </address>
Reference-contexts: Pearson [Pearson, 1894] modeled the forehead size of crabs with a mixture of two Gaussians; he used the method of moments to estimate the parameters. The arrival of computers in the last half-century allowed the estimation of mixture models in a maximum likelihood framework. <ref> [Titterington et al., 1985] </ref> cite more than 130 applications of mixture models on real problems. [Redner and Walker, 1984] review mixture density estimation as it was known a decade ago in the statistics community. 1 The problem of estimating local noise levels is known in the statistics literature as noise heterogeneity
Reference: [Tong and Lim, 1980] <author> Tong, H. and Lim, K. S. </author> <year> (1980). </year> <title> Threshold autoregression, limit cycles and cyclical data. </title> <journal> J. Roy. Stat. Soc. B, </journal> <volume> 42 </volume> <pages> 245-292. </pages>
Reference-contexts: In the time series community, the idea of splitting an input space into subspaces is not new. One of the first examples is the threshold autoregressive (TAR) model <ref> [Tong and Lim, 1980] </ref>. In contrast to gated experts, the splits there are very simple and ad hoc; there is no probabilistic interpretation. TAR models still are quite popular in economics and econometrics.
Reference: [Waterhouse et al., 1996] <author> Waterhouse, S. R., MacKay, D. J. C., and Robinson, A. J. </author> <year> (1996). </year> <title> Bayesian modesl for mixtures of epxerts. </title> <booktitle> In Advances in Neural Information Processing Systems 8 (NIPS*95). </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: The need to introduce terms such as 2 0 j shows the limitations of that framework. Gated experts can also be expressed in a MAP (maximum a posterior)-framework. Eq. 23 is an approximation to the general case of a Gamma prior <ref> [Waterhouse et al., 1996] </ref>. 9 Since we use nonlinear hidden units, the weights of the networks cannot be computed directly but require iterative techniques. <p> Choosing an appropriate prior is an important part of modeling, particularly for short and noisy data sets. 9 In our case of nonlinear experts, we do not subtract in the denominator the effective number of well-determined parameters of expert j, as <ref> [Waterhouse et al., 1996] </ref> do in their treatment of linear experts. <p> Eq. 23 is a soft version, penalizing the deviation from 2 0 j . Formally, this can be related to assuming ln ( 2 ) to be drawn from a Gamma distribution <ref> [Waterhouse et al., 1996] </ref>. Beliefs about the outputs of the gate, g j , are incorporated in a similar way.
Reference: [Waterhouse and Robinson, 1995] <author> Waterhouse, S. R. and Robinson, A. J. </author> <year> (1995). </year> <title> Non-linear prediction of acoustic vectors using hierarchical mixture of epxerts. </title> <editor> In Tesauro, G., Touretzky, D. S., and Leen, T. K., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7 (NIPS*94), </booktitle> <pages> pages 835-842. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Recently, [Jordan and Xu, 1995] proved the convergence, and <ref> [Waterhouse and Robinson, 1995] </ref> applied the hierarchical mixture of linear experts to time series prediction of the sunspots [Weigend et al., 1990, Nowlan and Hinton, 1992] and to nonlinear regression on an example of noise heterogeneity [Weigend and Nix, 1994, Nix and Weigend, 1995]. [Xu, 1994] applied this architecture to two
Reference: [Weigend and Gershenfeld, 1994] <author> Weigend, A. S. and Gershenfeld, N. A., </author> <title> editors (1994). Time Series Prediction: Forecasting the Future and Understanding the Past. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA. </address>
Reference-contexts: Section 3 summarizes the performance of the architecture and learning algorithm on several time series problems. More specifically then, Section 4 discusses the instructive example of computer generated data with regime switches. Section 5 predicts and analyzes the laser data from the Santa Fe Time Series Competition <ref> [Weigend and Gershenfeld, 1994] </ref> and Section 6 applies gated experts to predicting the electricity demand of France. Section 7 analyzes why gated experts help avoid overfitting and compares them with a method developed to predict local error bars [Weigend and Nix, 1994, Nix and Weigend, 1995]. <p> The minimum of the test error is 0.095. hidden units each. The minimum of the test error is 0.092. 5 Laboratory Data 5.1 Data: Laser (Deterministic Chaos) The second example applies gated experts to the laser time series from the Santa Fe Time Series Prediction and Analysis Competition <ref> [Weigend and Gershenfeld, 1994] </ref>. 13 The laser is a stationary system on the time scale of the observations of the time series used here. Its behavior can be approximated reasonably well by a set of three coupled, nonlinear differential equations, the Lorenz equations. These equations are invariant under time shift.
Reference: [Weigend et al., 1990] <author> Weigend, A. S., Huberman, B. A., and Rumelhart, D. E. </author> <year> (1990). </year> <title> Predicting the future: A connectionist approach. </title> <journal> International Journal of Neural Systems, </journal> <volume> 1 </volume> <pages> 193-209. </pages>
Reference-contexts: Recently, [Jordan and Xu, 1995] proved the convergence, and [Waterhouse and Robinson, 1995] applied the hierarchical mixture of linear experts to time series prediction of the sunspots <ref> [Weigend et al., 1990, Nowlan and Hinton, 1992] </ref> and to nonlinear regression on an example of noise heterogeneity [Weigend and Nix, 1994, Nix and Weigend, 1995]. [Xu, 1994] applied this architecture to two linear AR (2) processes (well suited for linear experts). <p> It counts the significantly nonzero weights; the weights are assumed to be drawn from a distribution that is the sum of a uniform distribution (for the weights that should be present) and a Gaussian (for the weights that should be absent) <ref> [Weigend et al., 1990] </ref>. 2.3.4 Improving segmentation through annealing A further modification to the learning improves the segmentation implied by the outputs of the gating network. <p> This indicates that the network is learning more features at that stage in training that do not generalize to new data than features that do. One technique against overfitting then simply consists of stopping training early and taking the network at the minimum of a validation set <ref> [Weigend et al., 1990] </ref>. As training proceeds, the network tends to shift its resources towards the high noise regions: the more noisy a data point, the bigger its error and thus the bigger its effect in error backpropagation.
Reference: [Weigend and LeBaron, 1994] <author> Weigend, A. S. and LeBaron, B. </author> <year> (1994). </year> <title> Evaluating neural network predictors by bootstrapping. </title> <booktitle> In Proceedings of International Conference on Neural Information Processing (ICONIP'94), </booktitle> <pages> pages 1207-1212. </pages> <note> Technical Report CU-CS-725-94, </note> <institution> Computer Science Department, University of Colorado at Boulder, ftp://ftp.cs.colorado.edu/pub/Time-Series/MyPapers/bootstrap.ps. </institution>
Reference-contexts: For gated experts with potentially only 10 a few patterns assigned to some expert, this scheme becomes statistically very unreliable; it strongly depends on the specific splits of the data into the different subsets <ref> [Weigend and LeBaron, 1994] </ref>.
Reference: [Weigend and Nix, 1994] <author> Weigend, A. S. and Nix, D. A. </author> <year> (1994). </year> <title> Predictions with confidence intervals (local error bars). </title> <booktitle> In Proceedings of the International Conference on Neural Information Processing (ICONIP'94), </booktitle> <pages> pages 1207-1212, </pages> <address> Seoul, Korea. </address>
Reference-contexts: Recently, [Jordan and Xu, 1995] proved the convergence, and [Waterhouse and Robinson, 1995] applied the hierarchical mixture of linear experts to time series prediction of the sunspots [Weigend et al., 1990, Nowlan and Hinton, 1992] and to nonlinear regression on an example of noise heterogeneity <ref> [Weigend and Nix, 1994, Nix and Weigend, 1995] </ref>. [Xu, 1994] applied this architecture to two linear AR (2) processes (well suited for linear experts). <p> Section 7 analyzes why gated experts help avoid overfitting and compares them with a method developed to predict local error bars <ref> [Weigend and Nix, 1994, Nix and Weigend, 1995] </ref>. <p> This article considers two alternatives to a standard network trained with global least mean squared errors. Section 7.2 reviews a technique, introduced in <ref> [Weigend and Nix, 1994, Nix and Weigend, 1995] </ref>, originally developed to obtain 22 local error bars for regression and prediction problems. <p> Reprinted from <ref> [Weigend and Nix, 1994] </ref>. The cost function was discussed in Section 2.4 as Eq. 30: C LEB = 2 d y (x) 2 (x) 2 # Its derivation in a maximum likelihood framework assumes Gaussian distributed errors on the outputs.
Reference: [Weigend and Srivastava, 1995] <author> Weigend, A. S. and Srivastava, A. N. </author> <year> (1995). </year> <title> Predicting conditional probability distributions: A connectionist approach. </title> <journal> International Journal of Neural Systems, </journal> <volume> 6 </volume> <pages> 109-118. </pages>
Reference-contexts: In that case, it can still be interesting to read off the entire conditional density and compare it to a simple density estimation with Gaussians [Bishop, 1994], or to nonparametric density estimation using fractional binning <ref> [Weigend and Srivastava, 1995] </ref>. In order to evaluate the likelihood of the data given the modelhow well the model predicts the observed datawe need to assume a specific distribution for the measurement errors. Since this paper focuses on regression (continuous, unbounded output), a Gaussian error model is a reasonable choice.
Reference: [Xu, 1994] <author> Xu, L. </author> <year> (1994). </year> <title> Signal segmentation by finite mixture model and EM algorithm. </title> <booktitle> In Proceedings of the 1994 International Symposium on Artificial Neural Networks (ISANN'94), </booktitle> <pages> pages 453-458, </pages> <address> Tainan, Taiwan. </address>
Reference-contexts: Xu, 1995] proved the convergence, and [Waterhouse and Robinson, 1995] applied the hierarchical mixture of linear experts to time series prediction of the sunspots [Weigend et al., 1990, Nowlan and Hinton, 1992] and to nonlinear regression on an example of noise heterogeneity [Weigend and Nix, 1994, Nix and Weigend, 1995]. <ref> [Xu, 1994] </ref> applied this architecture to two linear AR (2) processes (well suited for linear experts).
Reference: [Yule, 1927] <author> Yule, G. </author> <year> (1927). </year> <title> On a method of investigating periodicity in disturbed series with special reference to wolfer's sunspot numbers. </title> <journal> Phil. Trans. Roy. Soc. London, </journal> <volume> A 226 </volume> <pages> 267-298. 28 </pages>
Reference-contexts: 1 Introduction 1.1 Different regimes with different noise levels: the need for gated experts Conventional time series models are global models. They can be linear, assuming that the next value is a linear superposition of preceding values <ref> [Yule, 1927] </ref>, or they can be nonlinear, conveniently described in the quite general language of neural networks with hidden units [Rumelhart et al., 1986, Lapedes and Farber, 1987]. Such single, global, and traditionally univariate models are well suited to problems with stationary dynamics.
References-found: 66

