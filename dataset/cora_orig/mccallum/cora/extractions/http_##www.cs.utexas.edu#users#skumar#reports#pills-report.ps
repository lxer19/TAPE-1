URL: http://www.cs.utexas.edu/users/skumar/reports/pills-report.ps
Refering-URL: http://www.cs.utexas.edu/users/skumar/reports.html
Root-URL: 
Title: Learning General Propositions An intermediate approach to CNF and DNF learners.  
Author: Shailesh Kumar 
Date: December 18, 1996  
Address: Austin.  
Affiliation: Department of Computer Science University of Texas at  
Abstract: This paper proposes and evaluates an Inductive learning algorithm called Pills that learns general propositional clauses instead of learning specific forms of propositional clauses like disjunctive normal forms (DNF) or conjunctive normal forms (CNF). The learning algorithm makes use of a probabilistic criteria to first pick the best literal and then recursively calls itself to learn conjunctions or disjunctions with that literal and the remaining examples that are yet to be covered. The hypothesis space of this representation is the superset of that of the CNF and DNF representations. The performance of the algorithm is compared with that of the CNF and DNF learners over standard data sets from UCI repository [6]. It was found that even in the bigger hypothesis space, Pills could converge to a reasonably accurate concept that is atleaset as good as the CNF and DNF learners. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Brieman, L. Friedman, J, H., Olshen, R. A., & Stone, C. J., </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <address> Belmont, CA. </address>
Reference-contexts: The representation language not only determines the bias of the hypothesis being learnt, but it also determines the learning algorithms to be used. Various representation languages like classification trees like CART <ref> [1] </ref> ID3 [8] and C4.5 [10] ), Neural Networks, and disjunctive normal forms [2],[7] and Conjunctive normal forms [5] etc can be used to learn a concept. In this work, another representation language called Pills is introduced.
Reference: 2. <author> Clark, P. & Niblett, T. </author> <year> (1992). </year> <title> The CN2 induction Algorithm. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 261-284. </pages>
Reference: 3. <author> Michalski, R. S. </author> <year> (1983). </year> <title> A Theory and methodology of inductive learning. </title> <editor> In R. S. Michalski, J. G. Carbonell. & T.M.Mitchell, </editor> <booktitle> (Eds) Machine Learning:An Artificial Intelligence Approach </booktitle>
Reference: 4. <author> Mitchell, T. </author> <title> M (1996), Machine Learning, </title> <publisher> McGraw Hill. </publisher>
Reference: 5. <author> Mooney, R. J.(1995), </author> <title> Encouraging Experimental Results on Learning CNF,Machine Learning, </title> <type> 19 </type> <institution> 79-92,1995. </institution>
Reference-contexts: The representation language not only determines the bias of the hypothesis being learnt, but it also determines the learning algorithms to be used. Various representation languages like classification trees like CART [1] ID3 [8] and C4.5 [10] ), Neural Networks, and disjunctive normal forms [2],[7] and Conjunctive normal forms <ref> [5] </ref> etc can be used to learn a concept. In this work, another representation language called Pills is introduced. An inductive learning algorithm for learning concepts in this language is proposed and the language and the algorithm are evaluated for standard training data sets from UCI repository [6].
Reference: 6. <author> Murphy P. M. & Aha, D. W. </author> <title> (1993) UCI Repository of Machine Learning Databases. </title> <institution> Department of Information Computer and Science, University of California, </institution> <address> Irvine, CA. </address>
Reference-contexts: In this work, another representation language called Pills is introduced. An inductive learning algorithm for learning concepts in this language is proposed and the language and the algorithm are evaluated for standard training data sets from UCI repository <ref> [6] </ref>. This language essentially learns a boolean circuit where the inputs are propositions on feature value pairs. <p> Finally, the training time is the time it takes to train the learner from the given set of examples. Two data sets from UCI repository <ref> [6] </ref>, Soybean disease diagnosis and DNA promoter data were used in these experiments. The Soybean disease data had a total of 289 examples out of which 49 examples were kept aside as test data and the remaining were used to choose training examples from.
Reference: 7. <author> Pagallo, G., & Haussler, D. </author> <year> (1990). </year> <title> Boolean feature discovery in empirical learning. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 71-100. 16 </pages>
Reference: 8. <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of Decision Trees, </title> <booktitle> Machine Learning 1(1) </booktitle> <pages> 81-106 </pages>
Reference-contexts: The representation language not only determines the bias of the hypothesis being learnt, but it also determines the learning algorithms to be used. Various representation languages like classification trees like CART [1] ID3 <ref> [8] </ref> and C4.5 [10] ), Neural Networks, and disjunctive normal forms [2],[7] and Conjunctive normal forms [5] etc can be used to learn a concept. In this work, another representation language called Pills is introduced. <p> The results were compared with those of the DNF learner Pfoil <ref> [8] </ref> and the CNF learner PfoilCnf [8]. The three criteria that were used to evaluate the methods were the test accuracy, the concept complexity and the training time. The test accuracy is basically the fraction of the correctly classified test examples by the learnt concept. <p> The results were compared with those of the DNF learner Pfoil <ref> [8] </ref> and the CNF learner PfoilCnf [8]. The three criteria that were used to evaluate the methods were the test accuracy, the concept complexity and the training time. The test accuracy is basically the fraction of the correctly classified test examples by the learnt concept. These examples were not part of the training set of examples. <p> The concept complexity of all the three learners is nearly same for soybean data as can be seen 14 from figure 8 although for the DNA data the Pills concept complexity is significantly higher than PfoilCnf <ref> [8] </ref>. One reason for this could be the strong M-of-N bias of the DNA data. The current implementation of Pills is not using any pruning. Although the algorithm provides for pruning using the threshold parameter T used in steps 2 through 4 of the algorithm. <p> Although the algorithm provides for pruning using the threshold parameter T used in steps 2 through 4 of the algorithm. This could be leading to over fitting and learning cumbersome and less than expected accurate concepts by Pills. Finally the training time of Pills and Pfoil <ref> [8] </ref> are nearly same. This is because of the nature of the algorithms used. The time complexity analysis of Pills shown in section 2 and those for Pfoil [8] and PfoilCnf [8] are same although the difference in the complexity of the concept learnt causes the differences in the training time. <p> Finally the training time of Pills and Pfoil <ref> [8] </ref> are nearly same. This is because of the nature of the algorithms used. The time complexity analysis of Pills shown in section 2 and those for Pfoil [8] and PfoilCnf [8] are same although the difference in the complexity of the concept learnt causes the differences in the training time. 5 Future Work The Pills algorithm could be further improved by making proper use of the thresholding parameter T. <p> Finally the training time of Pills and Pfoil <ref> [8] </ref> are nearly same. This is because of the nature of the algorithms used. The time complexity analysis of Pills shown in section 2 and those for Pfoil [8] and PfoilCnf [8] are same although the difference in the complexity of the concept learnt causes the differences in the training time. 5 Future Work The Pills algorithm could be further improved by making proper use of the thresholding parameter T.
Reference: 9. <author> Quinlan, J. R. </author> <year> (1990). </year> <title> Learning Logical definitions from Relations. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 239-266. </pages>
Reference: 10. <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, </address> <month> CA. </month> <title> 17 statistically significant though Pills shows a marginal improvement over Pfoil and PfoilCnf. 18 The main reason being that the bias of the data is M-of-N and the Pfoil is using some prunning while the bias of Pills does not cover M-of-N bia s. </title> <journal> Although eventually, </journal> <note> Pills performs as well as Pfoil and PfoilCnf. 19 propositional clause) 20 21 22 </note>
Reference-contexts: The representation language not only determines the bias of the hypothesis being learnt, but it also determines the learning algorithms to be used. Various representation languages like classification trees like CART [1] ID3 [8] and C4.5 <ref> [10] </ref> ), Neural Networks, and disjunctive normal forms [2],[7] and Conjunctive normal forms [5] etc can be used to learn a concept. In this work, another representation language called Pills is introduced.
References-found: 10

