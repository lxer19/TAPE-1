URL: http://polaris.cs.uiuc.edu/reports/1413.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Email: Email: fkuba, cdp, gallivang@csrd.uiuc.edu  
Title: THE SYNERGETIC EFFECT OF COMPILER, ARCHITECTURE, AND MANUAL OPTIMIZATIONS ON THE PERFORMANCE OF CFD ON MULTIPROCESSORS  
Author: Masayuki Kuba Constantine D. Polychronopoulos, Kyle Gallivan 
Address: IL 61801  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign,  
Abstract: This paper discusses the comprehensive performance profiling, improvement and benchmark-ing of a Computational Fluid Dynamics code, one of the Grand Challenge applications, on three popular multiprocessors. In the process of analyzing performance we considered language, compiler, architecture, and algorithmic changes and quantified each of them and their incremental contribution to bottom-line performance. We demonstrate that parallelization alone cannot result in significant gains if the granularity of parallel threads and the effect of parallelization on data locality are not taken into account. Unlike benchmarking studies that often focus on the performance or effectiveness of parallelizing compilers on specific loop kernels, we used the entire CFD code to measure the global effectiveness of compilers and parallel architectures. We probed the performance bottlenecks in each case and derived solutions which eliminate or neutralize the performance inhibiting factors. The major conclusion of our work is that overall performance is extremely sensitive to the synergetic effects of compiler optimizations, algorithmic and code tuning, and architectural idiosyncrasies. fl Asahi Chemical Industry Co., LTD., Fuji, 416, Japan
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Alliant Computer Systems Corporation. FX/FORTRAN Language Manual: Volume1: Guideline, </institution> <year> 1986. </year>
Reference-contexts: Parallel: In each case, parallel execution time was computed by running the parallel version of the CFD obtained as follows. The original code was compiled by each system's native parallelizing compiler. For the Alliant, we used the Alliant compiler with the automatic parallelization/vectorization options <ref> [1] </ref>. For the SGI Challenge, we used the SGI Parallel Fortran Accelerator (PFA) [10]. Parafrase-2 and Polaris: The original code was compiled by each experimental source-to-source parallelizing compiler, Parafrase-2 [7] and Polaris [9].
Reference: [2] <institution> Alliant Computer Systems Corporation. FX/SERIES Architecture Manual, </institution> <year> 1988. </year>
Reference-contexts: The Alliant FX/80 is an 8-processor MIMD system where each processor has a vector unit and all processors share a 4-quadrant cache <ref> [2] </ref>. The Alliant FX/2800 was the next generation of MIMD machines from Alliant and is a 20-processor machine using the Intel i860 processor.
Reference: [3] <author> M. R. Haghighat. </author> <title> Symbolic Analysis for Parallelizing Compilers. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1994. </year>
Reference-contexts: Compilers need the ability to quantitatively analyze sections of code and determine the trade-off between the pay-off and the cost of a particular transformation. Symbolic program analysis <ref> [3] </ref> provides a powerful means for computing the symbolic size of code sections; however, with the exception of Parafrase-2 no other compiler provides this capability. 4 Architectural Bottlenecks After all compiler transformations were manually incorporated in our code, the performance of the paral-lelized version was compared on the SGI Challenge and
Reference: [4] <author> M. Kuba. </author> <title> On the parallelization of a cfd code. </title> <type> Technical Report 1412, </type> <institution> Center for Supercomputing Research and Development, </institution> <year> 1995. </year>
Reference-contexts: COEFV was extensively parallelized by the compiler optimizations discussed in Section 3. The structure of the calculations in SOLVEP and SOLVEV are similar, and we thus limit the discussion to SOLVEP a similar approach was used for SOLVEV. The details of the hand-restructuring of the code are given in <ref> [4] </ref>. In this paper, we outline the changes at the highest possible level. The computations in SOLVEP sweep a 3-dimensional structure as shown in Figure 10 (a).
Reference: [5] <author> J. A. Meijerink and H. A. Van Der Vorst. </author> <title> Guidelines for the usage of incomplete decompositions in solving sets of linear equations as they occur in practical problems. </title> <journal> Journal of Computational Physics, </journal> <volume> 44 </volume> <pages> 134-155, </pages> <year> 1981. </year>
Reference-contexts: Each of Steps 2 through 5 in Figure 1 consists of three modules: (i) Assemble a system of linear equations by discretization, (ii) modify those equations according to boundary conditions, and (iii) solve the system of linear equations. We employ MICCG (Modified Incomplete Cholesky Conjugate Gradient) <ref> [5] </ref> to solve the continuity equation (Module SOLVEP), and ILUCR (Incomplete LU Conjugate Residual) [6] to solve the Navier-Stokes Equations (Module SOLVEV). The performance is evaluated using the 3-dimensional space as shown in Figure 2.
Reference: [6] <author> T. Muraoka and et al. </author> <title> Large Scale Numerical Simulation. </title> <address> Iwanami, </address> <year> 1990. </year> <title> in Japanese. </title>
Reference-contexts: We employ MICCG (Modified Incomplete Cholesky Conjugate Gradient) [5] to solve the continuity equation (Module SOLVEP), and ILUCR (Incomplete LU Conjugate Residual) <ref> [6] </ref> to solve the Navier-Stokes Equations (Module SOLVEV). The performance is evaluated using the 3-dimensional space as shown in Figure 2.
Reference: [7] <author> David A. Padua and Rudolf Eigenmann. </author> <title> Polaris: A new generation parallelizing compiler for mpp's. </title> <type> Technical report, </type> <institution> Center for Supercomputing Research and Development, </institution> <year> 1993. </year>
Reference-contexts: For the Alliant, we used the Alliant compiler with the automatic parallelization/vectorization options [1]. For the SGI Challenge, we used the SGI Parallel Fortran Accelerator (PFA) [10]. Parafrase-2 and Polaris: The original code was compiled by each experimental source-to-source parallelizing compiler, Parafrase-2 <ref> [7] </ref> and Polaris [9]. The parallelized source output from the two parallelizers was compiled again by the back-end Fortran compiler of each machine. Both the SGI and the Alliant back-end compilers parallelize loops only when explicitly indicated.
Reference: [8] <author> S. V. Patankar. </author> <title> Numerical Heat Transfer and Fluid Flow. </title> <publisher> Hemisphere Publishing, </publisher> <year> 1980. </year>
Reference-contexts: Our work focused on the comprehensive and multi-level analysis of a complete commercial scientific application, a Computational Fluid Dynamics (CFD) code based on SIMPLE (Semi-Implicit Method for Pressure-Linked Equation) <ref> [8] </ref>. This paper reports on the performance bottlenecks of hardware, architectures, compilers and operating systems, and on the importance of manual optimizations and code tuning. <p> It comes as no surprise that runtime overhead plays a more important role on performance than the serial bottlenecks in Amdahl's law. 2 The CFD code and Experimental Environment 3 1 m cube, divided in 40 meshes in each direction. Our CFD program is based on the SIMPLE method <ref> [8] </ref>, which is outlined in Figure 1. The algorithm first increments the time step (Step 1 in Figure 1), and then iteratively solves the Navier-Stokes equations to compute the velocity (Steps 2-4) and the continuity equations (Step 5) until they converge.
Reference: [9] <author> C. D. Polychronopoulos and et al. </author> <title> Parafrase-2: An environment for parallelizing, partitioning, synchronizing and scheduling programs on multiprocessors. </title> <booktitle> In International Conference for Parallel Processing, </booktitle> <year> 1989. </year>
Reference-contexts: For the Alliant, we used the Alliant compiler with the automatic parallelization/vectorization options [1]. For the SGI Challenge, we used the SGI Parallel Fortran Accelerator (PFA) [10]. Parafrase-2 and Polaris: The original code was compiled by each experimental source-to-source parallelizing compiler, Parafrase-2 [7] and Polaris <ref> [9] </ref>. The parallelized source output from the two parallelizers was compiled again by the back-end Fortran compiler of each machine. Both the SGI and the Alliant back-end compilers parallelize loops only when explicitly indicated.
Reference: [10] <author> Silicon Graphics Inc. </author> <title> POWER Fortran Accelerator User's Guide. online. </title> <type> 23 </type>
Reference-contexts: The original code was compiled by each system's native parallelizing compiler. For the Alliant, we used the Alliant compiler with the automatic parallelization/vectorization options [1]. For the SGI Challenge, we used the SGI Parallel Fortran Accelerator (PFA) <ref> [10] </ref>. Parafrase-2 and Polaris: The original code was compiled by each experimental source-to-source parallelizing compiler, Parafrase-2 [7] and Polaris [9]. The parallelized source output from the two parallelizers was compiled again by the back-end Fortran compiler of each machine. <p> If iadd=0, there is no bus contention. If iadd=1, there is bus contention. values thus reducing performance. This is implicitly suggested in <ref> [10] </ref>. Although we can avoid false sharing by either method, using local variables appears to be more effective, possibly due to the opportunity of extensive register reuse.
References-found: 10

