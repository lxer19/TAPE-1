URL: http://www.cse.unsw.edu.au/~nickt/doc/tower.ps.gz
Refering-URL: http://www.cse.unsw.edu.au/~nickt/index.html
Root-URL: http://www.cse.unsw.edu.au
Email: tom-@cse.unsw.edu.au  
Title: Exploring Architecture Variations in Constructive Cascade Networks  
Author: N.K. Treadgold and T.D. Gedeon 
Address: nickt  
Affiliation: Department of Information Engineering School of Computer Science Engineering, The University of New South Wales  
Abstract: Constructive neural networks employing a cascade architecture face a number of problems. These include large propagation delays, high fan-in and irregular network connections. These problems are especially relevant with regards to VLSI implementation of these algorithms. This work explores the effect of limiting the depth of the cascades created by the CasPer algorithm, a constructive network algorithm. Instead of a single cascade of hidden neurons, a series of cascade towers are built. The maximum size of each tower is set prior to training, thus limiting maximum network depth, creating regular connections and enabling a reduction in maximum fan-in. The networks created in this manner are shown to maintain or better network generalization over a number of different tower sizes. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Treadgold, </author> <title> N.K. and Gedeon, T.D. A Cascade Network Employing Progressive RPROP, </title> <booktitle> Int. Work Conf. On Artificial and Natural Neural Networks, </booktitle> <year> 1997, </year> <pages> pp. 733-742. </pages>
Reference: [2] <author> Treadgold, </author> <title> N.K. and Gedeon, T.D. Extending CasPer: A Regression Survey, </title> <booktitle> Int. Conf. On Neural Information Processing, </booktitle> <year> 1997, </year> <note> to appear. </note>
Reference-contexts: CasPer is a constructive algorithm, which inserts hidden neurons one at a time to form a cascade architecture, similar to Cascade Correlation (CasCor) [4]. CasPer has been shown to produce networks with fewer hidden neurons than CasCor, while also improving the resulting network generalization, especially with regression tasks <ref> [2] </ref>. The reasons for CasPers improved performance is that it does not use either CasCors correlation measure, which can cause poor generalization performance [5], or weight freezing, which can lead to oversize networks [6].
Reference: [3] <author> Treadgold, </author> <title> N.K. and Gedeon, T.D. Extending and Benchmarking the CasPer Algorithm, </title> <address> AI97, Perth Australia, </address> <year> 1997, </year> <note> to appear. </note>
Reference: [4] <author> Fahlman, S.E. and Lebiere, C. </author> <booktitle> The cascade-correlation learning architecture, Advances in Neural Information Processing, </booktitle> <volume> vol. 2, </volume> <editor> D.S. Touretzky, (Ed.) </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kauffman, </publisher> <year> 1990, </year> <pages> pp. 524-532. </pages>
Reference-contexts: 1. Introduction The CasPer [1-3] algorithm has been shown to be a powerful method for training neural networks. CasPer is a constructive algorithm, which inserts hidden neurons one at a time to form a cascade architecture, similar to Cascade Correlation (CasCor) <ref> [4] </ref>. CasPer has been shown to produce networks with fewer hidden neurons than CasCor, while also improving the resulting network generalization, especially with regression tasks [2]. <p> A problem faced by constructive cascade algorithms such as CasPer is that the number of hidden neurons that are installed is potentially unlimited. Hence both the fan-in and propagation delay through cascade networks are inherently large, as noted early on by Fahlman <ref> [4] </ref>. In addition, the cascade structure results in irregular network connections. These features make VLSI implementation of such networks difficult. A further difficulty faced by the cascade architecture is that the number of weights added per hidden neuron is exponential. <p> Each spiral is made up of 97 points, which the network must learn to distinguish. Each training pattern consists of two inputs (the x,y coordinates) and a single output (the spiral classification). This problem was used by Fahlman <ref> [4] </ref> to demonstrate the effectiveness of the CasCor algorithm on a problem known to be very difficult for traditional Back Propagation to solve. The standard test set for the two spirals data set was used to measure the resulting generalization ability of the networks. <p> At this point the mean, standard deviation and median for the following characteristics were measured: number of connection crossings, hidden neurons inserted, percentage correct on the test set, and the number of network weights. Fahlman <ref> [4] </ref> defines the term connection crossings as the number of multiply-accumulate steps to propagate activation values forward through the network and error values backward. This measure of computational cost is used instead of epochs since the network sizes are continually changing. These results are shown in Table 1.
Reference: [5] <author> Hwang, J., You, S., Lay, S. and I. Jou, </author> <title> The Cascade-Correlation Learning: A Projection Pursuit Learning Perspective, </title> <journal> IEEE Trans. Neural Networks 7(2), </journal> <year> 1996, </year> <pages> pp. 278-289. </pages>
Reference-contexts: CasPer has been shown to produce networks with fewer hidden neurons than CasCor, while also improving the resulting network generalization, especially with regression tasks [2]. The reasons for CasPers improved performance is that it does not use either CasCors correlation measure, which can cause poor generalization performance <ref> [5] </ref>, or weight freezing, which can lead to oversize networks [6]. A problem faced by constructive cascade algorithms such as CasPer is that the number of hidden neurons that are installed is potentially unlimited. <p> ( ) sin ( ( . ) ) 1 2 1 1 13356 15 1 3 0 6 1 = - + - p - x3 0 5 2 2 4 0 9 sin ( ( . ) )).p The generation of training and test data follows the method of <ref> [5] </ref>: two sets of training data were created, one noiseless and one noisy, using 225 randomly selected pairs [0,1] of abscissa values -(x 1 ,x 2 )-.
Reference: [6] <author> Kwok, T. and Yeung, D. </author> <title> Experimental Analysis of Input Weight Freezing in Constructive Neural Networks, </title> <booktitle> Proc IEEE Int. Conf. On Neural Networks, </booktitle> <year> 1993, </year> <pages> pp. 511-516. </pages>
Reference-contexts: The reasons for CasPers improved performance is that it does not use either CasCors correlation measure, which can cause poor generalization performance [5], or weight freezing, which can lead to oversize networks <ref> [6] </ref>. A problem faced by constructive cascade algorithms such as CasPer is that the number of hidden neurons that are installed is potentially unlimited. Hence both the fan-in and propagation delay through cascade networks are inherently large, as noted early on by Fahlman [4].
Reference: [7] <author> Hwang, J., Lay, S., Maechler, R. and Martin, D. </author> <title> Regression Modeling in BackPropagation and Projection Pursuit Learning, </title> <journal> IEEE Trans. Neural Networks 5(3), </journal> <year> 1994, </year> <pages> pp. 342 353. </pages>
Reference: [8] <author> Shin, Y. and Ghost, J. </author> <title> Ridge Polynomial Networks, </title> <journal> IEEE Trans. Neural Networks 6(2), </journal> <year> 1995, </year> <pages> pp. 610-622. </pages>
Reference-contexts: Similar network architectures using a single layer of HONs are constructed by Projection Pursuit Learning [5,7], where the HON complexity is specified by the order of the Hermite polynomial used, and the Ridge Polynomial Network <ref> [8] </ref>, which uses increasing orders of sigma-pi networks as its HON. Previous work by Phatak and Koren [9] explored the effect of limiting network depth and fan-in with CasCor. <p> This would reduce the chances of producing too complex a HON at an early stage. This strategy is used by Ridge Polynomial Networks <ref> [8] </ref>, which start with a low degree sigma-pi network and increase the degree as training continues. Adapting this strategy to CasPer is the subject of future work. 0 0.04 0.08 0.12 0.16 0.2 Hidden Unit F V CasPer 3_tower 4_tower 5_tower 6_tower 7_tower 8_tower Table 3.
Reference: [9] <author> Phatak, D.S. and Koren, I. </author> <title> Connectivity and Performance Tradeoffs in the Cascade Correlation Learning Architecture, </title> <journal> IEEE Trans. Neural Networks 5(6), </journal> <year> 1994, </year> <pages> pp. 930-935. </pages>
Reference-contexts: Previous work by Phatak and Koren <ref> [9] </ref> explored the effect of limiting network depth and fan-in with CasCor. This was done by constructing traditional layered topologies, however tradeoffs in performance compared to the traditional cascade architecture were observed. 0 200 400 600 Hidden Units W i g t s cascade 6_tower 5_tower 4_tower 2.
Reference: [10] <author> Riedmiller, M. and Braun, H. </author> <title> A Direct Adaptive Method for Faster Backpropagation Learning: The RPROP Algorithm, </title> <booktitle> Proc IEEE Int. Conf. on Neural Networks, </booktitle> <year> 1993, </year> <pages> pp. 586-591. </pages>
Reference-contexts: This was done by constructing traditional layered topologies, however tradeoffs in performance compared to the traditional cascade architecture were observed. 0 200 400 600 Hidden Units W i g t s cascade 6_tower 5_tower 4_tower 2. The CasPer Algorithm CasPer uses a modified version of the RPROP algorithm <ref> [10] </ref> for network training. RPROP is a gradient descent algorithm, which uses separate adaptive learning rates for each weight. Each weight begins with an initial learning rate, which is then adapted depending on the sign of the error gradient seen by the weight as it traverses the error surface.
Reference: [11] <author> Treadgold, </author> <title> N.K. and Gedeon, T.D. A Simulated Annealing Enhancement to Resilient Backpropagation, </title> <booktitle> Proc. Int. Panel Conf. Soft and Intelligent Computing, </booktitle> <address> Budapest, </address> <year> 1996, </year> <pages> pp. 293-298. </pages>
Reference-contexts: Importantly, however, no weights are frozen, and hence if the network can gain benefit by modifying an old weight, this occurs, albeit at an initially slower rate than the weights connected to the new neuron. In addition, the L1 weights are trained by a variation of RPROP termed SARPROP <ref> [11] </ref>. The SARPROP algorithm is based on RPROP, but uses a noise factor to enhance the ability of the network to escape from local minima. <p> The amount of noise added falls as training continues via a Simulated Annealing (SA) term. This combination of techniques has been shown to improve RPROPs ability to escape local minima <ref> [11] </ref>. <p> After some experimentation it was found that the addition of a Simulated Annealing term applied to the weight decay, as used in the SARPROP algorithm <ref> [11] </ref>, often improved convergence and generalization. Each time a new hidden neuron is inserted, the weight decay begins with a large magnitude, which is then reduced by the SA term. The amount of weight decay is proportional to the square of the weight magnitude.
Reference: [12] <author> Kwok, T. and Yeung, D. </author> <title> Use of Bias Term in Projection Pursuit Learning Improves Approximation and Convergence Properties, </title> <journal> IEEE Trans. Neural Networks 7(5), </journal> <year> 1996, </year> <pages> pp. 1168-1183. </pages>
Reference-contexts: Table 2. Average FVU on the cif data set. NETWORK CIF CIF (NOISE) CasPer 0.01298 0.03967 3_Tower 0.00827 0.02925 4_Tower 0.01005 0.03244 5_Tower 0.00801 0.03194 6_Tower 0.00820 0.03149 7_Tower 0.01047 0.03228 8_Tower 0.01110 0.03163 PPL 0.03818 0.09919 Previous work done by Kwok and Yeung <ref> [12] </ref> allows a comparison of these results with Projection Pursuit Learning (PPL), a powerful regression tool. Kwok and Yeung used the cif function, taking average FVU results obtained from 10 training runs.
References-found: 12

