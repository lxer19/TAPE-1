URL: ftp://ftp.cs.man.ac.uk/pub/TR/UMCS-95-12-6.ps.Z
Refering-URL: http://www.cs.man.ac.uk/cstechrep/Abstracts/UMCS-95-12-6.html
Root-URL: http://www.cs.man.ac.uk
Title: The Prospective Student's Introduction to the Robot Learning Problem  
Author: Ulrich Nehmzow and Tom Mitchell 
Affiliation: Computer Science University of Manchester  
Pubnum: ISSN 1361 6161  
Abstract: Department of Computer Science University of Manchester Technical Report Series UMCS-95-12-6 
Abstract-found: 1
Intro-found: 1
Reference: [Connell & Mahadevan 93] <author> J. Connell and S. Mahadevan (eds.), </author> <title> Robot Learning, </title> <publisher> Kluwer 1993. </publisher>
Reference-contexts: In general, it is more difficult to learn from delayed feedback because the system faces the credit assignment problem: how much did each action of the sequence of actions actually contribute towards accomplishing the desired goal (e.g., <ref> [Samuel 59, Connell & Mahadevan 93] </ref>). Standard ways of addressing this problem of learning by delayed rewards are the bucket brigade algorithm ([Samuel 59, Holland et al. 86] or Q-learning ([Watkins 89]).
Reference: [Dickmanns & Graefe 88] <author> E. Dickmanns and V. Graefe, </author> <title> Dynamic monocular machine vision and Applications of dynamic monocular machine vision, </title> <institution> Universitat der Bundeswehr, Munich, Technical Report UniBwM/LRT/WE 13/FB/88-3, </institution> <year> 1988. </year>
Reference: [Holland et al. 86] <author> J. Holland, K. Holyoak, R. Nisbett and P. Thagard, </author> <title> Induction, </title> <publisher> MIT Press 1986. </publisher>
Reference: [Kaelbling 90] <author> L. Kaelbling, </author> <title> Learning in Embedded Systems, </title> <type> PhD thesis, Stanford Technical Report TR-90-04, </type> <year> 1990. </year>
Reference-contexts: A prototypical example is the supervised training data obtained from the human trainer for the ALVINN system described above. The backpropagation algorithm for training artificial neural networks is one common technique for supervised learning 7 Immediate Reward Delayed Reward Self supervised [Nehmzow et al. 89] <ref> [Kaelbling 90] </ref> Externally Supervised [Pomerleau 93, Martin & Nehmzow 95] [Sutton 84] Unsupervised [Kohonen 88] | ([Rumelhart et al 86]). <p> If the task description awards positive reinforcement upon reaching the actual light source, the learning process is extremely slow in taking off, because initially all actions appear equally good to the robot, and the search space is enormous. <ref> [Kaelbling 90] </ref> has implemented such a system and reports that the learning process was so slow that she returned to simulations of her learning controller 7 .
Reference: [Kohonen 88] <author> T. Kohonen, </author> <title> Self-organization and associative memory, </title> <publisher> Springer Verlag 1988. </publisher>
Reference-contexts: The backpropagation algorithm for training artificial neural networks is one common technique for supervised learning 7 Immediate Reward Delayed Reward Self supervised [Nehmzow et al. 89] [Kaelbling 90] Externally Supervised [Pomerleau 93, Martin & Nehmzow 95] [Sutton 84] Unsupervised <ref> [Kohonen 88] </ref> | ([Rumelhart et al 86]). The training information provided in supervised control learning may be examples of the action to be performed, as in ALVINN, or more limited information in the form of a scalar, performance-related reward, not indicating the correct action itself ([Sutton 91]).
Reference: [Lin] <author> L.J. Lin, </author> <title> Reinforcement learning for robots using neural networks, </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University 1993. </institution>
Reference-contexts: This kind of hierarchical organization offers one possible route to scaling up robot learning to tasks of more realistic complexity. Recent attempts to develop such approaches include [Singh 93] and <ref> [Lin] </ref>, but this remains an issue in need of further research. 7 It should be noted, though, that the purpose of Kaelbling's system was not primarily to build a light-seeking robot, but to investigate learning from delayed rewards. 10 3. <p> The questions are: how can the learning controller identify fundamental building blocks, and how can it assembly these to form (various) complex plans? (See [Singh 92] for an example of how concatenating elemental sequential decision tasks leads to more complex sequential decision tasks. See also <ref> [Lin] </ref>). 7.3 Dynamic robot learning The robot learning problem must not be viewed statically.
Reference: [Mahadevan & Connell 91] <author> S. Mahadevan and J. Connell, </author> <title> Automatic Programming of Behavior-based Robots using Reinforcement Learning, </title> <booktitle> Proc. AAAI 91. </booktitle>
Reference: [Nehmzow et al. 89] <author> U. Nehmzow, J. Hallam and T. Smithers, </author> <title> Really Useful Robots, </title> <editor> in T. Kanade, F.C.A. Groen and L.O. Hertzberger (eds.), </editor> <booktitle> Intelligent Autonomous Systems 2, Proceedings of IAS 2, </booktitle> <address> ISBN 90-800410-1-7, Amsterdam 1989 </address>
Reference-contexts: A prototypical example is the supervised training data obtained from the human trainer for the ALVINN system described above. The backpropagation algorithm for training artificial neural networks is one common technique for supervised learning 7 Immediate Reward Delayed Reward Self supervised <ref> [Nehmzow et al. 89] </ref> [Kaelbling 90] Externally Supervised [Pomerleau 93, Martin & Nehmzow 95] [Sutton 84] Unsupervised [Kohonen 88] | ([Rumelhart et al 86]). <p> We refer to the later as self-supervised learning, because one component of the robot is acting a a teacher for another component (examples of implementations of self-supervised learning controllers are given in <ref> [Nehmzow et al. 89, Nehmzow 94] </ref>). Unsupervised learning, on the other hand, performs a clustering of incoming information without using input-output pairs for training. Kohonen's self-organizing feature map is a well known example of an unsupervised learning mechanism ([Kohonen 88]).
Reference: [Nehmzow 92] <author> U. Nehmzow, </author> <title> Experiments in Competence Acquisition for Autonomous Mobile Robots, </title> <type> PhD thesis, </type> <institution> University of Edinburgh 1992. </institution>
Reference-contexts: Is it possible to identify a curriculum of robot learning? At the bottom level, there is probably the learning of relexive sensor-motor associations, expressed in competences such as obstacle avoidance, wall following, and following attractors. Competences at this level are state of the art in mobile robotics (see <ref> [Nehmzow 92] </ref> for a review). <p> Whilst Walter was dependent on electronic circuitry to implement his learning controller, and robots were therefore built for specific tasks, we now have controllers that allow a robot to acquire different tasks by merely redefining the reward function (see, for instance, <ref> [Nehmzow 92] </ref>, for an overview). Learning from immediate reward is the most effective way of learning. Competences such as photo-taxis can be acquired within minutes by robots ([Nehmzow 94]). In some cases, however, immediate reward is not available.
Reference: [Nehmzow 94] <author> U. Nehmzow, </author> <title> Autonomous acquisition of sensor-motor couplings in robots, </title> <institution> University of Manchester technical report UMCS-94-11-1, </institution> <year> 1994. </year>
Reference-contexts: We refer to the later as self-supervised learning, because one component of the robot is acting a a teacher for another component (examples of implementations of self-supervised learning controllers are given in <ref> [Nehmzow et al. 89, Nehmzow 94] </ref>). Unsupervised learning, on the other hand, performs a clustering of incoming information without using input-output pairs for training. Kohonen's self-organizing feature map is a well known example of an unsupervised learning mechanism ([Kohonen 88]).
Reference: [Martin & Nehmzow 95] <author> P. Martin and U. Nehmzow, </author> <title> "Programming" by teaching: neural network control in the Manchester mobile robot, </title> <booktitle> Conf. Intelligent Autonomous Vehicles IAV 95. </booktitle>
Reference-contexts: Alternatively, the teacher could be outside, in which case the robot would learn from external feedback. A robotics implementation of such a system is discussed in <ref> [Martin & Nehmzow 95] </ref>. 6 5.2 The Target Function Above we saw two ways to formulate the robot control learning task: learning the control function f : S ! A and learning the function Q : S fi A ! V . <p> A prototypical example is the supervised training data obtained from the human trainer for the ALVINN system described above. The backpropagation algorithm for training artificial neural networks is one common technique for supervised learning 7 Immediate Reward Delayed Reward Self supervised [Nehmzow et al. 89] [Kaelbling 90] Externally Supervised <ref> [Pomerleau 93, Martin & Nehmzow 95] </ref> [Sutton 84] Unsupervised [Kohonen 88] | ([Rumelhart et al 86]).
Reference: [O'Sullivan et al. 95] <author> J. O'Sullivan, T. Mitchell and S. Thrun, </author> <title> Explanation based learning for mobile robot perception, in Symbolic Visual Learning, </title> <publisher> Oxford University Press, </publisher> <year> 1995. </year>
Reference-contexts: Given a perfect next state function, one could compute f via look-ahead search to determine which action leads to the greatest reward. Even an imperfectly learned N extState has been shown to be helpful background knowledge to improve learning the function Q <ref> [O'Sullivan et al. 95] </ref>. One intriguing property of N extState is that it is task-independent (i.e., unlike f , Q, and H, it is not specific to any goal or reward function). Therefore a robot that learns N extState can use it regardless of the current goal. <p> Some attempts have been made to make real robots learn from delayed rewards, but the number of training examples required is substantially more in this case, leading some researchers to turn to computer simulations ([Kaelbling 90]), and others to seek new methods to overcome this problem ([Sutton 90], <ref> [O'Sullivan et al. 95] </ref>). 5.4 The robot learning curriculum Learning aims for lasting change in order to achieve a particular goal. It therefore requires a curriculum, an agenda of what is to be learned, and the order it is to be learned in. <p> Feature recognition One aspect of concept formation is the differentiation between salient features and dynamical features of the environment, both in terms of temporal patterns and perceptual patterns. Anticipation and prediction (i.e. some sort of generalization) can be useful mechanisms here (see <ref> [O'Sullivan et al. 95] </ref> for an example of an explanation based learning system on a mobile robot). Macro learning and planning In order to move towards complex functions, fundamental building blocks of such functions need to be identified, and subsequently assembled.
Reference: [Oreskes et al. 94] <author> N. Oreskes, K. Shrader-Frechette and K. Belitz, </author> <title> Verification, Validation, and Confirmation of Numerical models in the Earth Sciences, </title> <journal> Science, </journal> <volume> Vol 263, </volume> <month> 4 Feb </month> <year> 1994, </year> <pages> pp. 641-646. </pages>
Reference: [Pomerleau 93] <author> D. Pomerleau, </author> <title> Knowledge-based training of artificial neural nstworks for autonomous robot driving, </title> <note> in [Connell & Mahadevan 93] </note>
Reference-contexts: A prototypical example is the supervised training data obtained from the human trainer for the ALVINN system described above. The backpropagation algorithm for training artificial neural networks is one common technique for supervised learning 7 Immediate Reward Delayed Reward Self supervised [Nehmzow et al. 89] [Kaelbling 90] Externally Supervised <ref> [Pomerleau 93, Martin & Nehmzow 95] </ref> [Sutton 84] Unsupervised [Kohonen 88] | ([Rumelhart et al 86]).
Reference: [Rumelhart et al 86] <author> D. Rumelhart, G.Hinton, and R. Williams, </author> <title> Learning internal representations by error propagation, </title> <editor> in D. Rumelhart and J. McClelland (eds), </editor> <booktitle> Parallel Distributed Processing, </booktitle> <publisher> MIT Press 1986. </publisher>
Reference: [Russell 89] <author> S. Russell, </author> <title> The Use of Knowledge in Analogy and Induction, </title> <publisher> London: Pitman 1989. </publisher>
Reference: [Schmidthuber & Wahnsiedler 92] <author> J. Schmidthuber and R. Wahnsiedler, </author> <title> Planning simple trajectories using neural subgoal generators, </title> <editor> in J.A. Meyer, H. Roitblat and S. Wilson (eds.), </editor> <booktitle> From Animals to Animats 2, </booktitle> <publisher> MIT Press 1993. </publisher>
Reference: [Samuel 59] <author> A.L. Samuel, </author> <title> Some studies in machine learning using the game of checkers, </title> <journal> IBM Journal on Research and Development, </journal> <year> 1959. </year>
Reference-contexts: In general, it is more difficult to learn from delayed feedback because the system faces the credit assignment problem: how much did each action of the sequence of actions actually contribute towards accomplishing the desired goal (e.g., <ref> [Samuel 59, Connell & Mahadevan 93] </ref>). Standard ways of addressing this problem of learning by delayed rewards are the bucket brigade algorithm ([Samuel 59, Holland et al. 86] or Q-learning ([Watkins 89]).
Reference: [Singh 92] <author> S. Singh, </author> <title> Transfer of learning by composing solutions of elemental sequential tasks, </title> <booktitle> Machine Learning 8 </booktitle> <pages> 323-339, </pages> <year> 1992. </year> <month> 14 </month>
Reference-contexts: Macro learning and planning In order to move towards complex functions, fundamental building blocks of such functions need to be identified, and subsequently assembled. The questions are: how can the learning controller identify fundamental building blocks, and how can it assembly these to form (various) complex plans? (See <ref> [Singh 92] </ref> for an example of how concatenating elemental sequential decision tasks leads to more complex sequential decision tasks. See also [Lin]). 7.3 Dynamic robot learning The robot learning problem must not be viewed statically.
Reference: [Singh 93] <author> S. Singh, </author> <title> Learning to Solve Markovian Decision Processes, </title> <type> PhD thesis, </type> <institution> Department of Com--puter Science, University of Massachusetts, </institution> <year> 1993 </year>
Reference-contexts: This kind of hierarchical organization offers one possible route to scaling up robot learning to tasks of more realistic complexity. Recent attempts to develop such approaches include <ref> [Singh 93] </ref> and [Lin], but this remains an issue in need of further research. 7 It should be noted, though, that the purpose of Kaelbling's system was not primarily to build a light-seeking robot, but to investigate learning from delayed rewards. 10 3.
Reference: [Sutton 84] <author> R. Sutton, </author> <title> Temporal credit assignment in reinforcement learning, </title> <type> PhD thesis, </type> <institution> University of Masschusetts 1984. </institution>
Reference-contexts: The backpropagation algorithm for training artificial neural networks is one common technique for supervised learning 7 Immediate Reward Delayed Reward Self supervised [Nehmzow et al. 89] [Kaelbling 90] Externally Supervised [Pomerleau 93, Martin & Nehmzow 95] <ref> [Sutton 84] </ref> Unsupervised [Kohonen 88] | ([Rumelhart et al 86]). The training information provided in supervised control learning may be examples of the action to be performed, as in ALVINN, or more limited information in the form of a scalar, performance-related reward, not indicating the correct action itself ([Sutton 91]).
Reference: [Sutton 91] <author> R. Sutton, </author> <title> Reinforcement Learning Architectures for Animats, </title> <editor> in J.A. Meyer and S. Wilson (eds), </editor> <booktitle> From Animals to Animats, </booktitle> <publisher> MIT Press 1991. </publisher>
Reference-contexts: Defined in terms of the earlier Q function, H (s) = max a2A Q (s; a). This evaluation function, like the Q function, can be used to compute the desired control function f. It has been used, for example, in Sutton's Adaptive Heuristic Critic <ref> [Sutton 91] </ref>.
Reference: [Sutton 90] <author> R. Sutton, R. Sutton, </author> <title> Integrated architectures for learning, planning and reacting based on approximating dynamic programming, </title> <booktitle> Proc. Intern. Conf. on Machine Learning, </booktitle> <publisher> Morgan Kaufman 1990. </publisher>
Reference: [Walter 50] <author> W. Grey Walter, </author> <title> An imitation of life, </title> <publisher> Scientific American 182(5), </publisher> <month> 42-45, </month> <title> 1950; and A machine that learns, </title> <publisher> Scientific American 51, </publisher> <pages> 60-63, </pages> <year> 1951. </year>
Reference: [Watkins 89] <author> C. Watkins, </author> <title> Learning with delayed rewards, Doctoral dissertation, </title> <address> Cambridge University 1989. </address>
Reference-contexts: In this way, the currently learned evaluation function ^ Q applied to state s t+1 is used to estimate training values for the preceding state s t . Surprisingly, it can be shown that under certain conditions, this method converges to the optimal control procedure <ref> [Watkins 89] </ref>. This approach of Q learning is one style of reinforcement learning. In general, reinforcement learning involves learning to control the robot based on a (possibly delayed) scalar training reward, rather than training examples specifying the desired state-action associations.
References-found: 25

