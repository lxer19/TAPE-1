URL: ftp://ftp.cogsci.indiana.edu/pub/wang.semantics.ps
Refering-URL: http://www.cogsci.indiana.edu/farg/peiwang/papers.html
Root-URL: 
Email: pwang@cogsci.indiana.edu  
Title: Grounded on Experience: Semantics for intelligence  
Author: Pei Wang 
Address: 510 North Fess, Bloomington, IN 47408, USA  
Affiliation: Indiana University  
Note: Center for Research on Concepts and Cognition  
Abstract: Model-theoretic semantics is inappropriate for adaptive systems working with insufficient knowledge and resources. An experience-grounded semantics is introduced in this paper, using NARS, an intelligent reasoning system, as a concrete example. In NARS, the truth value of a sentence indicates the amount of available evidence, and the meaning of a term indicates its experienced relationship with other terms. Accordingly, both truth value and meaning are dynamic and subjective. This approach provides new ideas to the solution of some important problems in artificial intelligence.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Barwise and J. Perry. </author> <title> Situations and Attitudes. </title> <publisher> The MIT Press, </publisher> <address> Cam-bridge, Massachusetts, </address> <year> 1983. </year>
Reference-contexts: This idea sounds reasonable, but it does not answer the original question: how is the "concept" and "degree of belief" related to the outside world? Without an answer to that question, such a solution "simply pushes the problem of external significance from expressions to ideas." <ref> [1] </ref> In this paper, we show another possibility: to abandon model-theoretic semantics, and to find another semantics for an intelligent reasoning system, which still use a formal language and formal inference rules. 3 Model vs.
Reference: [2] <author> L. Birnbaum. Rigor mortis: </author> <title> a response to Nilsson's "Logic and artificial intelligence". </title> <journal> Artificial Intelligence, </journal> <volume> 47 </volume> <pages> 57-77, </pages> <year> 1991. </year>
Reference-contexts: It seems that natural language is too subtle and fluid to be put into the frame of model-theoretic semantics. Also, it hardly works for non-deductive inferences <ref> [2, 10] </ref>, though there are various attempts to extend the theory into more flexible variations by introducing ideas like possible world and multi-valued logic [3, 4, 8, 23]. The problems in model-theoretic semantics are often used as arguments against "strong AI". <p> Model-theoretic semantics has been criticized by many authors for its rigidness <ref> [2, 10] </ref>. However, without a powerful competitor, the solution is far from clear. <p> They try some other ideas, such as neural network and robots, with the hope that they can generate meaning and truth from perception and action <ref> [2, 5] </ref>.
Reference: [3] <author> R. Carnap. </author> <title> Logical Foundations of Probability. </title> <publisher> The University of Chicago Press, </publisher> <address> Chicago, </address> <year> 1950. </year>
Reference-contexts: Also, it hardly works for non-deductive inferences [2, 10], though there are various attempts to extend the theory into more flexible variations by introducing ideas like possible world and multi-valued logic <ref> [3, 4, 8, 23] </ref>. The problems in model-theoretic semantics are often used as arguments against "strong AI".
Reference: [4] <author> J. Halpern. </author> <title> An analysis of first-order logics of probability. </title> <journal> Artificial Intelligence, </journal> <volume> 46 </volume> <pages> 311-350, </pages> <year> 1990. </year>
Reference-contexts: Also, it hardly works for non-deductive inferences [2, 10], though there are various attempts to extend the theory into more flexible variations by introducing ideas like possible world and multi-valued logic <ref> [3, 4, 8, 23] </ref>. The problems in model-theoretic semantics are often used as arguments against "strong AI".
Reference: [5] <author> S. Hardad. </author> <title> The symbol grounding problem. </title> <journal> Physica D, </journal> <volume> 42 </volume> <pages> 335-346, </pages> <year> 1990. </year>
Reference-contexts: They try some other ideas, such as neural network and robots, with the hope that they can generate meaning and truth from perception and action <ref> [2, 5] </ref>. <p> This leads us to Searle's "Chinese room" argument [16] and Harnad's "symbol grounding" problem <ref> [5] </ref>. As mentioned previously, Searle's argument is based on the assumption that a symbol can only get meaning from a model. If we accept an experience-grounded semantics, it is no longer the case. <p> This idea sounds like what Harnad calls "dictionary-go-round" | he hopes that meaning of symbols can "be grounded in something other than just more meaningless symbols." <ref> [5] </ref> Here we should notice a subtle difference: in experience-grounded semantics, the meaning of a term is not reduced into the meaning of other terms (that will lead to circular definition in a finite language), but defined by its relations with other terms.
Reference: [6] <author> Hume. </author> <title> An enquiry concerning human understanding. </title> <journal> London, </journal> <volume> 1748. </volume>
Reference-contexts: However, here "objective" means "common" or "unbiased", rather than "observer-independent." Such a semantics provides a justification for non-deductive inferences. As revealed by Hume's "induction problem", our predication about future experience cannot be infallible <ref> [6] </ref>. From limited past experience, we cannot get general descriptions of "state of affairs", neither can we know how far our current knowledge is from such an "objective" descriptions. Based on this, Popper made the well-known conclusion that an inductive logic is impossible [13].
Reference: [7] <author> D. Krantz. </author> <title> From indices to mappings: The representational approach to measurement. </title> <editor> In D. Brown and J. Smith, editors, </editor> <booktitle> Frontiers of Mathematical Psychology: Essays in Honor of Clyde Coombs, Recent Research in Psychology, chapter 1. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, Germany, </address> <year> 1991. </year>
Reference-contexts: No matter how the truth value &lt; 0:75; 0:80 &gt; is practically generated (there are infinite possibilities to get it), it can always be understood in a unique way, as stated above. The "ideal experience" is used here as an "ideal meterstick" to measure degree of truth <ref> [7] </ref>. Another factor that makes actual experience different from ideal experience is the insufficiency of resources. Due to the lack of space, some knowledge in the experience is forgot by the system; due to the lack of time, some knowledge in the experience is ignored by the system. <p> Moreover, defining a truth value by a set of binary relations in a section of "ideal experience", we can explain a multi-valued statement about randomness, fuzziness, ignorance, or their mixture, by translating it into a set of two-valued statements. This is exactly what measure theory asks us to do <ref> [7] </ref>. 6 Meaning in NARS Like truth values, the meaning of terms in NARS is also dynamic and subjective. The meaning of a term is determined by its experienced relations with 10 other terms, and it determines how the term will be used by the system in future.
Reference: [8] <author> H. Kyburg. </author> <title> Semantics for probabilistic inference. </title> <booktitle> In Proceedings of the Eighth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 142-148, </pages> <year> 1992. </year>
Reference-contexts: Also, it hardly works for non-deductive inferences [2, 10], though there are various attempts to extend the theory into more flexible variations by introducing ideas like possible world and multi-valued logic <ref> [3, 4, 8, 23] </ref>. The problems in model-theoretic semantics are often used as arguments against "strong AI".
Reference: [9] <author> G. </author> <title> Lakoff. Cognitive semantics. </title> <editor> In U. Eco, M. Santambrogio, and Violi P., editors, </editor> <booktitle> Meaning and Mental Representation, </booktitle> <pages> pages 119-154. </pages> <institution> Indiana University Press, Bloomington, Indiana, </institution> <year> 1988. </year>
Reference-contexts: Human beings judge the truth value of a sentence according to personal experience and determine the meaning of a word according to its relations with other words. This is not a new idea to psychologists and linguistics <ref> [9, 12, 18] </ref>. However, few people tried to apply it to an artificial language defined by a formal grammar. This is caused by several assumptions, which, though seldom mentioned, are accepted by many people. It is implicitly assumed that the semantics of a "formal language" has to be model-theoretic.
Reference: [10] <author> D. McDermott. </author> <title> A critique of pure reason. </title> <journal> Computational Intelligence, </journal> <volume> 3 </volume> <pages> 151-160, </pages> <year> 1987. </year>
Reference-contexts: It seems that natural language is too subtle and fluid to be put into the frame of model-theoretic semantics. Also, it hardly works for non-deductive inferences <ref> [2, 10] </ref>, though there are various attempts to extend the theory into more flexible variations by introducing ideas like possible world and multi-valued logic [3, 4, 8, 23]. The problems in model-theoretic semantics are often used as arguments against "strong AI". <p> Model-theoretic semantics has been criticized by many authors for its rigidness <ref> [2, 10] </ref>. However, without a powerful competitor, the solution is far from clear. <p> However, without a powerful competitor, the solution is far from clear. As McDermott said: "The notation we use must be understandable to those using it and reading it; so it must have a semantics; so it must have a Tarskian semantics, because there is no other candidate." <ref> [10] </ref> Some people believe that it is the idea of "formalizing language and inference rules" that should be abandoned. They try some other ideas, such as neural network and robots, with the hope that they can generate meaning and truth from perception and action [2, 5].
Reference: [11] <author> N. </author> <title> Nilsson. </title> <journal> Logic and artificial intelligence. Artificial Intelligence, </journal> <volume> 47 </volume> <pages> 31-56, </pages> <year> 1991. </year>
Reference-contexts: Model-theoretic semantics was founded by Tarski's work. Although Tarski's primary target formal language, he also hoped that the ideas could be applied to reform everyday language [17]. This approach is accepted by the "logical approach" to AI <ref> [11] </ref>. For a language L, defined by a finite formal grammar, a model M consists of a description of the relevant part of a domain, in another language ML, and an interpretation I, which maps the items in L into the items in ML.
Reference: [12] <author> F. Palmer. </author> <title> Semantics. </title> <publisher> Cambridge University Press, </publisher> <address> New York, 2 edition, </address> <year> 1981. </year>
Reference-contexts: Tarski said, "As regards the applicability of semantics to mathematical science and their methodology, i.e., to meta-mathematics, we are in a much more favorable position than in the case of empirical sciences." [17] However, the attempt to apply this idea to the semantic study of natural language is not successful <ref> [12] </ref>. It seems that natural language is too subtle and fluid to be put into the frame of model-theoretic semantics. <p> Human beings judge the truth value of a sentence according to personal experience and determine the meaning of a word according to its relations with other words. This is not a new idea to psychologists and linguistics <ref> [9, 12, 18] </ref>. However, few people tried to apply it to an artificial language defined by a formal grammar. This is caused by several assumptions, which, though seldom mentioned, are accepted by many people. It is implicitly assumed that the semantics of a "formal language" has to be model-theoretic.
Reference: [13] <author> K. </author> <title> Popper. The logic of Scientific Discovery. </title> <publisher> Hutchinson, </publisher> <address> London, </address> <year> 1968. </year>
Reference-contexts: From limited past experience, we cannot get general descriptions of "state of affairs", neither can we know how far our current knowledge is from such an "objective" descriptions. Based on this, Popper made the well-known conclusion that an inductive logic is impossible <ref> [13] </ref>. However, from the previous discussion, we can see that what really pointed out by Hume and Popper is the impossibility of an inductive logic with a model-theoretic semantics.
Reference: [14] <author> W. V. Quine and J. Ullian. </author> <title> The Web of Belief. Random House, </title> <address> New York, </address> <year> 1970. </year>
Reference-contexts: To say that truth values are dynamic and subjective, it does not mean that they are arbitrary. As Quine said about the human mind, "Observations are the boundary conditions of a system of beliefs." <ref> [14] </ref> The systems in the same environment can achieve certain "objectivity" by communicating to one another to share experience. However, here "objective" means "common" or "unbiased", rather than "observer-independent." Such a semantics provides a justification for non-deductive inferences.
Reference: [15] <author> B. Russell. </author> <booktitle> Recent work on the principles of mathematics. International Monthly, </booktitle> <address> 4:84, </address> <year> 1901. </year>
Reference-contexts: On the other hand, what R does to the terms and sentences in L has no influence to their meaning and truth value. When working within such a system, as Russell said, "we never know what we are talking about, nor whether what we are saying is true," <ref> [15] </ref> unless R can set up models by itself. In that case, however, it no longer works in L only, and its models still cannot exclude other possible models.
Reference: [16] <author> J. Searle. </author> <title> Minds, brains, and programs. </title> <journal> The Behavioral and Brain Science, </journal> <volume> 3 </volume> <pages> 417-424, </pages> <year> 1980. </year>
Reference-contexts: The problems in model-theoretic semantics are often used as arguments against "strong AI". Actually, Searle's assertion that "computers are syntactic, but the human mind is semantic" in his "Chinese room" argument <ref> [16] </ref> is directly based on the assumption that all computerized symbol manipulations are bounded to model-theoretic semantics, so uninterpreted symbols are meaningless. Model-theoretic semantics has been criticized by many authors for its rigidness [2, 10]. However, without a powerful competitor, the solution is far from clear. <p> This leads us to Searle's "Chinese room" argument <ref> [16] </ref> and Harnad's "symbol grounding" problem [5]. As mentioned previously, Searle's argument is based on the assumption that a symbol can only get meaning from a model. If we accept an experience-grounded semantics, it is no longer the case.
Reference: [17] <author> A. Tarski. </author> <title> The semantic conception of truth. </title> <journal> Philosophy and Phenomenological Research, </journal> <volume> 4 </volume> <pages> 341-375, </pages> <year> 1944. </year>
Reference-contexts: Model-theoretic semantics was founded by Tarski's work. Although Tarski's primary target formal language, he also hoped that the ideas could be applied to reform everyday language <ref> [17] </ref>. This approach is accepted by the "logical approach" to AI [11]. <p> For a reasoning system, valid inference rules are those that only derive true conclusions from true premises. According to this opinion, as Tarski said <ref> [17] </ref>, "semantics is a discipline which deals with certain relations between expressions of a language and the objects `referred to' by those expressions." Let us see what is implied by the above definitions. <p> The study of semantics contributes remarkably to the development of mathematics. As Tarski said, "As regards the applicability of semantics to mathematical science and their methodology, i.e., to meta-mathematics, we are in a much more favorable position than in the case of empirical sciences." <ref> [17] </ref> However, the attempt to apply this idea to the semantic study of natural language is not successful [12]. It seems that natural language is too subtle and fluid to be put into the frame of model-theoretic semantics.
Reference: [18] <author> A. Tversky and D. Kahneman. </author> <title> Judgment under uncertainty: heuristics and biases. </title> <journal> Science, </journal> <volume> 185 </volume> <pages> 1124-1131, </pages> <year> 1974. </year>
Reference-contexts: Human beings judge the truth value of a sentence according to personal experience and determine the meaning of a word according to its relations with other words. This is not a new idea to psychologists and linguistics <ref> [9, 12, 18] </ref>. However, few people tried to apply it to an artificial language defined by a formal grammar. This is caused by several assumptions, which, though seldom mentioned, are accepted by many people. It is implicitly assumed that the semantics of a "formal language" has to be model-theoretic.
Reference: [19] <author> P. Wang. </author> <title> From inheritance relation to non-axiomatic logic. </title> <journal> International Journal of Approximate Reasoning. </journal> <note> Accepted in June 1994. </note>
Reference-contexts: model of L is represented in another language ML, and it is not neces sarily accessible to the system that use L, but experience is represented in L itself, and it is accessible to the system. 4 The Semantics of NARS In the following, we take Non-Axiomatic Reasoning System (NARS) <ref> [19, 21, 22] </ref> as an example, to show how to apply experience-ground semantics to a formal language used by a computerized reasoning system. As a general-purpose intelligent reasoning system, NARS is designed to be adaptive with insufficient knowledge and resources [22]. <p> The solution used in NARS is "bootstrapping" | taking a subset of L to define the truth value of sentences and meaning of terms in L. In the following, we only discuss the core language of NARS, defined in <ref> [19] </ref>, and ignore its extensions. 6 NARS uses a term-oriented language, in which each sentence consists of a subject term and a predicate term, related by a copula. Let us first define a copula for an ideal inheritance relation. <p> positive evidence) and the number of all checked instances w (call it the weight of available evidence), the truth value of "S P " can be naturally represented by a pair of real numbers &lt; f; c &gt;, where f = w + =w, and c = w=(w + 1) <ref> [19] </ref>. Here f is the frequency, or proportion, of positive evidence among all evidence, and c is called confidence, indicating the amount of relevant evidence that the system has collected. (See [19] for a detailed discussion about confidence.) Especially, "S &lt; P " is identical to "S P &lt; 1; 1 <p> of real numbers &lt; f; c &gt;, where f = w + =w, and c = w=(w + 1) <ref> [19] </ref>. Here f is the frequency, or proportion, of positive evidence among all evidence, and c is called confidence, indicating the amount of relevant evidence that the system has collected. (See [19] for a detailed discussion about confidence.) Especially, "S &lt; P " is identical to "S P &lt; 1; 1 &gt;", that is, "&lt;" is the limit of "" when both w and w + go to infinite, while w w + (the amount of negative evidence) has an upper bound. <p> Assuming that the experience of the system (until a certain instant) is represented by K, a finite set of sentences in L 0 , then it is easy to generate the reflexive and transitive closure K fl (see <ref> [19] </ref>). Based on K fl , we define an extension and an intension for each term in K fl : The extension of a term T is a set of terms E T = fx j x &lt; T 2 K fl g. <p> Though the above examples are insufficient to uniquely determine the induction rule and the revision rule in NARS (the general situation is discussed in <ref> [19] </ref>), they do provide boundary conditions that the rules should satisfy. Similar analysis can be done to other non-deductive inferences, such as abduction and analogy. In this way, the validity of the inference rules in NARS are justified. <p> On the other hand, ignorance, revealed by the phenomenon that judgments have different sensibility to new evidence, can be measured by "lack of confidence" in NARS, so become a function of available evidence <ref> [19] </ref>. Although these types of uncertainty have different origin, in NARS they are all represented by the truth value of sentences, and processed in a unified manner.
Reference: [20] <author> P. Wang. </author> <title> The interpretation of fuzziness. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics. </journal> <note> Accepted in October 1994. </note>
Reference-contexts: Another result of such an experience-based definition of truth value is that it provides a unified representation for the various types of uncertainty. As discussed in <ref> [20] </ref>, in a sentence "S P ," randomness usually happens when the extension of the subject term S is partially included in the extension of the predicate term P (some, but not all, instances of S are P ), and fuzziness usually happens when the intension of the predicate term P
Reference: [21] <author> P. Wang. </author> <title> Non-axiomatic reasoning system (version 2.2). </title> <type> Technical Report 75, </type> <institution> Center for Research on Concepts and Cognition, Indi-ana University, Bloomington, Indiana, </institution> <year> 1993. </year> <note> Available via WWW at http://www.cogsci.indiana.edu/farg/pwang papers.html. </note>
Reference-contexts: model of L is represented in another language ML, and it is not neces sarily accessible to the system that use L, but experience is represented in L itself, and it is accessible to the system. 4 The Semantics of NARS In the following, we take Non-Axiomatic Reasoning System (NARS) <ref> [19, 21, 22] </ref> as an example, to show how to apply experience-ground semantics to a formal language used by a computerized reasoning system. As a general-purpose intelligent reasoning system, NARS is designed to be adaptive with insufficient knowledge and resources [22]. <p> The semantic theory introduced in this paper can be extended into more complex systems. The term-oriented language defined previously can be extended to including other inheritance relations and compound terms <ref> [21] </ref>. In a system with sensory-motor capacity, truth value and meaning are no longer only determined within the language, but also determined by the "non-verbal" components in the experience.
Reference: [22] <author> P. Wang. </author> <title> On the working definition of intelligence. </title> <type> Technical Report 94, </type> <institution> Center for Research on Concepts and Cognition, Indiana University, Bloomington, Indiana, </institution> <year> 1994. </year> <note> Available via WWW at http://www.cogsci.indiana.edu/farg/pwang papers.html. </note>
Reference-contexts: It cannot be assumed, neither, that all desired results are deductively implied by current knowledge. It is easy to see that the human mind usually works in such an environment, but few current computer system can. In <ref> [22] </ref>, it is argued that "working with insufficient knowledge and resources" is a definitive property of intelligence. Model-theoretic semantics cannot be applied in such a situation. <p> model of L is represented in another language ML, and it is not neces sarily accessible to the system that use L, but experience is represented in L itself, and it is accessible to the system. 4 The Semantics of NARS In the following, we take Non-Axiomatic Reasoning System (NARS) <ref> [19, 21, 22] </ref> as an example, to show how to apply experience-ground semantics to a formal language used by a computerized reasoning system. As a general-purpose intelligent reasoning system, NARS is designed to be adaptive with insufficient knowledge and resources [22]. <p> As a general-purpose intelligent reasoning system, NARS is designed to be adaptive with insufficient knowledge and resources <ref> [22] </ref>. As discussed above, in such a system the truth value of a sentence is determined by its relationship with the experience of the system, rather than with "state of affairs" in a model. <p> As the system know more about "bird", its meaning becomes richer and more complex. The term "bird" may never means the same to NARS as it means to a human (because we cannot expect a computer system to have human experience <ref> [22] </ref>), but we cannot say "bird" is meaningless to the system for this (human chauvinistic) reason. This leads us to Searle's "Chinese room" argument [16] and Harnad's "symbol grounding" problem [5].
Reference: [23] <author> L. Zadeh. </author> <title> Test-score semantics as a basis for a computational approach to the representation of meaning. </title> <journal> Literary and Linguistic Computing, </journal> <volume> 1 </volume> <pages> 24-35, </pages> <year> 1986. </year>
Reference-contexts: Also, it hardly works for non-deductive inferences [2, 10], though there are various attempts to extend the theory into more flexible variations by introducing ideas like possible world and multi-valued logic <ref> [3, 4, 8, 23] </ref>. The problems in model-theoretic semantics are often used as arguments against "strong AI".
References-found: 23

