URL: http://www.cs.iastate.edu/~honavar/Papers/distal.ps
Refering-URL: http://www.cs.iastate.edu/~cs572/weekly.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: fyang|parekh|honavarg@cs.iastate.edu  
Title: DistAl: An Inter-pattern Distance-based Constructive Learning Algorithm  
Author: Jihoon Yang, Rajesh Parekh and Vasant Honavar 
Address: 226 Atanasoff Hall  Ames, IA 50011. U.S.A.  
Affiliation: Artificial Intelligence Research Group Department of Computer Science  Iowa State University  
Abstract: Multi-layer networks of threshold logic units offer an attractive framework for the design of pattern classification systems. A new constructive neural network learning algorithm (DistAl) based on inter-pattern distance is introduced. DistAl constructs a single hidden layer of spherical threshold neurons. Each neuron is designed to exclude a cluster of training patterns belonging to the same class. The weights and thresholds of the hidden neurons are determined directly by comparing the inter-pattern distances of the training patterns. This offers a significant advantage over other constructive learning algorithms that use an iterative (and often time consuming) weight modification strategy to train individual neurons. The individual clusters (represented by the hidden neurons) are combined by a single output layer of threshold neurons. Results of experiments on several artificial and real-world datasets show that DistAl compares favorably with other neural network learning algorithms for pattern classification. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Gallant, </author> <title> Neural Network Learning and Expert Systems, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference: [2] <author> R. G. Parekh, J. Yang, and V. G. Honavar, </author> <title> "Constructive neural network learning algorithms for multi-category real-valued pattern classification," </title> <type> Tech. Rep. </type> <institution> ISU-CS-TR97-06, Department of Computer Science, Iowa State University, </institution> <year> 1997, </year> <note> (Submitted for review to the IEEE Transactions on Neural Networks). </note>
Reference-contexts: DistAl worked fairly well (both in terms of speed and generalization) on the feature subset selection task. Detailed comparison of the performance DistAl with that of the different constructive learning algorithms discussed in <ref> [2] </ref> on a large number of datasets is currently in progress.
Reference: [3] <author> R. G. Parekh, J. Yang, and V. G. Honavar, </author> <title> "Mupstart a constructive neural network learning algorithm for multi-category pattern classification," </title> <booktitle> in Proceedings of the IEEE/INNS International Conference on Neural Networks, </booktitle> <address> ICNN'97, </address> <year> 1997, </year> <pages> pp. 1924-1929. </pages>
Reference: [4] <author> J. Yang, R. Parekh, and V. Honavar, </author> <title> "MTiling a constructive neural network learning algorithm for multi-category pattern classification," </title> <booktitle> in Proceedings of the World Congress on Neural Networks'96, </booktitle> <address> San Diego, </address> <year> 1996, </year> <pages> pp. 182-187. </pages>
Reference: [5] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams, </author> <title> "Learning internal representations by error propagation," in Parallel Distributed Processing: Explorations into the Microstructure of Cognition, </title> <booktitle> vol. 1 (Foundations). </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, Mas-sachusetts, </address> <year> 1986. </year>
Reference: [6] <author> F. Rosenblatt, </author> <title> "The perceptron: A probabilistic model for information storage and organization in the brain," </title> <journal> Psychological Review, </journal> <volume> vol. 65, </volume> <pages> pp. 386-408, </pages> <year> 1958. </year>
Reference: [7] <author> N.J. Nilsson, </author> <title> The Mathematical Foundations of Learning Machines, </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1965. </year>
Reference: [8] <author> J. K. Anlauf and M. Biehl, </author> <title> "Properties of an adaptive perceptron algorithm," </title> <booktitle> in Parallel Processing in Neural Systems and Computers, </booktitle> <year> 1990, </year> <pages> pp. 153-156. </pages>
Reference: [9] <author> M. Frean, </author> <title> Small Nets and Short Paths: Optimizing Neural Computation, </title> <type> Ph.D. thesis, </type> <institution> Center for Cognitive Science, Edinburgh University, UK, </institution> <year> 1990. </year>
Reference: [10] <author> H. Poulard, </author> <title> "Barycentric correction procedure: A fast method of learning threshold units," </title> <booktitle> in Proceedings of WCNN'95, </booktitle> <address> July 17-21, Washington D.C., </address> <booktitle> 1995, </booktitle> <volume> vol. 1, </volume> <pages> pp. 710-713. </pages>
Reference: [11] <author> B. Raffin and M. G. Gordon, </author> <title> "Learning and generalization with minimerror, a temperature-dependent learning algorithm," </title> <journal> Neural Computation, </journal> <volume> vol. 7, </volume> <pages> pp. 1206-1224, </pages> <year> 1995. </year>
Reference: [12] <author> R. Reed, </author> <title> "Pruning algorithms | a survey," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 4, no. 5, </volume> <pages> pp. 740-747, </pages> <year> 1993. </year>
Reference: [13] <author> R. G. Parekh, J. Yang, and V. G. Honavar, </author> <title> "Pruning strategies for constructive neural network learning algorithms," </title> <booktitle> in Proceedings of the IEEE/INNS International Conference on Neural Networks, </booktitle> <address> ICNN'97, </address> <year> 1997, </year> <pages> pp. 1960-1965. </pages>
Reference: [14] <author> V. Honavar, </author> <title> Generative Learning Structures and Processes for Generalized Connectionist Networks, </title> <type> Ph.D. thesis, </type> <institution> University of Wisconsin, Madison, </institution> <year> 1990. </year>
Reference: [15] <author> V. Honavar and L. Uhr, </author> <title> "Generative learning structures and processes for connectionist networks," </title> <journal> Information Sciences, </journal> <volume> vol. 70, </volume> <pages> pp. 75-108, </pages> <year> 1993. </year>
Reference: [16] <author> S. Gallant, </author> <title> "Perceptron based learning algorithms," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 1, no. 2, </volume> <pages> pp. 179-191, </pages> <month> June </month> <year> 1990. </year>
Reference: [17] <author> M. Mezard and J. Nadal, </author> <title> "Learning feed-forward networks: The tiling algorithm," </title> <journal> J. Phys. A: Math. Gen., </journal> <volume> vol. 22, </volume> <pages> pp. 2191-2203, </pages> <year> 1989. </year> <month> 5 </month>
Reference: [18] <author> M. Frean, </author> <title> "The upstart algorithm: A method for constructing and training feedforward neural networks," </title> <journal> Neural Computation, </journal> <volume> vol. 2, </volume> <pages> pp. 198-209, </pages> <year> 1990. </year>
Reference: [19] <author> N. Burgess, </author> <title> "A constructive algorithm that converges for real-valued input patterns," </title> <journal> International Journal of Neural Systems, </journal> <volume> vol. 5, no. 1, </volume> <pages> pp. 59-66, </pages> <year> 1994. </year>
Reference: [20] <author> M. Marchand, M. Golea, and P. Rujan, </author> <title> "A convergence theorem for sequential learning in two-layer perceptrons," </title> <journal> Europhysics Letters, </journal> <volume> vol. 11, no. 6, </volume> <pages> pp. 487-492, </pages> <year> 1990. </year>
Reference-contexts: The representation of the patterns at the hidden layer is linearly separable <ref> [20] </ref>. An iterative perceptron like learning rule can be used to train the output weights. The output weights can also be directly set as follows: The weights between output and hidden neurons are chosen such that each hidden neuron overwhelms the effect of the hidden neurons generated later.
Reference: [21] <author> P. Langley, </author> <title> Elements of Machine Learning, </title> <publisher> Morgan Kaufmann, </publisher> <address> Palo Alto, CA, </address> <year> 1995. </year>
Reference: [22] <author> J. Yang, R. Parekh, and V. Honavar, </author> <title> "DistAl: An inter-pattern distance-based constructive learning algorithm," </title> <type> Tech. Rep. </type> <institution> ISU-CS-TR 97-05, Iowa State University, </institution> <year> 1997. </year>
Reference-contexts: C. Convergence Proof Theorem: DistAl guarantees to converge to 100% training accuracy with a finite number of hidden neurons for a dataset with a finite number of non-contradictory patterns. Proof: See <ref> [22] </ref> for the detailed proof. D. <p> The network was allowed to train until it achieved 100% classification accuracy on the training set. The best generalization (during the process of training) was then reported. For further details on the experimental set up and the datasets see <ref> [22] </ref>. A thorough comparison of our algorithm with all the existing pattern classification algorithms is beyond the scope of our project.
Reference: [23] <author> R. O. Duda and P. E. Hart, </author> <title> Pattern Classification and Scene Analysis, </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1973. </year>
Reference: [24] <author> G. Salton and M. McGill, </author> <title> Introduction to Modern Information Retrieval, </title> <publisher> McGraw Hill, </publisher> <address> New York, </address> <year> 1983. </year>
Reference: [25] <author> D. Wilson and T. Martinez, </author> <title> "Improved heterogeneous distance functions," </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> vol. 6, </volume> <pages> pp. 1-34, </pages> <year> 1997. </year>
Reference-contexts: A thorough comparison of our algorithm with all the existing pattern classification algorithms is beyond the scope of our project. In what follows we compare the performance of DistAl with the results for the k nearest-neighbor algorithm reported in <ref> [25] </ref> and the results for the best generalization performance for some of the datasets from the results reported in the UCI repository. As we can see from Table 1, DistAl gave comparable results to other algorithms in most datasets (except Soybean (large)) despite its fast execution.

References-found: 25

