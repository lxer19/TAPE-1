URL: http://www.umiacs.umd.edu/users/yaser/2cworkshop.ps.Z
Refering-URL: http://www.umiacs.umd.edu/users/yaser/publications.html
Root-URL: 
Title: Human Emotion Recognition from Motion Using a Radial Basis Function Network Architecture  
Author: Mark Rosenblum, Yaser Yacoob, Larry Davis 
Date: Nov. 1994  
Note: Presented at the IEEE Workshop on Motion of Non-Rigid and Articulated Objects,  
Address: College Park, MD 20742  Austin, TX,  
Affiliation: Computer Vision Laboratory University of Maryland  
Abstract: In this paper a radial basis function network architecture is developed that learns the correlation of facial feature motion patterns and human emotions. We describe a hierarchical approach which at the highest level identifies emotions, at the mid level determines motion of facial features, and at the low level recovers motion directions. Individual emotion networks were trained to recognize the `smile' and `surprise' emotions. Each emotion network was trained by viewing a set of sequences of one emotion for many subjects. The trained neural network was then tested for retention, extrapolation and rejection ability. Success rates were about 88% for retention, 73% for extrapolation, and 79% for rejection. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Abdel-Mottaleb, R. Chellappa, and A. Rosen-feld, </author> <title> "Binocular motion stereo using MAP estimation", </title> <journal> IEEE CVPR, </journal> <pages> 321-327, </pages> <year> 1993. </year>
Reference-contexts: The system is composed of the following components: * Optical flow computation: Optical flow is computed at the points with high gradient at each frame. Our algorithm for flow computation is based on a correlation approach proposed by Abdel-Mottaleb et al. <ref> [1] </ref>. It computes subpixel flow assuming that the motion between two consecutive images is bounded within an nxn win dow. * Region tracking: We assume that, for each feature, we can initially compute a rectangular region that encloses it.
Reference: [2] <author> J.N. Bassili, </author> <title> "Emotion recognition: The role of facial movement and the relative importance of upper and lower areas of the face," </title> <journal> Journal of Personality and Social Psychology, </journal> <volume> Vol. 37, </volume> <pages> 2049-2059, </pages> <year> 1979. </year>
Reference-contexts: Few studies have directly investigated the influence of the motion and deformation of facial features on the interpretation of facial expressions (a review of the relevant psychological aspects of recognizing facial expressions appears in [11]). Bassili <ref> [2] </ref> suggested that motion in the image of a face would allow emotions to be identified even with minimal infor mation about the spatial arrangement of features.
Reference: [3] <author> J. Hertz, A. Krogh and R.G. Palmer, </author> <title> Introduction to the Theory of Neural Computation, </title> <publisher> Addison Wesley, </publisher> <year> 1991. </year>
Reference: [4] <author> K. Mase, </author> <title> "Recognition of facial expression from optical flow," </title> <journal> IEICE Transactions, </journal> <volume> Vol. E 74, No. 10, </volume> <pages> 3474-3483, </pages> <year> 1991. </year>
Reference: [5] <author> J. Moody and C. </author> <month> Darken </month> <year> 1988. </year> <title> "Learning with Localized Receptive Fields," </title> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <editor> eds Touretzky, Hinton, and Sejnowski. </editor> <publisher> Morgan-Kaufmann Publishers, </publisher> <year> 1988. </year>
Reference-contexts: of these component subnetworks through the output units of the individual emotion network. decomposition 5 The Basic Building Block Because of their ability to directly represent prototypical situations of the application in the receptive field centers, we chose to use a modified version of the radial basis function network (RBFN) <ref> [5] </ref> as the architecture for the basic building blocks. In the following section we discuss the enhancements we made to the basic RBFN architecture to handle the temporal relations associated with this problem.
Reference: [6] <author> D.A. Pomerleau, </author> <title> Neural Network Perception for Mobile Robot Guidance, </title> <type> Ph.D. thesis, </type> <institution> Carnegie Mellon University, Department of Computer Science, </institution> <year> 1992. </year>
Reference-contexts: The activation of an output unit in this representation corresponds to the network's confidence that the emotion of the current sequence is in the stage corresponding to the particular output unit. Pomerleau <ref> [6] </ref> found that when there exists a proximal relation between output units the supervised learning unit activations should reflect this relation. In our application, an output unit represents a stage of an emotion and the stages are related by the obvious temporal proximal relation.
Reference: [7] <author> M. Seibert and A.M. Waxman, </author> <title> "Adaptive 3-D Object Recognition from Multiple Views", </title> <journal> IEEE PAMI, </journal> <volume> Vol. 14, No. 2, </volume> <pages> 107-124. </pages>
Reference-contexts: Connectionist architectures have been used in visual classification problems with great success [6,3]. The classification of visual imagery, however, has mainly focused on static imagery. Seibert and Wax-man <ref> [7] </ref> recently developed a system that performed object recognition using the object's rigid motion. The neural network learned correlations between different aspect views of an object, and as the network observed a sequence of the object moving in space, it accumulated evidence of the object it was viewing.
Reference: [8] <author> D. Terzopoulos, and K. Waters, </author> <title> "Analysis and synthesis of facial image sequences using physical and anatomical models," </title> <journal> IEEE PAMI, </journal> <volume> Vol. 15, No. 6, </volume> <pages> 569-579, </pages> <year> 1993. </year>
Reference: [9] <author> Y. Yacoob, and L.S. Davis, </author> <title> "Labeling of human face components from range data," </title> <journal> IEEE CVPR, </journal> <pages> 592-593, </pages> <year> 1993. </year>
Reference-contexts: Such an algorithm has been recently proposed for range data by Yacoob and Davis <ref> [9] </ref> and a similar algorithm could be developed for intensity images. Our algorithm tracks these regions through the remainder of the sequence.
Reference: [10] <editor> Y. Yacoob, and L.S. Davis, </editor> <booktitle> "Computing Spatio-Temporal Representations of Human Faces" IEEE CVPR, </booktitle> <pages> 70-75, </pages> <year> 1994. </year>
Reference-contexts: Building on these results we explore the potential of motion analysis in an autonomous system. The problem of recognizing facial expressions has recently attracted attention in the computer vision community [4,8,10]. Yacoob and Davis proposed an approach for analyzing and representing the dynamics of facial expressions from image sequences <ref> [10] </ref>. <p> The neural network views variable length sequences of images of a human subject instead of a single static image. The connectionist approach could replace the expert rules developed in <ref> [10] </ref>, and may allow developing person-specific learning capabilities. 2 Overview of our approach The following constitute the framework within which our approach for analysis and recognition of facial expressions is developed: * The face is viewed from a near frontal view throughout the sequence. * The overall rigid motion of the <p> The system is similar to <ref> [10] </ref> in the tracking and optical flow computation but differs in the analysis and interpretation of motion patterns. The system is composed of the following components: * Optical flow computation: Optical flow is computed at the points with high gradient at each frame.
Reference: [11] <author> Y. Yacoob, and L.S. Davis, </author> <title> Recognizing Human Facial Expressions, </title> <type> Technical Report CAR-TR-706, </type> <institution> Center for Automation Research, University of Maryland, College Park, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: Few studies have directly investigated the influence of the motion and deformation of facial features on the interpretation of facial expressions (a review of the relevant psychological aspects of recognizing facial expressions appears in <ref> [11] </ref>). Bassili [2] suggested that motion in the image of a face would allow emotions to be identified even with minimal infor mation about the spatial arrangement of features.
References-found: 11

