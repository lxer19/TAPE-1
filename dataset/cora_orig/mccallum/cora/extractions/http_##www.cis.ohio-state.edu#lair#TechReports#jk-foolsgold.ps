URL: http://www.cis.ohio-state.edu/lair/TechReports/jk-foolsgold.ps
Refering-URL: http://www.cis.ohio-state.edu/lair/Papers/Directories/f-dir.html
Root-URL: 
Email: kolen-j@cis.ohio-state.edu  
Title: Fools Gold: Extracting Finite State Machines From Recurrent Network Dynamics  
Author: John F. Kolen 
Note: DRAFT: To appear in NIPS6  
Address: Columbus, OH 43202  
Affiliation: Laboratory for AI Research Department of Computer and Information Science The Ohio State University  
Abstract: Several recurrent networks have been proposed as representations for the task of formal language learning. After training a recurrent network, the next step is to understand the information processing carried out by the network. Some researchers (Giles et al., 1992; Watrous & Kuhn, 1992; Cleeremans et al., 1989) have resorted to extracting finite state machines from the internal state trajectories of their recurrent networks. This paper describes two conditions, sensitivity to initial conditions and frivolous computational explanations due to discrete measurements (Kolen & Pollack, 1993), which allow these extraction methods to return illusionary finite state descriptions.
Abstract-found: 1
Intro-found: 1
Reference: <author> Chomsky, N. </author> <year> (1957). </year> <title> Syntactic Structures. The Hague: </title> <publisher> Mounton & Co.. </publisher>
Reference: <author> Chomsky, N. </author> <year> (1965). </year> <title> Aspects of the Theory of Syntax. </title> <address> Cambridge, Mass.: </address> <publisher> MIT Press. </publisher>
Reference: <author> Cleeremans, A., Servan-Schreiber, D. & McClelland, J. L. </author> <year> (1989). </year> <title> Finite state automata and simple recurrent networks. </title> <journal> Neural Computation, </journal> <volume> 1, </volume> <month> 372--381. </month>
Reference: <author> Crutchfield, J. & Young, K. </author> <year> (1989). </year> <title> Computation at the Onset of Chaos. </title> <editor> In W. Zurek, (Ed.), </editor> <title> Entropy, Complexity, </title> <booktitle> and the Physics of Information. </booktitle> <address> Reading: Addison-Wesely. </address>
Reference-contexts: Any FSA extraction technique should embrace this definition, in fact it grounds the standard FSA minimization methods and the physical system modelling of Crutchfield and Young <ref> (Crutchfield & Young, 1989) </ref>. Consider a recurrent network with one output and three recurrent state units. The output unit performs a threshold at zero for state unit one. That is, when the current state is to the right of the boundary then the output is positive.
Reference: <author> Devaney, R. L. </author> <year> (1989). </year> <title> An Introduction to Chaotic Dynamical Systems. </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: And by fifty iterations we see the network reaching the its full extent of in state space. This behavior is known as sensitivity to initial conditions and is one of three conditions which have been used to characterize chaotic dynamical systems <ref> (Devaney, 1989) </ref>.
Reference: <author> Giles, C. L., Miller, C. B., Chen, D., Sun, G. Z., Chen, H. H. & C.Lee, Y. </author> <year> (1992). </year> <title> Extracting and Learning an Unknown Grammar with Recurrent Neural Networks. </title> <editor> In John E. Moody, Steven J. Hanson & Richard P. Lippman, (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 4. </booktitle> <publisher> Morgan Kaufman. </publisher>
Reference: <author> Gold, E. M. </author> <year> (1969). </year> <title> Language identification in the limit. </title> <journal> Information and Control, </journal> <volume> 10, </volume> <month> 372--381. </month>
Reference-contexts: Introduction Formal language learning <ref> (Gold, 1969) </ref> has been a topic of concern for cognitive science and artificial intelligence. Neural information processing approach to this problem involves the use of recurrent networks that embody the internal state mechanisms underlying automata models (Pol-lack, 1991; ; Cleeremans et al., 1989; Watrous & Kuhn, 1992).
Reference: <author> Hopcroft, J. E. & Ullman, J. D. </author> <year> (1979). </year> <title> Introduction to Automata Theory, Languages, and Computation. </title> <publisher> Addison-Wesely. </publisher>
Reference-contexts: The basis of this claim rests upon the definition of automata state. Recall that two information processing states are the same if and only if they generate the same output Fools Gold DRAFT 2 responses for all possible future inputs <ref> (Hopcroft & Ullman, 1979) </ref>. Any FSA extraction technique should embrace this definition, in fact it grounds the standard FSA minimization methods and the physical system modelling of Crutchfield and Young (Crutchfield & Young, 1989). Consider a recurrent network with one output and three recurrent state units.
Reference: <author> Kolen, J. F. & Pollack, J. B. </author> <year> (1993). </year> <title> The Apparent Computational Complexity of Physical Systems. </title> <booktitle> In Proceedings of the Fifteenth Annual Conference of the Cognitive Science Society. </booktitle> <editor> Laurence Earlbaum. </editor> <title> FIGURE 3. Decision regions which induce a context sensitive language. q c Fools Gold DRAFT 6 Newell, </title> <editor> A. & Simon, H. A. </editor> <year> (1976). </year> <title> Computer science as empirical inquiry: symbols and search. </title> <journal> Communications of the Association for Computing Machinery, </journal> <volume> 19, </volume> <month> 113--126. </month>
Reference-contexts: Unfortunately, it is very difficult, and probably intractable, to differentiate between a nondeterministic system with a small number of states or a deterministic with large number of states. In certain cases, however, it is possible to analytically ascertain this distinction. The Observers' Paradox Elsewhere <ref> (Kolen & Pollack, 1993) </ref>, we focused on some well-hidden problems with the symbol system approach to understanding the computational powers of physical systems.
Reference: <author> Watrous, R. L. & Kuhn, G. M. </author> <year> (1992). </year> <title> Induction of Finite-State Automata Using Second-Order Recurrent Networks. </title> <editor> In John E. Moody, Steven J. Hanson & Richard P. Lippman, (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 4. </booktitle> <publisher> Morgan Kaufman. </publisher>
References-found: 10

