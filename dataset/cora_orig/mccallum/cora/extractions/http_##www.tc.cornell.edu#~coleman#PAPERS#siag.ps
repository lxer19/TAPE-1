URL: http://www.tc.cornell.edu/~coleman/PAPERS/siag.ps
Refering-URL: http://www.tc.cornell.edu/~coleman/
Root-URL: http://www.tc.cornell.edu
Title: SIAG/OPT Views-and-News A Forum for the SIAM Activity Group on Optimization pPCx: Solving Linear Programs
Author: Thomas F. Coleman Chunguang Sun Michael Wagner 
Address: Ithaca, NY, 14853  Center,Cornell University,Ithaca, NY 14853  Engineering,Cornell University,Ithaca, NY 14853n  
Affiliation: Computer Science Department and Center for Applied Mathematics, Cornell University,  Advanced Computing Research Institute, Cornell Theory  School of Operations Research and Industrial  
Note: Contents Case Studies  
Date: 9, Fall 1997 1  9 Fall 1997  
Pubnum: No.  No.  
Abstract: Case Studies pPCx: Solving Linear Programs in Parallel, T.F. Coleman, C. Sun, M. Wagner : : : : : : : : : : : : : : 1 Chairman's Column Jorge More : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 5 Essays I Know It When I See It: Toward a Definition of Direct Search Methods, M.W. Trosset : : : : : : : : : 7 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. F. Coleman, J. Czyzyk, C. Sun, M. Wagner, and S. J. Wright, pPCx: </author> <title> Parallel Software for Linear Programming, </title> <booktitle> Proc. of the Eighth SIAM Conference on Parallel Processing in Scientific Computing, </booktitle> <address> Min-neapolis, MN, </address> <month> March </month> <year> 1997 </year>
Reference-contexts: To wrap up, Table 1.2 shows an overview of a few problems that enable us to get an impression of the differences between WGPP and MMD (for more computational results see <ref> [1] </ref>). We see that MMD and WGPP are very comparable in the first 2 problems and that WGPP performs significantly better on the PDS problems. In fact, PDS-20 could not be solved on less than 8 processors using the minimum degree ordering due to memory limitations.
Reference: [2] <author> J. Czyzyk, S. Mehrotra, and S. J. Wright, </author> <title> PCx User Guide, </title> <type> Technical Report OTC 96/01, </type> <institution> Optimization Technology Center, Argonne National Laboratory and Northwestern University, </institution> <month> October </month> <year> 1996. </year>
Reference-contexts: The serial code (PCx <ref> [2] </ref>) on which we based our implementation follows a variant proposed by Sanjay Mehrotra [5] that has proven itself to be very successful in practice. <p> The solution of the system (corrector & centering step) 0 @ 0 A T I 1 A B x s C 0 @ 0 1 A a heuristically chosen centering parameter, see <ref> [2] </ref>) keeps the residuals of the equality constraints at the same level while moving back towards the "central path" by readjusting the complementarity gaps x i s i . <p> Numerous heuristics are inherited by pPCx from the serial code, and we refer the interested reader to the PCx user guide <ref> [2] </ref> for more details on these and other issues. 4. Computational Results pPCx is implemented entirely in ANSI C with MPI No. 9, Fall 1997 4 extensions, making it thus a very portable software package.
Reference: [3] <author> A. Gupta, WGPP: </author> <title> Watson Graph Partitioning (and sparse matrix ordering) Package, </title> <institution> IBM Research Report RC 20453 (90427), </institution> <month> May </month> <year> 1996. </year>
Reference: [4] <author> J. W.-H. Liu, </author> <title> Modification of the minimum degree algorithm by multiple elimination, </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 11 (1985), </volume> <pages> pp. 141-153. </pages>
Reference-contexts: This problem is known to be NP-hard; the most successful and widely used heuristic approaches are variants of Liu's Multiple Minimum Degree Ordering <ref> [4] </ref>. In a parallel setting, the ordering has another effect: it will determine the workload for each processor. A good ordering for a parallel factorization will have to balance these two objectives.
Reference: [5] <author> S. Mehrotra, </author> <title> On the implementation of a primal-dual interior point method, </title> <journal> SIAM Journal on Optimization, </journal> <volume> 2 (1992), </volume> <pages> pp. 575-601. </pages>
Reference-contexts: The serial code (PCx [2]) on which we based our implementation follows a variant proposed by Sanjay Mehrotra <ref> [5] </ref> that has proven itself to be very successful in practice. The overall search direction in each iteration is a combination of the so-called affine scaling direction and a corrector step towards the interior of the positive orthant.
Reference: [6] <author> E. Rothberg and B. Hendrickson, </author> <title> Sparse matrix ordering methods for interior point linear programming, </title> <type> Sandia National Laboratories Technical Report 96-0475, </type> <month> January, </month> <year> 1996. </year>
Reference-contexts: In a parallel setting, the ordering has another effect: it will determine the workload for each processor. A good ordering for a parallel factorization will have to balance these two objectives. Recently, permutations derived from graph partitioning ideas, known as nested dissection orderings, have been investigated (e.g., <ref> [6] </ref>) and are naturally much more successful in balancing the load. Since the ordering is done only once and its runtime is negligible in comparison, we can afford to spend some time with it. In practice, hybrid strategies are used [6]. <p> ideas, known as nested dissection orderings, have been investigated (e.g., <ref> [6] </ref>) and are naturally much more successful in balancing the load. Since the ordering is done only once and its runtime is negligible in comparison, we can afford to spend some time with it. In practice, hybrid strategies are used [6]. The obvious source of parallelism in this algorithm is the Cholesky factorization. While the numerical part is performed once per iteration and can be done fully in parallel, the symbolic part is not easily par-allelized. We do it on one processor. The triangular solves are performed in parallel.
Reference: [7] <author> C. Sun, </author> <title> Efficient parallel solutions of large sparse SPD systems on distributed-memory multiprocessors, </title> <type> Technical Report CTC92TR102, </type> <institution> Advanced Computing Research Institute, Cornell Theory Center, Cor-nell University, </institution> <address> Ithaca, NY, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: For the parallel solution of the positive definite systems it uses the implementation of a parallel multifrontal Cholesky factorization in the package psspd that was developed at the Cornell Theory Center by Chunguang Sun <ref> [7] </ref>. psspd is a self-contained package in the sense that it performs the symbolic and numerical factorization phases as well as the triangular solves. To test the performance of nested dissection orderings (psspd provides an implementation of Liu's MMD ordering) we experimented with an implementation by A. Gupta called WGPP.

Reference: [1] <author> G. E. P. Box and K. B. Wilson. </author> <title> On the experimental attainment of optimum conditions. </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> 13 </volume> <pages> 1-45, </pages> <year> 1951. </year> <note> Includes discussion. </note>
Reference-contexts: To wrap up, Table 1.2 shows an overview of a few problems that enable us to get an impression of the differences between WGPP and MMD (for more computational results see <ref> [1] </ref>). We see that MMD and WGPP are very comparable in the first 2 problems and that WGPP performs significantly better on the PDS problems. In fact, PDS-20 could not be solved on less than 8 processors using the minimum degree ordering due to memory limitations.
Reference: [2] <author> A. G. Buckley and H. Ma. </author> <title> A Derivative-Free Algorithm for Parallel and Sequential Optimization. </title> <type> Technical Report, </type> <institution> Computer Science Department, University of Victoria, Victoria, Canada, </institution> <year> 1994. </year>
Reference-contexts: The serial code (PCx <ref> [2] </ref>) on which we based our implementation follows a variant proposed by Sanjay Mehrotra [5] that has proven itself to be very successful in practice. <p> The solution of the system (corrector & centering step) 0 @ 0 A T I 1 A B x s C 0 @ 0 1 A a heuristically chosen centering parameter, see <ref> [2] </ref>) keeps the residuals of the equality constraints at the same level while moving back towards the "central path" by readjusting the complementarity gaps x i s i . <p> Numerous heuristics are inherited by pPCx from the serial code, and we refer the interested reader to the PCx user guide <ref> [2] </ref> for more details on these and other issues. 4. Computational Results pPCx is implemented entirely in ANSI C with MPI No. 9, Fall 1997 4 extensions, making it thus a very portable software package.
Reference: [3] <author> A. R. Conn, K. Scheinberg, and Ph. L. Toint. </author> <title> On the convergence of derivative-free methods for unconstrained optimization. </title> <editor> In A. Iserles and M. Buh-mann, editors, </editor> <title> Approximation Theory and Optimization: </title> <editor> Tributes to M. J. D. </editor> <booktitle> Powell, </booktitle> <pages> pages 83-108, CUP, </pages> <year> 1997. </year>
Reference: [4] <author> A. R. Conn, K. Scheinberg, and Ph. L. Toint. </author> <title> Recent progress in unconstrained nonlinear optimization without derivatives. </title> <booktitle> Mathematical Programming, </booktitle> <year> 1997. </year> <note> To appear. </note>
Reference-contexts: This problem is known to be NP-hard; the most successful and widely used heuristic approaches are variants of Liu's Multiple Minimum Degree Ordering <ref> [4] </ref>. In a parallel setting, the ordering has another effect: it will determine the workload for each processor. A good ordering for a parallel factorization will have to balance these two objectives.
Reference: [5] <author> A. R. Conn and Ph. L. Toint. </author> <title> An algorithm using quadratic interpolation for unconstrained derivative free optimization. </title> <editor> In G. Di Pillo and F. Giannessi, editors, </editor> <booktitle> Nonlinear Optimization and Applications, </booktitle> <pages> pages 27-47, </pages> <publisher> Plenum Publishing, </publisher> <address> New York, </address> <year> 1996. </year>
Reference-contexts: The serial code (PCx [2]) on which we based our implementation follows a variant proposed by Sanjay Mehrotra <ref> [5] </ref> that has proven itself to be very successful in practice. The overall search direction in each iteration is a combination of the so-called affine scaling direction and a corrector step towards the interior of the positive orthant.
Reference: [6] <author> J. E. Dennis and R. B. Schnabel. </author> <title> Numerical Methods for Unconstrained Optimization and Nonlinear Equations. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1983. </year>
Reference-contexts: In a parallel setting, the ordering has another effect: it will determine the workload for each processor. A good ordering for a parallel factorization will have to balance these two objectives. Recently, permutations derived from graph partitioning ideas, known as nested dissection orderings, have been investigated (e.g., <ref> [6] </ref>) and are naturally much more successful in balancing the load. Since the ordering is done only once and its runtime is negligible in comparison, we can afford to spend some time with it. In practice, hybrid strategies are used [6]. <p> ideas, known as nested dissection orderings, have been investigated (e.g., <ref> [6] </ref>) and are naturally much more successful in balancing the load. Since the ordering is done only once and its runtime is negligible in comparison, we can afford to spend some time with it. In practice, hybrid strategies are used [6]. The obvious source of parallelism in this algorithm is the Cholesky factorization. While the numerical part is performed once per iteration and can be done fully in parallel, the symbolic part is not easily par-allelized. We do it on one processor. The triangular solves are performed in parallel.
Reference: [7] <author> C. Elster and A. Neumaier. </author> <title> A grid algorithm for bound constrained optimization of noisy functions. </title> <journal> IMA Journal of Numerical Analysis, </journal> <volume> 15 </volume> <pages> 585-608, </pages> <year> 1995. </year>
Reference-contexts: For the parallel solution of the positive definite systems it uses the implementation of a parallel multifrontal Cholesky factorization in the package psspd that was developed at the Cornell Theory Center by Chunguang Sun <ref> [7] </ref>. psspd is a self-contained package in the sense that it performs the symbolic and numerical factorization phases as well as the triangular solves. To test the performance of nested dissection orderings (psspd provides an implementation of Liu's MMD ordering) we experimented with an implementation by A. Gupta called WGPP.
Reference: [8] <author> P. E. Gill, W. Murray, and M. H. Wright. </author> <title> Practical Optimization. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: Readers interested in more details of the theory, definitions and convergence proofs are referred to the book by S. Wright <ref> [8] </ref>, though it is not essential to know these details to grasp the gist of this article. We begin by stating a simple form of the linear programming problem and its dual: max c T x x 0 min b T No. 9, Fall 1997 2 s.t.
Reference: [9] <author> T. Glad and A. Goldstein. </author> <title> Optimization of functions whose values are subject to small errors. </title> <journal> BIT, </journal> <volume> 17 </volume> <pages> 160-169, </pages> <year> 1977. </year>
Reference: [10] <author> R. Hooke and T. A. Jeeves. </author> <title> "Direct search" solution of numerical and statistical problems. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 8 </volume> <pages> 212-229, </pages> <year> 1961. </year>
Reference: [11] <author> J. A. Nelder and R. Mead. </author> <title> A simplex method for function minimization. </title> <journal> Computer Journal, </journal> <volume> 7 </volume> <pages> 308-313, </pages> <year> 1965. </year>
Reference: [12] <author> M. J. D. Powell. </author> <title> A direct search optimization method that models the objective function by linear interpolation. </title> <booktitle> In Advances in Optimization and Numerical Analysis, </booktitle> <pages> pages 51-67, </pages> <publisher> Kluwer Academic, </publisher> <address> Dor-drecht, Netherlands, </address> <year> 1994. </year> <booktitle> Proceedings of the Sixth Workshop on Optimization and Numerical Analysis, Oaxaca, </booktitle> <address> Mexico. </address>
Reference: [13] <author> M. J. D. Powell. </author> <title> A direct search optimization method that models the objective function by quadratic interpolation. </title> <booktitle> 1994. Presentation at the Fifth Stock-holm Optimization Days. </booktitle>
Reference: [14] <author> M. J. D. Powell. </author> <title> Trust region methods that employ quadratic interpolation to model the objective function. </title> <booktitle> 1996. Presentation at the Fifth SIAM Conference on Optimization, </booktitle> <address> Victoria, British Columbia. </address>
Reference: [15] <author> S. S. Stevens. </author> <title> On the theory of scales of measurement. </title> <journal> Science, </journal> <volume> 103 </volume> <pages> 677-680, </pages> <year> 1946. </year>
Reference: [16] <author> V. Torczon. </author> <title> Multi-Directional Search: A Direct Search Algorithm for Parallel Machines. </title> <type> Technical Report 90-7, </type> <institution> Department of Computational and Applied Mathematics, Rice University, Houston, TX, </institution> <year> 1990. </year> <type> Author's 1989 Ph.D. dissertation. </type>
Reference: [17] <author> V. Torczon. </author> <title> On the convergence of pattern search methods. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 7 </volume> <pages> 1-26, </pages> <year> 1997. </year>
Reference: [18] <author> M. H. Wright. </author> <title> Direct Search Methods: Once Scorned, Now Respectable. </title> <type> Technical Report 96-4-02, </type> <institution> Computing Science Research Center, AT&T Bell Laboratories, </institution> <address> Murray Hill, NJ, </address> <year> 1996. </year>
References-found: 25

