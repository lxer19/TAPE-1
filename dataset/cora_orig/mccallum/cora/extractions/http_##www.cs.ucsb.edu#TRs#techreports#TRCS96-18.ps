URL: http://www.cs.ucsb.edu/TRs/techreports/TRCS96-18.ps
Refering-URL: http://www.cs.ucsb.edu/TRs/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: fcfu,tyangg@cs.ucsb.edu  
Title: Efficient Sparse LU Factorization with Partial Pivoting on Distributed Memory Architectures  
Author: Cong Fu and Tao Yang 
Keyword: Sparse LU factorization, Column partial pivoting, Dense structures, Task scheduling, Irregular parallelism, Run-time support, Symbolic factorization.  
Address: Santa Barbara, CA 93106  
Affiliation: Department of Computer Science University of California  
Abstract: Sparse LU factorization with partial pivoting is important to many scientific applications, but the effective parallelization of this algorithm is still an open problem. The main difficulty is that partial pivoting operations make structures of L and U factors unpredictable beforehand. This paper presents a novel approach called S fl for parallelizing this problem on distributed memory machines. S fl incorporates static symbolic factorization to avoid run-time control overhead and uses nonsymmetric L/U supernode partitioning and amalgamation strategies to maximize the use of BLAS-3 routines. The irregular task parallelism embedded in sparse LU is exploited using graph scheduling and efficient run-time support techniques. The experimental results on the Cray-T3D with a set of Harwell-Boeing nonsymmetric matrices are very encouraging and good scalability has been achieved. Even compared to a highly optimized sequential code, the parallel speedups are still impressive considering the current status of sparse LU research.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Ashcraft, R. Grimes, J. Lewis, B. Peyton, and H. Simon. </author> <title> Progress in Sparse Matrix Methods for Large Sparse Linear Systems on Vector Supercomputers. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 1 </volume> <pages> 10-30, </pages> <year> 1987. </year>
Reference-contexts: Thus it is necessary to identify dense structures in a sparse matrix after the static symbolic factorization. 4 L/U supernode partitioning and dense structure iden tification Supernode partitioning is a commonly used technique to improve the cache performance of sparse code <ref> [1] </ref>. For a symmetric sparse matrix, a supernode is defined as a group of contiguous columns that have identical nonzero patterns. Excellent performance has been achieved in [18, 21, 22] using supernode partitioning for Cholesky factorization. However, the above definition is not directly applicable to sparse LU with nonsymmetric matrices.
Reference: [2] <author> T. Davis. </author> <title> User's guide for the Unsymmetric-pattern Multifrontal Package (UMFPACK). </title> <type> Technical Report TR-93-020, </type> <institution> Computer and Information Sciences Department, University of Florida, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: They perform symbolic factorization and generate supernodes on the fly as the factorization proceeds. Their code delivers impressive performance and is among the best sequential codes for sparse LU with partial pivoting <ref> [4, 2] </ref>. However it is challenging to parallelize their code to get scalable performance and so far we have not seen any published results on the parallelization of their method on distributed memory machines. <p> The SuperLU performs symbolic factorization and identifies supernodes on the fly. It also maximizes the use of BLAS-2 level operations to improve the cache performance of sparse LU. As a result, it outperforms several other sparse LU packages [4]. For instance, it is better than UMFPACK <ref> [2] </ref> for 15 out of 21 testing matrices. It is challenging to parallelize SuperLU and get good speedups because of the following reasons. * One has to parallelize the symbolic factorization part.
Reference: [3] <author> J. Demmel. </author> <title> Numerical Linear Algebra on Parallel Processors. </title> <booktitle> Lecture Notes for NSF-CBMS Regional Conference in the Mathematical Sciences, </booktitle> <month> June </month> <year> 1995. </year> <note> To be published as a book by SIAM. </note>
Reference-contexts: However in many applications such as circuit simulation, computational fluid dynamics and structural mechanics, the associated equation systems involve nonsymmetric matrices. Pivoting must be conducted to maintain numerical stability for such nonsymmetric linear systems and a typical strategy is partial column pivoting <ref> [3, 15] </ref>. Because the pivoting operations interchange rows based on the numerical values of matrix elements during the elimination process, it is impossible to predict the precise structures of L and U factors without 1 actually performing the numerical factorization. <p> We observe that on most current commodity processors with memory hierarchies, a highly optimized BLAS-3 subroutine usually outperforms a BLAS-2 subroutine in implementing the same numerical operations <ref> [3, 5] </ref>. We can afford to introduce 2 some extra BLAS-3 operations in re-designing the LU algorithm so that the new algorithm is easy to be parallelized but the sequential performance of this code is still competitive to the current best sequential code. <p> Section 6 addresses the scheduling and run-time support issues. Section 7 discusses a supernode amalgamation technique to adjust task granularity. Section 8 presents the experimental results. Section 9 concludes the paper. 2 Preliminaries Sparse LU factorization can be used to solve large linear systems arising from many important applications <ref> [3, 15, 20] </ref>. Given a nonsingular matrix A, it can be factorized into two matrices L and U using Gaussian Elimination with column partial pivoting. During each step of the elimination process, a row interchange may be needed to maintain numerical stability. <p> We observe that in SuperLU [4] the DGEMV routine (the BLAS-2 level dense matrix vector multiplication) accounts for 78% to 98% of the floating point operations (exclude the symbolic factorization part). It is also a fact <ref> [3] </ref> that BLAS-3 routine DGEMM (matrix-matrix multiplication) is usually much faster than BLAS-1 and BLAS-2 routines. On Cray-T3D with a matrix of size 25 fi 25, DGEMM can achieve 103 MFLOPS while DGEMV only reaches 85 MFLOPS. <p> Task F actor (k) is to factorize all the columns in the k-th column block, including finding the pivoting sequence associated with those columns. Instead of performing the row interchange to the other part of the matrix right after each pivoting search, a technique called "delayed-pivoting" is used <ref> [3] </ref>. In this technique, the pivoting sequence is held until the factorization of the k-th column block is completed. Then the pivoting sequence is applied to the rest of the matrix, i.e., interchange rows.
Reference: [4] <author> J. Demmel, S. Eisenstat, J. Gilbert, X. Li, and J. Liu. </author> <title> A Supernodal Approach to Sparse Partial Pivoting. </title> <type> Technical Report CSD-95-883, </type> <institution> UC Berkeley, </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: It is easy to get speedups by comparing a parallel code to a sequential code which does not fully exploit the uni-processor capability, but it is not as easy to parallelize a highly optimized sequential code. One such sequential code is SuperLU <ref> [4] </ref> which uses a supernode approach to conduct sequential sparse LU with column partial pivoting. The supernode partitioning makes it possible to perform most of the numerical updates using BLAS-2 level dense matrix-vector multiplications, and therefore to better exploit memory hierarchies. <p> They perform symbolic factorization and generate supernodes on the fly as the factorization proceeds. Their code delivers impressive performance and is among the best sequential codes for sparse LU with partial pivoting <ref> [4, 2] </ref>. However it is challenging to parallelize their code to get scalable performance and so far we have not seen any published results on the parallelization of their method on distributed memory machines. <p> For example, the SuperLU package <ref> [4] </ref> first makes a rough estimation on the total storage needed, or more precisely according to the results from previous factorization, and expands the storage when it becomes necessary later during the factorization. Caching behavior plays an important role in achieving good performance for scientific computations. <p> The SuperLU performs symbolic factorization and identifies supernodes on the fly. It also maximizes the use of BLAS-2 level operations to improve the cache performance of sparse LU. As a result, it outperforms several other sparse LU packages <ref> [4] </ref>. For instance, it is better than UMFPACK [2] for 15 out of 21 testing matrices. It is challenging to parallelize SuperLU and get good speedups because of the following reasons. * One has to parallelize the symbolic factorization part. <p> It is challenging to parallelize SuperLU and get good speedups because of the following reasons. * One has to parallelize the symbolic factorization part. Otherwise the speedups will be limited by the symbolic factorization which contributes 20 45% of the total execution time for most of the tested matrices <ref> [4] </ref>. * Even if the symbolic factorization is parallelizable, there exists another difficulty for paral-lelization, especially on distributed memory machines. The pivot selection and row interchange dynamically increase fill-ins, which in turn will cause changes of L/U data structures, data dependence structures and processor loads. <p> We will use this static strategy. But the overestimation does introduce extra fill-ins and some unnecessary operations in the numerical factorization. We observe that in SuperLU <ref> [4] </ref> the DGEMV routine (the BLAS-2 level dense matrix vector multiplication) accounts for 78% to 98% of the floating point operations (exclude the symbolic factorization part). It is also a fact [3] that BLAS-3 routine DGEMM (matrix-matrix multiplication) is usually much faster than BLAS-1 and BLAS-2 routines. <p> Excellent performance has been achieved in [18, 21, 22] using supernode partitioning for Cholesky factorization. However, the above definition is not directly applicable to sparse LU with nonsymmetric matrices. A good analysis for defining unsymmetric supernodes is available in <ref> [4] </ref>. Basically there could be several types of unsymmetric supernodes, namely T1, T2, T3 and T4 based on different criteria. Their results show that T2 and T3 are the best choices in terms of performance. In this paper we choose type T2 supernode. <p> We call this partitioning method L 7 supernode partitioning. Here by "subrow" we mean the contiguous part of a row in a supernode. However, the issue of identifying dense structures in a U factor is not addressed by supernode types T2 and T3 <ref> [4] </ref> and a standard compressed sparse column format is used to store a U factor. For the dynamic pivoting approach in SuperLU, it is difficult to explore structure regularity in a U factor after L supernode partitioning. <p> This results in very fine grained tasks. Amalgamating small supernodes can lead to great performance improvement for both parallel and sequential sparse codes because it can improve cache performance and reduce inter-processor communication overhead. There could be many ways to amalgamate supernodes <ref> [4, 21] </ref>. The basic idea is to relax the restriction that all the columns in a supernode must have exactly the same nonzero structure. The amalgamation is usually guided by a supernode elimination tree. <p> Our experiments show that when r is in the range of 4 6, it gives the best performance for the tested matrices and leads to 16 50% improvement on the execution times of the sequential code. Compared to the 515% running time improvement by relaxed supernodes in <ref> [4] </ref>, our improvement is much more. The reason is that by getting bigger supernodes, we are getting larger dense structures, although there may be a few zero entries in them, and we are taking more advantage of BLAS-3 kernels. <p> We estimate that 0:82 for the tested matrices based on the results in <ref> [4] </ref>. From Table 6 we can see that 0:67. We also calculate the ratios of the number of floating point operations performed in S fl and SuperLU for the tested matrices in Table 5. In average, the value of C 0 C is 3:98.
Reference: [5] <author> J. Dongarra, J. Du Croz, S. Hammarling, and R. Hanson. </author> <title> An Extended Set of Basic Linear Algebra Subroutines. </title> <journal> ACM Trans. on Mathematical Software, </journal> <volume> 14 </volume> <pages> 18-32, </pages> <year> 1988. </year>
Reference-contexts: We observe that on most current commodity processors with memory hierarchies, a highly optimized BLAS-3 subroutine usually outperforms a BLAS-2 subroutine in implementing the same numerical operations <ref> [3, 5] </ref>. We can afford to introduce 2 some extra BLAS-3 operations in re-designing the LU algorithm so that the new algorithm is easy to be parallelized but the sequential performance of this code is still competitive to the current best sequential code.
Reference: [6] <author> I. S. Duff. </author> <title> On Algorithms for Obtaining a Maximum Transversal. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 7(3) </volume> <pages> 315-330, </pages> <month> September </month> <year> 1981. </year>
Reference-contexts: For any nonsingular matrix which does not have a zero-free diagonal, it is always possible to permute the rows of the matrix so that the permuted matrix has a zero-free diagonal <ref> [6] </ref>. Though the symbolic factorization does work on a matrix that contains zero entries in the diagonal, it is not preferable because it makes the overestimation too generous. <p> This symbolic factorization is applied after an ordering is performed on the matrix A to reduce fill-ins. The ordering we are currently using is the multiple minimum degree ordering for A T A. We also permute the rows of the matrix using a transversal obtained from Duff's algorithm <ref> [6] </ref> to make A have a zero-free diagonal. The transversal can often help reduce fill-ins [7]. In the SuperLU, symbolic factorization is conducted dynamically according to the actual pivoting choice so that overestimation is not an issue.
Reference: [7] <author> I. S. Duff. </author> <type> Personal Communication, </type> <year> 1996. </year>
Reference-contexts: The ordering we are currently using is the multiple minimum degree ordering for A T A. We also permute the rows of the matrix using a transversal obtained from Duff's algorithm [6] to make A have a zero-free diagonal. The transversal can often help reduce fill-ins <ref> [7] </ref>. In the SuperLU, symbolic factorization is conducted dynamically according to the actual pivoting choice so that overestimation is not an issue. But the symbolic factorization contributes average 20 45% to the total factorization time for the tested matrices.
Reference: [8] <author> C. Fu and T. Yang. </author> <title> Efficient Run-time Support for Irregular Task Computations with Mixed Granularities. </title> <booktitle> In Proceedings of IEEE International Parallel Processing Symposium, </booktitle> <pages> pages 823-830, </pages> <address> Hawaii, </address> <month> April </month> <year> 1996. </year> <month> 24 </month>
Reference-contexts: We need to overlap computation with communication, balance loads and eliminate unnecessary communication overhead. We have examined two possible scheduling strategies. One is to use the compute-ahead scheduling [15] and another is to use sophisticated graph scheduling. We implemented our sparse LU method using a run-time system RAPID <ref> [8, 9] </ref> and conducted experiments with a set of Harwell-Boeing nonsymmetric matrices on Cray-T3D. Our experiments show that the scheduled code has achieved good scalability. The rest of the paper is organized as follows. Section 2 gives the problem definition. <p> Graph scheduling has already been shown effective for irregular problems such as adaptive n-body simulation [13] and sparse Cholesky factorization <ref> [8, 9] </ref>. For sparse LU, it can better exploit irregular parallelism compared to the compute-ahead scheduling. The major reason is that the compute-ahead scheduling is not as efficient in overlapping communication with computation. We demonstrate this point using the LU task graph in Figure 9. <p> It does not incur any copying/buffering during a data transfer. The functionality of RMA suffices to fit our task protocol and the overhead is kept as small as possible. RMA is available in modern multi-processor architectures such as Cray-T3D [23] and Meiko CS-2 <ref> [8] </ref>. 2. Eliminating redundant communication. A task may send the same message to several successors and some of these successors could be assigned to the same processor. In this case, it is enough to send this message once to the processor on which those successors reside [24]. 3. <p> However in the RAPID system, this hand-shaking process can be avoided by a carefully designed task communication protocol [9]. It should be mentioned that un-synchronized direct remote memory access may overwrite some live data and cause data inconsistencies during the execution. In <ref> [8, 9] </ref>, we prove that if a task graph is dependence-complete, i.e., all data dependencies are captured in this graph and the graph 16 is sequentializable, the task schedule execution by RAPID is always correct. It is easy to show that our sparse LU task graphs are dependence-complete.
Reference: [9] <author> C. Fu and T. Yang. </author> <title> Run-time Compilation for Parallel Sparse Matrix Computations. </title> <booktitle> In Proceedings of ACM International Conference on Supercomputing, </booktitle> <pages> pages 237-244, </pages> <address> Philadelphia, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: We need to overlap computation with communication, balance loads and eliminate unnecessary communication overhead. We have examined two possible scheduling strategies. One is to use the compute-ahead scheduling [15] and another is to use sophisticated graph scheduling. We implemented our sparse LU method using a run-time system RAPID <ref> [8, 9] </ref> and conducted experiments with a set of Harwell-Boeing nonsymmetric matrices on Cray-T3D. Our experiments show that the scheduled code has achieved good scalability. The rest of the paper is organized as follows. Section 2 gives the problem definition. <p> Graph scheduling has already been shown effective for irregular problems such as adaptive n-body simulation [13] and sparse Cholesky factorization <ref> [8, 9] </ref>. For sparse LU, it can better exploit irregular parallelism compared to the compute-ahead scheduling. The major reason is that the compute-ahead scheduling is not as efficient in overlapping communication with computation. We demonstrate this point using the LU task graph in Figure 9. <p> However, the efficient execution of a task graph schedule is the key to reach this goal. We discuss the run-time support strategies for executing tasks in the next subsection. 15 6.2 The RAPID run-time system We have implemented our scheme using the RAPID run-time system <ref> [9] </ref>. This system provides a set of library functions for specifying irregular data objects and tasks that access these objects. The system then extracts a task dependence graph from data access patterns, schedules and executes tasks efficiently on a distributed memory machine. <p> Thus for a general computation, a permission to write the remote address needs to be obtained before issuing a remote write. However in the RAPID system, this hand-shaking process can be avoided by a carefully designed task communication protocol <ref> [9] </ref>. It should be mentioned that un-synchronized direct remote memory access may overwrite some live data and cause data inconsistencies during the execution. <p> However in the RAPID system, this hand-shaking process can be avoided by a carefully designed task communication protocol [9]. It should be mentioned that un-synchronized direct remote memory access may overwrite some live data and cause data inconsistencies during the execution. In <ref> [8, 9] </ref>, we prove that if a task graph is dependence-complete, i.e., all data dependencies are captured in this graph and the graph 16 is sequentializable, the task schedule execution by RAPID is always correct. It is easy to show that our sparse LU task graphs are dependence-complete.
Reference: [10] <author> K. Gallivan, B. Marsolf, and H. Wijshoff. </author> <title> The Parallel Solution of Nonsymmetric Sparse Linear Systems using H* Reordering and an Assoicated Factorization. </title> <booktitle> In Proc. of ACM International Conference on Supercomputing, </booktitle> <pages> pages 419-430, </pages> <address> Manchester, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: There are several approaches that can be used for solving nonsymmetric systems. One approach is the unsymmetric-pattern multi-frontal method [17] that uses elimination graphs to model irregular parallelism and guide the parallel computation if the pivoting sequence can be known prior to numerical factorization. Another approach <ref> [10] </ref> is to restructure a sparse matrix into a bordered block upper triangular form and use a special pivoting technique which preserves the structure and maintains numerical stability at acceptable levels. It is implemented on Illinois Cedar multiprocessors based on Aliant shared memory clusters.
Reference: [11] <author> A. George and E. Ng. </author> <title> Symbolic Factorization for Sparse Gaussian Elimination with Partial Pivoting. </title> <journal> SIAM J. Scientific and Statistical Computing, </journal> <volume> 8(6) </volume> <pages> 877-898, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: Our paper focuses on the parallelization issues for a given matrix ordering with a commonly used pivoting strategy (column partial pivoting) to maintain numerical stability. Parallelization of sparse LU with partial column pivoting is also studied in <ref> [11, 12] </ref> on a shared memory machine. Their approaches overestimate the nonzero fill-ins by using a static symbolic LU factorization so that the dynamic variation of LU data structures is avoided. <p> We use the static symbolic factorization technique first proposed in <ref> [11, 12] </ref> to predict the worst possible structures of the L and U factors without knowing the actual numerical values, then we develop a nonsymmetric L/U supernode partitioning technique to identify the dense structures in both L and U factors, and maximize the use of BLAS-3 level subroutines (matrix-matrix multiplication) for <p> It has been shown that the structure of L c can be used as an upper bound for the structures of L and U factors regardless of the choice of the pivot row at each step <ref> [11] </ref>. But it turns out that this bound is not very tight. It often substantially overestimates the structures of the L and U factors. We consider another method instead [11]. The basic idea is to statically consider all the possible pivoting choices at each step. <p> upper bound for the structures of L and U factors regardless of the choice of the pivot row at each step <ref> [11] </ref>. But it turns out that this bound is not very tight. It often substantially overestimates the structures of the L and U factors. We consider another method instead [11]. The basic idea is to statically consider all the possible pivoting choices at each step. The space is allocated for all the possible nonzeros that would be introduced by any pivoting sequence that could occur during the numerical factorization. We summarize the symbolic factorization method briefly as follows.
Reference: [12] <author> A. George and E. Ng. </author> <title> Parallel Sparse Gaussian Elimination with Partial Pivoting. </title> <journal> Annals of Operations Research, </journal> <volume> 22 </volume> <pages> 219-240, </pages> <year> 1990. </year>
Reference-contexts: Our paper focuses on the parallelization issues for a given matrix ordering with a commonly used pivoting strategy (column partial pivoting) to maintain numerical stability. Parallelization of sparse LU with partial column pivoting is also studied in <ref> [11, 12] </ref> on a shared memory machine. Their approaches overestimate the nonzero fill-ins by using a static symbolic LU factorization so that the dynamic variation of LU data structures is avoided. <p> We use the static symbolic factorization technique first proposed in <ref> [11, 12] </ref> to predict the worst possible structures of the L and U factors without knowing the actual numerical values, then we develop a nonsymmetric L/U supernode partitioning technique to identify the dense structures in both L and U factors, and maximize the use of BLAS-3 level subroutines (matrix-matrix multiplication) for <p> Using the precise pivoting information at each elimination step can certainly optimize data space usage, reduce communication and improve load balance, but such benefits could be offset by high run-time control overhead. 4 The strategy of static data structure prediction in <ref> [12] </ref> is valuable in avoiding dynamic symbolic factorization, identifying the maximal data dependence patterns and minimizing dynamic control overhead. We will use this static strategy. But the overestimation does introduce extra fill-ins and some unnecessary operations in the numerical factorization. <p> Previous work on sparse Cholesky factorization has used elimination trees (e.g. [21]), which can accurately capture the available parallelism because the input matrix is symmetric and pivoting is not involved. For nonsymmetric sparse LU, one can also induce some parallelism from a row-wise elimination tree <ref> [12] </ref>, but it is not as straightforward as the one for sparse Cholesky. <p> For sparse Cholesky factorization, researchers have used heuristic mapping strategies [19, 22] based on elimination trees. For sparse LU, an elimination tree does not directly reflect the available parallelism and therefore scheduling based on the elimination tree is difficult. <ref> [12] </ref> uses a dynamic load balancing algorithm on a shared memory machine.
Reference: [13] <author> A. Gerasoulis, J. Jiao, and T. Yang. </author> <title> Scheduling of Structured and Unstructured Computation . In D. </title> <editor> Hsu, A. Rosenberg, and D. Sotteau, editors, </editor> <title> Interconnections Networks and Mappings and Scheduling Parallel Computation. </title> <journal> American Math. Society, </journal> <year> 1995. </year>
Reference-contexts: Graph scheduling has already been shown effective for irregular problems such as adaptive n-body simulation <ref> [13] </ref> and sparse Cholesky factorization [8, 9]. For sparse LU, it can better exploit irregular parallelism compared to the compute-ahead scheduling. The major reason is that the compute-ahead scheduling is not as efficient in overlapping communication with computation. We demonstrate this point using the LU task graph in Figure 9.
Reference: [14] <author> A. Gerasoulis and T. Yang. </author> <title> On the Granularity and Clustering of Directed Acyclic Task Graphs . IEEE Transactions on Parallel and Distributed Systems, </title> <booktitle> 4(6) </booktitle> <pages> 686-701, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Apparently the supernode amalgamation has brought significant improvement due to the increase of supernode size which implies an increase of the task granularities. This is important to obtain good parallel performance <ref> [14] </ref>.
Reference: [15] <author> G. Golub and J. M. Ortega. </author> <title> Scientific Computing: An Introduction with Parallel Computing Compilers . Academic Press, </title> <year> 1993. </year>
Reference-contexts: However in many applications such as circuit simulation, computational fluid dynamics and structural mechanics, the associated equation systems involve nonsymmetric matrices. Pivoting must be conducted to maintain numerical stability for such nonsymmetric linear systems and a typical strategy is partial column pivoting <ref> [3, 15] </ref>. Because the pivoting operations interchange rows based on the numerical values of matrix elements during the elimination process, it is impossible to predict the precise structures of L and U factors without 1 actually performing the numerical factorization. <p> Scheduling and executing DAG parallelism is a difficult job because parallelism in sparse problems is irregular and execution must be asynchronous. We need to overlap computation with communication, balance loads and eliminate unnecessary communication overhead. We have examined two possible scheduling strategies. One is to use the compute-ahead scheduling <ref> [15] </ref> and another is to use sophisticated graph scheduling. We implemented our sparse LU method using a run-time system RAPID [8, 9] and conducted experiments with a set of Harwell-Boeing nonsymmetric matrices on Cray-T3D. Our experiments show that the scheduled code has achieved good scalability. <p> Section 6 addresses the scheduling and run-time support issues. Section 7 discusses a supernode amalgamation technique to adjust task granularity. Section 8 presents the experimental results. Section 9 concludes the paper. 2 Preliminaries Sparse LU factorization can be used to solve large linear systems arising from many important applications <ref> [3, 15, 20] </ref>. Given a nonsingular matrix A, it can be factorized into two matrices L and U using Gaussian Elimination with column partial pivoting. During each step of the elimination process, a row interchange may be needed to maintain numerical stability. <p> support We discuss how sparse LU tasks can be mapped onto processors so that parallel time can be minimized and how a schedule can be executed efficiently. 6.1 Scheduling sparse LU tasks on distributed memory machines For dense problems, one can use a regular mapping such as block cyclic mapping <ref> [15] </ref>. These schemes usually result in good load balancing and decent speedup. However it is not agreed that what the best mapping strategy is for irregular applications such as sparse matrix computations. For sparse Cholesky factorization, researchers have used heuristic mapping strategies [19, 22] based on elimination trees. <p> This is to use block-cyclic mapping of tasks with a compute-ahead execution strategy, which is demonstrated in Figure 10. This idea has been used to speed up parallel dense factorizations <ref> [15] </ref>. It executes the numerical factorization layer by layer based on the current submatrix index. The parallelism is exploited for concurrent updating.
Reference: [16] <author> A. Gupta and V. Kumar. </author> <title> Optimally Scalable Parallel Sparse Cholesky Factorization. </title> <booktitle> In Proc. of 7th SIAM Conf. on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 442-447, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: 1 Introduction Sparse matrix factorization is the key to solving a sparse system of linear equations. If a matrix is symmetric and positive definite, Cholesky factorization can be used, for which fast sequential and parallel algorithms have been developed in <ref> [16, 21, 22] </ref>. However in many applications such as circuit simulation, computational fluid dynamics and structural mechanics, the associated equation systems involve nonsymmetric matrices. Pivoting must be conducted to maintain numerical stability for such nonsymmetric linear systems and a typical strategy is partial column pivoting [3, 15].
Reference: [17] <author> S. Hadfield and T. Davis. </author> <title> A Parallel Unsymmetric-pattern Multifrontal Method. </title> <type> Technical Report TR-94-028, </type> <institution> Computer and Information Sciences Departmenmt, University of Florida, </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: The adaptive and irregular nature of sparse LU data structures makes the efficient implementation of this algorithm very hard even on a modern sequential machine with memory hierarchies. There are several approaches that can be used for solving nonsymmetric systems. One approach is the unsymmetric-pattern multi-frontal method <ref> [17] </ref> that uses elimination graphs to model irregular parallelism and guide the parallel computation if the pivoting sequence can be known prior to numerical factorization. <p> For nonsymmetric sparse LU, one can also induce some parallelism from a row-wise elimination tree [12], but it is not as straightforward as the one for sparse Cholesky. Dynamically created DAGs have been used for modeling parallelism in a nonsymmetric multi-frontal method <ref> [17] </ref>. 12 (01) F actor (k) (02) for m = S (k) to S (k + 1) 1 (03) Find the pivoting position t in column m; (04) Interchange row t and row m of the column block k; (05) Scale column m and update rest of columns in this column
Reference: [18] <author> M. Heath, E. Ng, and B. Peyton. </author> <title> Parallel Algorithms for Sparse Linear Systems . SIAM Review, </title> <booktitle> 33(3) </booktitle> <pages> 420-460, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: To better exploit memory hierarchy in modern architectures, supernode partitioning is an important technique to exploit the regularity of sparse matrix computations and utilize BLAS operations to take advantage of dense operations. It has been successfully applied to Cholesky factorization <ref> [18, 21, 22] </ref>. The difficulty for the nonsymmetric case is that supernode structure depends on pivoting choices during the factorization and thus cannot be determined in advance. The SuperLU performs symbolic factorization and identifies supernodes on the fly. <p> For a symmetric sparse matrix, a supernode is defined as a group of contiguous columns that have identical nonzero patterns. Excellent performance has been achieved in <ref> [18, 21, 22] </ref> using supernode partitioning for Cholesky factorization. However, the above definition is not directly applicable to sparse LU with nonsymmetric matrices. A good analysis for defining unsymmetric supernodes is available in [4].
Reference: [19] <author> J. W. H. Liu. </author> <title> Computational Models and Task Scheduling for Parallel Sparse Cholesky Factorization . Parallel Computing, </title> <booktitle> 18 </booktitle> <pages> 327-342, </pages> <year> 1986. </year>
Reference-contexts: These schemes usually result in good load balancing and decent speedup. However it is not agreed that what the best mapping strategy is for irregular applications such as sparse matrix computations. For sparse Cholesky factorization, researchers have used heuristic mapping strategies <ref> [19, 22] </ref> based on elimination trees. For sparse LU, an elimination tree does not directly reflect the available parallelism and therefore scheduling based on the elimination tree is difficult. [12] uses a dynamic load balancing algorithm on a shared memory machine.
Reference: [20] <author> R. Lucas, T. Blank, and J. Tiemann. </author> <title> A Parallel Solution Method for Large Sparse Systems of Equations. </title> <journal> IEEE Trans. on Computer-Aided Design, </journal> <volume> CAD-6(6):981-991, </volume> <month> November </month> <year> 1987. </year>
Reference-contexts: Section 6 addresses the scheduling and run-time support issues. Section 7 discusses a supernode amalgamation technique to adjust task granularity. Section 8 presents the experimental results. Section 9 concludes the paper. 2 Preliminaries Sparse LU factorization can be used to solve large linear systems arising from many important applications <ref> [3, 15, 20] </ref>. Given a nonsingular matrix A, it can be factorized into two matrices L and U using Gaussian Elimination with column partial pivoting. During each step of the elimination process, a row interchange may be needed to maintain numerical stability.
Reference: [21] <author> E. Rothberg. </author> <title> Exploiting the Memory Hierarchy in Sequential and Parallel Sparse Cholesky Factorization . PhD thesis, </title> <institution> Dept. of Computer Science, Stanford, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Sparse matrix factorization is the key to solving a sparse system of linear equations. If a matrix is symmetric and positive definite, Cholesky factorization can be used, for which fast sequential and parallel algorithms have been developed in <ref> [16, 21, 22] </ref>. However in many applications such as circuit simulation, computational fluid dynamics and structural mechanics, the associated equation systems involve nonsymmetric matrices. Pivoting must be conducted to maintain numerical stability for such nonsymmetric linear systems and a typical strategy is partial column pivoting [3, 15]. <p> To better exploit memory hierarchy in modern architectures, supernode partitioning is an important technique to exploit the regularity of sparse matrix computations and utilize BLAS operations to take advantage of dense operations. It has been successfully applied to Cholesky factorization <ref> [18, 21, 22] </ref>. The difficulty for the nonsymmetric case is that supernode structure depends on pivoting choices during the factorization and thus cannot be determined in advance. The SuperLU performs symbolic factorization and identifies supernodes on the fly. <p> For a symmetric sparse matrix, a supernode is defined as a group of contiguous columns that have identical nonzero patterns. Excellent performance has been achieved in <ref> [18, 21, 22] </ref> using supernode partitioning for Cholesky factorization. However, the above definition is not directly applicable to sparse LU with nonsymmetric matrices. A good analysis for defining unsymmetric supernodes is available in [4]. <p> Previous work on sparse Cholesky factorization has used elimination trees (e.g. <ref> [21] </ref>), which can accurately capture the available parallelism because the input matrix is symmetric and pivoting is not involved. For nonsymmetric sparse LU, one can also induce some parallelism from a row-wise elimination tree [12], but it is not as straightforward as the one for sparse Cholesky. <p> This results in very fine grained tasks. Amalgamating small supernodes can lead to great performance improvement for both parallel and sequential sparse codes because it can improve cache performance and reduce inter-processor communication overhead. There could be many ways to amalgamate supernodes <ref> [4, 21] </ref>. The basic idea is to relax the restriction that all the columns in a supernode must have exactly the same nonzero structure. The amalgamation is usually guided by a supernode elimination tree.
Reference: [22] <author> E. Rothberg and R. Schreiber. </author> <title> Improved Load Distribution in Parallel Sparse Cholesky Factorization. </title> <booktitle> In Proc. of Supercomputing'94, </booktitle> <pages> pages 783-792, </pages> <month> November </month> <year> 1994. </year> <month> 25 </month>
Reference-contexts: 1 Introduction Sparse matrix factorization is the key to solving a sparse system of linear equations. If a matrix is symmetric and positive definite, Cholesky factorization can be used, for which fast sequential and parallel algorithms have been developed in <ref> [16, 21, 22] </ref>. However in many applications such as circuit simulation, computational fluid dynamics and structural mechanics, the associated equation systems involve nonsymmetric matrices. Pivoting must be conducted to maintain numerical stability for such nonsymmetric linear systems and a typical strategy is partial column pivoting [3, 15]. <p> To better exploit memory hierarchy in modern architectures, supernode partitioning is an important technique to exploit the regularity of sparse matrix computations and utilize BLAS operations to take advantage of dense operations. It has been successfully applied to Cholesky factorization <ref> [18, 21, 22] </ref>. The difficulty for the nonsymmetric case is that supernode structure depends on pivoting choices during the factorization and thus cannot be determined in advance. The SuperLU performs symbolic factorization and identifies supernodes on the fly. <p> For a symmetric sparse matrix, a supernode is defined as a group of contiguous columns that have identical nonzero patterns. Excellent performance has been achieved in <ref> [18, 21, 22] </ref> using supernode partitioning for Cholesky factorization. However, the above definition is not directly applicable to sparse LU with nonsymmetric matrices. A good analysis for defining unsymmetric supernodes is available in [4]. <p> These schemes usually result in good load balancing and decent speedup. However it is not agreed that what the best mapping strategy is for irregular applications such as sparse matrix computations. For sparse Cholesky factorization, researchers have used heuristic mapping strategies <ref> [19, 22] </ref> based on elimination trees. For sparse LU, an elimination tree does not directly reflect the available parallelism and therefore scheduling based on the elimination tree is difficult. [12] uses a dynamic load balancing algorithm on a shared memory machine.
Reference: [23] <author> T. Stricker, J. Stichnoth, D. O'Hallaron, S. Hinrichs, and T. Gross. </author> <title> Decoupling Synchronization and Data Transfer in Message Passing Systems of Parallel Computers. </title> <booktitle> In Proc. of International Conference on Supercomputing, </booktitle> <pages> pages 1-10, </pages> <address> Barcelona, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: It does not incur any copying/buffering during a data transfer. The functionality of RMA suffices to fit our task protocol and the overhead is kept as small as possible. RMA is available in modern multi-processor architectures such as Cray-T3D <ref> [23] </ref> and Meiko CS-2 [8]. 2. Eliminating redundant communication. A task may send the same message to several successors and some of these successors could be assigned to the same processor. <p> The communication network of the T3D is a 3-D torus. The Cray provides a shared memory access library called shmem which can achieve 126 MBytes/s bandwidth and 2:7s communication overhead using shmem put () primitive <ref> [23] </ref>. The implementation of the RAPID system on T3D uses shmem put () for remote memory access. 8.1 Sequential performance The sequential performance on the testing matrices from Table 1 are listed in Table 3.
Reference: [24] <author> T. Yang and A. Gerasoulis. </author> <title> PYRROS: Static Task Scheduling and Code Generation for Message-Passing Multiprocessors . In Proc. </title> <booktitle> of 6th ACM International Conference on Supercomputing, </booktitle> <pages> pages 428-437, </pages> <year> 1992. </year> <month> 26 </month>
Reference-contexts: We assign clusters to processors and order task execution within each 14 processor using the algorithms in <ref> [24] </ref>. The basic optimizations are balancing processor loads and overlapping computation with communication to hide communication latency. <p> Eliminating redundant communication. A task may send the same message to several successors and some of these successors could be assigned to the same processor. In this case, it is enough to send this message once to the processor on which those successors reside <ref> [24] </ref>. 3. Eliminating unnecessary synchronizations. Since the RMA directly writes data to a remote address, it is possible that the content at the remote address is still being used by other tasks and then the execution at the remote processor could be incorrect.
References-found: 24

