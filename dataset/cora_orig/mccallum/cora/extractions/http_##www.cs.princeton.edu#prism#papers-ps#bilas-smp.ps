URL: http://www.cs.princeton.edu/prism/papers-ps/bilas-smp.ps
Refering-URL: http://www.cs.princeton.edu/prism/html/all-papers.html
Root-URL: http://www.cs.princeton.edu
Title: Shared Virtual Memory Across SMP Nodes Using Automatic Update: Protocols and Performance  
Author: Angelos Bilas, Liviu Iftode, David Martin, and Jaswinder Pal Singh 
Address: Princeton, NJ 08544  
Note: Submitted for publication. Please do not distribute.  
Affiliation: Princeton university  Department of Computer Science, Princeton University,  
Pubnum: Technical Report TR-517-96.  
Abstract: As the workstation market moves form single processor to small-scale shared memory multiprocessors, it is very attractive to construct larger-scale multiprocessors by connecting widely available symmetric multiprocessors (SMPs) in a less tightly coupled way. Using a shared virtual memory (SVM) layer for this purpose preserves the shared memory programming abstraction across nodes. We explore the feasibility and performance implications of one such approach by extending the AURC (Automatic Update Release Consistency) protocol, used in the SHRIMP multicom-puter, to connect hardware-coherent SMPs rather than uniprocessors. We describe the extended AURC protocol, and compare its performance with both the AURC uniprocessor node case as well as with an all-software Lazy Release Consistency (LRC) protocol extended for SMPs. We present results based on detailed simulations of two protocols (AURC and LRC) and two architectural configurations of a system with 16 processors; one with one processor per node (16 nodes) and one with four processors per node (4 nodes). We find that, unless the bandwidth of the network interface is increased, the network interface becomes the bottleneck in a clustered architecture especially for AURC. While a LRC protocol can benefit from the reduction in per processor communication in a clustered architecture, the write-through traffic in AURC increases significantly the communication demands per network interface. This causes more traffic contention and either prevents the performance of AURC from improving under SMP or hurts it severely for applications with significant communication requirements. Thus, while AURC performs better than LRC, for applications with high communication needs, the reverse may be true in clustered architectures. Among possible solutions, two are investigated in the paper: protocol changes and bandwidth increases. Further work is clearly needed on the systems and application sides to evaluate whether AURC can be extended for multiprocessor node systems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Blumrich, K. Li, R. Alpert, C. Dubnicki, E. Felten, and J. Sandberg. </author> <title> A Virtual Memory Mapped Network Interface for the SHRIMP Multicomputer. </title> <booktitle> In Proceedings of the 21st Annual Symposium on Computer Architecture, </booktitle> <pages> pages 142-153, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Many research efforts have recently investigated supporting a coherent shared address space with less hardware support, ranging from less customized and integrated coherence controllers [12] to supporting shared virtual memory (SVM) at page level through the operating system [9, 6]. For example, the SHRIMP multicomputer <ref> [1] </ref> takes the approach of using commodity PCs and attaching a hardware network interface to them.
Reference: [2] <author> A.L. Cox, S. Dwarkadas, P. Keleher, H. Lu, R. Rajamony, and W. Zwaenepoel. </author> <title> Software Versus Hardware Shared-Memory Implementation: </title>
Reference-contexts: Their cost can be alleviated in by using a hierarchical ap-proach <ref> [2] </ref>. After all processors within a cluster have arrived at the barrier, one of them collects the write-notices from all four and sends them to the barrier manager (rather than have each processor send a message). <p> We are in the process of evaluating more applications, particularly irregular ones, and will be able to report on the results in the final version of the paper, if accepted. 7 Related Work Cox et al. in <ref> [2] </ref> discuss three different shared memory configurations. For small numbers of processors they compare a bus based cache coherent shared memory system to a SVM implementation (TradeMarks). <p> Karlsson et al. in [8] evaluates the performance of a SVM multiprocessor configuration built exclusively with commodity parts. They consider a cluster-based multiprocessor built from ATM switches and bus-based SMPs. As in <ref> [2] </ref>, the protocols used are essentially the ones in TreadMarks. They find that, although the bandwidth of present as well as next gener ation ATM technology is enough, the major bottleneck lies in current ATM interfaces whose latencies are not acceptable.
References-found: 2

