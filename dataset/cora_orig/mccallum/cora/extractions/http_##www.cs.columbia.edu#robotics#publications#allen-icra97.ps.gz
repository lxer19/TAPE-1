URL: http://www.cs.columbia.edu/robotics/publications/allen-icra97.ps.gz
Refering-URL: http://www.cs.columbia.edu/~amiller/
Root-URL: http://www.cs.columbia.edu
Title: Using Tactile and Visual Sensing with a Robotic Hand  
Author: Peter K. Allen Andrew T. Miller Paul Y. Oh Brian S. Leibowitz 
Address: New York, NY 10027  
Affiliation: Department of Computer Science, Columbia University,  
Abstract: Most robotic hands are either sensorless or lack the ability to accurately and robustly report position and force information relating to contact. This paper describes a robotic hand system that uses a limited set of native joint position and force sensing along with custom designed tactile sensors and real-time vision modules to accurately compute finger contacts and applied forces for grasping tasks. Three experiments are described: integration of real-time visual trackers in conjunction with internal strain gauge sensing to correctly localize and compute finger forces, determination of contact points on the inner and outer links of a finger through tactile sensing and visual sensing, and determination of vertical displacement by tactile sensing for a grasping task. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. Allen, A. Timcenko, B. Yoshimi, and P. Michel-man. </author> <title> Automated tracking and grasping of a moving object with a robotic hand-eye system. </title> <journal> IEEE Trans. on Robotics and Automation, </journal> <volume> 9(2) </volume> <pages> 152-165, </pages> <year> 1993. </year>
Reference-contexts: An excellent overview of this field is provided by Nicholls [10]. Two recent papers that discuss using tactile sensors without vision to estimate forces and contacts are [8, 9]. Our own work has explored the capability of vision systems to track and grasp moving objects <ref> [1] </ref> and use uncalibrated visual servoing to perform alignment tasks [17]. This work motivated us to use stereo vision to control an uninstrumented gripper in simple grasping tasks [18, 16].
Reference: [2] <author> A. Bendiksen and G. Hager. </author> <title> A vision-based grasping system for unfamiliar planar objects. </title> <booktitle> In IEEE International Conference of Robotics and Automation, </booktitle> <pages> pages 2844-2849, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Taylor et al. have used 3-D vision to guide the grasping task [15]. Castano and Hutchin-son [3] use visual constraint planes to create compliant surfaces for constraint robot movement in the real world. Bendiksen and Hager <ref> [2] </ref> have used vision in conjunction with gripper forces to achieve stable grasps. Sharma et al. [11] use perceptual 3D surfaces to represent the workspace of the gripper and object and they plan their positioning tasks along these surfaces.
Reference: [3] <author> A. Castano and S. Hutchinson. </author> <title> Visual compliance: </title> <journal> Task-directed visual servo control. IEEE Trans. on Robotics and Automation, </journal> <volume> 10(3) </volume> <pages> 334-342, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Hollingshurst and Cipolla [6] have developed a system for positioning a gripper above an object in the environment using an affine stereo transform to estimate the object's position. Taylor et al. have used 3-D vision to guide the grasping task [15]. Castano and Hutchin-son <ref> [3] </ref> use visual constraint planes to create compliant surfaces for constraint robot movement in the real world. Bendiksen and Hager [2] have used vision in conjunction with gripper forces to achieve stable grasps.
Reference: [4] <author> R. S. Fearing. </author> <title> Tactile sensing for shape interpretation. </title> <editor> In S. T. Venkataraman and T. Iberall, editors, </editor> <title> Dextrous Robot Hands. </title> <publisher> Springer-Verlag, </publisher> <year> 1989. </year>
Reference-contexts: We have designed a set of tactile sensors that can be mounted on the fingers of the hand, covering the active surfaces of the fingers and the palm. The pads use a capacitive tactile sensor designed by Howe [14] that is based upon an earlier design of Fearing's <ref> [4] </ref>. The sensor is designed to be slipped on to the links of the finger as shown in Figure 2. The electronics package is mounted on the robot wrist with wiring to each pad on the fingers and palm.
Reference: [5] <author> G. D. Hager and K. Toyama. </author> <title> X vision: A portable substrate for real-time vision applications. </title> <type> Technical report, </type> <institution> Department of Computer Science, Yale University, </institution> <year> 1995. </year>
Reference-contexts: The methods described here can be easily extended to a stereo perspective camera model that can provide full 3-D depth recovery as well [16]. The vision system we are using is a modification of Hager's X Vision system <ref> [5] </ref>. Each tracker has a state vector consisting of position and orientation information which is updated after each iteration of the tracking loop. Once a line or region tracker is initialized on an edge or window within the image it will track the feature at frame rates.
Reference: [6] <author> N. Hollinghurst and R. Cipolla. </author> <title> Uncalibrated stereo hand-eye coordination. </title> <type> Technical Report CUED/F-INFENG/TR126, </type> <institution> Department of Engineering, University of Cambridge, </institution> <year> 1993. </year>
Reference-contexts: Houshangi [7] tracked moving objects for grasping. Hollingshurst and Cipolla <ref> [6] </ref> have developed a system for positioning a gripper above an object in the environment using an affine stereo transform to estimate the object's position. Taylor et al. have used 3-D vision to guide the grasping task [15].
Reference: [7] <author> N. Houshangi. </author> <title> Control of a robot manipulator to grasp a moving target using vision. </title> <booktitle> In IEEE International Conference on Robotics and Automation, </booktitle> <pages> pages 604-609, </pages> <address> Cincinnati, </address> <month> May 13-18 </month> <year> 1990. </year>
Reference-contexts: Houshangi <ref> [7] </ref> tracked moving objects for grasping. Hollingshurst and Cipolla [6] have developed a system for positioning a gripper above an object in the environment using an affine stereo transform to estimate the object's position. Taylor et al. have used 3-D vision to guide the grasping task [15].
Reference: [8] <author> D. Johnston, P. Zhang, J. Hollerbach, and S. Jacobsen. </author> <title> A full tactile sensing suite for dextrous robot hands and use in contact force control. </title> <booktitle> In Proc. of the 1996 IEEE International Conference on Robotics and Automation, </booktitle> <pages> pages 3222-3227, </pages> <year> 1996. </year>
Reference-contexts: There have been many previous efforts to include a robust set of tactile sensors on a robotic hand. An excellent overview of this field is provided by Nicholls [10]. Two recent papers that discuss using tactile sensors without vision to estimate forces and contacts are <ref> [8, 9] </ref>. Our own work has explored the capability of vision systems to track and grasp moving objects [1] and use uncalibrated visual servoing to perform alignment tasks [17]. This work motivated us to use stereo vision to control an uninstrumented gripper in simple grasping tasks [18, 16].
Reference: [9] <author> H. Maekawa, K. Tanie, and K. Komoriya. </author> <title> Dynamic grasping force control using tactile feedback for grasp of multifingered hand. </title> <booktitle> In Proc. of the 1996 IEEE International Conference on Robotics and Automation, </booktitle> <pages> pages 2462-2469, </pages> <year> 1996. </year>
Reference-contexts: There have been many previous efforts to include a robust set of tactile sensors on a robotic hand. An excellent overview of this field is provided by Nicholls [10]. Two recent papers that discuss using tactile sensors without vision to estimate forces and contacts are <ref> [8, 9] </ref>. Our own work has explored the capability of vision systems to track and grasp moving objects [1] and use uncalibrated visual servoing to perform alignment tasks [17]. This work motivated us to use stereo vision to control an uninstrumented gripper in simple grasping tasks [18, 16].
Reference: [10] <author> H. R. </author> <title> Nicholls. Advanced Tactile Sensing for Robotics. </title> <publisher> World Scientific Press, </publisher> <year> 1992. </year>
Reference-contexts: There have been many previous efforts to include a robust set of tactile sensors on a robotic hand. An excellent overview of this field is provided by Nicholls <ref> [10] </ref>. Two recent papers that discuss using tactile sensors without vision to estimate forces and contacts are [8, 9]. Our own work has explored the capability of vision systems to track and grasp moving objects [1] and use uncalibrated visual servoing to perform alignment tasks [17].
Reference: [11] <author> R. Sharma, J. Herve, and P. Cucka. </author> <title> Analysis of dynamic hand positioning tasks using visual feedback. </title> <type> Technical Report CAR-TR-574, </type> <institution> Center for Auto. Res., University of Maryland, </institution> <year> 1991. </year>
Reference-contexts: Castano and Hutchin-son [3] use visual constraint planes to create compliant surfaces for constraint robot movement in the real world. Bendiksen and Hager [2] have used vision in conjunction with gripper forces to achieve stable grasps. Sharma et al. <ref> [11] </ref> use perceptual 3D surfaces to represent the workspace of the gripper and object and they plan their positioning tasks along these surfaces. Sobh and Ba-jcsy [13] examined how finite state machines can be used to monitor the grasping process through vision.
Reference: [12] <author> C. Smith and N. Papanikolopoulos. </author> <title> Vision-guided robotic grasping: Issues and experiments. </title> <booktitle> In Proc. of the 1996 IEEE International Conference on Robotics and Automation, </booktitle> <pages> pages 3203-3208, </pages> <year> 1996. </year>
Reference-contexts: Sharma et al. [11] use perceptual 3D surfaces to represent the workspace of the gripper and object and they plan their positioning tasks along these surfaces. Sobh and Ba-jcsy [13] examined how finite state machines can be used to monitor the grasping process through vision. Smith and Papanikolopolous <ref> [12] </ref> have recently extended their visual servoing and control algorithms to create a hand-eye tracker capable of grasping static and moving objects. There have been many previous efforts to include a robust set of tactile sensors on a robotic hand.
Reference: [13] <author> T. M. Sobh and R. </author> <title> Bajcsy. Autonomous observation under uncertainty. </title> <booktitle> In Proc. of IEEE International Conference on Robotics and Automation, </booktitle> <pages> pages 1792-1798, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Bendiksen and Hager [2] have used vision in conjunction with gripper forces to achieve stable grasps. Sharma et al. [11] use perceptual 3D surfaces to represent the workspace of the gripper and object and they plan their positioning tasks along these surfaces. Sobh and Ba-jcsy <ref> [13] </ref> examined how finite state machines can be used to monitor the grasping process through vision. Smith and Papanikolopolous [12] have recently extended their visual servoing and control algorithms to create a hand-eye tracker capable of grasping static and moving objects.
Reference: [14] <author> J. Son and R. Howe. </author> <title> Tactile sensing and stiffness control with multifingered hands. </title> <booktitle> In Proc. of the 1996 IEEE International Conference on Robotics and Automation, </booktitle> <pages> pages 3228-3233, </pages> <year> 1996. </year>
Reference-contexts: We have designed a set of tactile sensors that can be mounted on the fingers of the hand, covering the active surfaces of the fingers and the palm. The pads use a capacitive tactile sensor designed by Howe <ref> [14] </ref> that is based upon an earlier design of Fearing's [4]. The sensor is designed to be slipped on to the links of the finger as shown in Figure 2. The electronics package is mounted on the robot wrist with wiring to each pad on the fingers and palm.
Reference: [15] <author> M. Taylor, A. Blake, and A. Cox. </author> <title> Visually guided grasping in 3d. </title> <booktitle> In IEEE International Conference of Robotics and Automation, </booktitle> <pages> pages 761-766, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Houshangi [7] tracked moving objects for grasping. Hollingshurst and Cipolla [6] have developed a system for positioning a gripper above an object in the environment using an affine stereo transform to estimate the object's position. Taylor et al. have used 3-D vision to guide the grasping task <ref> [15] </ref>. Castano and Hutchin-son [3] use visual constraint planes to create compliant surfaces for constraint robot movement in the real world. Bendiksen and Hager [2] have used vision in conjunction with gripper forces to achieve stable grasps.
Reference: [16] <author> B. Yoshimi. </author> <title> Visual Control of Robotics Tasks. </title> <type> PhD thesis, </type> <institution> Dept.of Computer Science, Columbia University, </institution> <year> 1995. </year>
Reference-contexts: Our own work has explored the capability of vision systems to track and grasp moving objects [1] and use uncalibrated visual servoing to perform alignment tasks [17]. This work motivated us to use stereo vision to control an uninstrumented gripper in simple grasping tasks <ref> [18, 16] </ref>. <p> The methods described here can be easily extended to a stereo perspective camera model that can provide full 3-D depth recovery as well <ref> [16] </ref>. The vision system we are using is a modification of Hager's X Vision system [5]. Each tracker has a state vector consisting of position and orientation information which is updated after each iteration of the tracking loop.
Reference: [17] <author> B. Yoshimi and P. Allen. </author> <title> Alignment using an uncalibrated camera system. </title> <journal> IEEE Trans. on Robotics and Automation, </journal> <volume> 11(4) </volume> <pages> 516-521, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: Two recent papers that discuss using tactile sensors without vision to estimate forces and contacts are [8, 9]. Our own work has explored the capability of vision systems to track and grasp moving objects [1] and use uncalibrated visual servoing to perform alignment tasks <ref> [17] </ref>. This work motivated us to use stereo vision to control an uninstrumented gripper in simple grasping tasks [18, 16].
Reference: [18] <author> B. H. Yoshimi and P. Allen. </author> <title> Visual control of grasping and manipulation tasks. </title> <booktitle> In MFI '94: 1994 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems, </booktitle> <pages> pages 575-582, </pages> <year> 1994. </year>
Reference-contexts: Our own work has explored the capability of vision systems to track and grasp moving objects [1] and use uncalibrated visual servoing to perform alignment tasks [17]. This work motivated us to use stereo vision to control an uninstrumented gripper in simple grasping tasks <ref> [18, 16] </ref>.
References-found: 18

