URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-198.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Email: trevor@media.mit.edu eero@media.mit.edu  
Title: use of "Nulling" Filters to Separate Transparent Motions  
Author: Trevor Darrell and Eero Simoncelli 
Address: 20 Ames Street Cambridge MA, 02139  
Affiliation: Vision and Modeling Group The Media Laboratory Massachusetts Institute of Technology  
Note: On the  
Abstract: M.I.T. Media Laboratory Vision and Modeling Group Technical Report No. 198 ABSTRACT Transparent motions can be isolated in the Fourier domain, using derivative prefilters that selectively null the spatio-temporal energy consistent with the gradient constraint imposed by each component velocity. We use these "nulling" prefilters to robustly compute the support a particular velocity has at a point despite other velocities that may also be present at that point. Images are decomposed into a set of global constraints that account for a maximal amount of the motion information in the sequence. Adopting global motion hypotheses and a selection mechanism based on finding a parsimonious subset of these hypotheses, we show results separating motion stimuli into their constituent layers, even when those layers are transparently combined in the stimuli. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. H. Adelson. </author> <title> Layered representations for motion sequences. </title> <type> Technical Report TR-181, </type> <institution> Vision and Modeling Group Technical Report, MIT Media Lab, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: We have adopted a model that combines the best of both approaches, using a global constraint mechanism with arbitrary spatial grouping, and local measurements that are robust in the presence of transparent motion. Our grouping mechanism is based on the idea of a "layered representation" proposed by Adelson <ref> [2, 1] </ref>, in which each object or process in a scene is represented by a data structure describing aspects of the image attributable to that object.
Reference: [2] <author> E. H. Adelson and P. Anandan. </author> <title> Ordinal characteristics of transparency. </title> <type> Technical Report TR-150, </type> <institution> Vision and Modeling Group Technical Report, MIT Media Lab, </institution> <month> July </month> <year> 1990. </year>
Reference-contexts: We have adopted a model that combines the best of both approaches, using a global constraint mechanism with arbitrary spatial grouping, and local measurements that are robust in the presence of transparent motion. Our grouping mechanism is based on the idea of a "layered representation" proposed by Adelson <ref> [2, 1] </ref>, in which each object or process in a scene is represented by a data structure describing aspects of the image attributable to that object.
Reference: [3] <author> J. R. Bergen, P. J. Burt, K. Hanna, R. Hingorani, P. Jeanne, and S. Peleg. </author> <title> Dynamic multiple-motion computation. </title> <editor> In Y. A. Feldman and A. Bruckstein, editors, </editor> <booktitle> Artificial Intelligence and Computer Vision, </booktitle> <pages> pages 147-156. </pages> <publisher> Elsevier Science Publishers B.V., </publisher> <year> 1991. </year>
Reference-contexts: The algorithm of Shizawa and Mase [13, 14] directly computes two velocity vectors for each location in the image, but does not address the problem of perceptual grouping of coherently moving regions of the scene. The algorithms of Bergen et. al. <ref> [3] </ref> and Irani and Peleg [9] compute global affine optical flow fields, but use local measurements that are only capable of determining a single velocity estimate at each point. <p> This operation is similar in spirit to the predictive cancellation of Bergen et. al <ref> [3] </ref>, however we perform the computation locally rather than globally. We want to find the conditional probability of v given the observed image gradient and a known transparent velocity v 0 , P (vjrI; v 0 ).
Reference: [4] <author> M. Black and P. Anandan. </author> <title> Constraints for the early detection of discontinuity from motion. </title> <booktitle> In Proc Eighth National Conf on Artificial Intelligence, </booktitle> <pages> pages 1060-1066, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: A common approach to motion grouping has been to use an edge field in combination with a velocity field, and use a smoothing mechanism that does not integrate information across edge boundaries <ref> [4] </ref>. This type of "line-process" approach works well when the scene contains only simple occlusion, in which each object projects to a relatively large, connected region in the image.
Reference: [5] <author> T. Darrell and A. P. Pentland. </author> <title> Segmentation by minimal description. </title> <booktitle> In Proceedings Thrid Intl. Conference on Computer Vision, </booktitle> <address> Osaka, Japan, </address> <month> December </month> <year> 1990. </year>
Reference-contexts: To accommodate these types of phenomena, one needs a representation that allows for multiple velocities in a single neighborhood, and for the integration of information across disjoint regions of an image. Instead of a line-process, we have advocated the use of a set of layered "support processes" <ref> [5, 6] </ref>. Based on the notion of support found in the robust estimation literature [10], the support for a given global motion is computed as the conditional probability of the velocity field implied by the motion given the observed image sequence.
Reference: [6] <author> T. Darrell and A. P. Pentland. </author> <title> On the representation of occluded shapes. </title> <booktitle> In Proceedings IEEE Conference on Computer Vision, </booktitle> <address> Maui, Hawaii, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: To accommodate these types of phenomena, one needs a representation that allows for multiple velocities in a single neighborhood, and for the integration of information across disjoint regions of an image. Instead of a line-process, we have advocated the use of a set of layered "support processes" <ref> [5, 6] </ref>. Based on the notion of support found in the robust estimation literature [10], the support for a given global motion is computed as the conditional probability of the velocity field implied by the motion given the observed image sequence.
Reference: [7] <author> T. Darrell and A. P. Pentland. </author> <title> Robust estimation of a multi-layer motion representation. </title> <booktitle> In Proceedings IEEE Workshop on Visual Motion, </booktitle> <address> Princeton, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: Our grouping mechanism is based on the idea of a "layered representation" proposed by Adelson [2, 1], in which each object or process in a scene is represented by a data structure describing aspects of the image attributable to that object. In <ref> [7] </ref> we used the layered representation idea for the processing of motion information, allowing for arbitrary occlusion using layers with explicit regions of support for each moving object in the scene. <p> Here we remedy this deficit, incorporating a model of local motion measurements that accounts for motion transparency using the principle of superposition [13] together with a probabilistic model of velocity computation [15]. The grouping mechanism adopted in <ref> [7] </ref> assumed a purely spatial notion of support and thus could not explicitly represent transparent phenomena, since the support was constrained to be non-overlapping in the final solution. This paper extends the definition of support from an exclusively spatial notion, to include the spatio-temporal energy domain. <p> They have a computationally efficient mechanism that iteratively selects the layer with the largest support among uncovered portions of the image, and then marks the pixels associated with that layer as covered. In <ref> [7] </ref>, we presented a method that generated initial hypotheses either by sampling the parameter space, or by sampling the image domain (into small patches, for example) and fitting parameters to each sample. <p> Other authors have experimented with higher-order approaches, such as affine models (see [9, 16]). In previously reported work <ref> [7] </ref>, we demonstrated a system that used this velocity model, a simple version of the the support computation described in eq. 6, and a selection method equivalent to the one presented above without the model of non-conflicting velocities (thus it could not handle pure transparency). <p> We expect to report results on such scenes shortly. Results on sequences with non-transparent looming are reported in <ref> [7] </ref>. 10 5.3 Results We ran our system on several example images. We constructed sequences that contain two additively combined subimages moving over each other in different directions: v 1 = (0:8; 0:8), v 2 = (0:0; 0:8). <p> Figure 5 shows an example sequence with a person moving behind a stationary plant. The support field computed using the non-transparent support function (eq. 6) is shown in Figure 5 (b), as was reported in <ref> [7] </ref>.
Reference: [8] <author> M. Husain, S. Treue, and R. Andersen. </author> <title> Surface interpolation in three-dimensional structure-from-motion perception. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 324-333, </pages> <year> 1989. </year>
Reference: [9] <author> M. Irani and S. Peleg. </author> <title> Image sequence enhancement using multiple motions analysis. </title> <type> Technical Report 91-15, </type> <institution> Department of Computer Science Technical Report, The Hebrew University of Jerusalem, Israel, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: The algorithm of Shizawa and Mase [13, 14] directly computes two velocity vectors for each location in the image, but does not address the problem of perceptual grouping of coherently moving regions of the scene. The algorithms of Bergen et. al. [3] and Irani and Peleg <ref> [9] </ref> compute global affine optical flow fields, but use local measurements that are only capable of determining a single velocity estimate at each point. <p> One approach, presented by <ref> [9] </ref>, is to assume a spatially dominant "background" whose parameters can be estimated based on the entire image data, since the outlier contamination from the foreground will be relatively small. The support of the background is then computed using the weighted residual error computation given above. <p> Other authors have experimented with higher-order approaches, such as affine models (see <ref> [9, 16] </ref>).
Reference: [10] <author> P. Meer. </author> <title> Robust regression methods for computer vision: A review. </title> <journal> Intl. J. Computer Vision, </journal> <volume> 6 </volume> <pages> 60-70, </pages> <year> 1991. </year>
Reference-contexts: Instead of a line-process, we have advocated the use of a set of layered "support processes" [5, 6]. Based on the notion of support found in the robust estimation literature <ref> [10] </ref>, the support for a given global motion is computed as the conditional probability of the velocity field implied by the motion given the observed image sequence. Typically, this reduces to a weighted (normalized) residual error computation. <p> With multiple objects, this approach fails when two of the objects exist at the same scale. (In this case the percentage of outliers the foreground support will exceed the breakdown point of the robust estimation method <ref> [10] </ref>.) 4.1 Hypothesize and select methods An alternative approach is to test many different motion hypotheses for a given image sequence and decide which are worth retaining.
Reference: [11] <author> J. Mulligan. </author> <title> Motion transparency is restricted to two planes. </title> <institution> Investigative Opthal-mology and Visual Science Supplement (ARVO), 33:1049, </institution> <year> 1992. </year>
Reference-contexts: three motions, we could similarly construct P (vjrI; v 0 ; v 00 ): P (vjrI; v 0 ; v 00 ) / jjD (v)(D (v 0 )(D (v 00 )I))jj 2 + jjvjj 2 v for which the support function would be: s (x; y) = max 1 Mulligan <ref> [11] </ref> has shown that most human observers cannot discriminate more than 2 local motions at a single point. 6 Higher-order support functions are possible, but at the expense of considerably increasing computational complexity.
Reference: [12] <author> H. H. Nagel. </author> <title> On the estimation of optical flow: relations between different approaches and some new results. </title> <journal> Artificial Intelligence, </journal> <volume> 33 </volume> <pages> 299-324, </pages> <year> 1987. </year>
Reference-contexts: If the scene contains only a single motion at every point, this is a straightforward task using the well-known gradient-based models of velocity computation <ref> [12] </ref>. 3.1 The gradient constraint Given a particular velocity field v (x; y) and an image sequence I (x; y; t), we wish to compute the support field s (x; y) indicating those regions of the sequence with motion matching v (x; y) (at a particular time t 0 ).
Reference: [13] <author> M. Shizawa and K. Mase. </author> <title> Simultaneous multiple optical flow estimation. </title> <booktitle> In IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <address> Atlantic City, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: The algorithm of Shizawa and Mase <ref> [13, 14] </ref> directly computes two velocity vectors for each location in the image, but does not address the problem of perceptual grouping of coherently moving regions of the scene. <p> Here we remedy this deficit, incorporating a model of local motion measurements that accounts for motion transparency using the principle of superposition <ref> [13] </ref> together with a probabilistic model of velocity computation [15]. The grouping mechanism adopted in [7] assumed a purely spatial notion of support and thus could not explicitly represent transparent phenomena, since the support was constrained to be non-overlapping in the final solution. <p> Shizawa and Mase <ref> [13] </ref> introduced this formulation and proceeded to develop methods for the analytic estimation of two velocities at each point given the local image derivative information. Our focus, however, is on the computation of support, i.e. the likelihood of a velocity given the image information.
Reference: [14] <author> M. Shizawa and K. Mase. </author> <title> A unified computational theory for motion transparency and motion boundaries based on eigenenergy analysis. </title> <booktitle> In IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <address> Maui, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: The algorithm of Shizawa and Mase <ref> [13, 14] </ref> directly computes two velocity vectors for each location in the image, but does not address the problem of perceptual grouping of coherently moving regions of the scene.
Reference: [15] <author> E. P. Simoncelli and E. H. Adelson. </author> <title> Probability distributions of optical flow. </title> <booktitle> In IEEE Conference on Computer Vision and Pattern Recognition, Mauii, Hawaii, </booktitle> <month> June </month> <year> 1991. </year>
Reference-contexts: Here we remedy this deficit, incorporating a model of local motion measurements that accounts for motion transparency using the principle of superposition [13] together with a probabilistic model of velocity computation <ref> [15] </ref>. The grouping mechanism adopted in [7] assumed a purely spatial notion of support and thus could not explicitly represent transparent phenomena, since the support was constrained to be non-overlapping in the final solution. <p> Following <ref> [15] </ref>, we introduce a noise model into the gradient constraint equation: D (v (x; y) + n 1 (x; y))I (x; y; t) = n 2 (x; y) n i = N (0; i ); (4) where the noise term n 1 describes errors in the planarity assumption, and n 2
Reference: [16] <author> J. Y. A. Wang and E. Adelson. </author> <title> Layered representations for image sequence coding. </title> <booktitle> Submitted to Fourth International Conference on Computer Vision. </booktitle>
Reference-contexts: With this approach, one has to specify a method of generating the initial set of hypothetical motion fields, and a selection mechanism that acts upon them. Wang et. al. <ref> [16] </ref> have developed a layer description algorithm that fits motion parameters to initial patches of the image, and then computes the support associated with the estimated parameters. <p> Other authors have experimented with higher-order approaches, such as affine models (see <ref> [9, 16] </ref>).

References-found: 16

