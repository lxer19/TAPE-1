URL: http://delicias.dia.fi.upm.es/WORKSHOP/ECAI98/papers/marcel.ps
Refering-URL: http://delicias.dia.fi.upm.es/WORKSHOP/ECAI98/papers.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: vdriet@cs.vu.nl  
Title: Using Multiple Ontologies in a Framework for Natural Language Generation  
Author: Marcel Frohlich and Reind van de Riet flfl 
Keyword: Natural Language Generation, NLG, Ontologies, Knowl edge Representation, Databases  
Date: March 13, 1998  
Address: Amsterdam  
Affiliation: Universitat Tubingen marcel.froehlich@uni-tuebingen.de **Vrije Universiteit  
Abstract: A framwork for Natural Language Generation (NLG) is described, that can be used with arbitrarily structured knowledge sources (for example a database). It is built around three layers of domain knowledge representation, supporting different tasks. The benefit of this approach is that a given representation can be used as input to NLG rather than a redundant representation that has to be maintained separately. Domain dependent components are clearly distinguished. Domain independent tools for natural language generation are integrated. 
Abstract-found: 1
Intro-found: 1
Reference: [Bat97] <author> John Bateman. </author> <title> KPML Development Environment, Release 1.0. </title> <institution> German Center for Information Technology (GMD-IPSI), </institution> <year> 1997. </year>
Reference-contexts: It can be much simpler than the conceptualisation of layer 1. The different situations model different aspectual properties. The lowest layer (layer 1) is a domain independent ontology (called Upper Model) [BKMW90] developed with the Penman [Pen89] and KPML <ref> [Bat97] </ref> sentence generation systems. We use KPML for sentence generation. The concepts of the Upper Model (UM) encode semantic distinctions found in different languages, suitable to motivate distinctions in lexicogrammatical expression. 5 Not encoded within the UM are textual or interpersonal differences, but only the propositional differences (experiential meaning). <p> The measure is seconds per sentence. But there are a lot of opportunities to increase speed. An improved system should be implemented in C++. It is just standard for fast server software. Reimplementations of existing LISP systems with C++ show speedups of a factor 5 to 10. KPML <ref> [Bat97] </ref> is a grammar development tool. A monolingual generation server which implements only the core functions for generation could be much smaller. The algorithm for sentence generation has a lot of opportunities for parallel processing. A multi-threaded implementation could increase the processing considerably for multi-processor hardware.
Reference: [BKMW90] <author> John A. Bateman, Robert T. Kasper, Johanna D. Moore, and Richard A. Whitney. </author> <title> A general organization of knowledge for natural language processing: the penman upper model. </title> <type> Technical report, </type> <institution> USC/Information Sciences Institute, Marina del Rey, Cali-fornia, </institution> <year> 1990. </year>
Reference-contexts: Layer 2 does not model the whole domain, only the parts relevant for the NLG system. It can be much simpler than the conceptualisation of layer 1. The different situations model different aspectual properties. The lowest layer (layer 1) is a domain independent ontology (called Upper Model) <ref> [BKMW90] </ref> developed with the Penman [Pen89] and KPML [Bat97] sentence generation systems. We use KPML for sentence generation.
Reference: [Bri96] <author> David Brill. </author> <title> Loom Reference Manual Version 2.0. </title> <type> ISI, </type> <institution> University of Southern California, </institution> <month> September </month> <year> 1996. </year> <note> DRAFT. </note>
Reference-contexts: The goal of the system is to generate automatically administrative application forms in multiple languages from an abstract specification of the bureaucratic procedure. Some tools were identical: LOOM <ref> [Bri96] </ref>, for knowledge representation with the Upper Model and KPML for the English sentence generation. The interesting difference to many other projects is that GIST was built to be actually used as a real application. <p> The prototype can only generate single sentences. The original idea to connect it to a Case-Tool failed, because the meta models implemented in the Case-Tool did 8 not capture the full semantics of the diagrams, so we implemented new meta models in LOOM <ref> [Bri96] </ref>. On the left side is the knowledge base holding the meta-models, and the two ontologies. The circles in the middle represent the three different processing units.
Reference: [cFP96] <author> (contact) Fabio Pianesi. </author> <title> GIST Generating Instructional Text. Final Report 1.1, IRST, </title> <address> Trento, Italy, </address> <month> 9 </month> <year> 1996. </year>
Reference-contexts: One lexical item covers at least one concept of the SitSpec. The partial SemSpecs are combined to a well-formed SemSpec, which is passed to the generator. 4.4 Comparison: Knowledge Representation in GIST GIST <ref> [cFP96] </ref> is a big text generation application developed from 1993 until 1996 in a project with five European institutions involving more than 30 researchers. The goal of the system is to generate automatically administrative application forms in multiple languages from an abstract specification of the bureaucratic procedure.
Reference: [FLP94] <author> Giovanni Fabris, Keith Vander Linden, and Richard Power. </author> <title> Implementation of the upper and domain models. </title> <booktitle> GIST deliverable TSP-1b, IRST, </booktitle> <address> Trento, Italy, </address> <month> 11 </month> <year> 1994. </year>
Reference-contexts: The GIST consortium followed the knowledge representation scheme given by KPML, extending the Upper Model with domain concepts. But they kept a separate root for the domain knowledge. Imagine two DAGs with common leaves. The domain knowledge is modularised along the dimension of specialisation <ref> [FLP94] </ref>. A 'common' model on the top, refined by a model about administration, in turn refined by a model about pension applications. Domain concepts inherit from Upper Model concepts. This can be seen as a fixed mapping from the domain model to the Upper Model.
Reference: [Fro98] <author> Marcel Frohlich. </author> <title> NLG(db) Natural Language Generation from Databases. </title> <type> Master's thesis, </type> <institution> University of Tubingen, </institution> <year> 1998. </year>
Reference-contexts: The next section discusses the necessity of this separation to make the technology of natural language generation fit the requirements for industrial usage. Section 3 gives a quick overview of the architecture of a typical natural language generation system. Section 4 presents the framework called NLG (db) <ref> [Fro98] </ref> and compares it with another state-of-the-art text generation system. It is followed by a small case study using the framework: a prototype called CoMo for verbalising conceptual models. <p> domain independent modules.The more the former can be isolated, the easier it becomes to develop an NLG application within a predefined framework with reusable NLG components. 3 4 Knowledge Representation in NLG (db) In this section we will describe the different representation layers used in our framework called NLG (db) <ref> [Fro98] </ref>. 4.1 What is an ontology? We adopt Wielinga's view on the O word [WSJ + 94] : An ontology is a meta theory. <p> The sentence finally generated is (see screenshot): The cashier transfers money from a cash-drawer to a cash drawer. A more detailed description of the prototype can be found in M. Frohlich's Master's thesis <ref> [Fro98] </ref>. 6 Open Questions for Research 6.1 Integrating missing processing steps Several steps have to be integrated with the described framework for "real" text generation.
Reference: [Kri94] <author> Gerald Kristen. </author> <title> Object Orientation: The KISS Method: From Information Architecture to Information System. </title> <publisher> Addison-Wesley, </publisher> <address> Wokingham, England, </address> <year> 1994. </year>
Reference-contexts: If we want to be able to generate texts from any such diagram, then our domain we have to look at is the ontology (meta model) of the modelling technique. In the example we use a so-called Object Interaction diagram from a system analysis/design method called KISS Method <ref> [Kri94] </ref>. 5.1 Architecture and Implementation In the next diagram you can see the overall architecture of the prototype. The gray shaded textplan operator library has not been implemented. The prototype can only generate single sentences.
Reference: [KW89] <author> Robert Kasper and Richard Whitney. SPL: </author> <title> A sentence plan language for text generation. </title> <type> Technical Report forthcoming, </type> <institution> Information Sciences Institute, </institution> <address> 1989. 4676 Admiralty Way, Marina del Rey, California 90292-6695. </address>
Reference-contexts: SitSpecs are specifications that reference elements of layer 2. They are input to the MOOSE tool from M. Stede [Ste96]. MOOSE maps the SitSpecs to so-called SemSpecs. A SemSpecs are lexicalised SPL expressions <ref> [KW89] </ref>, which is the input format to the sentence generator. 4.3.1 From Speech Act to SitSpec This mapping , like the representation layers involved is domain dependent. But as layer 2 can be seen as a simplified form of layer 3 it is straightforward and not very complex.
Reference: [Lin94] <author> Keith Vander Linden. </author> <title> Specification of the extended sentence planning language. </title> <booktitle> GIST deliverable TST-0, </booktitle> <address> ITRI, Brighton, UK, </address> <month> 12 </month> <year> 1994. </year>
Reference-contexts: They chose an augmented version of Upper Model/SPL. The representation language is called ESPL <ref> [Lin94] </ref> and is augmented with mechanisms to represent interpersonal and textual parameters. For each sentence generator they programmed a mapping module, producing the generators native input format from ESPL. Compared to the hierarchy presented in the last section, layer 3 is further subdivided (generator independent/dependent sentence representation).
Reference: [Mel96] <author> Chris Mellsih. </author> <booktitle> Lecture notes. GLDV Autumn School in Magdeburg, </booktitle> <year> 1996. </year>
Reference-contexts: It is followed by a small case study using the framework: a prototype called CoMo for verbalising conceptual models. Section 6 collects some open questions and the last section presents the conclusions of this paper. 1 2 Motivation Automatically generated documents have some highly desirable properties <ref> [Mel96] </ref>. <p> Unfortunately, text generation systems capable of handling some of these advantages are still very complex pieces of software that hardly ever step across the border from academic research to industrial use. But the diagram <ref> [Mel96] </ref> comparing requirements of industrial and research NLG systems shows that it is not merely a problem of complexity. The approaches used for research prototypes are usually not transferable on industrial systems.
Reference: [MP93] <author> Johanna D. Moore and Cecile L. Paris. </author> <title> Planning texts for advisory dialogs: capturing intentional and rhetorical information. </title> <journal> Computational Linguistics, </journal> <volume> 19(4):651 - 694, </volume> <month> December </month> <year> 1993. </year>
Reference-contexts: It remains stable across different domains, because it uses a conceptualisation inherent to natural language. 4.3 Processing Several processing steps can be distinguished. First, text planning down to the level of atomic speech acts (simplified one per sentence), which will not be discussed ( see for example <ref> [MP93] </ref>). Then, mapping the the speech act specifications, which include references to concepts of layer 3 to so-called SitSpecs. SitSpecs are specifications that reference elements of layer 2. They are input to the MOOSE tool from M. Stede [Ste96]. MOOSE maps the SitSpecs to so-called SemSpecs.
Reference: [Pen89] <author> Penman Project. penman documentation: </author> <title> the Primer, the User Guide, the Reference Manual, and the Nigel manual. </title> <type> Technical report, </type> <institution> USC/Information Sciences Institute, Marina del Rey, Cali-fornia, </institution> <year> 1989. </year>
Reference-contexts: It can be much simpler than the conceptualisation of layer 1. The different situations model different aspectual properties. The lowest layer (layer 1) is a domain independent ontology (called Upper Model) [BKMW90] developed with the Penman <ref> [Pen89] </ref> and KPML [Bat97] sentence generation systems. We use KPML for sentence generation.
Reference: [Ste96] <author> Manfred Stede. </author> <title> Lexical Semantics and Knowledge Representation in Multilingual Sentence Generation. </title> <type> PhD thesis, </type> <institution> University of Toronto, </institution> <year> 1996. </year>
Reference-contexts: A situation determines the maximal conceptual content of a phrase (in most cases one sentence) but it does not determine how it is expressed. The idea of this ontology and the corresponding tools (mapping to layer 1, semantic lexicon) come from M. Stede's PhD thesis <ref> [Ste96] </ref>. His system, called MOOSE is part of our framework. Layer 2 does not model the whole domain, only the parts relevant for the NLG system. It can be much simpler than the conceptualisation of layer 1. The different situations model different aspectual properties. <p> Then, mapping the the speech act specifications, which include references to concepts of layer 3 to so-called SitSpecs. SitSpecs are specifications that reference elements of layer 2. They are input to the MOOSE tool from M. Stede <ref> [Ste96] </ref>. MOOSE maps the SitSpecs to so-called SemSpecs. A SemSpecs are lexicalised SPL expressions [KW89], which is the input format to the sentence generator. 4.3.1 From Speech Act to SitSpec This mapping , like the representation layers involved is domain dependent. <p> A name, a denotation, a coverage 6 and a partial SemSpec (lookup element). The distinction between denotation and coverage opens the possibility to formulate complex patterns of applicability beyond the covered denotation. The previous figure shows an examples of a SitSpecs from a tank domain. MOOSE <ref> [Ste96] </ref> finds all possible combinations of lexical items to cover all elements of the SitSpec exactly once. One lexical item covers at least one concept of the SitSpec.
Reference: [WSJ + 94] <author> B. Wielinga, A. Th. Schreiber, W. Jansweijer, A. Anjewierden, and F. van Harmelen. </author> <title> Framework and formalism for expressing ontologies. </title> <booktitle> deliverable DO1b.1, </booktitle> <address> UvA, Amsterdam, The Netherlands, </address> <year> 1994. </year> <title> ESPRIT Project 8145 KACTUS. </title> <type> 14 </type>
Reference-contexts: develop an NLG application within a predefined framework with reusable NLG components. 3 4 Knowledge Representation in NLG (db) In this section we will describe the different representation layers used in our framework called NLG (db) [Fro98]. 4.1 What is an ontology? We adopt Wielinga's view on the O word <ref> [WSJ + 94] </ref> : An ontology is a meta theory.
References-found: 14

