URL: http://www.cis.upenn.edu/~ungar/papers/acc95.ps
Refering-URL: http://www.cis.upenn.edu/~ungar/papers.html
Root-URL: 
Email: ungar@cis.upenn.edu deveaux@williams.edu  
Title: EMRBF: A Statistical Basis for Using Radial Basis Functions for Process Control  
Author: Lyle H. Ungar Richard D. De Veaux 
Address: College  
Affiliation: Department of Chemical Engineering Mathematics Department University of Pennsylvania Williams  
Abstract: Radial Basis Function (RBF) neural networks offer an attractive equation form for use in model-based control because they can approximate highly nonlinear plants and yet are well suited for linear adaptive control. We show how interpreting RBFs as mixtures of Gaussians allows the application of many statistical tools including the EM algorithm for parameter estimation. The resulting EMRBF models give uncertainty estimates and warn when they are extrapolating beyond the region where training data was available. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A.R. Barron. </author> <title> Universal approximation bounds for superpositions of a sigmoidal function. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 39(3) </volume> <pages> 930-945, </pages> <year> 1993. </year>
Reference-contexts: However, when one is modeling a system with multiple inputs, picking basis functions adaptively, i.e. based on the data at hand, gives much more accurate models with less data than using fixed basis functions <ref> [1] </ref>.
Reference: [2] <author> A.P. Dempster, N.M. Laird, and D.B. Rubin. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> 93 </volume> <pages> 1-88, </pages> <year> 1977. </year>
Reference-contexts: The excess basis functions are then removed and the equations for w j ; j and j rearranged to avoid division by zero. The EM algorithm, which we use to estimate the model parameters, is an iterative approach to maximum likelihood estimation <ref> [2] </ref> Each iteration of an EM algorithm is composed of two step, which for this problem take the following form: an Expectation (E) step in which, given assumed centers and widths for the basis functions, one determines the probability that each point in the training set comes from each Gaussian (Eqn.
Reference: [3] <author> F. Girosi and T. Poggio. </author> <title> Networks and the best approximation property, </title> <booktitle> Biological Cybernetics 63, </booktitle> <pages> 169-176, </pages> <year> 1990. </year>
Reference: [4] <author> K.J. Hunt, D. Sbarbaro, R. Zbikowski, and P.J. Gawthrop. </author> <title> Neural networks for control-systems-A survey. </title> <journal> Automatica, </journal> <volume> 28(6) </volume> <pages> 1083-1112, </pages> <year> 1992. </year>
Reference: [5] <author> M.A. Kramer, </author> <title> M.L. Thompson, and P.M. Bha-gat. Embedding theoretical models in neural networks. </title> <booktitle> Proceedings of the 1992 ACC. </booktitle> <pages> 475-479, </pages> <year> 1992. </year>
Reference-contexts: RBFs offer a number of advantages over backpropagation networks. As we will show below, RBFs lend themselves to producing error bars on their predictions. They can easily be used to learn model mismatch when an approximate first principles model is available <ref> [5] </ref>. And, most attractively for adaptive control purposes, if one fixes the basis functions (i.e. picks the centers and widths of Gaussians), then the predictions are linear in the coefficients (the weights). These weights can then be adapted, and all of the standard mathematics of linear systems can be applied.
Reference: [6] <author> J.A. Leonard, M.A. Kramer, and L.H. Ungar. </author> <title> Using radial basis functions to approximate a function and its error bounds. </title> <journal> IEEE Transactions on Neural Networks. </journal> <volume> 3 </volume> <pages> 624-627, </pages> <year> 1992. </year>
Reference: [7] <author> MacLachlan and Basford. </author> <title> Mixture Models. </title> <publisher> Chapman and Hall, </publisher> <year> 1988. </year>
Reference: [8] <author> J. Moody and C.J. Darken. </author> <title> Fast learning in networks of locally tuned processing units. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 281-294, </pages> <year> 1989. </year>
Reference-contexts: This method, called EM-RBF (Expectation Maximization Radial Basis Functions), picks basis functions taking into account both the x's and the y's unlike the standard k-means clus-tering on the x's <ref> [8] </ref>. We then present results comparing EMRBF with standard RBF estimation methods. The final section summarizes and mentions additional benefits from using EMRBF for control. 2. Mixture Model Formulation One way of describing radial basis functions is to view them as mixtures of Gaussians.
Reference: [9] <author> K. Parthasarathy and K. Narendra. </author> <title> Stable adaptive control of a class of discrete-time nonlinear systems using radial basis neural networks. </title> <type> Report No. 9103, </type> <institution> Electrical Engineering, Yale University, </institution> <year> 1991. </year>
Reference-contexts: These weights can then be adapted, and all of the standard mathematics of linear systems can be applied. There is a large literature on the use of RBFs for control applications; see, for example, Parthasarathy and Narendra <ref> [9] </ref> and Sanner and Slotine [12]. We present a statistical interpretation of RBFs as a mixture of Gaussians, and show how this leads to an efficient algorithm for simultaneously estimated the centers and widths, j and j , of the basis functions and the coefficients, w j .
Reference: [10] <author> T. Poggio and F. Girosi. </author> <title> Regularization algorithms for learning that are equivalent to multilayer networks. </title> <journal> Science. </journal> <volume> 247 </volume> <pages> 978-982, </pages> <year> 1990. </year>
Reference-contexts: In the limit, both types of networks, with enough neurons and enough data, can approximate any well-behaved function arbitrarily well <ref> [10] </ref>. In practice, RBFs are often superior to backpropagation networks when the input is of relatively low dimension (5-10 inputs), but are usually inferior for high dimensional problems (over 20 inputs). RBFs offer a number of advantages over backpropagation networks.
Reference: [11] <author> D.C. Psichogios and L.H. Ungar. </author> <title> Direct and indirect model based control using artificial neural networks. </title> <journal> Industrial Engineering Chemical Research. </journal> <volume> 30 </volume> <pages> 2564-2573, </pages> <year> 1991. </year>
Reference: [12] <author> R.M. Sanner and J.-J.E. Slotine. </author> <title> Gaussian networks for direct adaptive control. </title> <journal> IEEE Trans. Neural Networks. </journal> <volume> 3 </volume> <pages> 837-863, </pages> <year> 1992. </year>
Reference-contexts: These weights can then be adapted, and all of the standard mathematics of linear systems can be applied. There is a large literature on the use of RBFs for control applications; see, for example, Parthasarathy and Narendra [9] and Sanner and Slotine <ref> [12] </ref>. We present a statistical interpretation of RBFs as a mixture of Gaussians, and show how this leads to an efficient algorithm for simultaneously estimated the centers and widths, j and j , of the basis functions and the coefficients, w j .
Reference: [13] <author> L.H. Ungar, T. Johnson and R.D. </author> <title> DeVeaux Radial Basis Functions for Process Control, </title> <booktitle> CIMPRO Proceedings. </booktitle> <year> 1994 </year>
Reference-contexts: One can equally easily assume that the x's are correlated with themselves and the y's have correlation as well. This gives a covariance with a block matrix form and produces elliptical basis functions <ref> [13] </ref>. One can also use a full covariance matrix. The following results then follow from the mixture models literature (See e.g. MacLachlan and Basford, 1988, p. 37), which we have specialized for the case where j takes the diagonal form described above.
References-found: 13

