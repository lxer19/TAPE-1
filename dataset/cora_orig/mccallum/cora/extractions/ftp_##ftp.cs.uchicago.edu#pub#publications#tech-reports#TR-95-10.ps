URL: ftp://ftp.cs.uchicago.edu/pub/publications/tech-reports/TR-95-10.ps
Refering-URL: http://cs-www.uchicago.edu/publications/tech-reports/
Root-URL: 
Email: schaefer@cs.uchicago.edu  
Title: Deciding the Vapnik-ervonenkis dimension is  giving a rare example of a p 3 -complete problem.  
Author: p -complete Marcus Schfer 
Note: 3 -complete, thereby  
Address: 1100 East 58th Street Chicago, Illinois 60637, USA  
Affiliation: Department of Computer Science University of Chicago  
Abstract: Linial et al. raised the question of how dicult the computation of the Vapnik-ervonenkis dimension of a concept class over a tnite universe is. Papadmimitriou and Yannakakis obtained a trst answer using matrix representations of concept classes. We choose a more natural representation, which leads us to redetne the vc dimension problem. We establish that vc dimension is p 
Abstract-found: 1
Intro-found: 1
Reference: [AB92] <author> Martin Anthony, Norman Biggs. </author> <title> Computational Learning Theory. </title> <publisher> Cambridge University Press, </publisher> <year> 1992. </year>
Reference-contexts: Typically the concepts in C can be generated in some eective manner. To illustrate this, consider M n , the monomials in n variables, a standard example taken from learning theory (see <ref> [AB92] </ref>). If we identify a monomial with the assignments in X = f0; 1g n , which make it true, then every monomial is a concept, and M n is a concept class as detned above.
Reference: [Ass83] <author> Patrick Assouad. </author> <title> Densit et dimension. </title> <institution> Ann. Inst. Fourier, Grenoble, 33,3:233282, </institution> <year> 1983. </year>
Reference: [BKS93] <author> Richard Beigel, Martin Kummer, Frank Stephan. </author> <title> Quantifying the amount of verboseness. Information and Computation, </title> <publisher> 118:7390,1995. </publisher>
Reference-contexts: Their idea was successfully taken up by Blumer, Ehrenfeucht, Haussler and Warmuth and applied to learning theory [BE89]. Later applications include tnite automata [IT93], recursion theory <ref> [BKS93, KKta] </ref> and computational geometry [KS95], where an eective version of the Vapnik-ervonenkis dimension was suggested. Over some universe X a concept is a subset of the universe, and a concept class is a collection of concepts.
Reference: [BE89] <author> Anselm Blumer, Andzej Ehrenfeucht, David Haussler, Manfred K. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the Association for Computing Machinery, 36,4: </journal> <volume> 929965, </volume> <year> 1989. </year>
Reference-contexts: The notion originated in a paper in probability theory [VC71], where Vapnik and ervonenkis used it to prove uniform convergence of empirical distributions (see [Dud84]). Their idea was successfully taken up by Blumer, Ehrenfeucht, Haussler and Warmuth and applied to learning theory <ref> [BE89] </ref>. Later applications include tnite automata [IT93], recursion theory [BKS93, KKta] and computational geometry [KS95], where an eective version of the Vapnik-ervonenkis dimension was suggested. Over some universe X a concept is a subset of the universe, and a concept class is a collection of concepts. <p> Let us have a closer look at the VC dimension in learning theory. Blumer et al. showed in <ref> [BE89] </ref> that the VC dimension is a direct measure for the diculty of learning a concept class.
Reference: [Dud84] <author> Richard Mansteld Dudley. </author> <title> A course on empirical processes. In: cole d't de Probabilits de Saint-Flour XI I - 1982. </title> <booktitle> Lecture Notes in Mathematics 1097, </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, 2142, </address> <year> 1984. </year>
Reference-contexts: 1 Introduction An important contribution of mathematics to computer science has been the notion of the Vapnik-ervonenkis dimension, or VC dimension for short. The notion originated in a paper in probability theory [VC71], where Vapnik and ervonenkis used it to prove uniform convergence of empirical distributions (see <ref> [Dud84] </ref>). Their idea was successfully taken up by Blumer, Ehrenfeucht, Haussler and Warmuth and applied to learning theory [BE89]. Later applications include tnite automata [IT93], recursion theory [BKS93, KKta] and computational geometry [KS95], where an eective version of the Vapnik-ervonenkis dimension was suggested.
Reference: [IT93] <author> Yoshiyasu Ishigami, Sei'ichi Tani. </author> <title> The VC dimensions of Finite Automata with n States. </title> <booktitle> Proceedings of the 4th Workshop in Algorithmic Learning Theory, </booktitle> <volume> 328341, </volume> <year> 1993. </year>
Reference-contexts: The notion originated in a paper in probability theory [VC71], where Vapnik and ervonenkis used it to prove uniform convergence of empirical distributions (see [Dud84]). Their idea was successfully taken up by Blumer, Ehrenfeucht, Haussler and Warmuth and applied to learning theory [BE89]. Later applications include tnite automata <ref> [IT93] </ref>, recursion theory [BKS93, KKta] and computational geometry [KS95], where an eective version of the Vapnik-ervonenkis dimension was suggested. Over some universe X a concept is a subset of the universe, and a concept class is a collection of concepts.
Reference: [KKta] <author> Susanne Kaufmann, Martin Kummer. </author> <title> On a Quantitative Notion of Uniformity. </title> <note> to appear in: Fundamenta Informaticae, an extended abstract appeared in Proceedings MFCS'95, LNCS 969, 169178, Springer-Verlag, </note> <year> 1995. </year>
Reference-contexts: Their idea was successfully taken up by Blumer, Ehrenfeucht, Haussler and Warmuth and applied to learning theory [BE89]. Later applications include tnite automata [IT93], recursion theory <ref> [BKS93, KKta] </ref> and computational geometry [KS95], where an eective version of the Vapnik-ervonenkis dimension was suggested. Over some universe X a concept is a subset of the universe, and a concept class is a collection of concepts.
Reference: [KS95] <author> Martin Kummer, Marcus Schfer. </author> <title> Computability of Convex Sets. </title> <booktitle> Proceedings of the 12th Symposium on Theoretical Aspects of Computer Science, </booktitle> <year> 1995. </year>
Reference-contexts: Their idea was successfully taken up by Blumer, Ehrenfeucht, Haussler and Warmuth and applied to learning theory [BE89]. Later applications include tnite automata [IT93], recursion theory [BKS93, KKta] and computational geometry <ref> [KS95] </ref>, where an eective version of the Vapnik-ervonenkis dimension was suggested. Over some universe X a concept is a subset of the universe, and a concept class is a collection of concepts.
Reference: [LMR88] <author> Nathan Linial, Yishay Mansour, Ronald L. Rivest. </author> <title> Results on learnability and Vapnik-er-vonenkis dimension. </title> <booktitle> Proceedings of FOCS, </booktitle> <year> 1988. </year> <month> 4 </month>
Reference-contexts: This made the actual computation of the VC dimension of primary interest, and led to the question, what is the complexity of computing the VC dimension of a concept class? This question was trst asked by Linial, Mansour and Rivest in <ref> [LMR88] </ref>. To make it meaningful, we restrict ourselves to tnite universes X. This is sometimes called the discrete VC dimension problem. Since the universe is tnite, so is the concept class, say C = fC 1 ; : : : ; C m g. <p> So in addition to classifying the vc dimension problem, we present a rare natural example of a p 3 complete problem. The question of classifying the complexity of computing the VC dimension was trst asked by Linial et al. in <ref> [LMR88] </ref>. They approached the problem dierently by restricting representations of concept classes to matrix representations. This is oblivious to the fact that natural concept classes typically have an eective parameterization, which allows for representations that are exponentially smaller than the matrix representation, as exemplited above in the case of monomials.
Reference: [Pap94] <author> Christos H. Papadimitriou. </author> <title> Computational Complexity. </title> <publisher> Addison-Wesley, </publisher> <year> 1994. </year>
Reference-contexts: Similarly Shinohara [Shi93] classited matrix vc-dimension with regard to two other classes. 2 Computing the VC dimension Most of the notation is standard and can be found in <ref> [Pap94] </ref>. We will let denote the alphabet f0; 1g. The notation x [j::k] is the substring of x that starts at the jth bit and ends with the kth bit. So x is the same as x [1::n], if n is the length of x.
Reference: [PY93] <author> Christos H. Papadimitriou, Mihalis Yannakakis. </author> <title> On Limited Nondeterminism and the Complexity of the V-C Dimension. </title> <booktitle> Proceedings of the 8th Conference on Structure in Complexity Theory, </booktitle> <volume> 1218, </volume> <year> 1993. </year>
Reference-contexts: Let us call Linial et al.'s approach the matrix vc-dimension problem. They observed that matrix vc-dimension can be solved in time O (n log n ), where n is the size of the matrix. Later Papadimitriou and Yannakakis in <ref> [PY93] </ref> detned a new complexity class LOGNP and showed the problem to be complete for this class. Similarly Shinohara [Shi93] classited matrix vc-dimension with regard to two other classes. 2 Computing the VC dimension Most of the notation is standard and can be found in [Pap94].
Reference: [Sak93] <author> Akito Sakurai. </author> <title> On the VC dimension of Depth Four Threshold Circuits and the Complexity of Boolean-valued Functions. </title> <booktitle> Proceedings of the 4th Workshop in Algorithmic Learning Theory, </booktitle> <volume> 279287, </volume> <year> 1993. </year>
Reference: [Shi93] <author> Ayumi Shinohara. </author> <title> Complexity of Computing Vapnik-ervonenkis Dimension. </title> <booktitle> Proceedings of the 4th Workshop in Algorithmic Learning Theory, </booktitle> <volume> 279287, </volume> <year> 1993. </year>
Reference-contexts: They observed that matrix vc-dimension can be solved in time O (n log n ), where n is the size of the matrix. Later Papadimitriou and Yannakakis in [PY93] detned a new complexity class LOGNP and showed the problem to be complete for this class. Similarly Shinohara <ref> [Shi93] </ref> classited matrix vc-dimension with regard to two other classes. 2 Computing the VC dimension Most of the notation is standard and can be found in [Pap94]. We will let denote the alphabet f0; 1g.
Reference: [VC71] <author> V. N. Vapnik, A. Ya. ervonenkis. </author> <title> On the uniform convergence of relative frequencies of events to their probabilities. </title> <journal> Theory Probab. Appl., </journal> <volume> 16:264280, </volume> <year> 1971. </year> <month> 5 </month>
Reference-contexts: 1 Introduction An important contribution of mathematics to computer science has been the notion of the Vapnik-ervonenkis dimension, or VC dimension for short. The notion originated in a paper in probability theory <ref> [VC71] </ref>, where Vapnik and ervonenkis used it to prove uniform convergence of empirical distributions (see [Dud84]). Their idea was successfully taken up by Blumer, Ehrenfeucht, Haussler and Warmuth and applied to learning theory [BE89].
References-found: 14

