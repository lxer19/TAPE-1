URL: ftp://ftp.stat.duke.edu/pub/WorkingPapers/92-a03.ps
Refering-URL: http://www.isds.duke.edu/~mw/papers.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: SUMMARY  
Title: Hyperparameter estimation in Dirichlet process mixture models  
Author: By MIKE WEST 
Keyword: Some key words: Data augmentation; Dirichlet process; Gibbs sampling; Mixture models  
Note: This research was partially financed by the National Science Foundation under grant DMS-9024793  
Address: Durham NC 27706, USA.  
Affiliation: Institute of Statistics and Decision Sciences Duke University,  
Abstract: In Bayesian density estimation and prediction using Dirichlet process mixtures of standard, exponential family distributions, the precision or total mass parameter of the mixing Dirichlet process is a critical hyperparame-ter that strongly influences resulting inferences about numbers of mixture components. This note shows how, with respect to a flexible class of prior distributions for this parameter, the posterior may be represented in a simple conditional form that is easily simulated. As a result, inference about this key quantity may be developed in tandem with the existing, routine Gibbs sampling algorithms for fitting such mixture models. The concept of data augmentation is important, as ever, in developing this extension of the existing algorithm. A final section notes an simple asymptotic approx imation to the posterior. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Abramowitz, M. and I.A. </author> <title> Stegun (1965), Handbook of Mathematical Functions, with Formulas, Graphs, and Mathematical Tables, </title> <publisher> Dover: </publisher> <address> New York. </address>
Reference: <author> Antoniak, C.E. </author> <title> (1974) Mixtures of Dirichlet processes with applications to non-parametric problems, </title> <journal> The Annals of Statistics, </journal> <volume> 2, </volume> <pages> 1152-1174. </pages>
Reference-contexts: Take equation (1), P (kjff; n) = c n (k)n!ff k (ff) (ff + n) 5 with c n (k) / jS (k) n j <ref> (using results in Antoniak 1974) </ref>, where S (k) are Stirling numbers of the first kind (Abramowitz & Stegun section 24.1.3, p824).
Reference: <author> Escobar, D.M. and West, M. </author> <title> (to appear) Bayesian density estimation and inference using mixtures, </title> <journal> Journal of the American Statistical Association. </journal>
Reference: <author> Ferguson, </author> <title> T.S. (1983) Bayesian density estimation by mixtures of normal distributions, in Recent Advances in Statistics, </title> <editor> eds. H. Rizvi and J. Rustagi, </editor> <address> New York: </address> <publisher> Academic Press, </publisher> <pages> pp. 287-302. </pages>
Reference: <author> Gelfand, A.E. and Smith, A.F.M. </author> <year> (1990), </year> <title> Sampling based approaches to calculating marginal densities, </title> <journal> Journal of the American Statistical Association, </journal> <volume> 85, </volume> <pages> 398-409. </pages>
Reference: <author> M. West and G. </author> <title> Cao (to appear) Assessing mechanisms of neural synaptic activity. </title> <booktitle> In Bayesian Statistics in Science and Technology: Case Studies. </booktitle> <pages> 6 </pages>
References-found: 6

