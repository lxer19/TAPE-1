URL: http://www-robotics.usc.edu/~maja/publications/aa98-wfrancois.ps.gz
Refering-URL: http://www-robotics.usc.edu/~maja/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: michaudf@gel.usherb.ca mataric@cs.usc.edu  
Phone: tel: (819) 821-8000 ext. 2107 tel: (213) 740-4520  
Title: A History-Based Approach for Adaptive Robot Behavior in Dynamic Environments  
Author: Fran~cois Michaud Maja J Mataric 
Web: http://www.gel.usherb.ca/michaudf http://www-robotics.usc.edu/~agents  
Address: Sherbrooke (Quebec Canada) J1K 2R1 Los Angeles, CA 90089-0781  
Affiliation: Department of Electrical and Computer Engineering Computer Science Department Universite de Sherbrooke University of Southern California  
Abstract: The behavior-based approach has proven to be useful in making mobile agents work in dynamic and complex situations. Since the behaviors are responsible for managing the interactions between the agent and its environment, observing their use can be exploited to model these interactions. In our approach, the agent is initially given a set of behaviors to choose from, and the algorithm adapts the selection of behaviors according to the history of behavior use, represented in a tree structure. The approach is validated using a vision- and sonar-based Pioneer I robot in non-stationary conditions, in the context of a multi-robot foraging task. Results show the effectiveness of the approach in taking advantage of any regularities experienced in the world, leading to fast and adaptable specialization for the learning agent. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Agre, P. E. </author> <year> (1988), </year> <title> The Dynamic Structure of Everyday Life, </title> <type> PhD thesis, </type> <institution> MIT. </institution>
Reference-contexts: These considerations must be as independent as possible of the agent's operating environment, and of the agent's own limitations in perception, action and processing. Since performance of situated agents is based on their ability to cope with and exploit the dynamics of interaction with their environment <ref> (Agre 1988, Brooks 1991) </ref>, it is appropriate to model these interactions using a representation consistent with the control mechanism used, in our case the behavior-based framework. Doing so also preserves two key strengths of the behavior-based approach, situated-ness and emergence (Brooks 1991).
Reference: <author> Asada, M., Uchibe, E., Noda, S., Tawaratsumida, S. & Hosoda, K. </author> <year> (1994), </year> <title> Coordination of multiple behaviors acquired by a vision-based reinforcement learning, </title> <booktitle> in `Proc. IEEE/RSJ/GI Int'l Conf. on Intelligent Robots and Systems', </booktitle> <address> Munich, Germany. </address>
Reference: <author> Brooks, R. A. </author> <year> (1986), </year> <title> `A robust layered control system for a mobile robot', </title> <journal> IEEE Journal of Robotics and Automation RA-2(1), </journal> <pages> 14-23. </pages>
Reference-contexts: 1 Introduction Various kinds of autonomous agents, especially in mobile robotics, have been designed using the behavior-based paradigm <ref> (Brooks 1986) </ref>. This approach has been praised for its robustness and simplicity of construction. In a typical behavior-based system, the constituent behaviors are designed parsimoniously, executed in parallel, and prioritized using some fixed or flexible arbitration mechanism (Mataric 1997a). <p> Activated behaviors for the Searching Task, with Turn-randomly as a chosen alternative-behavior, are depicted in bold. The overall organization of the behaviors is represented in Figure 2, following the Subsumption Architecture <ref> (Brooks 1986) </ref> with the difference that the activated behaviors, i.e., those allowed to issue outputs, change dynamically. Behaviors that are not activated cannot participate in the control of the robot.
Reference: <author> Brooks, R. A. </author> <year> (1991), </year> <title> `Intelligence without representation', </title> <booktitle> Artificial Intelligence 47, </booktitle> <pages> 139-159. </pages>
Reference-contexts: Doing so also preserves two key strengths of the behavior-based approach, situated-ness and emergence <ref> (Brooks 1991) </ref>. Modeling the interaction dynamics with the environment can be especially useful in the case of dynamic multi-agent/robot environments (Goldberg & Mataric 1997) where topological and metric models do not directly apply.
Reference: <author> Brooks, R. A. </author> <year> (1996), </year> <title> MARS: Multiple Agency Reactivity System, </title> <type> Technical report, </type> <note> IS Robotics. </note>
Reference-contexts: The robot is programmed using MARS (Multiple Agency Reactivity System), a language for programming multiple concurrent processes and behaviors <ref> (Brooks 1996) </ref>. The experiments were conducted in an enclosed 12'fi11' rectangular pen containing pink blocks and a home region, marked with a green cylinder.
Reference: <author> Dorigo, M. & Colombetti, M. </author> <year> (1994), </year> <title> `Robot shaping: Developing autonomous agents through learning', </title> <booktitle> Artificial Intelligence 71(4), </booktitle> <pages> 321-370. </pages>
Reference: <author> Goldberg, D. & Mataric, M. J. </author> <year> (1997), </year> <title> Interference as a tool for designing and evaluating multi-robot controllers, </title> <booktitle> in `Proc. National Conf. on Artificial Intelligence (AAAI)', </booktitle> <address> Providence, </address> <publisher> RI, </publisher> <pages> pp. 637-642. </pages>
Reference-contexts: Doing so also preserves two key strengths of the behavior-based approach, situated-ness and emergence (Brooks 1991). Modeling the interaction dynamics with the environment can be especially useful in the case of dynamic multi-agent/robot environments <ref> (Goldberg & Mataric 1997) </ref> where topological and metric models do not directly apply. These dynamics are difficult for the designer to characterize and fully predict a priori, and are thus best captured by the agent itself.
Reference: <author> Kaelbling, L. P., Littman, M. L. & Moore, A. W. </author> <year> (1996), </year> <title> `Reinforcement learning: A survey', </title> <journal> Journal of Artificial Intelligence Research 4, </journal> <pages> 237-285. </pages>
Reference-contexts: Overall, the results obtained show that the different components of the learning algorithm try to establish a compromise between exploration (learning to adapt to Task tree in a multi-robot experiment. noise and changes in the environment) and exploitation <ref> (Kaelbling, Littman & Moore 1996) </ref> of a stable behavior selection strategy. The evaluation function characterizes the current situation and past experiences. The tree representation captures sequences of behavior use in a compact fashion to make a decision based on past experiences. <p> As in Dorigo & Colombetti (1994), Asada et al. (1994), and Mataric (1997b), our algorithm learns at the behavior gating level (i.e., decides which behavior's action should be switched through and executed <ref> (Kaelbling et al. 1996) </ref>), and the main difference from the above approaches is in our use of history of behavior use instead of percepts as states for selecting behaviors. <p> Globally, it learns a variable-width window that serves simultaneously as a model of the environment and a finite-memory policy <ref> (Kaelbling et al. 1996) </ref>. In our case, the algorithm learns a finite-memory strategy of behaviors use and selection, since it uses behaviors as an abstraction. Learning from observed behavior activation was also explored within the case-based reasoning framework, in work by Ram & Santamaria (1993).
Reference: <author> Maes, P. </author> <year> (1989), </year> <title> The dynamics of action selection, </title> <booktitle> in `Proc. Int'l Joint Conf. on Artificial Intelligence (IJ CAI)', </booktitle> <address> Detroit, MI, </address> <pages> pp. 991-997. </pages>
Reference-contexts: A fixed and parsimonious behavior set, however, does not allow an agent to easily adapt to changes in the environment, thus resulting in diminished flexibility <ref> (Maes 1989) </ref>. The ability to adapt to changing dynamics, however, is especially important for agents operating in unpredictable and non-stationary environments. One possible improvement would be to dynamically select behaviors by introducing a cognitive component capable of modeling the environment (Michaud, Lachiver & Le Dinh 1996).
Reference: <author> Mataric, M. J. </author> <year> (1997a), </year> <title> `Behavior-based control: Examples from navigation, learning, and group behavior', </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence. </journal>
Reference-contexts: This approach has been praised for its robustness and simplicity of construction. In a typical behavior-based system, the constituent behaviors are designed parsimoniously, executed in parallel, and prioritized using some fixed or flexible arbitration mechanism <ref> (Mataric 1997a) </ref>. A fixed and parsimonious behavior set, however, does not allow an agent to easily adapt to changes in the environment, thus resulting in diminished flexibility (Maes 1989). The ability to adapt to changing dynamics, however, is especially important for agents operating in unpredictable and non-stationary environments.
Reference: <author> Mataric, M. J. </author> <year> (1997b), </year> <title> `Reinforcement learning in the multi-robot domain', </title> <booktitle> Autonomous Robots 4(1). </booktitle>
Reference: <author> McCallum, A. K. </author> <year> (1996), </year> <title> Learning to use selective attention and short-term memory in sequential tasks, </title> <editor> in P. Maes, M. J. Mataric, J.-A. Meyer, J. Pollack & S. W. Wilson, eds, </editor> <booktitle> `From Animals to Animats: Proc. 4th Int'l Conf. on Simulation of Adaptive Behavior', </booktitle> <publisher> The MIT Press, </publisher> <address> Cape Cod, </address> <pages> pp. 315-324. </pages>
Reference: <author> Michaud, F. & Mataric, M. J. </author> <year> (1997), </year> <title> A history-based learning approach for adaptive robot behavior selection, </title> <type> Tech. Report CS-97-192, </type> <institution> Computer Science Department, Volen Center for Complex System, Bran-deis University, </institution> <address> Waltham, MA. </address>
Reference-contexts: The representation of the "interaction model" is based on the agent's history of behavior use, i.e., on any regularities in the sequences of behavior use that reflect the interactions between the agent and the world <ref> (Michaud & Mataric 1997, Michaud & Mataric 1998) </ref>. The agent derives this model in a reinforcement learning fashion using a performance criterion also derived from behavior use. The proposed algorithm focuses on continuous life-time adaptation rather than on learning a one-time, static policy. <p> Even in those simpler static conditions <ref> (Michaud & Mataric 1997) </ref>, unexpected behavior selection was learned by the robot.
Reference: <author> Michaud, F. & Mataric, M. J. </author> <year> (1998), </year> <title> `Learning from history for behavior-based mobile robots in nonstationary conditions', to appear in the joint special issue of Machine Learning and Autonomous Robots on "Robot Learning", </title> <editor> H. Hexmoor and M. Mataric, </editor> <publisher> eds. </publisher>
Reference: <author> Michaud, F., Lachiver, G. & Dinh, C. T. L. </author> <year> (1996), </year> <title> A new control architecture combining reactivity, deliberation and motivation for situated autonomous agent, </title> <editor> in P. Maes, M. J. Mataric, J.-A. Meyer, J. Pollack & S. W. Wilson, eds, </editor> <booktitle> `From Animals to Animats: Proc. 4th Int'l Conf. on Simulation of Adaptive Behavior', </booktitle> <publisher> The MIT Press, </publisher> <address> Cape Cod, </address> <pages> pp. 245-254. </pages>
Reference-contexts: The ability to adapt to changing dynamics, however, is especially important for agents operating in unpredictable and non-stationary environments. One possible improvement would be to dynamically select behaviors by introducing a cognitive component capable of modeling the environment <ref> (Michaud, Lachiver & Le Dinh 1996) </ref>. When behavior selection is changed according to external and internal states, knowing when to use a behavior becomes as important as the behavior itself. However, deriving a model of a dynamic and unpredictable environment is a very challenging problem. <p> However, the computational complexity of their approach is much greater than ours. Finally, the development of our algorithm was done in the context of a general control architecture for an intelligent agent <ref> (Michaud et al. 1996) </ref> based on dynamic selection of behaviors. 6 Conclusion Learning in dynamic and unpredictable environments, as in the multi-robot domain, is a very challenging problem.
Reference: <author> Ram, A. & Santamaria, J. C. </author> <year> (1993), </year> <title> `Multistrategy learning in reactive control systems for autonomous robotic navigation', </title> <journal> Informatica 17(4), </journal> <pages> 347-369. </pages>
References-found: 16

