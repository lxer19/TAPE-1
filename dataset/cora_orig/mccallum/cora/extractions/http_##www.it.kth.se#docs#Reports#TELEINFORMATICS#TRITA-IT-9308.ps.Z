URL: http://www.it.kth.se/docs/Reports/TELEINFORMATICS/TRITA-IT-9308.ps.Z
Refering-URL: http://www.it.kth.se/docs/Reports/TELEINFORMATICS/
Root-URL: http://www.it.kth.se
Title: A  Data Parallel Programming: A Survey and a Proposal for a New Model  
Author: Per Hammarlund and Bjorn Lisper 
Note: TRITA-IT-9308 ISSN 1103-535X ISRN KTH/IT/R 93/8-SE  
Date: September 17, 1993  
Affiliation: Royal Institute of Technology Department of Teleinformatics  
Abstract-found: 0
Intro-found: 1
Reference: [Albert et al., 1991] <author> Albert E, Lukas J. D, & Steele Jr. G. L, </author> <year> (1991). </year> <title> Data Parallel Computers and the FORALL Statement. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13 </volume> <pages> 185-192. </pages>
Reference-contexts: If array indices correspond to processing units, then the use of such expressions usually implies communication. A number of communication patterns are not possible to express in this syntax, which limits the applicability of FORTRAN 90 for parallel programming purposes. For a discussion on FORTRAN 90, see <ref> [Albert et al., 1991] </ref>. Code Example 13 This example uses CM FORTRAN, a Connection Machine dialect of FORTRAN 90. PROGRAM EXAMPLE INTEGER D PARAMETER (N=8) C Static parallel data DIMENSION A (N), B (N), C (N) C Initiate A and B.
Reference: [Backus, 1978] <author> Backus J, </author> <year> (1978). </year> <title> Can Programming Be Liberated from the von Neuman Style? A Functional Style and Its Algebra of Programs. </title> <journal> Comm. ACM, </journal> <volume> 21(8) </volume> <pages> 613-641. </pages>
Reference-contexts: CM Lisp never became a commercial language for the Connection Machine, but it is still interesting as 12 a research language. It has been said that it was hard to implement the language efficiently on the CM1. CM Lisp takes inspiration from both APL and FP <ref> [Backus, 1978] </ref>. The language is an extension to Common Lisp [Steele Jr., 1990], but the carrier language could equally well have been Scheme [Sussman and Steele Jr., 1975]. CM Lisp introduces one additional data type, the xapping. <p> We do not believe that a large number of primitives necessarily gives a language large expressive power. On the contrary, a large number of primitives complicates the syntax and it becomes hard to learn them all. A similar tendency has been seen for sequential languages, as observed in <ref> [Backus, 1978] </ref>. Along the same lines, with inspiration from functional programming, we would like to propose a view of data parallelism that we believe can lead to data parallel languages with a larger degree of generality, clarity and semantic soundness.
Reference: [Blelloch and Sabot, 1990] <author> Blelloch G. E & Sabot G. W, </author> <year> (1990). </year> <title> Compiling Collection-Oriented Languages onto Massively Parallel Computers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8 </volume> <pages> 119-134. </pages>
Reference-contexts: We would, though, like to argue that: 1. There is a comparatively large research initiative in data parallel implementations of functional programs. Related to our proposed model work is being done on compiling composition languages [Budd, 1988b] and collection oriented languages <ref> [Blelloch and Sabot, 1990] </ref>. We believe that high-level programs can be compiled to code that is only a constant factor worse than the corresponding "hand coded" program. 2. The higher level of abstraction of high level languages makes them less dependent on a certain architecture.
Reference: [Brainerd et al., 1990] <author> Brainerd W. S, Goldberg C. H, & Adams J. C, </author> <year> (1990). </year> <title> Programmer's Guide to FORTRAN 90. Programming Languages. </title> <publisher> McGraw-Hill. </publisher>
Reference-contexts: In the standard proposal FORTRAN 8X, and now in the FORTRAN 90 <ref> [Brainerd et al., 1990] </ref> standard, a number of "parallel" features are added, like index manipulations of arrays, some intrinsic operators for doing summations, and min/max operations. These intrinsic operators can then be implemented in a parallel way if the target machine is parallel.
Reference: [Budd, 1988a] <author> Budd T. A, </author> <year> (1988). </year> <title> A New Approach to Vector Code Generation for Applicative Languages. </title> <type> Technical report, </type> <institution> Department of Computer Science, Oregon State University, Corvallis, </institution> <note> Ore-gon 97331. 23 </note>
Reference-contexts: If both portable and efficient code is desired, then it seems that high-level languages with advanced compilation methods is the only way to go. Work is currently done on compiling data parallel languages for vector machines <ref> [Budd, 1988a] </ref> and for SIMD computers [Gilbert and Schreiber, 1991]. A high-level language compiler can both 22 use algorithmical skills [Hillis and Steele Jr., 1986] and knowledge about how a parallel data structure can be optimally mapped onto a machine [Knobe et al., 1990].
Reference: [Budd, 1988b] <author> Budd T. A, </author> <year> (1988). </year> <title> Composition and Compilation in Func--tional Programming Languages. </title> <type> Technical report, </type> <institution> Department of Computer Science, Oregon State University, Corvallis, Oregon 97331. </institution>
Reference-contexts: With the present state of affairs this is certainly true. We would, though, like to argue that: 1. There is a comparatively large research initiative in data parallel implementations of functional programs. Related to our proposed model work is being done on compiling composition languages <ref> [Budd, 1988b] </ref> and collection oriented languages [Blelloch and Sabot, 1990]. We believe that high-level programs can be compiled to code that is only a constant factor worse than the corresponding "hand coded" program. 2.
Reference: [Dietz and Klappholz, 1985] <author> Dietz H & Klappholz D, </author> <year> (1985). </year> <title> Refined C: a sequential language for parallel programming. </title> <editor> In DeGroot D (ed), </editor> <booktitle> Proc. 1985 International Conference on Parallel Processing, </booktitle> <pages> pp 442-449. </pages> <publisher> IEEE Computer Society, </publisher> <month> August </month> <year> 1985. </year>
Reference-contexts: Arrays can be manipulated by a special index syntax for scatter/gather etc. The language also introduces a number of intrinsic operations on vectors, all starting with a @, like @. for inner product and @&gt; for maximum in a vector. Refined C Refined C <ref> [Dietz and Klappholz, 1985] </ref> is perhaps more aimed at helping the compiler to parallelize the code than to provide actual parallel extensions. (A version for FORTRAN was reported in [Dietz and Klappholz, 1986].) Parallel-C Parallel-C [Kuehn and Siegel, 1985] is a set of extensions to C where anything can be declared
Reference: [Dietz and Klappholz, 1986] <author> Dietz H & Klappholz D, </author> <year> (1986). </year> <title> Refined FORTRAN: another sequential language for parallel programming. </title> <editor> In Hwang K, Jacobs S. M, & Swartzlander E. E (eds), </editor> <booktitle> Proc. 1986 International Conference on Parallel Processing, </booktitle> <pages> pp 184-191. </pages> <publisher> IEEE Computer Society, </publisher> <month> August </month> <year> 1986. </year>
Reference-contexts: Refined C Refined C [Dietz and Klappholz, 1985] is perhaps more aimed at helping the compiler to parallelize the code than to provide actual parallel extensions. (A version for FORTRAN was reported in <ref> [Dietz and Klappholz, 1986] </ref>.) Parallel-C Parallel-C [Kuehn and Siegel, 1985] is a set of extensions to C where anything can be declared as parallel. Parallel-C has no built in communication. There is supposed to be support for both SIMD and MIMD programming.
Reference: [Dijkstra, 1976] <author> Dijkstra E. W, </author> <year> (1976). </year> <title> A Discipline of Programming. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, N.J. </address>
Reference-contexts: In the FORALL statement, all increments take place at once. Within this statement, processor p + 1 cannot use the result of the increment from processor p. This parallel operation is actually a kind of concurrent assignment, where a set of variables are concurrently assigned values. <ref> [Dijkstra, 1976, Gries, 1978] </ref> Parallel Data In the sequential case each individual element of the array is identified by an index. The corresponding operations are performed in the strict order by which the indices are generated in the loop.
Reference: [Edelman, 1990] <author> Edelman A, </author> <year> (1990). </year> <title> Optimal Matrix Transposition and Bit Reversal on Hypercubes: All-To-All Personalized Communication, 1990. </title> <type> Personal communication. </type>
Reference: [Falkoff and Iverson, 1973] <author> Falkoff A & Iverson K, </author> <year> (1973). </year> <title> The Design of APL. </title> <journal> IBM Journal of Research and Development, </journal> <pages> pp 324-333. </pages>
Reference-contexts: APL started as a notation to describe vector computations on the blackboard. The first implementation similar to APL today was available in early 1966. The background of the language can be found in <ref> [Iverson, 1962, McDonnel, 1979, Falkoff and Iverson, 1973] </ref>. [Metzger, 1981] talks about the difference in programming methods when using APL as compared to using other languages. It should be noted that APL was not designed to run on parallel machines.
Reference: [Galil and Paul, 1983] <author> Galil Z & Paul W. J, </author> <year> (1983). </year> <title> An efficient General-Purpose Parallel Computer. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 30(2) </volume> <pages> 360-387. </pages>
Reference-contexts: We hope to be able to include some of these results in our research on implementations of data parallel languages. Finally it should be noted that there are other research efforts working towards much the same goals <ref> [Rice, 1990, Galil and Paul, 1983, Yang and Choo, 1992] </ref>. 8 Conclusions We introduced the concept of data parallel computation and we showed a few examples of basic computations like elementwise operations, communication, and reductions.
Reference: [Gilbert and Schreiber, 1991] <author> Gilbert J. R & Schreiber R, </author> <year> (1991). </year> <title> Optimal Expression Evaluation for Data Parallel Architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13 </volume> <pages> 58-64. </pages>
Reference-contexts: If both portable and efficient code is desired, then it seems that high-level languages with advanced compilation methods is the only way to go. Work is currently done on compiling data parallel languages for vector machines [Budd, 1988a] and for SIMD computers <ref> [Gilbert and Schreiber, 1991] </ref>. A high-level language compiler can both 22 use algorithmical skills [Hillis and Steele Jr., 1986] and knowledge about how a parallel data structure can be optimally mapped onto a machine [Knobe et al., 1990].
Reference: [Gries, 1978] <author> Gries D, </author> <year> (1978). </year> <title> The Multiple Assignment Statement. </title> <journal> IEEE Trans. Software Eng., SE-4(2):89-93. </journal>
Reference-contexts: In the FORALL statement, all increments take place at once. Within this statement, processor p + 1 cannot use the result of the increment from processor p. This parallel operation is actually a kind of concurrent assignment, where a set of variables are concurrently assigned values. <ref> [Dijkstra, 1976, Gries, 1978] </ref> Parallel Data In the sequential case each individual element of the array is identified by an index. The corresponding operations are performed in the strict order by which the indices are generated in the loop.
Reference: [Hammarlund and Lisper, 1993] <author> Hammarlund P & Lisper B, </author> <year> (1993). </year> <title> On the relation between functional and data parallel programming languages. </title> <booktitle> In Proc. of the Sixth Conference on Functional Programming Languages and Computer Architecture, </booktitle> <pages> pp 210-222. </pages> <publisher> ACM Press, </publisher> <month> june </month> <year> 1993. </year>
Reference-contexts: Along the same lines, with inspiration from functional programming, we would like to propose a view of data parallelism that we believe can lead to data parallel languages with a larger degree of generality, clarity and semantic soundness. For a more elaborate description of the below, see <ref> [Hammarlund and Lisper, 1993] </ref>. 6.1 Parallel Data as Functions We define parallel data as parallel data fields. A parallel data field is a function from a index domain I to a data domain A, i.e. an entity of type I ! A.
Reference: [Hillis and Steele Jr., 1986] <author> Hillis W. D & Steele Jr. G. L, </author> <year> (1986). </year> <title> Data Parallel Algorithms. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1170-1183. </pages>
Reference-contexts: Work is currently done on compiling data parallel languages for vector machines [Budd, 1988a] and for SIMD computers [Gilbert and Schreiber, 1991]. A high-level language compiler can both 22 use algorithmical skills <ref> [Hillis and Steele Jr., 1986] </ref> and knowledge about how a parallel data structure can be optimally mapped onto a machine [Knobe et al., 1990]. For parallel architectures the communication problem is becoming more understood [Saltz et al., 1991, Ho, 1991, Edelman, 1990, Papadimitriou and Sipser, 1984].
Reference: [Hillis, 1987] <author> Hillis W. D, </author> <year> (1987). </year> <title> The Connection Machine. </title> <journal> Scientific American, </journal> <volume> 256 </volume> <pages> 108-115. </pages>
Reference-contexts: Code Example 10 This example uses the J language. Input to the J evaluator is indented. a =. i. 10 a c =. a * b 0 1 4 9 16 25 36 49 64 81 285 4.2 CM Lisp When developing the Connection Machine (CM) <ref> [Hillis, 1987] </ref>, Danny Hillis and Guy L. Steele Jr. thought about a programming language for the machine. This language developed into the Connection Machine Lisp (CM Lisp) [Steele Jr. and Hillis, 1986, Hillis, 1987]. <p> Steele Jr. thought about a programming language for the machine. This language developed into the Connection Machine Lisp (CM Lisp) <ref> [Steele Jr. and Hillis, 1986, Hillis, 1987] </ref>. CM Lisp never became a commercial language for the Connection Machine, but it is still interesting as 12 a research language. It has been said that it was hard to implement the language efficiently on the CM1.
Reference: [Ho, 1991] <author> Ho C.-T, </author> <year> (1991). </year> <title> Optimal Broadcasting on SIMD Hypercubes Without Indirect Addressing Capability. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13 </volume> <pages> 246-255. 24 </pages>
Reference: [Iverson, 1962] <author> Iverson K. E, </author> <year> (1962). </year> <title> A Programming Language. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference-contexts: 1 Introduction The benefits and necessities of parallelism have been known for a long time in computer science. Data parallel languages as such started to be discussed explicitly in early to mid 1980. Actually, a data parallel language has been around since mid 1960, namely APL <ref> [Iverson, 1962, McDonnel, 1979] </ref>. The new thrust in research in data parallel languages started with the introduction of vector processors, like the CRAY, and large SIMD computers, like the Connection Machine [TMC, 1989]. Often data parallel programming paradigms have been introduced to make the compilation or vectorization process easier. <p> APL started as a notation to describe vector computations on the blackboard. The first implementation similar to APL today was available in early 1966. The background of the language can be found in <ref> [Iverson, 1962, McDonnel, 1979, Falkoff and Iverson, 1973] </ref>. [Metzger, 1981] talks about the difference in programming methods when using APL as compared to using other languages. It should be noted that APL was not designed to run on parallel machines.
Reference: [Iverson, 1991] <author> Iverson K. E, </author> <year> (1991). </year> <note> Programming in J. </note> <institution> Iverson Software Inc., Toronto. </institution>
Reference-contexts: This is probably due to the poorly developed parsing technology of the time when APL was defined. J APL has a descendant called J, where some of the anomalies of APL have been corrected. <ref> [Iverson, 1991] </ref> For instance, J uses plain symbol names for its operators, as opposed to the special characters of APL. Code Example 10 This example uses the J language.
Reference: [Knobe et al., 1990] <author> Knobe K, Lukas J. D, & Steele Jr. G. L, </author> <year> (1990). </year> <title> Data Optimization: Allocation of Arrays to Reduce Communication on SIMD Machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8 </volume> <pages> 102-118. </pages>
Reference-contexts: A high-level language compiler can both 22 use algorithmical skills [Hillis and Steele Jr., 1986] and knowledge about how a parallel data structure can be optimally mapped onto a machine <ref> [Knobe et al., 1990] </ref>. For parallel architectures the communication problem is becoming more understood [Saltz et al., 1991, Ho, 1991, Edelman, 1990, Papadimitriou and Sipser, 1984]. We hope to be able to include some of these results in our research on implementations of data parallel languages.
Reference: [Kogge and Stone, 1973] <author> Kogge P. M & Stone H. S, </author> <year> (1973). </year> <title> A Parallel Algorithm for the Efficient Solution of a General Class of Recurrence Equations. </title> <journal> IEEE Trans. Comput., C-22:786-793. </journal>
Reference-contexts: Again, the scan of a parallel datum of size n is possible to compute sequentially using n 1 operations. For all associative binary operations this can be done in parallel by cyclic reduction methods <ref> [Kogge and Stone, 1973] </ref>. Cyclic reduction on n elements takes O (n log n) operations 5 and runs in O (log n) time steps on n processing units. It is possible to implement this very efficiently on a parallel machine with only uniform communication.
Reference: [Kondo et al., 1986] <author> Kondo T, Tsuchiya T, Kitamura Y, Sugiyama Y, Kimura T, & Nakashima T, </author> <year> (1986). </year> <title> Pseudo MIMD Array Processor | AAP2. </title> <publisher> IEEE. </publisher>
Reference-contexts: Currently we do not know of any complete APL implementation on a parallel computer. A small version on APL, called AAPL, was used as application language on the AAP2 computer of NTT <ref> [Kondo et al., 1986] </ref>. CM Lisp has only been simulated on conventional computers. *Lisp, Old C*, Paralation Lisp, CM FORTRAN (a modified subset of FORTRAN 90), and New C* all run on the Connection Machine. FORTRAN 90 has been, or is being implemented on many vector machines like CRAY.
Reference: [Kuehn and Siegel, 1985] <author> Kuehn J. T & Siegel H. J, </author> <year> (1985). </year> <title> Extensions to the C Programming Language for SIMD/MIMD Parallelism. </title> <editor> In Deg-root D (ed), </editor> <booktitle> Proc. 1985 International Conference on Parallel Processing, </booktitle> <pages> pp 232-235. </pages> <publisher> IEEE Computer Society, </publisher> <month> August </month> <year> 1985. </year>
Reference-contexts: Refined C Refined C [Dietz and Klappholz, 1985] is perhaps more aimed at helping the compiler to parallelize the code than to provide actual parallel extensions. (A version for FORTRAN was reported in [Dietz and Klappholz, 1986].) Parallel-C Parallel-C <ref> [Kuehn and Siegel, 1985] </ref> is a set of extensions to C where anything can be declared as parallel. Parallel-C has no built in communication. There is supposed to be support for both SIMD and MIMD programming.
Reference: [Li, 1986] <author> Li K.-C, </author> <year> (1986). </year> <title> A Note on the Vector C Language. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 21(1). </volume>
Reference-contexts: Parallel execution comes from doing the elementwise operations on xappings in parallel. 4.3 Miscellaneous Extensions to C Around 1985/86 quite a few proposals for data parallel extensions of the C programming language appeared. Vector C Vector C <ref> [Li, 1986] </ref> introduces a more powerful handling of arrays. Arrays can be manipulated by a special index syntax for scatter/gather etc. The language also introduces a number of intrinsic operations on vectors, all starting with a @, like @. for inner product and @&gt; for maximum in a vector.
Reference: [MasPar, 1990] <institution> MasPar, </institution> <year> (1990). </year> <title> MasPar data-parallel programming languages. </title> <institution> MasPar Computer Corporation, 749 North Mary Avenue, </institution> <address> Sunnyvale, California 94086. </address>
Reference-contexts: Parallel-C has no built in communication. There is supposed to be support for both SIMD and MIMD programming. MasPar C MasPar C (MPL) has been introduced by the manufacturer MasPar as the C-like language for their parallel machine. <ref> [MasPar, 1990] </ref> It is quite similar to the New C* language described below. POMPC The POMPC language is a data parallel extension to C, much like New C* (see section 4.8).
Reference: [McDonnel, 1979] <author> McDonnel E, </author> <year> (1979). </year> <title> The Socio-Technical Beginnings of APL. </title> <journal> APL Quote Quad, </journal> <pages> pp 13-18. </pages>
Reference-contexts: 1 Introduction The benefits and necessities of parallelism have been known for a long time in computer science. Data parallel languages as such started to be discussed explicitly in early to mid 1980. Actually, a data parallel language has been around since mid 1960, namely APL <ref> [Iverson, 1962, McDonnel, 1979] </ref>. The new thrust in research in data parallel languages started with the introduction of vector processors, like the CRAY, and large SIMD computers, like the Connection Machine [TMC, 1989]. Often data parallel programming paradigms have been introduced to make the compilation or vectorization process easier. <p> APL started as a notation to describe vector computations on the blackboard. The first implementation similar to APL today was available in early 1966. The background of the language can be found in <ref> [Iverson, 1962, McDonnel, 1979, Falkoff and Iverson, 1973] </ref>. [Metzger, 1981] talks about the difference in programming methods when using APL as compared to using other languages. It should be noted that APL was not designed to run on parallel machines.
Reference: [Metzger, 1981] <author> Metzger R. C, </author> <year> (1981). </year> <title> APL Thinking, Finding Array-Oriented Solutions. </title> <booktitle> ACM, </booktitle> <pages> pp 212-218. </pages>
Reference-contexts: APL started as a notation to describe vector computations on the blackboard. The first implementation similar to APL today was available in early 1966. The background of the language can be found in [Iverson, 1962, McDonnel, 1979, Falkoff and Iverson, 1973]. <ref> [Metzger, 1981] </ref> talks about the difference in programming methods when using APL as compared to using other languages. It should be noted that APL was not designed to run on parallel machines. At the time when APL was implemented it was not possible to build (very large) parallel machines.
Reference: [Papadimitriou and Sipser, 1984] <author> Papadimitriou C. H & Sipser M, </author> <year> (1984). </year> <title> Communication Complexity. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 28 </volume> <pages> 260-269. </pages>
Reference: [Paris, 1992] <author> Paris N, </author> <year> (1992). </year> <title> The POMPC data parallel language. </title> <booktitle> Presented at the first European Connection Machine Workshop, </booktitle> <address> Wuppertal., </address> <month> June </month> <year> 1992. </year>
Reference-contexts: As one example one can mention that POMPC supports virtual processors but that it is at the same time possible to access these as they are mapped on the actual hardware, i.e. the programmer can use different levels of abstraction for the same data. <ref> [Paris, 1992] </ref> 4.4 *Lisp *Lisp [TMC, 1991d, TMC, 1991a] is a programming language for the Connection Machine. *Lisp saw the light of day in March 1986. It is an extension to Common Lisp [Steele Jr., 1990].
Reference: [Rice, 1990] <author> Rice M. D, </author> <year> (1990). </year> <title> Semantics for Data Parallel Computation. </title> <journal> International Journal for Parallel Programming, </journal> <volume> 19(6) </volume> <pages> 477-509. </pages>
Reference-contexts: We hope to be able to include some of these results in our research on implementations of data parallel languages. Finally it should be noted that there are other research efforts working towards much the same goals <ref> [Rice, 1990, Galil and Paul, 1983, Yang and Choo, 1992] </ref>. 8 Conclusions We introduced the concept of data parallel computation and we showed a few examples of basic computations like elementwise operations, communication, and reductions.
Reference: [Rose and Steele Jr., 1987] <author> Rose J. R & Steele Jr. G. L, </author> <year> (1987). </year> <title> C*: An Extended C Language for Data Parallel Programming. </title> <booktitle> In Proceedings of the 1987 Second International Conference on Supercomputing, </booktitle> <volume> volume II, </volume> <pages> pp 2-16. </pages> <booktitle> International Supercomputing Institute, </booktitle> <year> 1987. </year>
Reference-contexts: Some algorithms are also easy to express in languages like these, e.g. many algorithms in linear algebra. The increase in data size has also inspired this research. This is very minutely captured in an article about the old C* language for the Connection Machine <ref> [Rose and Steele Jr., 1987] </ref>: "It is not unusual for an application to involve the handling of 10 9 data values. <p> For the CM5 there will also be a real compiler. *Lisp was originally directed towards the bit-serial architecture of the CM1/CM2 which shows in its very rich type possibilities; one can for instance declare signed integers of any length and specialized floats. 4.5 Old C* The "Old" C* language <ref> [Rose and Steele Jr., 1987] </ref> was designed to be an extension to C, such that the extensions had the same flavor as C. It was supposed to be an strict superset of C.
Reference: [Sabot, 1988] <author> Sabot G. W, </author> <year> (1988). </year> <title> Paralation Lisp Reference Manual. </title> <type> Technical Report PL87-11, </type> <institution> Thinking Machines Corporation, Thinking Machines Corporation, </institution> <address> 245 First Street, Cambridge, Massachusetts 02142. </address>
Reference-contexts: Old C* took a lot from C++, like the new data type domain, which was very closely modeled after class in C++. The Old C* language was never completely implemented. 4.6 Paralation Lisp In Paralation Lisp <ref> [Sabot, 1988] </ref> the single new parallel data type is the paralation. A paralation is a vector-like collection of "sites." There are a number of specialized control structures for operations on the parallel data, like "(elwise (...) ...)" that performs scalar operations elementwise on paralations. The sites are identified by integers.
Reference: [Saltz et al., 1991] <author> Saltz J, Petiton S, Berryman H, & Rifkin A, </author> <year> (1991). </year> <title> Performance Effects of Irregular Communication Patterns on Massively Parallel Multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13 </volume> <pages> 202-212. </pages>
Reference: [Steele Jr. and Hillis, 1986] <author> Steele Jr. G. L & Hillis W. D, </author> <year> (1986). </year> <title> Connection Machine Lisp: Fine-Grained Parallel Symbolic Processing. </title> <type> Technical Report PL86-2, </type> <institution> Thinking Machine Corporation, Thinking Machines Corporation, </institution> <address> 245 First Street, Cambridge, Mas-sachusetts 02142. </address>
Reference-contexts: Steele Jr. thought about a programming language for the machine. This language developed into the Connection Machine Lisp (CM Lisp) <ref> [Steele Jr. and Hillis, 1986, Hillis, 1987] </ref>. CM Lisp never became a commercial language for the Connection Machine, but it is still interesting as 12 a research language. It has been said that it was hard to implement the language efficiently on the CM1. <p> The index of each element must be unique, which means that a xapping represents a partial function from lisp objects to lisp objects. An example of a xapping (from <ref> [Steele Jr. and Hillis, 1986] </ref>) is f sky!blue, apple!red, grass!green g.
Reference: [Steele Jr., 1990] <author> Steele Jr. G. L, </author> <year> (1990). </year> <title> Common Lisp the Language II. </title> <publisher> Digital Press, </publisher> <address> 12 Crosby Drive, Bedford, Massachusetts 01730, </address> <note> second edition. </note>
Reference-contexts: It has been said that it was hard to implement the language efficiently on the CM1. CM Lisp takes inspiration from both APL and FP [Backus, 1978]. The language is an extension to Common Lisp <ref> [Steele Jr., 1990] </ref>, but the carrier language could equally well have been Scheme [Sussman and Steele Jr., 1975]. CM Lisp introduces one additional data type, the xapping. <p> It is an extension to Common Lisp <ref> [Steele Jr., 1990] </ref>. The 13 structure of *Lisp very much reflects the PARIS programming model [TMC, 1991b] of the Con--nection Machine. In this model variables are allocated in vp-sets, Virtual Processor-sets. A program executes on a parallel virtual machine that consists of these virtual processors. <p> It could well be stored in local memory and accessed sequentially. In that case a conventional array implementation results. Currying of data fields could then be interpreted as selecting 3 Note that we can allow a polymorphic type system, with hierarchical types much like in Common Lisp <ref> [Steele Jr., 1990] </ref>. Then operators will only have to be applicable to that subtree of the type hierarchy. 21 some dimensions to be stored distributed over different processing units and other dimensions to be stored locally as arrays.
Reference: [Sussman and Steele Jr., 1975] <author> Sussman G. J & Steele Jr. G. L, </author> <year> (1975). </year> <title> Scheme: An interpreter for an extended lambda calculus. </title> <type> AI Memo 349, </type> <institution> MIT Artificial Intelligence Laboratory, Cambridge, Massachusetts. </institution>
Reference-contexts: It has been said that it was hard to implement the language efficiently on the CM1. CM Lisp takes inspiration from both APL and FP [Backus, 1978]. The language is an extension to Common Lisp [Steele Jr., 1990], but the carrier language could equally well have been Scheme <ref> [Sussman and Steele Jr., 1975] </ref>. CM Lisp introduces one additional data type, the xapping. A xapping is a combination of an array and a hash table: every element has an index, which can be any lisp object, and a data value, which can also be any lisp object.
Reference: [TMC, 1989] <institution> TMC, Thinking Machines Corporation, </institution> <address> 245 First Street, Cambridge, Massachusetts 02142, </address> <year> (1989). </year> <title> Connection Machine Model CM-2 Technical Summary, </title> <year> 1989. </year>
Reference-contexts: Actually, a data parallel language has been around since mid 1960, namely APL [Iverson, 1962, McDonnel, 1979]. The new thrust in research in data parallel languages started with the introduction of vector processors, like the CRAY, and large SIMD computers, like the Connection Machine <ref> [TMC, 1989] </ref>. Often data parallel programming paradigms have been introduced to make the compilation or vectorization process easier. In FORTRAN, for instance, it is usually much easier to vectorize a direct operation on arrays than a complicated loop statement.
Reference: [TMC, 1991a] <institution> TMC, Thinking Machines Corporation, </institution> <address> 245 First Street, Cambridge, Massachusetts 02142, </address> <year> (1991). </year> <title> Connection Machine: *Lisp Dictionary, </title> <address> 6.1 edition, </address> <year> 1991. </year>
Reference-contexts: As one example one can mention that POMPC supports virtual processors but that it is at the same time possible to access these as they are mapped on the actual hardware, i.e. the programmer can use different levels of abstraction for the same data. [Paris, 1992] 4.4 *Lisp *Lisp <ref> [TMC, 1991d, TMC, 1991a] </ref> is a programming language for the Connection Machine. *Lisp saw the light of day in March 1986. It is an extension to Common Lisp [Steele Jr., 1990]. The 13 structure of *Lisp very much reflects the PARIS programming model [TMC, 1991b] of the Con--nection Machine.
Reference: [TMC, 1991b] <institution> TMC, Thinking Machines Corporation, </institution> <address> 245 First Street, Cambridge, Massachusetts 02142, </address> <year> (1991). </year> <title> Connection Machine: Parallel Instruction Set (Paris), </title> <address> 6.1 edition, </address> <year> 1991. </year>
Reference-contexts: It is an extension to Common Lisp [Steele Jr., 1990]. The 13 structure of *Lisp very much reflects the PARIS programming model <ref> [TMC, 1991b] </ref> of the Con--nection Machine. In this model variables are allocated in vp-sets, Virtual Processor-sets. A program executes on a parallel virtual machine that consists of these virtual processors. This virtual machine is emulated on the real parallel hardware.
Reference: [TMC, 1991c] <institution> TMC, Thinking Machines Corporation, </institution> <address> 245 First Street, Cambridge, Massachusetts 02142, </address> <year> (1991). </year> <title> Connection Machine: Programming in C*, </title> <address> 6.1 edition, </address> <year> 1991. </year>
Reference-contexts: This language is the "New" C*. <ref> [TMC, 1991c] </ref> This is a much less bold language than Old C*. It reflects the underlying hardware of the Connection Machine much more, in the same way as *Lisp. As in *Lisp, all parallel variables exist in the same name space. Parallel variables are defined over shapes.
Reference: [TMC, 1991d] <institution> TMC, Thinking Machines Corporation, </institution> <address> 245 First Street, Cambridge, Massachusetts 02142, </address> <year> (1991). </year> <title> Connection Machine: </title> <booktitle> Programming in *Lisp, </booktitle> <address> 6.1 edition, </address> <year> 1991. </year> <month> 26 </month>
Reference-contexts: As one example one can mention that POMPC supports virtual processors but that it is at the same time possible to access these as they are mapped on the actual hardware, i.e. the programmer can use different levels of abstraction for the same data. [Paris, 1992] 4.4 *Lisp *Lisp <ref> [TMC, 1991d, TMC, 1991a] </ref> is a programming language for the Connection Machine. *Lisp saw the light of day in March 1986. It is an extension to Common Lisp [Steele Jr., 1990]. The 13 structure of *Lisp very much reflects the PARIS programming model [TMC, 1991b] of the Con--nection Machine.
Reference: [Yang and Choo, 1992] <author> Yang J. A & Choo Y, </author> <year> (1992). </year> <title> Data fields as parallel programs. </title> <booktitle> In Proceedings of the Second International Workshop on Array Structures, </booktitle> <address> Montreal, Canada, </address> <month> June/July </month> <year> 1992. </year> <month> 27 </month>
Reference-contexts: With a suitable syntax for function definitions, more general index domains can be expressed than is possible in existing data parallel languages. An example of practical interest is sparse index domains. Our view of data parallelism is close to the interpretation of arrays in the language Crystal <ref> [Yang and Choo, 1992] </ref>. In a functional language there is no semantic difference between parallel data fields and program code. Whether to consider a function as a parallel data field or as a program is merely a matter of implementation. <p> We hope to be able to include some of these results in our research on implementations of data parallel languages. Finally it should be noted that there are other research efforts working towards much the same goals <ref> [Rice, 1990, Galil and Paul, 1983, Yang and Choo, 1992] </ref>. 8 Conclusions We introduced the concept of data parallel computation and we showed a few examples of basic computations like elementwise operations, communication, and reductions.
References-found: 43

