URL: http://www.cs.ucla.edu/~ltaylor/aaai93.ps
Refering-URL: http://www.cs.ucla.edu/~ltaylor/
Root-URL: http://www.cs.ucla.edu
Title: EBL techniques have used finite state automata for automatic proof generation Cohen, 1987 or have
Author: Aho, A. V.; Sethi, R.; and Ullman, J. D. Dillenburg, J. F. and Nelson, P. C. . Elkan, C. . Hansson, O.; Mayer, A.; and Yung, M. . Criticizing Hart, P. E.; Nilsson, N. J.; and Raphael, B. . Ibaraki, T. . Korf, R. E. . :-. Korf, R. E. Pearl, J. . Pohl, I. . Sen, A. K. and Bagchi, A. . Shavlik, Jude W. . 
Affiliation: of Computer and Information Science  
Address: Reading, Mass.  7:315-343.  Palo Alto, Calif.  
Date: 1990  
Note: Related Work Several  Conclusions References  1986. Compilers: Principles, Techniques, and Tools. Addison-Wesley,  Dijkstra, E. W. 1959.  International Journal  1993. Linear-space best-first search. Artificial Intelligence. To appear. Nilsson, N. J. 1980. Principles of Artificial Intelligence. Morgan Kaufman Publishers, Inc.,  5:39-70.  
Pubnum: Shavlik,  
Abstract: Learning duplicate operator sequences can be compared to the learning of concepts in explanation-based learning (EBL) [ Minton, 1990 ] . These techniques share an exploratory phase of learning, capturing information from a small search which will be used in a larger one. The purpose of these operator sequences, however, is not accomplishment of specific goals, but the avoidance of duplicate nodes. In machine learning terms, we are learning only one class of control information, i.e., control of redundancy [ Minton, 1990 ] . We are learning nothing about success or failure of goals, or about goal interference, at which EBL techniques are directed. The introduction of macros into a search usually means the loss of optimality and an increase in the branching factor; eliminating duplicate sequences preserves optimality, and reduces the branching factor. EBL-aided searches may be applied to general sets of operators, while the FSM technique of this paper is limited to domains which have sets of operators that may be applied at any point. We have presented a technique for reducing the number of duplicate nodes generated by a depth-first search. The FSM method begins with a breadth-first search to identify operator strings that produce duplicate nodes. These redundant strings are then used to automatically generate a finite state machine that recognizes and rejects the duplicate strings. The FSM is then used to generate operators in the depth-first search. Producing the FSM is a preprocessing step that does not affect the complexity of the depth-first search. The additional time overhead to use the FSM in the depth-first search is negligible, although the FSM requires memory proportional to the number of states in the machine. This technique reduces the asymptotic complexity of depth-first search on a grid from O(3 r ) to O(r 2 ). On the Fifteen Puzzle, it reduces the brute force branching factor from 2.13 to 1.98, and reduced the time of an IDA* search by 70%. On the Twenty-Four Puzzle, a similar FSM reduced the time of WIDA* by 94.23%. It reduces the branching factor of the 2x2x2 Rubik's Cube from 6 to 4.73, and for the 3x3x3 Cube from 13.50 to 13.26. Chakrabarti, P. P.; Ghose, S.; Acharya, A.; and de Sarkar, S. C. 1989. Heuristic search in restricted memory. Artificial Intelligence 41:197-221. Cohen, W. W. 1987. A technique for generalizing number in explanation-based learning. Technical Report ML-TR-19, Computer Science Department, Rut-gers University, New Brunswick, NJ. Minton, S. 1990. Quantitative results concerning the utility of explanation-based learning. Artificial Intelligence 42(2-3):363-91. Taylor, Larry A. 1992. Pruning duplicate nodes in depth-first search. Technical Report CSD-920049, UCLA Computer Science Department, Los Angeles, CA 90024-1596. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aho, A. V.; Sethi, R.; and Ullman, J. D. </author> <year> 1986. </year> <title> Compilers: Principles, Techniques, and Tools. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass. </address>
Reference: <author> Chakrabarti, P. P.; Ghose, S.; Acharya, A.; and de Sarkar, S. C. </author> <year> 1989. </year> <title> Heuristic search in restricted memory. </title> <booktitle> Artificial Intelligence 41 </booktitle> <pages> 197-221. </pages>
Reference: <author> Cohen, W. W. </author> <year> 1987. </year> <title> A technique for generalizing number in explanation-based learning. </title> <type> Technical Report ML-TR-19, </type> <institution> Computer Science Department, Rut-gers University, </institution> <address> New Brunswick, NJ. </address>
Reference: <author> Dijkstra, E. W. </author> <year> 1959. </year> <title> A note on two problems in con-nexion with graphs. </title> <journal> Numerische Mathematik 1 </journal> <pages> 269-271. </pages>
Reference: <author> Dillenburg, J. F. and Nelson, P. C. </author> <year> 1993. </year> <title> Improving the efficiency of depth-first search by cycle elimination. </title> <journal> Information Processing Letters (forthcoming). </journal>
Reference: <author> Elkan, C. </author> <year> 1989. </year> <title> Conspiracy numbers and caching for searching and/or trees and theorem-proving. </title> <booktitle> In Proceedings of IJCAI-89. </booktitle> <volume> 1 </volume> <pages> 341-346. </pages>
Reference: <author> Hansson, O.; Mayer, A.; and Yung, M. </author> <year> 1992. </year> <title> Criticizing solutions to relaxed models yields powerful admissible heuristics. </title> <booktitle> Information Sciences 63(3) </booktitle> <pages> 207-227. </pages>
Reference: <author> Hart, P. E.; Nilsson, N. J.; and Raphael, B. </author> <year> 1968. </year> <title> A formal basis for the heuristic determination of minimum cost paths. </title> <journal> IEEE Transactions on Systems Science and Cybernetics 4(2) </journal> <pages> 100-107. </pages>
Reference: <author> Ibaraki, T. </author> <year> 1978. </year> <title> Depth m search in branch and bound algorithms. </title> <journal> International Journal of Computer and Information Science 7 </journal> <pages> 315-343. </pages>
Reference: <author> Korf, R. E. </author> <year> 1985. </year> <title> Depth-first iterative deepening: An optimal admissible tree search. </title> <booktitle> Artificial Intelligence 27 </booktitle> <pages> 97-109. </pages>
Reference: <author> Korf, R. E. </author> <year> 1993. </year> <title> Linear-space best-first search. </title> <journal> Artificial Intelligence. </journal> <note> To appear. </note>
Reference: <author> Minton, S. </author> <year> 1990. </year> <title> Quantitative results concerning the utility of explanation-based learning. </title> <journal> Artificial Intelligence 42(2-3):363-91. </journal>
Reference: <author> Nilsson, N. J. </author> <year> 1980. </year> <booktitle> Principles of Artificial Intelligence. </booktitle> <publisher> Morgan Kaufman Publishers, Inc., </publisher> <address> Palo Alto, Calif. </address>
Reference: <author> Pearl, J. </author> <year> 1984. </year> <title> Heuristics. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass. </address>
Reference: <author> Pohl, I. </author> <year> 1970. </year> <title> Heuristic search viewed as path finding in a graph. </title> <booktitle> Artificial Intelligence 1 </booktitle> <pages> 193-204. </pages>
Reference: <author> Sen, A. K. and Bagchi, A. </author> <year> 1989. </year> <title> Fast recursive formulations for best-first search that allow controlled use of memory. </title> <booktitle> In Proceedings of IJCAI-89. </booktitle> <volume> 1 </volume> <pages> 297-302. </pages>
Reference: <author> Shavlik, Jude W. </author> <year> 1990. </year> <title> Acquiring recursive and iterative concepts with explanation-based learning. </title> <booktitle> Machine Learning 5 </booktitle> <pages> 39-70. </pages>

References-found: 17

