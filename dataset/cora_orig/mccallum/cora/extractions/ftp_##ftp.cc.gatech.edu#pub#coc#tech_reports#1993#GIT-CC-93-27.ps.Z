URL: ftp://ftp.cc.gatech.edu/pub/coc/tech_reports/1993/GIT-CC-93-27.ps.Z
Refering-URL: http://www.cs.gatech.edu/tech_reports/index.93.html
Root-URL: 
Title: A Simulation-based Scalability Study of Parallel Systems  
Author: Anand Sivasubramaniam Aman Singla Umakishore Ramachandran H. Venkateswaran 
Keyword: Key words: parallel systems, parallel kernels, scalability, execution-driven simulation, perfor mance evaluation, performance metrics.  
Note: This work has been funded in part by NSF grants MIPS-9058430 and MIPS-9200005, and an equipment grant from DEC.  
Address: Atlanta, Ga 30332-0280  
Affiliation: College of Computing Georgia Institute of Technology  
Pubnum: Technical Report GIT-CC-93/27  
Email: e-mail: rama@cc.gatech.edu  
Phone: Phone: (404) 894-5136  
Date: April 1993  
Abstract: Scalability studies of parallel architectures have used scalar metrics to evaluate their performance. Very often, it is difficult to glean the sources of inefficiency resulting from the mismatch between the algorithmic and architectural requirements using such scalar metrics. Low-level performance studies of the hardware are also inadequate for predicting the scalability of the machine on real applications. We propose a top-down approach to scalability study that alleviates some of these problems. We characterize applications in terms of the frequently occurring kernels, and their interaction with the architecture in terms of overheads in the parallel system. An overhead function is associated with the algorithmic characteristics as well as their interaction with the architectural features. We present a simulation platform called SPASM (Simulator for Parallel Architectural Scalability Measurements) that quantifies these overhead functions. SPASM separates the algorithmic overhead into its components (such as serial and work-imbalance overheads), and interaction overhead into its components (such as latency and contention). Such a separation is novel and has not been addressed in any previous study. We illustrate the top-down approach by considering a case study in implementing three NAS parallel kernels on two simulated message-passing platforms. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anant Agarwal. </author> <title> Limits on Interconnection Network Performance. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 398-412, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Architects are usually concerned with low-level performance issues such as latency, contention and synchronization. The scalability of synchronization primitives supported by the hardware [3, 20], the limits on interconnection network performance <ref> [1, 21] </ref>, and the performance of scheduling policies [30, 17] are examples of such studies undertaken over the years. While such issues are extremely important, it is appropriate to put the impact of these factors into perspective by considering them in the context of overall application performance.
Reference: [2] <author> G. M. </author> <title> Amdahl. Validity of the Single Processor Approach to achieving Large Scale Computing Capabilities. </title> <booktitle> In Proceedings of the AFIPS Spring Joint Computer Conference, </booktitle> <pages> pages 483-485, </pages> <month> April </month> <year> 1967. </year>
Reference-contexts: Several performance metrics such as speedup <ref> [2] </ref>, scaled speedup [11], sizeup [27], experimentally determined serial fraction [14], and isoefficiency function [15] have been proposed over the years for capturing the scalability of parallel systems. <p> Parallel system overheads may be broadly classified into a purely algorithmic component (algorithmic overhead), and a component arising due to the interaction of the algorithm and the architecture (interaction overhead). The algorithmic overhead is due to the inherent serial part <ref> [2] </ref> and the work-imbalance in the algorithm, and is independent of the architectural characteristics. For instance, if in certain parallel phases of an algorithm the number of processors utilized changes then it would create work imbalance.
Reference: [3] <author> Thomas E. Anderson. </author> <title> The Performance of Spin Lock Alternatives for Shared-Memory Multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(1) </volume> <pages> 6-16, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: Architects are usually concerned with low-level performance issues such as latency, contention and synchronization. The scalability of synchronization primitives supported by the hardware <ref> [3, 20] </ref>, the limits on interconnection network performance [1, 21], and the performance of scheduling policies [30, 17] are examples of such studies undertaken over the years.
Reference: [4] <author> D. Bailey et al. </author> <title> The NAS Parallel Benchmarks. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 5(3) </volume> <pages> 63-73, </pages> <year> 1991. </year>
Reference-contexts: We illustrate the top-down approach through a case study, implementing a few NAS parallel kernels <ref> [4] </ref> on two message-passing platforms (a bus and a binary hypercube) simulated on SPASM. The algorithmic characteristics of these kernels are discussed in Section 4, details of the two architectural platforms are presented in Section 5, and the results of our study are summarized in Section 6. <p> As one goes lower in the hierarchy, the outcome of the evaluation becomes less realistic. Our top-down approach uses a hierarchical method to benchmarking based on the granularity of the benchmarks. The Perfect Club Benchmarks [5], SPLASH [23] and the NAS Benchmarks <ref> [4] </ref> are examples of application suites that have been proposed for studying the performance of parallel machines. Such applications are representative of real workloads and appear at the top of our hierarchy. <p> Despite its limitations, we believe that the scalability of an application with respect to an architecture can be captured by studying its kernels, since they represent the computationally intensive phases of an application. Therefore, we have used kernels in this study, in particular the NAS parallel kernels <ref> [4] </ref> that have been derived from a large number of Computational Fluid Dynamics applications. One would like to see a performance improvement (speedup) that is linear with the increase in the number of processors (as shown by the curve for linear behavior in Figure 1). <p> On the other hand, if a shared memory style programming is used, the communication pattern is not explicit and gets merged with the data access pattern. The Numerical Aerodynamic Simulation (NAS) program at NASA Ames has identified a set of kernels <ref> [4] </ref> that are representative of a number of large scale Computational Fluid Dynamics codes. In this study, we consider three of these kernels for the purposes of illustrating the top-down approach using SPASM. In this section, we identify their characteristics in a message-passing style implementation. Phase Description Comp. Gran.
Reference: [5] <author> M. Berry et al. </author> <title> The Perfect Club Benchmarks: Effective Performance Evaluation of Supercomputers. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 3(3) </volume> <pages> 5-40, </pages> <year> 1989. </year>
Reference-contexts: As one goes lower in the hierarchy, the outcome of the evaluation becomes less realistic. Our top-down approach uses a hierarchical method to benchmarking based on the granularity of the benchmarks. The Perfect Club Benchmarks <ref> [5] </ref>, SPLASH [23] and the NAS Benchmarks [4] are examples of application suites that have been proposed for studying the performance of parallel machines. Such applications are representative of real workloads and appear at the top of our hierarchy.
Reference: [6] <author> Eric A. Brewer, Chrysanthos N. Dellarocas, Adrian Colbrook, and William E. Weihl. </author> <title> PRO-TEUS : A high-performance parallel-architecture simulator. </title> <type> Technical Report MIT-LCS-TR-516, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, MA 02139, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: This strategy has considerably lowered the time overhead for simulation (the simulation time is at most a factor of two compared to the real time) for the applications considered. The approach has also been used in other recent simulation studies <ref> [6, 8, 22] </ref>. is abstracted by a processor (PE), a cache module (Cache) and a network interface (Deliver Daemon). These three entities are implemented as CSIM processes. The CSIM process representing a processor executes the code associated with the processor. <p> Hence, we have 23 resorted to calculating these time quanta manually and introducing the appropriate instrumentation code in our source programs. These manual measurements may have contributed to the inaccuracies in the estimation. We propose to use the augmentation technique used in other similar simulation studies <ref> [6, 8] </ref> to overcome these inaccuracies. 7 Concluding Remarks Theoretical and analytical models for studying the scalability of parallel systems have their limitations. We have proposed a new top-down approach to identify and quantify the different overheads in a parallel system that affects its scalability.
Reference: [7] <author> D. Chen, H. Su, and P. Yew. </author> <title> The Impact of Synchronization and Granularity on Parallel Systems. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 239-248, </pages> <year> 1990. </year>
Reference-contexts: There are studies that use real applications to address specific issues like the effect of sharing in parallel programs on the cache and bus performance [10] and the impact of synchronization and task granularity on parallel system performance <ref> [7] </ref>. Cypher et al. [9], identify the architectural requirements such as floating point operations, communications, and input/output for scientific applications. However, there have been very few attempts at quantifying the effects of algorithmic and architectural interactions in a parallel system.
Reference: [8] <author> R. G. Covington, S. Madala, V. Mehta, J. R. Jump, and J. B. Sinclair. </author> <title> The Rice parallel processing testbed. </title> <booktitle> In Proceedings of the ACM SIGMETRICS 1988 Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 4-11, </pages> <address> Santa Fe, NM, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: This strategy has considerably lowered the time overhead for simulation (the simulation time is at most a factor of two compared to the real time) for the applications considered. The approach has also been used in other recent simulation studies <ref> [6, 8, 22] </ref>. is abstracted by a processor (PE), a cache module (Cache) and a network interface (Deliver Daemon). These three entities are implemented as CSIM processes. The CSIM process representing a processor executes the code associated with the processor. <p> Hence, we have 23 resorted to calculating these time quanta manually and introducing the appropriate instrumentation code in our source programs. These manual measurements may have contributed to the inaccuracies in the estimation. We propose to use the augmentation technique used in other similar simulation studies <ref> [6, 8] </ref> to overcome these inaccuracies. 7 Concluding Remarks Theoretical and analytical models for studying the scalability of parallel systems have their limitations. We have proposed a new top-down approach to identify and quantify the different overheads in a parallel system that affects its scalability.
Reference: [9] <author> R. Cypher, A. Ho, S. Konstantinidou, and P. Messina. </author> <title> Architectural requirements of parallel scientific applications with explicit communication. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1993. </year> <note> To appear. </note>
Reference-contexts: There are studies that use real applications to address specific issues like the effect of sharing in parallel programs on the cache and bus performance [10] and the impact of synchronization and task granularity on parallel system performance [7]. Cypher et al. <ref> [9] </ref>, identify the architectural requirements such as floating point operations, communications, and input/output for scientific applications. However, there have been very few attempts at quantifying the effects of algorithmic and architectural interactions in a parallel system.
Reference: [10] <author> Susan J. Eggers and Randy H. Katz. </author> <title> The Effect of Sharing on the Cache and Bus Performance of Parallel Programs. </title> <booktitle> In Proceedings of the Third International Conference on Architectural 26 Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 257-270, </pages> <address> Boston, Mas--sachusetts, </address> <month> April </month> <year> 1989. </year>
Reference-contexts: There are studies that use real applications to address specific issues like the effect of sharing in parallel programs on the cache and bus performance <ref> [10] </ref> and the impact of synchronization and task granularity on parallel system performance [7]. Cypher et al. [9], identify the architectural requirements such as floating point operations, communications, and input/output for scientific applications.
Reference: [11] <author> John L. Gustafson, Gary R. Montry, and Robert E. Benner. </author> <title> Development of Parallel Methods for a 1024-node Hypercube. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 9(4) </volume> <pages> 609-638, </pages> <year> 1988. </year>
Reference-contexts: Several performance metrics such as speedup [2], scaled speedup <ref> [11] </ref>, sizeup [27], experimentally determined serial fraction [14], and isoefficiency function [15] have been proposed over the years for capturing the scalability of parallel systems.
Reference: [12] <institution> Intel Corporation, Oregon. Intel iPSC/2 and iPSC/860 User's Guide, </institution> <year> 1989. </year>
Reference-contexts: They support blocking and non-blocking modes of message transfer. The semantics of these modes are the same as those available on an iPSC/860 <ref> [12] </ref>. A blocking send blocks the sender until the message has left the sending buffer. Such a send does not necessarily imply that the message has reached the destination processor or even entered the network.
Reference: [13] <author> Leah H. Jamieson. </author> <title> Characterizing Parallel Algorithms. </title> <editor> In L. H. Jamieson, D. B. Gannon, and R. J. Douglas, editors, </editor> <booktitle> The Characteristics of Parallel Algorithms, </booktitle> <pages> pages 65-100. </pages> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: An understanding of the interaction between the algorithmic and architectural characteristics of a parallel system can give us such information. Studies undertaken by Kung [16] and Jamieson <ref> [13] </ref> help identify some of these characteristics from a theoretical perspective but they do not provide any means of quantifying their effects. 1 The term, parallel system, is used to denote an algorithm-architecture combination. 1 Parallel algorithms designed for an idealized machine model, project asymptotic estimates for their performance that may
Reference: [14] <author> Alan H. Karp and Horace P. Flatt. </author> <title> Measuring Parallel processor Performance. </title> <journal> Communications of the ACM, </journal> <volume> 33(5) </volume> <pages> 539-543, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Several performance metrics such as speedup [2], scaled speedup [11], sizeup [27], experimentally determined serial fraction <ref> [14] </ref>, and isoefficiency function [15] have been proposed over the years for capturing the scalability of parallel systems.
Reference: [15] <author> Vipin Kumar and V. Nageswara Rao. </author> <title> Parallel Depth-First Search. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 16(6) </volume> <pages> 501-519, </pages> <year> 1987. </year>
Reference-contexts: Several performance metrics such as speedup [2], scaled speedup [11], sizeup [27], experimentally determined serial fraction [14], and isoefficiency function <ref> [15] </ref> have been proposed over the years for capturing the scalability of parallel systems. While these metrics are extremely useful for tracking performance trends, they do not provide the information needed to understand the sources of inefficiency in a given architecture with respect to a given algorithm.
Reference: [16] <author> H. T. Kung. </author> <title> The Structure of Parallel Algorithms. </title> <booktitle> Advances in Computers, </booktitle> <volume> 19 </volume> <pages> 65-112, </pages> <year> 1980. </year> <title> Edited by Marshall C. </title> <publisher> Yovits and Published by Academic Press, </publisher> <address> New York. </address>
Reference-contexts: An understanding of the interaction between the algorithmic and architectural characteristics of a parallel system can give us such information. Studies undertaken by Kung <ref> [16] </ref> and Jamieson [13] help identify some of these characteristics from a theoretical perspective but they do not provide any means of quantifying their effects. 1 The term, parallel system, is used to denote an algorithm-architecture combination. 1 Parallel algorithms designed for an idealized machine model, project asymptotic estimates for their
Reference: [17] <author> Scott T. Leutenegger and Mary K. Vernon. </author> <title> The Performance of Multiprogrammed Multiprocessor Scheduling Policies. </title> <booktitle> In Proceedings of the ACM SIGMETRICS 1990 Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 226-236, </pages> <year> 1990. </year>
Reference-contexts: Architects are usually concerned with low-level performance issues such as latency, contention and synchronization. The scalability of synchronization primitives supported by the hardware [3, 20], the limits on interconnection network performance [1, 21], and the performance of scheduling policies <ref> [30, 17] </ref> are examples of such studies undertaken over the years. While such issues are extremely important, it is appropriate to put the impact of these factors into perspective by considering them in the context of overall application performance.
Reference: [18] <author> Sridhar Madala and James B. Sinclair. </author> <title> Performance of Synchronous Parallel Algorithms with Regular Structures. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(1) </volume> <pages> 105-116, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Analytical models for parallel systems are even more difficult to build and often use simplistic assumptions about the system to keep the complexity of such models reasonable for purposes of analysis <ref> [28, 18] </ref>. Scalability is a notion frequently used to signify the "goodness" of parallel systems.
Reference: [19] <author> F. H. McMahon. </author> <title> The Livermore Fortran Kernels : A Computer Test of the Numerical Performance Range. </title> <type> Technical Report UCRL-53745, </type> <institution> Lawrence Livermore National Laboratory, Livermore, </institution> <address> CA, </address> <month> December </month> <year> 1986. </year>
Reference-contexts: Such abstractions of real applications that capture the main phases of the computation are called kernels. One can go even lower than kernels by abstracting the main loops in the computation (like the Lawrence Livermore loops <ref> [19] </ref>) and evaluating their performance. As one goes lower in the hierarchy, the outcome of the evaluation becomes less realistic. Our top-down approach uses a hierarchical method to benchmarking based on the granularity of the benchmarks.
Reference: [20] <author> John M. Mellor-Crummey and Michael L. Scott. </author> <title> Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> February </month> <year> 1991. </year> <month> 27 </month>
Reference-contexts: Architects are usually concerned with low-level performance issues such as latency, contention and synchronization. The scalability of synchronization primitives supported by the hardware <ref> [3, 20] </ref>, the limits on interconnection network performance [1, 21], and the performance of scheduling policies [30, 17] are examples of such studies undertaken over the years.
Reference: [21] <author> Gregory F. Pfister and V. Alan Norton. </author> <title> Hot Spot Contention and Combining in Multistage Interconnection Networks. </title> <journal> IEEE Transactions on Computer Systems, </journal> <volume> C-34(10):943-948, </volume> <month> Oc-tober </month> <year> 1985. </year>
Reference-contexts: Architects are usually concerned with low-level performance issues such as latency, contention and synchronization. The scalability of synchronization primitives supported by the hardware [3, 20], the limits on interconnection network performance <ref> [1, 21] </ref>, and the performance of scheduling policies [30, 17] are examples of such studies undertaken over the years. While such issues are extremely important, it is appropriate to put the impact of these factors into perspective by considering them in the context of overall application performance.
Reference: [22] <author> S. K. Reinhardt et al. </author> <title> The Wisconsin Wind Tunnel : Virtual prototyping of parallel computers. </title> <booktitle> In Proceedings of the ACM SIGMETRICS 1993 Conference on Measurement and Modeling of Computer Systems, </booktitle> <address> Santa Clara, CA, </address> <month> May </month> <year> 1993. </year> <note> To appear. </note>
Reference-contexts: This strategy has considerably lowered the time overhead for simulation (the simulation time is at most a factor of two compared to the real time) for the applications considered. The approach has also been used in other recent simulation studies <ref> [6, 8, 22] </ref>. is abstracted by a processor (PE), a cache module (Cache) and a network interface (Deliver Daemon). These three entities are implemented as CSIM processes. The CSIM process representing a processor executes the code associated with the processor.
Reference: [23] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <type> Technical Report CSL-TR-91-469, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <year> 1991. </year>
Reference-contexts: As one goes lower in the hierarchy, the outcome of the evaluation becomes less realistic. Our top-down approach uses a hierarchical method to benchmarking based on the granularity of the benchmarks. The Perfect Club Benchmarks [5], SPLASH <ref> [23] </ref> and the NAS Benchmarks [4] are examples of application suites that have been proposed for studying the performance of parallel machines. Such applications are representative of real workloads and appear at the top of our hierarchy.
Reference: [24] <author> Anand Sivasubramaniam, Umakishore Ramachandran, and H. Venkateswaran. </author> <title> Message-Passing: Computational Model, Programming Paradigm, and Experimental Studies. </title> <type> Technical Report GIT-CC-91/11, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <month> February </month> <year> 1991. </year>
Reference-contexts: One can use either simulation or direct experimental evaluation of the applications on the real hardware to implement the top-down approach. We adopted the latter technique in our earlier studies by experimenting with frequently used parallel algorithms on shared memory [26] and message-passing <ref> [24] </ref> platforms.
Reference: [25] <author> Anand Sivasubramaniam, Umakishore Ramachandran, and H. Venkateswaran. </author> <title> A Computational Model for Message-Passing. </title> <booktitle> In Proceedings of the Sixth International Parallel Processing Symposium, </booktitle> <pages> pages 358-361, </pages> <address> Beverly Hills, California, </address> <month> March </month> <year> 1992. </year>
Reference-contexts: Algorithmic overhead is the difference between the linear curve and that which would be obtained (the "ideal" curve in Figure 1) by executing the algorithm on an ideal machine such as the PRAM <ref> [29, 25] </ref>. Such a machine idealizes the parallel architecture by assuming an infinite number of processors, and unit costs for communication and synchronization. Hence, the real execution could deviate significantly from the ideal execution due to the overheads such as latency, contention, synchronization, scheduling and cache effects.
Reference: [26] <author> Anand Sivasubramaniam, Gautam Shah, Joonwon Lee, Umakishore Ramachandran, and H. Venkateswaran. </author> <title> Experimental Evaluation of Algorithmic Performance on Two Shared Memory Multiprocessors. In Norihisa Suzuki, editor, </title> <booktitle> Shared Memory Multiprocessing, </booktitle> <pages> pages 81-107. </pages> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: One can use either simulation or direct experimental evaluation of the applications on the real hardware to implement the top-down approach. We adopted the latter technique in our earlier studies by experimenting with frequently used parallel algorithms on shared memory <ref> [26] </ref> and message-passing [24] platforms.
Reference: [27] <author> Xian-He Sun and John L. Gustafson. </author> <title> Towards a better Parallel Performance Metric. </title> <journal> Parallel Computing, </journal> <volume> 17 </volume> <pages> 1093-1109, </pages> <year> 1991. </year>
Reference-contexts: Several performance metrics such as speedup [2], scaled speedup [11], sizeup <ref> [27] </ref>, experimentally determined serial fraction [14], and isoefficiency function [15] have been proposed over the years for capturing the scalability of parallel systems.
Reference: [28] <author> D. F. Vrsalovic, D. P. Siewiorek, Z. Z. Segall, and E. Gehringer. </author> <title> Performance Prediction and Calibration for a Class of Multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(11) </volume> <pages> 1353-1365, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: Analytical models for parallel systems are even more difficult to build and often use simplistic assumptions about the system to keep the complexity of such models reasonable for purposes of analysis <ref> [28, 18] </ref>. Scalability is a notion frequently used to signify the "goodness" of parallel systems.
Reference: [29] <author> J. C. Wyllie. </author> <title> The Complexity of Parallel Computations. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Cornell University, </institution> <year> 1979. </year> <month> 28 </month>
Reference-contexts: Algorithmic overhead is the difference between the linear curve and that which would be obtained (the "ideal" curve in Figure 1) by executing the algorithm on an ideal machine such as the PRAM <ref> [29, 25] </ref>. Such a machine idealizes the parallel architecture by assuming an infinite number of processors, and unit costs for communication and synchronization. Hence, the real execution could deviate significantly from the ideal execution due to the overheads such as latency, contention, synchronization, scheduling and cache effects. <p> The algorithmic overhead is quantified by computing the time taken for execution of a given parallel program on an ideal machine such as the PRAM <ref> [29] </ref> and measuring its deviation from a linear speedup curve. Further, we separate this overhead into that due to the serial part (serial overhead) and that due 5 to work imbalance (work-imbalance overhead). As we mentioned earlier, the interaction overhead should be separated into its component parts.
Reference: [30] <author> John Zahorjan and Cathy McCann. </author> <title> Processor Scheduling in Shared Memory Multiprocessors. </title> <booktitle> In Proceedings of the ACM SIGMETRICS 1990 Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 214-225, </pages> <booktitle> 1990. </booktitle> <volume> 29 16 18 19 21 22 24 </volume>
Reference-contexts: Architects are usually concerned with low-level performance issues such as latency, contention and synchronization. The scalability of synchronization primitives supported by the hardware [3, 20], the limits on interconnection network performance [1, 21], and the performance of scheduling policies <ref> [30, 17] </ref> are examples of such studies undertaken over the years. While such issues are extremely important, it is appropriate to put the impact of these factors into perspective by considering them in the context of overall application performance.
References-found: 30

