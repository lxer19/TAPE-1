URL: http://grid.let.rug.nl/~erikt/papers/clin93.ps
Refering-URL: http://grid.let.rug.nl/~erikt/papers/clin93.html
Root-URL: 
Email: erikt@let.rug.nl  
Title: Acquiring Digital Phonology  
Author: Erik F. Tjong Kim Sang Alfa-informatica 
Address: Groningen  
Affiliation: University of  
Abstract: In the phonological feature theory presented in "Phonological Networks" by D.G. Gilbers language specific digital phonology (DIGPHON) models are constructed by carefully modifying a universal model. I present a learning strategy that can be used for deriving language specific DIGPHON models from a universal model. The strategy amounts to recognizing obligatory constraints on the combinations of articulatory features present in a language. The constraints can be used to convert a universal DIGPHON model into a language specific model mechanically.
Abstract-found: 1
Intro-found: 1
Reference: [Bouma 91] <author> Gosse Bouma. </author> <title> A logical reconstruction of digital phonology. </title> <editor> In Steven Bird, editor, </editor> <booktitle> Declarative Perspectives on Phonology, Edinburgh Working Papers in Cognitive Science. </booktitle> <institution> University of Edinburgh, </institution> <year> 1991. </year>
Reference-contexts: I will use the notation [switch XY :Y] for describing that switch XY is in position Y and the notation [f eature i :1] ([f eature i :0]) for describing that f eature i is on (off). I will use the term state of the model (conform <ref> [Bouma 91] </ref>) to refer to the positions of the switches of the model at a particular moment.
Reference: [Gilbers 92] <author> D.G. Gilbers. </author> <title> Phonological Networks. </title> <type> PhD thesis, </type> <institution> University of Groningen, </institution> <year> 1992. </year> <pages> ISSN 0928-0030. </pages>
Reference-contexts: Of course, remaining errors and peculiarities remain my responsibility. This research was granted by the Dutch Foundation for Language, Logic and Information. production. I start with a short description of the universal vowel model described in <ref> [Gilbers 92] </ref>. After this, I present a learning strategy for deriving language specific constraints. The constraints are based on the optional implications defined in [Gilbers 92]. The outline of the learning strategy will be followed by a discussion of the phonological principles DIGPHON models have to satisfy. <p> I start with a short description of the universal vowel model described in <ref> [Gilbers 92] </ref>. After this, I present a learning strategy for deriving language specific constraints. The constraints are based on the optional implications defined in [Gilbers 92]. The outline of the learning strategy will be followed by a discussion of the phonological principles DIGPHON models have to satisfy. After that, I present results which were achieved by using the strategy. 2 The universal vowel model The universal vowel model developed in [Gilbers 92] is drawn in <p> optional implications defined in <ref> [Gilbers 92] </ref>. The outline of the learning strategy will be followed by a discussion of the phonological principles DIGPHON models have to satisfy. After that, I present results which were achieved by using the strategy. 2 The universal vowel model The universal vowel model developed in [Gilbers 92] is drawn in figure 1. It consists of connections, diodes, feature boxes and different types of switches: binary switches (switch F G and switch KL ), ternary switches (switch CDE and switch HIJ ) and polarity switches (switch AB and switch MN ). <p> Most language specific models use switches and connections which are not present in the universal model drawn in figure 1. Gilbers states that this universal model is incomplete. It also contains hidden paths, called optional implications, that are necessary for deriving language specific models (see <ref> [Gilbers 92] </ref>, section 4.2.1). The optional implications do not change the behavior of the universal model. specific model. A hidden binary switch is present between the two inputs of switch MN . <p> I believe that humans, after a development stage during the first six months of their life, have a vowel production system that resembles the universal vowel model of <ref> [Gilbers 92] </ref> combined with optional implications of the formats described in B1, B2 and B3. Acquiring a language specific vowel model amounts to recognizing the implications that are obligatory in the language specific model. <p> the DIGPHON model for L that produces the combination and that contains the HLrev switch in positive polarity position. 3 Every language specific model is capable of producing at least one feature combination that contains [HLrev:0] and satisfies [back:1] , [round:1]: [0; 0; 0; 0; 0; 0; 0; 0] //. <ref> [Gilbers 92] </ref>, page 130 predicts that this vowel is universal. principle 3b. In case a language L contains feature combinations that do not satisfy the equivalence relation [back:1] , [round:1] 3 : 1. The DIGPHON model for L should contain the round reverse switch (switch MN ) 2. <p> I applied this program to a corpus of 32 vowel patterns. The corpus contains the four vowel patterns discussed in the fourth chapter of <ref> [Gilbers 92] </ref> and an additional 28 vowel patterns randomly chosen from [Maddieson 84]. <p> The error-fixing process proved to be non-trivial and I cannot guarantee that the routine I use at this moment will be successful for other languages. Apart from generating 28 new language specific vowel models, the model generation program actually managed to improve one of the vowel models presented in <ref> [Gilbers 92] </ref>. From the four vowel models mentioned in the fourth chapter of [Gilbers 92] three are identical to the ones generated by the program. One model differed: the model of the Dutch vowel pattern. <p> Apart from generating 28 new language specific vowel models, the model generation program actually managed to improve one of the vowel models presented in <ref> [Gilbers 92] </ref>. From the four vowel models mentioned in the fourth chapter of [Gilbers 92] three are identical to the ones generated by the program. One model differed: the model of the Dutch vowel pattern.
Reference: [Gold 67] <author> E. Mark Gold. </author> <title> Language identification in the limit. </title> <journal> Information and Control, </journal> <volume> 10 </volume> <pages> 447-474, </pages> <year> 1967. </year>
Reference-contexts: This is a language learning task that makes use of both positive and (implicit) negative information about elements present in the language. According to elementary learning theory developing a model that decides if a feature combination is in the language, is a trivial task ([Pinker 79], <ref> [Gold 67] </ref>). However, the markedness constraint on the DIGPHON models makes the task of deriving language specific DIGPHON models non-trivial. It is difficult to imagine that the derivational plan used in the second program described in section 5 is cognitively valid.
Reference: [Kas 89] <author> Mark Kas. </author> <title> De fonologie de computer in: een testprogramma. (Dutch). </title> <type> unpublished manuscript, </type> <institution> University of Groningen, </institution> <year> 1989. </year>
Reference-contexts: However, this model will not always be a DIGPHON model. Gilbers has 2 The DIGPHON feature representation of the vowels contains six vowels with two possible representations (first mentioned in <ref> [Kas 89] </ref>). Deciding which one of these representations to use in a language depends on characteristics of other vowels in the language. This decision will influence the optional implications needed for modeling a language.
Reference: [Maddieson 84] <author> Ian Maddieson. </author> <title> Patterns of sounds. </title> <publisher> Cambridge University Press, </publisher> <year> 1984. </year>
Reference-contexts: I applied this program to a corpus of 32 vowel patterns. The corpus contains the four vowel patterns discussed in the fourth chapter of [Gilbers 92] and an additional 28 vowel patterns randomly chosen from <ref> [Maddieson 84] </ref>.
Reference: [Pinker 79] <author> Steven Pinker. </author> <title> Formal models of language learning. </title> <journal> Cognition, </journal> <volume> 7 </volume> <pages> 217-283, </pages> <year> 1979. </year>
References-found: 6

