URL: http://c.gp.cs.cmu.edu:5103/afs/cs/usr/katia/www/cabins-papers/ictai95-final.ps.gz
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs/usr/katia/www/cabins.html
Root-URL: http://www.cs.cmu.edu
Email: zeng+@cs.cmu.edu katia@cs.cmu.edu  
Phone: (412) 268-8815 (412) 268-8825  
Title: Using Case-Based Reasoning as a Reinforcement Learning Framework for Optimization with Changing Criteria  
Author: Dajun Zeng Katia Sycara 
Address: Pittsburgh, PA 15213, U.S.A.  
Affiliation: The Robotics Institute Carnegie Mellon University  
Abstract: Practical optimization problems such as job-shop scheduling often involve optimization criteria that change over time. Repair-based frameworks have been identified as flexible computational paradigms for difficult combinatorial optimization problems. Since the control problem of repair-based optimization is severe, Reinforcement Learning (RL) techniques can be potentially helpful. However, some of the fundamental assumptions made by traditional RL algorithms are not valid for repair-based optimization. Case-Based Reasoning (CBR) compensates for some of the limitations of traditional RL approaches. In this paper, we present a Case-Based Reasoning RL approach, implemented in the C A B I N S system, for repair-based optimization. We chose job-shop scheduling as the testbed for our approach. Our experimental results show that C A B I N S is able to effectively solve problems with changing optimization criteria which are not known to the system and only exist implicitly in a extensional manner in the case base. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. P. Singh, </author> <title> "Tansfer of learning by composing solutions of elemental sequential tasks," </title> <journal> Machine Learning, </journal> <volume> vol. 8, </volume> <pages> pp. 323-339, </pages> <year> 1992. </year>
Reference-contexts: Typically, actions may affect not only the immediate reward, but also the next situation, and through that all subsequent rewards <ref> [1] </ref>. In this paper, we present a learning agent that solves one of the "hardest" [2] combinatorial optimization problems, i.e., job-shop scheduling problems. <p> Recently, repair-based optimization has been identified as a very flexible framework for solving optimization problems [4]. Reinforcement learning (RL) is particularly relevant and potentially useful within a repair-based framework. However, some basic assumptions that typical reinforcement learning methods <ref> [5, 1] </ref> make about the problem domain are violated for complex optimization tasks: (1) The reinforcement signal is typically assumed to be a scalar, which doesn't hold for real-world optimization tasks where evaluation criteria are situation-dependent and changing, (2) RL methods assume that there is an explicit criterion to tell the <p> The subscript j takes the value <ref> [1; 2; 3; 4; 5] </ref> to refer to one of the five subsets of the problems (each of them contains 5 problem instances), respectively.
Reference: [2] <author> S. </author> <title> French, Sequencing and Scheduling: An Introduction to the Mathematics of the Job-Shop. </title> <publisher> Lon-don: Ellis Horwood, </publisher> <year> 1982. </year>
Reference-contexts: Typically, actions may affect not only the immediate reward, but also the next situation, and through that all subsequent rewards [1]. In this paper, we present a learning agent that solves one of the "hardest" <ref> [2] </ref> combinatorial optimization problems, i.e., job-shop scheduling problems. Our approach, implemented in the C A B I N S system, is shown experimentally to be able to learn scheduling problem solving knowledge even when the scheduling criteria change over time. This capability is very important for the following reasons. <p> One of the most difficult scheduling problem classes is job shop scheduling <ref> [2] </ref>. In job-shop scheduling, each task (interchangeably called an order or a job) consists of a set of activities to be scheduled according to a given partial ordering which reflects precedence constraints. <p> The goal of a scheduling system is to optimize the resulting schedule based on a set of objectives, such as minimize weighted tardiness, minimize inventory cost of Work-In-Process (WIP), etc. The scheduling problem is difficult to solve for a number of reasons. First, it is an NP-complete problem <ref> [2, 14] </ref>. Second, scheduling objectives are typically not well-defined and may be changing over time. For example, the user might want to minimize both weighted tardiness and work-in-process to meet due dates and to diminish the inventory cost. <p> The subscript j takes the value <ref> [1; 2; 3; 4; 5] </ref> to refer to one of the five subsets of the problems (each of them contains 5 problem instances), respectively.
Reference: [3] <author> C. Reeves, ed., </author> <title> Modern Heuristic Techniques for Combinatorial Problems. </title> <address> New York: </address> <publisher> Halsted Press, </publisher> <year> 1993. </year>
Reference-contexts: This capability is very important for the following reasons. First, traditional search methods, both Operations Research-based and AI-based, that are used in combinatorial optimization, need explicit representation of the optimization objectives that must be defined in advance of problem solving <ref> [3] </ref>. In many practical problems, such as scheduling and design, optimization criteria often involve context- and user-dependent tradeoffs which are impossible to represent as an explicit and static optimization function. <p> The subscript j takes the value <ref> [1; 2; 3; 4; 5] </ref> to refer to one of the five subsets of the problems (each of them contains 5 problem instances), respectively.
Reference: [4] <author> B. D. M. Zweben, E. Davis and M. Deale, </author> <title> "Rescheduling with iterative repair," </title> <booktitle> in Proceedings of AAAI-92 workshop on Production Planning, Scheduling and control, </booktitle> <address> (San Jose, CA.), </address> <publisher> AAAI, </publisher> <year> 1992. </year>
Reference-contexts: On the other hand, approaches that utilize machine learn ing techniques to adapt their behavior to the chang-ing objective criteria and problem solving context are much more promising. Recently, repair-based optimization has been identified as a very flexible framework for solving optimization problems <ref> [4] </ref>. Reinforcement learning (RL) is particularly relevant and potentially useful within a repair-based framework. <p> The advantage of the repair-based approach for optimization problems for which there is no known efficient constructive algorithm has been recently realized by both Operations Research and AI communities <ref> [4, 14] </ref>. Within a repair-based optimization framework, the search space consists of all the possible solutions. <p> The subscript j takes the value <ref> [1; 2; 3; 4; 5] </ref> to refer to one of the five subsets of the problems (each of them contains 5 problem instances), respectively.
Reference: [5] <author> R. S. Sutton, </author> <title> "Learning to predict by the methods of temporal differences," </title> <journal> Machine Learning, </journal> <volume> vol. 3, </volume> <pages> pp. 9-44, </pages> <year> 1988. </year>
Reference-contexts: Recently, repair-based optimization has been identified as a very flexible framework for solving optimization problems [4]. Reinforcement learning (RL) is particularly relevant and potentially useful within a repair-based framework. However, some basic assumptions that typical reinforcement learning methods <ref> [5, 1] </ref> make about the problem domain are violated for complex optimization tasks: (1) The reinforcement signal is typically assumed to be a scalar, which doesn't hold for real-world optimization tasks where evaluation criteria are situation-dependent and changing, (2) RL methods assume that there is an explicit criterion to tell the <p> Our experimental results show that CBR could be effectively incorporated within a RL context. Due to the approximate nature of CBR, when CBR-based selection and evaluation are applied in decision-making, we lose many nice properties which Temporal Differences-based approach <ref> [5] </ref> can provide, such as asymptotic convergence. We believe, however, that our CBR-based approach has good potential for handling much bigger search spaces since it doesn't require an explicit representation of the problem space, and attacking task domains with complicated and dynamically changing decision-making criteria and constraints. <p> The subscript j takes the value <ref> [1; 2; 3; 4; 5] </ref> to refer to one of the five subsets of the problems (each of them contains 5 problem instances), respectively.
Reference: [6] <author> C. J. C. H. Watkins, </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> University of Cambridge, Eng-land, </institution> <year> 1989. </year>
Reference-contexts: However, for optimization tasks, except for toy problems, it is not possible to verify the optimality of a certain solution short of using exhaustive search, which is computationally prohibitive. To address these fundamental issues, instead of using classic reinforcement learning techniques, such as Q-learning <ref> [6] </ref>, or connectionist-based approaches [7], we apply Case-Based Reasoning (CBR) [8] as the primary tool to (1) represent the state space implicitly and approximately in a case-base, (2) generate expected rewards associated with sample points in the state space based on previous problem solving experiences and knowledge about optimization criteria, (in
Reference: [7] <author> L.-J. Lin, </author> <title> "Self-improving reactive agents based on reinforcement learning, planning and teaching," </title> <journal> Machine Learning, </journal> <volume> vol. 8, </volume> <pages> pp. 293-321, </pages> <year> 1992. </year>
Reference-contexts: However, for optimization tasks, except for toy problems, it is not possible to verify the optimality of a certain solution short of using exhaustive search, which is computationally prohibitive. To address these fundamental issues, instead of using classic reinforcement learning techniques, such as Q-learning [6], or connectionist-based approaches <ref> [7] </ref>, we apply Case-Based Reasoning (CBR) [8] as the primary tool to (1) represent the state space implicitly and approximately in a case-base, (2) generate expected rewards associated with sample points in the state space based on previous problem solving experiences and knowledge about optimization criteria, (in some sense, an approximation
Reference: [8] <author> J. Kolodner, </author> <title> Case-Based Reasoning. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1994. </year>
Reference-contexts: To address these fundamental issues, instead of using classic reinforcement learning techniques, such as Q-learning [6], or connectionist-based approaches [7], we apply Case-Based Reasoning (CBR) <ref> [8] </ref> as the primary tool to (1) represent the state space implicitly and approximately in a case-base, (2) generate expected rewards associated with sample points in the state space based on previous problem solving experiences and knowledge about optimization criteria, (in some sense, an approximation of Q used in Q-learning is
Reference: [9] <author> K. Sycara and K. Miyashita, </author> <title> "Learning control knowledge through case-based acquisition of user optimization preferences in ill-structured domain," in Machine Learning and Knowledge Acquisition: Integrated Approaches (G. </title> <editor> Tecuci and Y. Kodratoff, eds.), </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: The work reported here extends previous work on the C A B I N S system <ref> [9, 10, 11, 12, 13] </ref>. It tests the hypothesis that our CBR-based incremental repair methodology shows good potential within a reinforcement learning context to solve problems with opti mization criteria that change over time.
Reference: [10] <author> K. Sycara and K. Miyashita, </author> <title> "Adaptive schedule repair," in Knowledge Based Reactive Scheduling (E. </title> <editor> Szelke and R. Kerr, </editor> <booktitle> eds.), </booktitle> <pages> pp. 107-124, </pages> <address> Ams-terdam, </address> <publisher> Holland: North Holland, </publisher> <year> 1994. </year>
Reference-contexts: The work reported here extends previous work on the C A B I N S system <ref> [9, 10, 11, 12, 13] </ref>. It tests the hypothesis that our CBR-based incremental repair methodology shows good potential within a reinforcement learning context to solve problems with opti mization criteria that change over time.
Reference: [11] <author> D. Zeng, </author> <title> "Combined machine learning techniques in predictive and reactive scheduling." </title> <institution> Graduate School of Industrial Administration, summer paper, Carnegie Mellon University, </institution> <year> 1993. </year>
Reference-contexts: The work reported here extends previous work on the C A B I N S system <ref> [9, 10, 11, 12, 13] </ref>. It tests the hypothesis that our CBR-based incremental repair methodology shows good potential within a reinforcement learning context to solve problems with opti mization criteria that change over time. <p> schedule instance (a state) to an appropriate "abstracted" state in order to extract the appropriate control action and evaluates the applicability of this action in the current problem solving situation. 4 Experimental Evaluation of Captur ing Changing Preferences Extensive experiments have been conducted with C A B I N S <ref> [19, 11, 12] </ref>.
Reference: [12] <author> K. Sycara and K. Miyashita, </author> <title> "Case-based acquisition of user preferences for solution improvement in ill-structured domains," </title> <booktitle> in Proceedings of AAAI-94, </booktitle> <address> (Seattle, Washington), </address> <publisher> AAAI, </publisher> <month> August </month> <year> 1994. </year>
Reference-contexts: The work reported here extends previous work on the C A B I N S system <ref> [9, 10, 11, 12, 13] </ref>. It tests the hypothesis that our CBR-based incremental repair methodology shows good potential within a reinforcement learning context to solve problems with opti mization criteria that change over time. <p> eighted T ardiness + W:I:P: make more sense than W eighted T ardiness fi W:I:P: or the opposite? In this paper, we focus on solving schedule optimization problems where optimization criteria change over time 2 For experimental comparison of our approach with other scheduling methods, interested readers are referred to <ref> [12, 13] </ref>. The basic assumption we made about changing optimization criteria is that the changes occur smoothly, (e.g., we are not expecting that the user will rapidly shift from maximizing a certain objective to minimizing it), and the problem solving context will not change drastically over the problem solving horizon. <p> schedule instance (a state) to an appropriate "abstracted" state in order to extract the appropriate control action and evaluates the applicability of this action in the current problem solving situation. 4 Experimental Evaluation of Captur ing Changing Preferences Extensive experiments have been conducted with C A B I N S <ref> [19, 11, 12] </ref>.
Reference: [13] <author> K. Miyashita and K. Sycara, "Cabins: </author> <title> A framework of knowledge acquisition and iterative revision for schedule improvement and reactive repair," </title> <journal> Artificial Intelligence, </journal> <volume> vol. 76, no. </volume> <pages> 1-2, </pages> <year> 1995. </year>
Reference-contexts: The work reported here extends previous work on the C A B I N S system <ref> [9, 10, 11, 12, 13] </ref>. It tests the hypothesis that our CBR-based incremental repair methodology shows good potential within a reinforcement learning context to solve problems with opti mization criteria that change over time. <p> eighted T ardiness + W:I:P: make more sense than W eighted T ardiness fi W:I:P: or the opposite? In this paper, we focus on solving schedule optimization problems where optimization criteria change over time 2 For experimental comparison of our approach with other scheduling methods, interested readers are referred to <ref> [12, 13] </ref>. The basic assumption we made about changing optimization criteria is that the changes occur smoothly, (e.g., we are not expecting that the user will rapidly shift from maximizing a certain objective to minimizing it), and the problem solving context will not change drastically over the problem solving horizon. <p> For details, refer to <ref> [13] </ref>. In order to bound the ripple effects of repair, a repair tactic is used only within a bounded time horizon, the time interval between the end of the activity preceding the focal activity in the same focal job and the end of the focal activity. <p> As a case retrieval mechanism, C A B I N S uses a variation of k-Nearest Neighbor method (k-NN) [18]. For the detailed formula for similarity calculation, see <ref> [13] </ref>. Since the number of possible schedules for job-shop scheduling is potentially infinite, explicit representation of the state space is impossible. In C A B I N S, the case-base reflects samples of state transition sequences that have been tried out. <p> By incorporating explicit objectives into the RBR so they could be reflected in the case base we got an experimental baseline against which to evaluate the schedules generated by C A B I N S <ref> [13] </ref>.
Reference: [14] <author> T. E. Morton and D. W. Pentico, </author> <title> Heuristic Scheduling Systems: With Application to Production Systems and Product Management. </title> <address> New York, N.Y.: </address> <publisher> John Wiley and Sons Inc., </publisher> <year> 1993. </year>
Reference-contexts: The advantage of the repair-based approach for optimization problems for which there is no known efficient constructive algorithm has been recently realized by both Operations Research and AI communities <ref> [4, 14] </ref>. Within a repair-based optimization framework, the search space consists of all the possible solutions. <p> The goal of a scheduling system is to optimize the resulting schedule based on a set of objectives, such as minimize weighted tardiness, minimize inventory cost of Work-In-Process (WIP), etc. The scheduling problem is difficult to solve for a number of reasons. First, it is an NP-complete problem <ref> [2, 14] </ref>. Second, scheduling objectives are typically not well-defined and may be changing over time. For example, the user might want to minimize both weighted tardiness and work-in-process to meet due dates and to diminish the inventory cost. <p> Each activity can be executed on two substitutable machines. Bottleneck machines, however, have no substitutes. Although the size of these problems may seem small to researchers outside the scheduling community, job-shop scheduling problems of this size have been recognized as very hard problems by AI and OR researchers <ref> [20, 14] </ref> and there are no known optimal solutions yet for these problems due to the large number of constraints. A cross-validation method was used to evaluate the performance of C A B I N S. Each problem set in each group was divided in half.
Reference: [15] <author> L. Dent, J. Boticario, J. McDermott, T. Mitchell, and D. Zabowski, </author> <title> "A personal learning apprentice," </title> <booktitle> in Proceedings of the Tenth National Conference on Artificial Intelligence, AAAI, </booktitle> <year> 1992. </year>
Reference-contexts: However, in this paper, we restrict our attention on the role of CBR as a complementary framework for reinforcement learning. changing preferences <ref> [15, 16] </ref>.
Reference: [16] <author> T. Mitchell, R. Caruana, D. Freitag, J. McDer-mott, and D. Zabowski, </author> <title> "Experience with a learning personal assistant," </title> <journal> Communications of the ACM, </journal> <volume> vol. 37, </volume> <month> July </month> <year> 1994. </year>
Reference-contexts: However, in this paper, we restrict our attention on the role of CBR as a complementary framework for reinforcement learning. changing preferences <ref> [15, 16] </ref>.
Reference: [17] <author> K. Sycara, D. Zeng, and K. Miyashita, </author> <title> "Using case-based reasoning to acquire user scheduling preferences that change over time," </title> <booktitle> in The Proceedings of the Eleventh IEEE Conference on Artificial Intelligence Applications (CAIA '95), </booktitle> <address> (Los Angeles), </address> <publisher> IEEE, </publisher> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: However, in this paper, we restrict our attention on the role of CBR as a complementary framework for reinforcement learning. changing preferences [15, 16]. Fore more detailed dis-cussion, see <ref> [17] </ref>. 3 Overview of C A B I N S C A B I N S uses a repair-based approach for schedule optimization, i.e., a complete but suboptimal schedule is generated by OR-based dispatch heuristics or a constraint-based scheduler and then incrementally revised using revision actions, called repair tactics.
Reference: [18] <author> B. V. Dasarathy, ed., </author> <title> Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques. </title> <address> Los Alamos, CA: </address> <publisher> IEEE Computer Society Press, </publisher> <year> 1990. </year>
Reference-contexts: As a case retrieval mechanism, C A B I N S uses a variation of k-Nearest Neighbor method (k-NN) <ref> [18] </ref>. For the detailed formula for similarity calculation, see [13]. Since the number of possible schedules for job-shop scheduling is potentially infinite, explicit representation of the state space is impossible. In C A B I N S, the case-base reflects samples of state transition sequences that have been tried out.
Reference: [19] <author> K. Miyashita and K. Sycara, </author> <title> "Improving schedule quality through case-based reasoning," </title> <booktitle> in Proceedings of AAAI-93 Workshop on Case-Based Reasoning, </booktitle> <address> (Washington, DC), </address> <pages> pp. 101-110, </pages> <publisher> AAAI, </publisher> <year> 1993. </year>
Reference-contexts: schedule instance (a state) to an appropriate "abstracted" state in order to extract the appropriate control action and evaluates the applicability of this action in the current problem solving situation. 4 Experimental Evaluation of Captur ing Changing Preferences Extensive experiments have been conducted with C A B I N S <ref> [19, 11, 12] </ref>.
Reference: [20] <author> N. Sadeh, </author> <title> Look-Ahead Techniques for Micro-Opportunistic Job Shop Scheduling. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <year> 1991. </year>
Reference-contexts: Each activity can be executed on two substitutable machines. Bottleneck machines, however, have no substitutes. Although the size of these problems may seem small to researchers outside the scheduling community, job-shop scheduling problems of this size have been recognized as very hard problems by AI and OR researchers <ref> [20, 14] </ref> and there are no known optimal solutions yet for these problems due to the large number of constraints. A cross-validation method was used to evaluate the performance of C A B I N S. Each problem set in each group was divided in half.
References-found: 20

