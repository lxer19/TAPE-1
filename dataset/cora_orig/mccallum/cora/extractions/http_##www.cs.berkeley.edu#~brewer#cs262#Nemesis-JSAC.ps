URL: http://www.cs.berkeley.edu/~brewer/cs262/Nemesis-JSAC.ps
Refering-URL: http://www.cs.berkeley.edu/~brewer/cs262/
Root-URL: 
Title: The Design and Implementation of an Operating System to Support Distributed Multimedia Applications  
Author: Ian Leslie, Derek McAuley, Richard Black, Timothy Roscoe, Paul Barham, David Evers, Robin Fairbairns, and Eoin Hyden 
Abstract: Support for multimedia applications by general purpose computing platforms has been the subject of considerable research. Much of this work is based on an evolutionary strategy in which small changes to existing systems are made. The approach adopted here is to start ab initio with no backward compatibility constraints. This leads to a novel structure for an operating system. The structure aims to decouple applications from one another and to provide multiplexing of all resources, not just the CPU, at a low level. The motivation for this structure, a design based on the structure, and its implementation on a number of hardware platforms is described. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. J. Mullender, I. M. Leslie, and D. R. McAuley, </author> <title> "Operating-system support for distributed multimedia", </title> <booktitle> in Proceedings of Summer 1994 Usenix Conference, </booktitle> <address> Boston, Massachusetts, USA, </address> <month> June </month> <year> 1994, </year> <pages> pp. 209-220, </pages> <note> Also available as Pegasus Paper 94-6. </note>
Reference: [2] <author> C. J. Lindblad, D. J. Wetherall, and D. L. Tennenhouse, </author> <title> "The VuSystem: A programming system for visual processing of digital video", </title> <booktitle> in Proceedings of ACM Multimedia, </booktitle> <address> San Francisco, CA, USA, </address> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: If the current situation is typified as using processors to control continuous media, future systems will be typified as systems in which the data types operated on have been extended to include continuous media streams <ref> [2] </ref>. What we are concerned with here is to provide an environment in which such applications can be developed and run.
Reference: [3] <author> G. Coulson, A.Campbell, P. Robin, G. Blair, M. Papathomas, and D. Sheperd, </author> <title> "The design of a QoS-controlled ATM-based communication system in chorus", </title> <journal> IEEE Journal on Selected Areas In Communications, </journal> <volume> vol. 13, no. 4, </volume> <pages> pp. 686-699, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: In between are a range of options which are more appropriate for multimedia systems. In general the approach is to provide probabilistic guarantees and to expect applications to monitor their performance and adapt their behaviour when resource allocation changes. Some QoS architectures, for example <ref> [3] </ref> assume a context in which applications specify their QoS requirements to a layer below them which then determines how that requirement is to be met and in turn specifies derived QoS requirements to the next layer below.
Reference: [4] <author> D.R. McAuley, </author> <title> "Protocol Design for High Speed Networks", </title> <type> Tech. Rep. 186, </type> <institution> University of Cambridge Computer Laboratory, </institution> <month> January </month> <year> 1990, </year> <type> Ph.D. Dissertation. </type>
Reference: [5] <author> D. L. Tennenhouse, </author> <title> "Layered multiplexing considered harmful", in Protocols for High Speed Networks, </title> <editor> Rudin and Williamson, Eds. </editor> <booktitle> 1989, </booktitle> <publisher> Elsevier. </publisher>
Reference: [6] <author> J. Mogul, </author> <title> "Efficient Use of Workstations for Passive Monitoring of Local Area Networks", in Computer Communication Review. </title> <booktitle> ACM SIGCOMM, </booktitle> <month> September </month> <year> 1990, </year> <note> vol. 20. </note>
Reference-contexts: Knowledgeable applications may make special use of the buffer pools in the special memory. It has been recent practice in operating systems to support a protocol independent scheme for determining the process for which packets arriving at an interface are destined. This is known as packet filtering <ref> [6] </ref> and this technology is now highly advanced [7],[35]. For non-self-selecting interfaces, packet filtering can determine which I/O path the data will travel along as easily as it can determine which process will be the receiver. This property is assumed in the Nemesis buffer mechanism derived below.
Reference: [7] <author> S. McCanne and V. Jacobson, </author> <title> "The BSD Packet Filter: A New Architecture for User-level Packet Capture", </title> <booktitle> in USENIX Winter 1993 Conference, </booktitle> <month> January </month> <year> 1993, </year> <pages> pp. 259-269. </pages>
Reference: [8] <author> A. Demers, S. Keshav, and S. Shenker, </author> <title> "Analysis and Simulation of Fair Queueing Algorithm", </title> <journal> Journal of Internetworking: Research and Experience, </journal> <volume> vol. 1, no. 1, </volume> <year> 1990. </year>
Reference-contexts: While this particular line of work grew out of the use of virtual circuits in ATM networks, it can also be employed in IP networks by the use of packet filters [6],[7] and fair queueing schemes <ref> [8] </ref>. Analogously, application QoS Crosstalk occurs when operating system services and physical resources are multiplexed among client applications. In addition to network protocol processing, components such as device I/O, filing systems and directory services, memory management, link-loaders, and window systems are needed by client applications.
Reference: [9] <author> M. V. Wilkes and R. M. Needham, </author> <title> The Cambridge CAP Computer and its Operating System, </title> <publisher> North Holland, </publisher> <year> 1979. </year>
Reference-contexts: A number of approaches have been taken to try and minimize the cost of interacting with such servers. One technique is to support thread migration; there are systems which allow threads to undergo protection domain switches, both in specialised hardware architectures <ref> [9] </ref> and conventional workstations [10]. However, such threads cannot easily be scheduled by their parent application, and must be implemented by a kernel which manages the protection domain boundaries. <p> The thread tunnelling model achieves very low latency by combining all components into one operation: the transfer of the thread from client to server, using the kernel to simulate the protected procedure calls implemented in hardware on, for example, Multics [28] and some capability systems such as the CAP <ref> [9] </ref>. An example is the replacement of the original TAOS RPC mechanism by Lightweight RPC [10].
Reference: [10] <author> Brian N. Bershad, Thomas E. Anderson, Edward D. Lazowska, and Henry M. Levy, </author> <title> "Lightweight Remote Procedure Call", </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> vol. 8, no. 1, </volume> <pages> pp. 37-55, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: A number of approaches have been taken to try and minimize the cost of interacting with such servers. One technique is to support thread migration; there are systems which allow threads to undergo protection domain switches, both in specialised hardware architectures [9] and conventional workstations <ref> [10] </ref>. However, such threads cannot easily be scheduled by their parent application, and must be implemented by a kernel which manages the protection domain boundaries. This kernel must as a consequence, provide synchronisation mechanisms for its threads, and applications are no longer in control of their own resource tradeoffs. <p> An example is the replacement of the original TAOS RPC mechanism by Lightweight RPC <ref> [10] </ref>. In these cases, the performance advantage of thread tunnelling comes at a price; since the thread has left the client domain, it has the same effect as having blocked as far as the client is concerned.
Reference: [11] <author> Clifford W. Mercer, Stefan Savage, and Hideyuki Tokuda, </author> <title> "Processor capacity reserves: Operating system support for multimedia applications", </title> <booktitle> in Proceedings of the IEEE International Conference on Multimedia Computing and Systems, </booktitle> <month> May </month> <year> 1994. </year> <title> 7 This is used in an embedded application where the number of processes is small. </title> <type> 21 </type>
Reference-contexts: The alternative is to implement servers as separate schedulable entities. Some systems allow a client to transfer some of their resources to the server to preserve a given QoS across server calls. The Processor Capacity Reserves mechanism <ref> [11] </ref> is the most prominent of these; the kernel implements objects called reserves which can be transferred from client threads to servers.
Reference: [12] <author> Chandramohan A. Thekkath, Thu D. Nguyen, Evelyn Moy, and Edward D. Lazowska, </author> <title> "Implementing network protocols at user level", </title> <type> Tech. Rep. </type> <institution> 93-03-01, Department of Computer Science and Engineering, University of Washington, </institution> <address> Seattle, WA 98195, </address> <year> 1993. </year>
Reference-contexts: However, there are a number of examples in recent literature of services being implemented as client libraries instead of within a kernel or server. Efficient user-level threads packages have already been mentioned. Other examples of user level libraries include network protocols <ref> [12] </ref>, window system rendering [13] and Unix emulation [14]. Nemesis is designed to use these techniques. In addition, most of the support for creating and linking new domains, setting up inter-domain communication, and networking is performed in the context of the application.
Reference: [13] <author> P. Barham, M. Hayter, D. McAuley, and I. Pratt, </author> <title> "Devices on the Desk Area Network", </title> <journal> IEEE Journal on Selected Areas In Communications, </journal> <volume> vol. 13, no. 4, </volume> <month> May </month> <year> 1995. </year>
Reference-contexts: However, there are a number of examples in recent literature of services being implemented as client libraries instead of within a kernel or server. Efficient user-level threads packages have already been mentioned. Other examples of user level libraries include network protocols [12], window system rendering <ref> [13] </ref> and Unix emulation [14]. Nemesis is designed to use these techniques. In addition, most of the support for creating and linking new domains, setting up inter-domain communication, and networking is performed in the context of the application.
Reference: [14] <author> Yousef A. Khalidi and Michael N. Nelson, </author> <title> "An Implementation of UNIX on an Object-oriented Operating System", </title> <type> Tech. Rep. 92-3, </type> <institution> Sun Microsystems Laboratories, Inc., </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: However, there are a number of examples in recent literature of services being implemented as client libraries instead of within a kernel or server. Efficient user-level threads packages have already been mentioned. Other examples of user level libraries include network protocols [12], window system rendering [13] and Unix emulation <ref> [14] </ref>. Nemesis is designed to use these techniques. In addition, most of the support for creating and linking new domains, setting up inter-domain communication, and networking is performed in the context of the application. The result is a `vertically integrated' operating system architecture, illustrated in figure 2. <p> In the local case an interface reference and invocation reference have the same representation - a pointer to a closure binding is an implicit and trivial operation. In Nemesis, as in Spring <ref> [14] </ref>, all interfaces are strongly typed, and these types are defined in an interface definition language (IDL). The IDL used in Nemesis, called Middl, is similar in functionality to the IDLs used in object-based RPC systems, with some additional constructs to handle local and low-level operating system interfaces.
Reference: [15] <author> Thomas E. Anderson, Brian N. Bershad, Edward D. Lazowska, and Henry M. Levy, </author> <title> "Scheduler Activations: Effective Kernel Support for the User-Level Management of Parallelism", </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> vol. 10, no. 1, </volume> <pages> pp. 53-79, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: A privileged service called the Domain Manager creates DCBs and links them into the scheduler data structures. The details of some of the fields in the DCB are described below. 6 A.1 Activations The concept of activations is similar to that presented in <ref> [15] </ref>. When a domain is allocated the CPU by the kernel, the domain is normally upcalled rather than being resumed at the point where it lost the CPU. This allows the domain to consider scheduling actions as soon as it obtains CPU resource.
Reference: [16] <author> D. Reed and R. Kanodia, </author> <title> "Synchronization with eventcounts and sequencers", </title> <type> Tech. Rep., </type> <institution> MIT Laboratory for Computer Science, </institution> <year> 1977. </year>
Reference-contexts: Currently only "point-to-point" events are implemented, although there is nothing to prevent "multicast" events if needed in the future. Exactly what an event represents is not known by the kernel, but only by the domains themselves. The relationship between these Nemesis events and event counts and sequencers <ref> [16] </ref> within the standard user thread library is discussed in section IV-D. Events are also used as the underlying notification mechanism supporting interrupt dispatch and inter-domain communication facilities at a higher level of abstraction currently support is provided for inter-domain invocations (section VI), and streaming data operations (section VI-C). <p> It may be very simple (in the case of a single threaded domain), or more complex. The base technique for synchronization that was adopted within domains was to extend the use of the core Nemesis events already present for interdomain communication, and provide event counts and sequencers <ref> [16] </ref>. These event counts and sequencers can be purely local within the domain or attached to either outbound events (those which can be propagated to another domain using send ()) or inbound events (those which change asynchronously as a result of some other domain issuing a send ()).
Reference: [17] <author> D. Mills, </author> <title> "Internet Time Synchronisation: The Network Time Protocol", Internet Request for Comment Number 1129, </title> <month> October </month> <year> 1989. </year>
Reference-contexts: CPU scheduling is done based on this ticker, and the system time is updated when this ticker interrupt occurs taking into account an adjustment to keep the time consistent with universal time (UT) (e.g. using NTP <ref> [17] </ref>). In these circumstances, a 8 domain may only be able to obtain a time value accurate to the time of the last timer interrupt and even then the value actually read may be subject to significant skew due to adjustments.
Reference: [18] <author> M.J. Dixon, </author> <title> "System Support for Multi-Service Traffic", </title> <type> Tech. Rep. 245, </type> <institution> University of Cambridge Computer Laboratory, </institution> <month> September </month> <year> 1991, </year> <type> Ph.D. Dissertation. </type>
Reference-contexts: The interrupt service routine (ISR) for a high interrupt rate device can therefore hog the processor for long periods, since the scheduler itself hardly gets a chance to run, let alone a user process. Such high frequency interruptions can be counter productive; <ref> [18] </ref> describes a situation where careful prioritising of interrupts led to high throughput, but with most interrupts disabled for a high proportion of the time. <p> C. Rbufs The inter-process communication mechanism described above fits the needs of inter-domain invocation quite well, but is not appropriate for stream based bulk transfer of data. Besides Pipes and Streams, schemes for controlling such transfers are more often integrated with network buffering and include Mbufs [30], IOBufs <ref> [18] </ref>, Fbufs [31] and other schemes to support application data unit (ADU) transfer such as the IP trailers [32] scheme found in some versions of BSD. A full discussion of these schemes can be found in [20].
Reference: [19] <author> C. L. Liu and James W. Layland, </author> <title> "Scheduling Algorithms for Multiprogramming in a Hard-Real-Time Environment", </title> <journal> Journal of the ACM, </journal> <volume> vol. 20, no. 1, </volume> <pages> pp. 46-61, </pages> <month> January </month> <year> 1973. </year>
Reference-contexts: nanoseconds' as the QoS Manager has ensured that the total share of the processor allocated is less than 100% (i.e. s i =p i &lt; 1), and slices can be executed at any point during their period this approach satisfies the conditions required for an EDF algorithm to function correctly <ref> [19] </ref>, This argument relies on two simplifications: firstly, that scheduling overhead is negligible, and secondly that the system is in a steady state.
Reference: [20] <author> R.J. Black, </author> <title> "Explicit Network Scheduling", </title> <type> Tech. Rep. 361, </type> <institution> University of Cambridge Computer Laboratory, </institution> <month> December </month> <year> 1994, </year> <type> Ph.D. Dissertation. </type>
Reference-contexts: The fixed Jubilees remove the need for EDF scheduling and is particular suited to situations where the application load is well understood and where the a single Jubilee size can be chosen. Complete details can be found in <ref> [20] </ref>. D. Intra-domain scheduling This section considers the implementation of an intra-domain scheduler to provide a familiar threaded environment. The intra-domain scheduler is the code which sits above the virtual processor interface. The code is not privileged and can differ from domain to domain. <p> The mutexes and conditional variables of SRC threads [21], POSIX threads, and the semaphores used in the Wanda system have all been implemented straightforwardly and efficiently over event counts. Details can be found in <ref> [20] </ref>. Implementing threads packages over the upcall interface has proved remarkably easy. <p> A full discussion of these schemes can be found in <ref> [20] </ref>. The scheme presented here, RBufs, is intended as the principal mechanism for both interdomain streams and for streams between devices and application domains.
Reference: [21] <author> A.D. Birrell and J.V. Guttag, </author> <title> "Synchronization Primitives for a Multiprocessor: A formal specification", </title> <type> Tech. Rep. 20, </type> <institution> Digital Equipment Corporation Systems Research Center, </institution> <year> 1987. </year>
Reference-contexts: The mutexes and conditional variables of SRC threads <ref> [21] </ref>, POSIX threads, and the semaphores used in the Wanda system have all been implemented straightforwardly and efficiently over event counts. Details can be found in [20]. Implementing threads packages over the upcall interface has proved remarkably easy.
Reference: [22] <author> Timothy Roscoe, </author> <title> The Structure of a Multi-Service Operating System, </title> <type> PhD thesis, </type> <institution> University of Cambridge Computer Laboratory, </institution> <month> April </month> <year> 1995, </year> <note> Available as Technical Report no. 376. </note>
Reference-contexts: In order to describe the inter-domain communciation system of Nemesis it is necessary to present some of the higher level constructs used in Nemesis to complement the single virtual address space approach. A full account can be found in <ref> [22] </ref>. The key aspects are the extensive use of typing, transparency and modularity in the definition of the Nemesis interfaces and the use of closures to provide comprehensible, safe and extensive sharing of data and code.
Reference: [23] <author> Bjarne Stroustrup, </author> <title> The Design and Evolution of C++, </title> <publisher> Addison-Wesley, </publisher> <year> 1994. </year>
Reference-contexts: The basic paradigm adopted is then dictated by the Middl 3 This is different from C++ where there is no distinction between class and type, and hence no clear notion of an interface. C++ abstract classes often contain implementation details, and were added as an afterthought <ref> [23, p. 277] </ref>. 13 interface definition language: Remote Procedure Call (RPC) with the addition of `announcement' operations, which allow use of message passing semantics.
Reference: [24] <author> Dave Otway, </author> <title> "The ANSA Binding Model", ANSA Phase III document APM.1314.01, Architecture Projects Management Limited, </title> <publisher> Poseidon House, </publisher> <address> Castle Park, Cambridge, CB3 0RD, UK, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: By contrast, the field of distributed processing has sophisticated and well-established notions of interfaces and binding, for example the "Trader" within the ANSA architecture <ref> [24] </ref>. The Nemesis binding model shares many features with the ANSA model.
Reference: [25] <author> Andrew Birrell, Greg Nelson, Susan Owicki, and Ted Wobber, </author> <title> "Network Objects", </title> <booktitle> Proceedings of the 14th ACM SIGOPS Symposium on Operating Systems Principles, Operating Systems Review, </booktitle> <volume> vol. 27, no. 5, </volume> <pages> pp. 217-230, </pages> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: The key feature of the implicit binding paradigm is that information about the binding itself is hidden from the client, who is presented with a surrogate interface indistinguishable from the `real thing'. This is the approach adopted by many distributed object systems, for example Modula-3 Network Objects <ref> [25] </ref> and CORBA [26]. It is intuitive and easy to use from the point of view of a client programmer, and for many applications provides all the functionality required, provided that a garbage collector is available to destroy the binding when it is no longer in use.
Reference: [26] <author> Object Management Group, </author> <title> The Common Object Request Broker: Architecture and Specification, Draft 10th December 1991, OMG Document Number 91.12.1, revision 1.1. </title>
Reference-contexts: This is the approach adopted by many distributed object systems, for example Modula-3 Network Objects [25] and CORBA <ref> [26] </ref>. It is intuitive and easy to use from the point of view of a client programmer, and for many applications provides all the functionality required, provided that a garbage collector is available to destroy the binding when it is no longer in use.
Reference: [27] <author> Brian N. Bershad, Thomas E. Anderson, Edward D. Lazowska, and Henry M. Levy, </author> <title> "User-Level Interprocess Communication for Shared Memory Multiprocessors", </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> vol. 9, no. 2, </volume> <pages> pp. 175-198, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: This separation of information transfer from control transfer is especially beneficial in a shared memory multiprocessor, as described in <ref> [27] </ref>.
Reference: [28] <author> E.I. Organick, </author> <title> The Multics System: An Examination of Its Structure, </title> <publisher> MIT Press, </publisher> <year> 1972. </year>
Reference-contexts: The thread tunnelling model achieves very low latency by combining all components into one operation: the transfer of the thread from client to server, using the kernel to simulate the protected procedure calls implemented in hardware on, for example, Multics <ref> [28] </ref> and some capability systems such as the CAP [9]. An example is the replacement of the original TAOS RPC mechanism by Lightweight RPC [10].
Reference: [29] <author> Jeffrey S. Chase, Henry M. Levy, Michael J. Feeley, and Edward D. Lazowska, </author> <title> "Sharing and Protection in a Single Address Space Operating System", </title> <type> Technical Report 93-04-02, </type> <note> revised January 1994, </note> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <month> Apr. </month> <year> 1993. </year>
Reference-contexts: Measurements have been taken of null RPC times between two domains an otherwise unloaded DEC3000/400 Sandpiper. Most calls take about 30s, which compares very favourably with those reported in <ref> [29] </ref> for Mach (88s) and Opal (122s) on the same hardware. Some calls (20% in the experiments) take between 55s and 65s; these have experienced more than one reschedule between event transmissions.
Reference: [30] <author> S. Le*er, M. McKusick, M. Karels, and J. Quarterman, </author> <title> The Design and Implementation of the 4.3BSD UNIX Operating System, </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: C. Rbufs The inter-process communication mechanism described above fits the needs of inter-domain invocation quite well, but is not appropriate for stream based bulk transfer of data. Besides Pipes and Streams, schemes for controlling such transfers are more often integrated with network buffering and include Mbufs <ref> [30] </ref>, IOBufs [18], Fbufs [31] and other schemes to support application data unit (ADU) transfer such as the IP trailers [32] scheme found in some versions of BSD. A full discussion of these schemes can be found in [20].
Reference: [31] <author> P. Druschel and L. Peterson, "Fbufs: </author> <title> A High-Bandwidth Cross-Domain Transfer Facility", </title> <booktitle> in Proceedings of the fourteenth ACM Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1993, </year> <pages> pp. 189-202. </pages>
Reference-contexts: Rbufs The inter-process communication mechanism described above fits the needs of inter-domain invocation quite well, but is not appropriate for stream based bulk transfer of data. Besides Pipes and Streams, schemes for controlling such transfers are more often integrated with network buffering and include Mbufs [30], IOBufs [18], Fbufs <ref> [31] </ref> and other schemes to support application data unit (ADU) transfer such as the IP trailers [32] scheme found in some versions of BSD. A full discussion of these schemes can be found in [20].
Reference: [32] <author> S. Le*er and M. Karels, "Trailer Encapsulations", </author> <title> Internet Request for Comment Number 893, </title> <month> April </month> <year> 1984. </year>
Reference-contexts: Besides Pipes and Streams, schemes for controlling such transfers are more often integrated with network buffering and include Mbufs [30], IOBufs [18], Fbufs [31] and other schemes to support application data unit (ADU) transfer such as the IP trailers <ref> [32] </ref> scheme found in some versions of BSD. A full discussion of these schemes can be found in [20]. The scheme presented here, RBufs, is intended as the principal mechanism for both interdomain streams and for streams between devices and application domains.
Reference: [33] <author> P. Druschel, L. Peterson, and B. Davie, </author> <title> "Experiences with a High-Speed Network Adaptor: A Software Perspective", </title> <journal> in Computer Communication Review. ACM SIGCOMM, September 1994, </journal> <volume> vol. 24, </volume> <pages> pp. 2-13. </pages>
Reference-contexts: Self-selecting interfaces use the VCI (or similar) in the header of arriving data to access the correct buffer control information. These are typically high bandwidth interfaces with DMA support. Non self-selecting interfaces require software copying of data (e.g. Ethernet). Examples of self-selecting interfaces include the Aurora TURBOchannel interface <ref> [33] </ref> and the Jetstream / Afterburner combination [34]. In Jetstream the arriving packets enter a special buffer memory based on the arriving VCI. The device driver then reads the headers and instructs a special DMA engine to copy the data to the final location.
Reference: [34] <author> A. Edwards, G. Watson, J. Lumley, D. Banks, C. Calamvokis, and C. Dalton, </author> <title> "User-space protocols deliver high performance to applications on a low-cost Gb/s LAN", </title> <journal> in Computer Communication Review. ACM SIGCOMM, September 1994, </journal> <volume> vol. 24, </volume> <pages> pp. 14-23. </pages>
Reference-contexts: These are typically high bandwidth interfaces with DMA support. Non self-selecting interfaces require software copying of data (e.g. Ethernet). Examples of self-selecting interfaces include the Aurora TURBOchannel interface [33] and the Jetstream / Afterburner combination <ref> [34] </ref>. In Jetstream the arriving packets enter a special buffer memory based on the arriving VCI. The device driver then reads the headers and instructs a special DMA engine to copy the data to the final location.
Reference: [35] <author> M. Yuhara, C. Maeda, B. Bershad, and J. Moss, </author> <title> "The MACH Packet Filter: Efficient Packet Demultiplexing for Multiple Endpoints and Large Messages", </title> <booktitle> in USENIX Winter 1994 Conference, </booktitle> <month> January </month> <year> 1994, </year> <pages> pp. 153-165. </pages>
Reference: [36] <institution> Digital Equipment Corporation TURBOchannel Industry Group, </institution> <note> TURBOchannel Specifications Version 3.0, </note> <year> 1993. </year>
Reference-contexts: This property is assumed in the Nemesis buffer mechanism derived below. On older hardware many devices which used DMA required a single non-interrupted access to a contiguous buffer. On more recent platforms such as the TURBOchannel <ref> [36] </ref> the bus architecture requires that a device burst for some maximum period before relinquishing the bus. This is to prevent the cache and write buffer being starved of memory bandwidth and halting the CPU. Devices are expected to have enough internal buffering to weather such gaps.

References-found: 36

