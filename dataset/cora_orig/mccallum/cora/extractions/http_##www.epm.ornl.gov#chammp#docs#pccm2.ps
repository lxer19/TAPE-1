URL: http://www.epm.ornl.gov/chammp/docs/pccm2.ps
Refering-URL: 
Root-URL: 
Title: PARALLEL COMPUTING (in press) DESIGN AND PERFORMANCE OF A SCALABLE PARALLEL COMMUNITY CLIMATE MODEL  
Author: JOHN DRAKE IAN FOSTER JOHN MICHALAKES BRIAN TOONEN PATRICK WORLEY 
Abstract: We describe the design of a parallel global atmospheric circulation model, PCCM2. This parallel model is functionally equivalent to the National Center for Atmospheric Research's Community Climate Model, CCM2, but is structured to exploit distributed memory multicomputers. PCCM2 incorporates parallel spectral transform, semi-Lagrangian transport, and load balancing algorithms. We present detailed performance results on the IBM SP2 and Intel Paragon. These results provide insights into the scalability of the individual parallel algorithms and of the parallel model as a whole. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Barros and T. Kauranne, </author> <title> On the parallelization of global spectral Eulerian shallow-water models, </title> <booktitle> in Parallel Supercomputing in Atmospheric Science: Proceedings of the Fifth ECMWF Workshop on Use of Parallel Processors in Meteorology, </booktitle> <editor> G.-R. Hoffman and T. Kauranne, eds., </editor> <publisher> World Scientific Publishing Co. Pte. Ltd., </publisher> <address> Singapore, </address> <year> 1993, </year> <pages> pp. 36-43. </pages>
Reference-contexts: All these developments build on previous work on parallel algorithms for atmospheric circulation models, as described in the papers referenced above and in <ref> [1, 3, 14, 18, 19, 24] </ref>. Space does not permit a description of the primitive equations used to model the fluid dynamics of the atmosphere [30] or of CCM2. <p> To evaluate the spectral coefficients numerically, a fast Fourier transform (FFT) is used to find ~ m () for any given . The Legendre transform (LT) is approximated using a Gaussian quadrature rule. Denoting the Gauss points in <ref> [1; 1] </ref> by j and the Gauss weights by w j , ~ m J X ~ m ( j )P m Here J is the number of Gauss points. (For simplicity, we will henceforth refer to (3) as the forward Legendre transform.) The point values are recovered from the spectral <p> Without the reordering of the wavenumbers generated by the parallel FFT, a noticeable load imbalance would occur, as those processor columns associated with larger wavenumbers would have very few spectral coefficients. The particular ordering used, which was first proposed by Barros and Kauranne <ref> [1] </ref>, minimizes this imbalance. 3.3. Parallel Fast Fourier Transform. The data decompositions used in PCCM2 allow "physics" computations to proceed without communication within each vertical column. However, communication is required for the FFT and LT. As noted above, communication for the FFT can be achieved in two different ways.
Reference: [2] <author> W. Bourke, </author> <title> An efficient, one-level, primitive-equation spectral model, </title> <journal> Mon. Wea. Rev., </journal> <volume> 102 (1972), </volume> <pages> pp. 687-701. </pages>
Reference-contexts: This stage entails a data dependency in the west-east dimension. Second, a LT is applied to each wavenumber of the resulting latitude/wavenumber grid to integrate the results of the FFT in the north-south, or latitudinal, direction, transforming Fourier space data to spectral space <ref> [2] </ref>. This stage entails a data dependency in the north-south dimension. In the inverse transform, the order of these operations is reversed. 3. PCCM2 Parallel Algorithms. We now introduce the parallel algorithms used in PCCM2.
Reference: [3] <author> D. Dent, </author> <title> The ECMWF model on the Cray Y-MP8, in The Dawn of Massively Parallel Processing in Meteorology, </title> <editor> G.-R. Hoffman and D. K. Maretis, eds., </editor> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1990. </year>
Reference-contexts: All these developments build on previous work on parallel algorithms for atmospheric circulation models, as described in the papers referenced above and in <ref> [1, 3, 14, 18, 19, 24] </ref>. Space does not permit a description of the primitive equations used to model the fluid dynamics of the atmosphere [30] or of CCM2.
Reference: [4] <author> D. Dent, </author> <title> The IFS Model: A Parallel Production Weather Code, </title> <note> submitted to Parallel Computing, </note> <year> 1995. </year>
Reference-contexts: PCCM2 was the first spectral, semi-Lagrangian model developed for distributed-memory multicomputers [6]. However, we are not the only group to have examined parallel formulations of such models. At the European Center for Medium-range Weather Forecasts (ECMWF), Dent <ref> [4] </ref> and his colleagues have developed the Integrated Forecast System for both vector multiprocessors and distributed memory multiprocessors. Sela [25] has adapted the National Meteorological Center's spectral model for use on the CM5. <p> Scaling arguments show that for large problem sizes and large numbers of processors, transpose algorithms are the most efficient, as they communicate the least data. (These algorithms are used in the ECMWF's Integrated Forecast System model, which is designed to operate at T213 resolution using 31 vertical levels <ref> [4] </ref>.) However, both experimental and analytic studies indicate that for the smaller problem sizes considered in climate modeling, nontranspose algorithms are often competitive, particularly for the LT. 3.2. PCCM2 Data Decompositions. <p> This approach has proved successful in the IFS model <ref> [4] </ref>, and is currently being investigated for use in CCM2 and PCCM2. 5. Load Balancing. As noted in the introduction, load imbalances can arise in parallel climate models as a result of spatial and temporal variations in the computation requirements of physics routines [20, 21].
Reference: [5] <author> U.S. </author> <title> Department of Energy, Building an advanced climate model: Progress plan for the CHAMMP climate modeling program, </title> <type> DOE Tech. Report DOE/ER-0479T, U.S. </type> <institution> Department of Energy, </institution> <address> Washington, D.C., </address> <month> December, </month> <year> 1990. </year>
Reference: [6] <author> J. B. Drake, R. E. Flanery, I. T. Foster, J. J. Hack, J. G. Michalakes, R. L. Stevens, D. W. Walker, D. L. Williamson, and P. H. Worley, </author> <title> The message-passing version of the parallel community climate model, </title> <booktitle> in Parallel Supercomputing in Atmospheric Science, </booktitle> <publisher> World Scientific Publishing, </publisher> <address> Singapore, </address> <year> 1993, </year> <pages> pp. 500-513. </pages>
Reference-contexts: We have also incorporated some of the more promising algorithms into a production parallel climate model called PCCM2 <ref> [6] </ref>. The latter development has allowed us to validate performance results obtained using simpler testbed codes, and to investigate issues such as load balancing and parallel I/O. <p> However, they also introduce significant spatial and temporal variations in computational load, and it proves useful to incorporate load balancing algorithms that compensate for these variations. PCCM2 was the first spectral, semi-Lagrangian model developed for distributed-memory multicomputers <ref> [6] </ref>. However, we are not the only group to have examined parallel formulations of such models. At the European Center for Medium-range Weather Forecasts (ECMWF), Dent [4] and his colleagues have developed the Integrated Forecast System for both vector multiprocessors and distributed memory multiprocessors.
Reference: [7] <author> E. Eliasen, B. Machenhauer, and E. Rasmussen, </author> <title> On a numerical method for integration of the hydrodynamical equations with a spectral representation of the horizontal fields, </title> <type> Report No. 2, </type> <institution> Institut for Teoretisk Meteorologi, Kobenhavns Universitet, Denmark, </institution> <year> 1970. </year>
Reference-contexts: CCM2 is a comprehensive, three-dimensional global atmospheric general circulation model for use in the analysis and prediction of global climate [15, 16]. It uses two different numerical methods to simulate the fluid dynamics of the atmosphere. The spherical harmonic (spectral) transform method <ref> [7, 22] </ref> is used for the horizontal discretization of vortic-ity, divergence, temperature, and surface pressure; this method features regular, static, global data dependencies.
Reference: [8] <author> I. T. Foster, W. Gropp, and R. Stevens, </author> <title> The parallel scalability of the spectral transform method, </title> <journal> Mon. Wea. Rev., </journal> <volume> 120 (1992), </volume> <pages> pp. 835-850. </pages>
Reference-contexts: In the course of this study, we have developed new parallel algorithms for numerical methods used in atmospheric circulation models, and evaluated these and other parallel algorithms using both testbed codes and analytic performance models <ref> [8, 10, 11, 29, 32] </ref>. We have also incorporated some of the more promising algorithms into a production parallel climate model called PCCM2 [6]. The latter development has allowed us to validate performance results obtained using simpler testbed codes, and to investigate issues such as load balancing and parallel I/O. <p> Physical space is best partitioned by latitude and longitude, as a decomposition in the vertical dimension requires considerable communication in the physics component. However, Fourier and spectral space can be partitioned in several different ways. A latitude/wavenumber decomposition of Fourier space requires communication within the FFT <ref> [8, 29] </ref>. Alternatively, a latitude/vertical decomposition of Fourier space allows FFTs to proceed without communication, if a transpose operation is first used to transform physical space from a latitude/longitude to a latitude/vertical decomposition.
Reference: [9] <author> I. T. Foster and B. Toonen, </author> <title> Load balancing in climate models, </title> <booktitle> Proc. 1994 Scalable High-Performance Computing Conf., </booktitle> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1994. </year>
Reference-contexts: A flexible load-balancing library has been developed as part of the PCCM2 effort, suitable for use in PCCM2 and other similar climate models. This library has been incorporated into PCCM2 and used to experiment with alternative load-balancing strategies <ref> [9] </ref>. One simple strategy swaps every second grid point with its partner 180 degrees apart in longitude during radiation calculations. This does a good job of balancing the diurnal cycle: because CCM2 pairs N/S latitudes, it ensures that each processor always has approximately equal numbers of day and night points. <p> Currently, the former scheme gives better performance in most cases, and succeeds in eliminating about 75 per cent of the inefficiency attributable to load imbalances when PCCM2 runs on 512 processors <ref> [9] </ref>. This scheme is incorporated in the production PCCM2, and is used in the performance studies reported in the next section. Figure 2 illustrates the impacts of load balancing. 10 J. DRAKE, I. FOSTER, J. MICHALAKES, B. TOONEN, AND P. WORLEY Fig. 2.
Reference: [10] <author> I. T. Foster and P. H. Worley, </author> <title> Parallelizing the spectral transform method: A comparison of alternative parallel algorithms, in Parallel Processing for Scientific Computing, </title> <editor> R. F. Sincovec, D. E. Keyes, M. R. Leuze, L. R. Petzold, and D. A. Reed, eds., </editor> <booktitle> Society for Industrial and Applied Mathematics, </booktitle> <address> Philadelphia, PA, </address> <year> 1993, </year> <pages> pp. 100-107. </pages>
Reference-contexts: In the course of this study, we have developed new parallel algorithms for numerical methods used in atmospheric circulation models, and evaluated these and other parallel algorithms using both testbed codes and analytic performance models <ref> [8, 10, 11, 29, 32] </ref>. We have also incorporated some of the more promising algorithms into a production parallel climate model called PCCM2 [6]. The latter development has allowed us to validate performance results obtained using simpler testbed codes, and to investigate issues such as load balancing and parallel I/O. <p> However, we restrict our attention to two dimensional decompositions, as these provide adequate parallelism on hundreds or thousands of processors and simplify code development. A variety of different decompositions, and hence parallel algorithms, are possible in spectral atmospheric models <ref> [10, 11] </ref>. Physical space is best partitioned by latitude and longitude, as a decomposition in the vertical dimension requires considerable communication in the physics component. However, Fourier and spectral space can be partitioned in several different ways. A latitude/wavenumber decomposition of Fourier space requires communication within the FFT [8, 29]. <p> We have obtained a detailed understanding of the relative performance of these different algorithms by incorporating them in a sophisticated testbed code called PSTSWM <ref> [10, 11, 33] </ref>. Extensive studies with this code have allowed us to identify optimal algorithm choices, processor mesh aspect ratios, and communication protocols for different problem sizes, computers, and numbers of processors. As much as possible, these optimal choices are incorporated in PCCM2. <p> A square or almost square processor mesh was used in most cases, as experiments indicated that these generally gave the best performance. Communication protocols are for the most part those identified as efficient in studies with PSTSWM <ref> [10, 11, 33] </ref>. 6.1. Computers. Table 1 describes the two parallel computer systems on which experiments were performed: the Intel Paragon XP/S MP 150 and the IBM SP2. These systems have similar architectures and programming models, but vary considerably in their communication and computational capabilities.
Reference: [11] <author> I. T. Foster and P. H. Worley, </author> <title> Parallel algorithms for the spectral transform method, </title> <note> submitted for publication. Also available as Tech. Report ORNL/TM-12507, </note> <institution> Oak Ridge National Laboratory, Oak Ridge, TN, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: In the course of this study, we have developed new parallel algorithms for numerical methods used in atmospheric circulation models, and evaluated these and other parallel algorithms using both testbed codes and analytic performance models <ref> [8, 10, 11, 29, 32] </ref>. We have also incorporated some of the more promising algorithms into a production parallel climate model called PCCM2 [6]. The latter development has allowed us to validate performance results obtained using simpler testbed codes, and to investigate issues such as load balancing and parallel I/O. <p> However, we restrict our attention to two dimensional decompositions, as these provide adequate parallelism on hundreds or thousands of processors and simplify code development. A variety of different decompositions, and hence parallel algorithms, are possible in spectral atmospheric models <ref> [10, 11] </ref>. Physical space is best partitioned by latitude and longitude, as a decomposition in the vertical dimension requires considerable communication in the physics component. However, Fourier and spectral space can be partitioned in several different ways. A latitude/wavenumber decomposition of Fourier space requires communication within the FFT [8, 29]. <p> We have obtained a detailed understanding of the relative performance of these different algorithms by incorporating them in a sophisticated testbed code called PSTSWM <ref> [10, 11, 33] </ref>. Extensive studies with this code have allowed us to identify optimal algorithm choices, processor mesh aspect ratios, and communication protocols for different problem sizes, computers, and numbers of processors. As much as possible, these optimal choices are incorporated in PCCM2. <p> The resulting "block" decomposition of the physical domain is illustrated in The latitude-wavenumber decomposition used for Fourier space in PCCM2 when using the double transpose parallel FFT is illustrated in Figure 1. (A different decomposition in used in the other parallel FFT <ref> [11] </ref>.) The latitude dimension is partitioned as in the physical domain, while the wavenumber dimension is partitioned into P sets 6 J. DRAKE, I. FOSTER, J. MICHALAKES, B. TOONEN, AND P. WORLEY Fig. 1. <p> However, communication is required for the FFT and LT. As noted above, communication for the FFT can be achieved in two different ways. One approach is to use a conventional parallel FFT; we do not describe this here as the basic techniques are described in <ref> [11] </ref>. An alternative approach that is more efficient in many cases uses matrix transpose operations to transform the physical space data passed to the FFT from a latitude/longitude decomposition to a latitude/vertical+field decomposition, partitioning the fields being transformed as well as the vertical layers. <p> Parallel Legendre Transform. Communication is also required for the Leg-endre transform used to move between Fourier and spectral space. In PCCM2, this operation is achieved using a parallel vector sum algorithm; other algorithms are slightly more efficient in some cases <ref> [11, 32] </ref>, but are harder to integrate into PCCM2. 8 J. DRAKE, I. FOSTER, J. MICHALAKES, B. TOONEN, AND P. WORLEY The forward and inverse Legendre transforms are ~ m J X ~ m ( j )P m and N (m) X ~ m n ( j ) respectively. <p> This sum can be computed in a number of ways; we support both a variant of the recursive halving algorithm [28], and a ring algorithm <ref> [11] </ref>. For the inverse transform, calculation of ~ m ( j ) requires only spectral coefficients associated with wavenumber m, all of which are local to every processor in the corresponding processor column. Thus, no interprocessor communication is required in the inverse transform. <p> It also leaves the vertical dimension undecomposed, which avoids the need for communication in the vertical coupling in spectral space that is required by the semi-implicit timestepping algorithm. Finally, it is easily integrated into CCM2. (Other methods, such as the interleaved ring and transpose algorithms, require substantial restructuring <ref> [11, 32] </ref>.) A disadvantage is that all computations within the spectral domain must be calculated redundantly within each processor column. However, since CCM2 performs relatively little work in the spectral domain, the redundant work has not proved to be significant. <p> A square or almost square processor mesh was used in most cases, as experiments indicated that these generally gave the best performance. Communication protocols are for the most part those identified as efficient in studies with PSTSWM <ref> [10, 11, 33] </ref>. 6.1. Computers. Table 1 describes the two parallel computer systems on which experiments were performed: the Intel Paragon XP/S MP 150 and the IBM SP2. These systems have similar architectures and programming models, but vary considerably in their communication and computational capabilities. <p> This suggests that there may be advantages to using either a distributed FFT or a O (log P ) transpose FFTs <ref> [11] </ref> when solving small problems on large number of nodes. 7. Summary. PCCM2 is a production climate model designed to execute on massively parallel computer systems. It is functionally equivalent to the shared-memory 18 J. DRAKE, I. FOSTER, J. MICHALAKES, B. TOONEN, AND P.
Reference: [12] <author> G. C. Fox, M. A. Johnson, G. A. Lyzenga, S. W. Otto, J. K. Salmon, and D. W. Walker, </author> <title> Solving Problems on Concurrent Processors, </title> <journal> vol. </journal> <volume> 1, </volume> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: Algorithmic Alternatives. The spectral transform is a composition of FFT and LT phases, and parallel FFT and LT algorithms are well understood (e.g., see <ref> [12, 23, 26, 27] </ref>).
Reference: [13] <author> H. Franke, P. Hochschild, P. Pattnaik, J.-P. Prost, and M. Snir, </author> <title> MPI-F: Current status and future directions, </title> <booktitle> Proc. Scalable Parallel Libraries Conference, </booktitle> <address> Mississippi State, October, </address> <publisher> IEEE Computer Society Press, </publisher> <year> 1994. </year>
Reference: [14] <author> U. Gartel, W. Joppich, and A. Schuller, </author> <title> Parallelizing the ECMWF's weather forecast program: The 2D case, </title> <booktitle> Parallel Computing, 19 (1993), </booktitle> <pages> pp. 1413-1426. </pages>
Reference-contexts: All these developments build on previous work on parallel algorithms for atmospheric circulation models, as described in the papers referenced above and in <ref> [1, 3, 14, 18, 19, 24] </ref>. Space does not permit a description of the primitive equations used to model the fluid dynamics of the atmosphere [30] or of CCM2.
Reference: [15] <author> J. J. Hack, B. A. Boville, B. P. Briegleb, J. T. Kiehl, P. J. Rasch, and D. L. Williamson, </author> <title> Description of the NCAR Community Climate Model (CCM2), </title> <type> NCAR Technical Note TN-382+STR, </type> <institution> National Center for Atmospheric Research, Boulder, Colo., </institution> <year> 1992. </year>
Reference-contexts: PCCM2 is a scalable parallel implementation of the National Center for Atmospheric Research (NCAR)'s Community Climate Model version 2 (CCM2). CCM2 is a comprehensive, three-dimensional global atmospheric general circulation model for use in the analysis and prediction of global climate <ref> [15, 16] </ref>. It uses two different numerical methods to simulate the fluid dynamics of the atmosphere. The spherical harmonic (spectral) transform method [7, 22] is used for the horizontal discretization of vortic-ity, divergence, temperature, and surface pressure; this method features regular, static, global data dependencies.
Reference: [16] <author> J. J. Hack, J. M. Rosinski, D. L. Williamson, B. A. Boville, and J. E. Truesdale, </author> <title> Computational design of the NCAR Community Climate Model, </title> <note> to appear in this issue of Parallel Computing, </note> <year> 1995. </year>
Reference-contexts: PCCM2 is a scalable parallel implementation of the National Center for Atmospheric Research (NCAR)'s Community Climate Model version 2 (CCM2). CCM2 is a comprehensive, three-dimensional global atmospheric general circulation model for use in the analysis and prediction of global climate <ref> [15, 16] </ref>. It uses two different numerical methods to simulate the fluid dynamics of the atmosphere. The spherical harmonic (spectral) transform method [7, 22] is used for the horizontal discretization of vortic-ity, divergence, temperature, and surface pressure; this method features regular, static, global data dependencies. <p> Sela [25] has adapted the National Meteorological Center's spectral model for use on the CM5. Hammond et al. [17] have developed a data-parallel implementation of CCM2, and Hack et al. <ref> [16] </ref> have adapted a vector multiprocessor version of CCM2 to execute on small multicomputers. All these developments build on previous work on parallel algorithms for atmospheric circulation models, as described in the papers referenced above and in [1, 3, 14, 18, 19, 24].
Reference: [17] <author> S. Hammond, R. Loft, J. Dennis, and R. Sato, </author> <title> Implementation and performance issues of a massively parallel atmospheric model, Parallel Computing, </title> <note> this issue. </note>
Reference-contexts: At the European Center for Medium-range Weather Forecasts (ECMWF), Dent [4] and his colleagues have developed the Integrated Forecast System for both vector multiprocessors and distributed memory multiprocessors. Sela [25] has adapted the National Meteorological Center's spectral model for use on the CM5. Hammond et al. <ref> [17] </ref> have developed a data-parallel implementation of CCM2, and Hack et al. [16] have adapted a vector multiprocessor version of CCM2 to execute on small multicomputers.
Reference: [18] <author> T. Kauranne and S. Barros, </author> <title> Scalability estimates of parallel spectral atmospheric models, </title> <note> in Par 20 J. </note> <author> DRAKE, I. FOSTER, J. MICHALAKES, B. TOONEN, AND P. </author> <title> WORLEY allel Supercomputing in Atmospheric Science, </title> <editor> G.-R. Hoffman and T. Kauranne, eds., </editor> <publisher> World Scientific Publishing Co. Pte. Ltd., </publisher> <address> Singapore, </address> <year> 1993, </year> <pages> pp. 312-328. </pages>
Reference-contexts: All these developments build on previous work on parallel algorithms for atmospheric circulation models, as described in the papers referenced above and in <ref> [1, 3, 14, 18, 19, 24] </ref>. Space does not permit a description of the primitive equations used to model the fluid dynamics of the atmosphere [30] or of CCM2.
Reference: [19] <author> R. D. Loft and R. K. Sato, </author> <title> Implementation of the NCAR CCM2 on the Connection Machine, </title> <booktitle> in Parallel Supercomputing in Atmospheric Science, </booktitle> <editor> G.-R. Hoffman and T. Kauranne, eds., </editor> <publisher> World Scientific Publishing Co. Pte. Ltd., </publisher> <address> Singapore, </address> <year> 1993, </year> <pages> pp. 371-393. </pages>
Reference-contexts: All these developments build on previous work on parallel algorithms for atmospheric circulation models, as described in the papers referenced above and in <ref> [1, 3, 14, 18, 19, 24] </ref>. Space does not permit a description of the primitive equations used to model the fluid dynamics of the atmosphere [30] or of CCM2.
Reference: [20] <author> J. Michalakes, </author> <title> Analysis of workload and load balancing issues in NCAR Community Climate Model, </title> <type> Technical Report ANL/MCS-TM-144, </type> <institution> Argonne National Laboratory, Argonne, Ill., </institution> <year> 1991. </year>
Reference-contexts: Load Balancing. As noted in the introduction, load imbalances can arise in parallel climate models as a result of spatial and temporal variations in the computation requirements of physics routines <ref> [20, 21] </ref>. For example, CCM2 performs radiation computations only for grid points that are in sunlight. This situation can be corrected by employing load-balancing algorithms that, either statically or dynamically, map computation to processors in different ways.
Reference: [21] <author> J. Michalakes and R. Nanjundiah, </author> <title> Computational load in model physics of the parallel NCAR Community Climate Model, </title> <type> Technical Report ANL/MCS-TM-186, </type> <institution> Argonne National Laboratory, Argonne, Ill., </institution> <year> 1994. </year>
Reference-contexts: Load Balancing. As noted in the introduction, load imbalances can arise in parallel climate models as a result of spatial and temporal variations in the computation requirements of physics routines <ref> [20, 21] </ref>. For example, CCM2 performs radiation computations only for grid points that are in sunlight. This situation can be corrected by employing load-balancing algorithms that, either statically or dynamically, map computation to processors in different ways.
Reference: [22] <author> S. A. Orszag, </author> <title> Transform method for calculation of vector-coupled sums: Application to the spectral form of the vorticity equation, </title> <journal> J. Atmos. Sci., </journal> <volume> 27, </volume> <pages> 890-895, </pages> <year> 1970. </year>
Reference-contexts: CCM2 is a comprehensive, three-dimensional global atmospheric general circulation model for use in the analysis and prediction of global climate [15, 16]. It uses two different numerical methods to simulate the fluid dynamics of the atmosphere. The spherical harmonic (spectral) transform method <ref> [7, 22] </ref> is used for the horizontal discretization of vortic-ity, divergence, temperature, and surface pressure; this method features regular, static, global data dependencies. <p> To allow exact, unaliased transforms of quadratic terms, we select I to be the minimum multiple of two such that I 3M + 1, and set J = I=2 <ref> [22] </ref>. The value of M can then be used to characterize the horizontal grid resolution, and we use the term "TM " to denote a particular discretization. Thus with M = 42, we choose I = 128 and J = 64 for T42 resolution.
Reference: [23] <author> M. Pease, </author> <title> An adaptation of the fast Fourier transform for parallel processing, </title> <journal> J. Assoc. Comput. Mach., </journal> <volume> 15 (1968), </volume> <pages> pp. 252-264. </pages>
Reference-contexts: Algorithmic Alternatives. The spectral transform is a composition of FFT and LT phases, and parallel FFT and LT algorithms are well understood (e.g., see <ref> [12, 23, 26, 27] </ref>).
Reference: [24] <author> R. B. Pelz and W. F. Stern, </author> <title> A balanced parallel algorithm for spectral global climate models, in Parallel Processing for Scientific Computing, </title> <editor> R. F. Sincovec, D. E. Keyes, M. R. Leuze, L. R. Petzold, and D. A. Reed, eds., </editor> <booktitle> Society for Industrial and Applied Mathematics, </booktitle> <address> Philadelphia, PA, </address> <year> 1993, </year> <pages> pp. 126-128. </pages>
Reference-contexts: All these developments build on previous work on parallel algorithms for atmospheric circulation models, as described in the papers referenced above and in <ref> [1, 3, 14, 18, 19, 24] </ref>. Space does not permit a description of the primitive equations used to model the fluid dynamics of the atmosphere [30] or of CCM2.
Reference: [25] <author> J.G. Sela, </author> <title> Weather forecasting on parallel architectures, </title> <note> to appear in this issue of Parallel Computing, </note> <year> 1995. </year>
Reference-contexts: However, we are not the only group to have examined parallel formulations of such models. At the European Center for Medium-range Weather Forecasts (ECMWF), Dent [4] and his colleagues have developed the Integrated Forecast System for both vector multiprocessors and distributed memory multiprocessors. Sela <ref> [25] </ref> has adapted the National Meteorological Center's spectral model for use on the CM5. Hammond et al. [17] have developed a data-parallel implementation of CCM2, and Hack et al. [16] have adapted a vector multiprocessor version of CCM2 to execute on small multicomputers.
Reference: [26] <author> P. N. Swarztrauber, </author> <title> Multiprocessor FFTs, </title> <booktitle> Parallel Computing, 5 (1987), </booktitle> <pages> pp. 197-210. </pages>
Reference-contexts: Algorithmic Alternatives. The spectral transform is a composition of FFT and LT phases, and parallel FFT and LT algorithms are well understood (e.g., see <ref> [12, 23, 26, 27] </ref>).
Reference: [27] <author> P. N. Swarztrauber, W. L. Briggs, R. A. Sweet, V. E. Henson, and J. Otto, </author> <title> Bluestein's FFT for arbitrary n on the hypercube, </title> <booktitle> Parallel Computing, 17 (1991), </booktitle> <pages> pp. 607-618. </pages>
Reference-contexts: Algorithmic Alternatives. The spectral transform is a composition of FFT and LT phases, and parallel FFT and LT algorithms are well understood (e.g., see <ref> [12, 23, 26, 27] </ref>).
Reference: [28] <author> R. A. van de Geijn, </author> <title> On Global Combine Operations, </title> <note> LAPACK Working Note 29, CS-91-129, </note> <month> April </month> <year> 1991, </year> <institution> Computer Science Department, University of Tennessee. </institution>
Reference-contexts: This sum can be computed in a number of ways; we support both a variant of the recursive halving algorithm <ref> [28] </ref>, and a ring algorithm [11]. For the inverse transform, calculation of ~ m ( j ) requires only spectral coefficients associated with wavenumber m, all of which are local to every processor in the corresponding processor column. Thus, no interprocessor communication is required in the inverse transform.
Reference: [29] <author> D. W. Walker, P. H. Worley, and J. B. Drake, </author> <title> Parallelizing the spectral transform method. Part II, </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 4 (1992), </volume> <pages> pp. 509-531. </pages>
Reference-contexts: In the course of this study, we have developed new parallel algorithms for numerical methods used in atmospheric circulation models, and evaluated these and other parallel algorithms using both testbed codes and analytic performance models <ref> [8, 10, 11, 29, 32] </ref>. We have also incorporated some of the more promising algorithms into a production parallel climate model called PCCM2 [6]. The latter development has allowed us to validate performance results obtained using simpler testbed codes, and to investigate issues such as load balancing and parallel I/O. <p> Physical space is best partitioned by latitude and longitude, as a decomposition in the vertical dimension requires considerable communication in the physics component. However, Fourier and spectral space can be partitioned in several different ways. A latitude/wavenumber decomposition of Fourier space requires communication within the FFT <ref> [8, 29] </ref>. Alternatively, a latitude/vertical decomposition of Fourier space allows FFTs to proceed without communication, if a transpose operation is first used to transform physical space from a latitude/longitude to a latitude/vertical decomposition.
Reference: [30] <author> W. Washington and C. Parkinson, </author> <title> An Introduction to Three-Dimensional Climate Modeling, </title> <publisher> University Science Books, </publisher> <address> Mill Valley, Calif., </address> <year> 1986. </year>
Reference-contexts: All these developments build on previous work on parallel algorithms for atmospheric circulation models, as described in the papers referenced above and in [1, 3, 14, 18, 19, 24]. Space does not permit a description of the primitive equations used to model the fluid dynamics of the atmosphere <ref> [30] </ref> or of CCM2. Instead, we proceed directly in Sections 2 and 3 to a description of the spectral transform method and the techniques used to parallelize it in PCCM2. In Sections 4 and 5, we describe the parallel semi-Lagrangian transport and load balancing algorithms used in PCCM2.
Reference: [31] <author> D. L. Williamson and P. J. Rasch, </author> <title> Two-dimensional semi-Lagrangian transport with shape-preserving interpolation, </title> <journal> Mon. Wea. Rev., </journal> <volume> 117, </volume> <pages> 102-129, </pages> <year> 1989. </year>
Reference-contexts: It uses two different numerical methods to simulate the fluid dynamics of the atmosphere. The spherical harmonic (spectral) transform method [7, 22] is used for the horizontal discretization of vortic-ity, divergence, temperature, and surface pressure; this method features regular, static, global data dependencies. A semi-Lagrangian transport scheme <ref> [31] </ref> is used for highly accurate advection of water vapor and an arbitrary number of other scalar fields, such 1 Oak Ridge National Laboratory, P.O. Box 2008, Bldg. 6012, Oak Ridge, TN 37831-6367. 2 Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, IL 60439. 1 2 J. DRAKE, I. <p> From this trajectory the departure point, D, is calculated, and the moisture field is interpolated at D using shape preserving interpolation <ref> [31] </ref>. SCALABLE PARALLEL COMMUNITY CLIMATE MODEL 9 PCCM2 uses a simple strategy to parallelize the SLT. Since the SLT occurs entirely in physical space, the associated calculations can be decomposed by using the latitude/longitude decomposition used to parallelize the physics and spectral transform.
Reference: [32] <author> P. H. Worley and J. B. Drake, </author> <title> Parallelizing the spectral transform method, </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 4 (1992), </volume> <pages> pp. 269-291. </pages>
Reference-contexts: In the course of this study, we have developed new parallel algorithms for numerical methods used in atmospheric circulation models, and evaluated these and other parallel algorithms using both testbed codes and analytic performance models <ref> [8, 10, 11, 29, 32] </ref>. We have also incorporated some of the more promising algorithms into a production parallel climate model called PCCM2 [6]. The latter development has allowed us to validate performance results obtained using simpler testbed codes, and to investigate issues such as load balancing and parallel I/O. <p> Parallel Legendre Transform. Communication is also required for the Leg-endre transform used to move between Fourier and spectral space. In PCCM2, this operation is achieved using a parallel vector sum algorithm; other algorithms are slightly more efficient in some cases <ref> [11, 32] </ref>, but are harder to integrate into PCCM2. 8 J. DRAKE, I. FOSTER, J. MICHALAKES, B. TOONEN, AND P. WORLEY The forward and inverse Legendre transforms are ~ m J X ~ m ( j )P m and N (m) X ~ m n ( j ) respectively. <p> It also leaves the vertical dimension undecomposed, which avoids the need for communication in the vertical coupling in spectral space that is required by the semi-implicit timestepping algorithm. Finally, it is easily integrated into CCM2. (Other methods, such as the interleaved ring and transpose algorithms, require substantial restructuring <ref> [11, 32] </ref>.) A disadvantage is that all computations within the spectral domain must be calculated redundantly within each processor column. However, since CCM2 performs relatively little work in the spectral domain, the redundant work has not proved to be significant.
Reference: [33] <author> P. H. Worley and I. Foster, </author> <year> 1994. </year> <title> Parallel spectral transform shallow water model: A runtime-tunable parallel benchmark code, </title> <booktitle> in Proc. Scalable High Performance Computing Conf., </booktitle> <publisher> IEEE Computer Society Press, Los Alamitos, Calif., </publisher> <pages> pp. 207-214. </pages>
Reference-contexts: We have obtained a detailed understanding of the relative performance of these different algorithms by incorporating them in a sophisticated testbed code called PSTSWM <ref> [10, 11, 33] </ref>. Extensive studies with this code have allowed us to identify optimal algorithm choices, processor mesh aspect ratios, and communication protocols for different problem sizes, computers, and numbers of processors. As much as possible, these optimal choices are incorporated in PCCM2. <p> A square or almost square processor mesh was used in most cases, as experiments indicated that these generally gave the best performance. Communication protocols are for the most part those identified as efficient in studies with PSTSWM <ref> [10, 11, 33] </ref>. 6.1. Computers. Table 1 describes the two parallel computer systems on which experiments were performed: the Intel Paragon XP/S MP 150 and the IBM SP2. These systems have similar architectures and programming models, but vary considerably in their communication and computational capabilities. <p> N Paragon SUNMOS 1.6.5 i860XP 16 fi 64 mesh 1024 SP2 AIX + MPL Power 2 multistage crossbar 128 Name t s (sec) t b (sec) MB/sec MFlop/sec (swap) Single Double Paragon 72 0.007 282 11.60 8.5 SP2 70 0.044 45 44.86 53.8 served by running the PSTSWM testbed code <ref> [33] </ref> on a single processor for a variety of problem resolutions, and so is an achieved peak rate rather than a theoretical peak rate. N in Table 1 and the X axis for all figures refer to the number of nodes, not the number of processors.
References-found: 33

