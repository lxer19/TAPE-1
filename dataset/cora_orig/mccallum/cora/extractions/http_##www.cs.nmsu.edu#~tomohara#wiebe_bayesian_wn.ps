URL: http://www.cs.nmsu.edu/~tomohara/wiebe_bayesian_wn.ps
Refering-URL: http://www.cs.nmsu.edu/~tomohara/
Root-URL: http://www.cs.nmsu.edu
Email: wiebe, tomohara@cs.nmsu.edu  bruce@cs.unca.edu  
Title: Constructing Bayesian Networks from WordNet for Word-Sense Disambiguation: Representational and Processing Issues  
Author: Janyce Wiebe and Tom O'Hara Rebecca Bruce 
Address: Las Cruces, NM 88003  Asheville, NC 28804-3299  
Affiliation: Department of Computer Science and Computing Research Laboratory New Mexico State University  Department of Computer Science University of North Carolina at Asheville  
Abstract: This paper describes a probabilistic model that is formed from the integration of an analytical and empirical component. The analytical component is a Bayesian network derived from WordNet, and the empirical component is composed of compatible probabilistic models formulated from tagged training data. The components are integrated in a formal, uniform framework based on the semantics of causal dependence. The paper explores various representational issues that must be addressed when formulating a Bayesian network representation of lexical information such as that expressed in Word-Net. These issues are essential to the design of such a network and they have not been previously explored. We describe two choices for the representation of lexical items and two choices for the representation lexical relations. The effect of each combination of choices on evidence propagation in the network is discussed. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bruce, R., and Wiebe, J. </author> <year> (1994), </year> <title> "Word-sense disambiguation using decomposable models", </title> <booktitle> in Proc. of the 32nd Annual Meeting of the Association for Computational Linguistics (ACL-94), </booktitle> <pages> pp 139-146. </pages>
Reference-contexts: These properties form the basis of the evidence propagation scheme used for Bayesian networks discussed in Section 7. We also make use of these properties in formulating the empirical classifiers as described in <ref> (Bruce and Wiebe, 1994) </ref>. Bayesian networks are a very rich and complex representational framework. They support easy integration of diverse information sources and form the basis for much of the current work on reasoning under uncertainty (Pearl, 1988). <p> Each classifier defines a probability distribution describing the likelihood of each sense of the targeted word given the automatically derived features of the context. An example of the type of feature used is the part-of-speech of the word to the right; see <ref> (Bruce and Wiebe, 1994) </ref> for the other ones we use. The distributions determined by the empirical classifiers are added as evidence to the Bayesian network, initiating belief propagation.
Reference: <author> Charniak, E., and R. </author> <title> Goldman (1993), "A Bayesian Model of Plan Recognition", </title> <booktitle> Artificial Intelligence 64 </booktitle> <pages> 53-79. </pages>
Reference: <author> Cowie, J., J. Guthrie, and L. </author> <month> Guthrie </month> <year> (1992), </year> <title> "Lexical Disambiguation Using Simulated Annealing", </title> <booktitle> Proc. COLING-92, </booktitle> <pages> pp. 359-365. </pages>
Reference: <author> Eizirik, L. , V. Barbosa, and S. </author> <title> Mendes (1993), "A Bayesian-Network Approach to Lexical Disambiguation", </title> <journal> Cognitive Science, </journal> <volume> 17 </volume> <pages> 257-283. </pages>
Reference: <author> Heckerman, D. and J. </author> <title> Breese (1994), "Causal Independence for Probability Assessment and Inference Using Bayesian Networks", </title> <type> Technical Report MSR-TR-94-08, </type> <institution> Microsoft Research, </institution> <note> (Revised Oc-tober, </note> <year> 1995). </year>
Reference-contexts: If no additional independence assumptions are made regarding the interactions among the parent nodes, then the number of probabilities that must be specified is exponential in n, and probabilistic inference is made correspondingly more complex <ref> (Heckerman and Breese, 1994) </ref>. To overcome this problem, the noisy-OR model (Pearl, 1988) is often adopted. Under this model, certain independence assumptions are made regarding the interactions among the parent nodes, with the effect that the number of probabilities that must be specified is linear in n.
Reference: <author> Hirst, G. </author> <year> (1988), </year> <title> "Resolving Lexical Ambiguity Computationally with Spreading Activation and Polaroid Words", in Lexical Ambiguity Resolution, </title> <editor> S. Small, G. Cottrell, and M. Tanenhaus (eds), </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers, </publisher> <pages> pp. 73-107. </pages>
Reference: <author> Lesk, M. </author> <year> (1986), </year> <title> "Automatic Sense Disambiguation Using Machine Readable Dictionaries: How to Tell a Pine Cone from an Ice Cream Cone", </title> <booktitle> in Proc. </booktitle> <address> SIGDOC, Toronto. </address>
Reference-contexts: Furthermore, heuristics were used to give preference to shorter paths and to avoid connections through nodes with many out-going arcs. There have been several approaches that have relied upon word-overlap in dictionary definitions to resolve word-sense ambiguities in context, starting with <ref> (Lesk, 1986) </ref>. Cowie et al. (1992) extend the idea by using simulated annealing to optimize a configuration of word senses simultaneously in terms of degree of word overlap. Veronis and Ide (1990) developed a neural network model to overcome the limitation of addressing only pairwise dependencies in word-overlap approaches.
Reference: <author> Miller, G., </author> <year> (1990), </year> <title> "WordNet: An On-line Lexical Database", </title> <journal> International Journal of Lexicography 3(4). </journal>
Reference-contexts: The lexical relations are derived from an existing knowledge source, because this information cannot be automatically extracted from training data with existing techniques. The knowledge source we use is the WordNet is-a hierarchy, i.e., the hyper-nym/hyponym taxonomy <ref> (Miller, 1990) </ref>. Although this hierarchy was developed for other purposes, it fl This research was supported in part by the Office of Naval Research under grant number N00014-95-1-0776. has been frequently applied to word-sense disambiguation (Resnik, 1995; Sussna, 1993).
Reference: <author> Pearl, J. </author> <year> (1988), </year> <title> Probabilistic Reasoning in Intelligent Systems, </title> <address> San Mateo, CA: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: Conditional independence relationships simplify the formulation of the joint distribution making it possible to work with a large number of variables. Further, models that characterize conditional independence relationships have desirable computational properties (e.g., see the discussion on decomposable models in <ref> (Pearl, 1988) </ref>). These properties form the basis of the evidence propagation scheme used for Bayesian networks discussed in Section 7. We also make use of these properties in formulating the empirical classifiers as described in (Bruce and Wiebe, 1994). Bayesian networks are a very rich and complex representational framework. <p> Bayesian networks are a very rich and complex representational framework. They support easy integration of diverse information sources and form the basis for much of the current work on reasoning under uncertainty <ref> (Pearl, 1988) </ref>. This paper explores the representational issues that must be addressed when mapping the lexical information in WordNet to a Bayesian network. The implications of the various choices are analyzed in depth. <p> If no additional independence assumptions are made regarding the interactions among the parent nodes, then the number of probabilities that must be specified is exponential in n, and probabilistic inference is made correspondingly more complex (Heckerman and Breese, 1994). To overcome this problem, the noisy-OR model <ref> (Pearl, 1988) </ref> is often adopted. Under this model, certain independence assumptions are made regarding the interactions among the parent nodes, with the effect that the number of probabilities that must be specified is linear in n. <p> Under this model, certain independence assumptions are made regarding the interactions among the parent nodes, with the effect that the number of probabilities that must be specified is linear in n. Basically, one need only specify the conditional probabilities of the child and each parent individually. As presented in <ref> (Pearl, 1988) </ref>, the noisy-OR model assumes that all of the variables are binary. Heck-erman and Breese (1994) present a generalization of the noisy-OR model, causal independence. In this model, the parents are assumed to be independent causes for the child. <p> In this technique, support from the empirical classifiers is added to the Bayesian network using virtual evidence nodes <ref> (Pearl, 1988) </ref>. The usual way to add evidence to a Bayesian network is to instantiate a node to a particular value (called "clamping"); the influence of this evidence is then propagated through the network.
Reference: <author> Quillian, M.(1968), </author> <title> "Semantic Memory", in Semantic Information and Processing, </title> <editor> M. Minsky, ed., </editor> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: As the evidence is dispersed among the various possibilities at subsequent nodes, the evidence for any single possibility tends to decrease. This is so for either edge direction. 8 Comparison to Related Work Spreading activation schemes have been common in various forms, starting with Quillian's <ref> (Quillian, 1968) </ref> work on semantic memory. Quillian used spreading activation to identify paths between concepts for the purpose of comparison and contrast. To construct the semantic networks, dictionary def initions were manually encoded in the form a graph. Hirst (1988) also used spreading activation to perform word-sense disambiguation.
Reference: <author> Resnik, P. </author> <year> (1995), </year> <title> "Disambiguating Noun Groupings with Respect to WordNet Senses", </title> <booktitle> in Proc. Third Workshop on Very Large Corpora, </booktitle> <address> Cam-bridge, MA, </address> <month> June </month> <year> 1995. </year>
Reference: <author> Sussna, M. </author> <year> (1993), </year> <title> "Word Sense Disambiguation for Free-text Indexing Using a Massive Semantic Network", </title> <booktitle> in Proc. Second International Conference on Information and Knowledge Management (CIKM-93), </booktitle> <address> Arlington, Virginia. </address>
Reference: <author> Veronis, J., and N. </author> <title> Ide (1990), "Word Sense Disambiguation with Very Large Neural Networks Extracted from Machine Readable Dictionaries", </title> <booktitle> in Proc. COLING-90, </booktitle> <address> Helsinki, </address> <month> August </month> <year> 1990. </year>
Reference: <author> Voorhees, E. </author> <title> (1993) "Using WordNet to Disambiguate Word Senses for Text Retrieval", </title> <booktitle> in Proc. 16th Annual ACM SIGIR Conference on Research and Development in Information Retrieval, Pitts-burgh, </booktitle> <pages> pp. 171-180. </pages>
Reference: <author> Yarowsky, D. </author> <year> (1992), </year> <title> "Word-Sense Disambiguation Using Statistical Models of Roget's Categories Trained on Large Corpora", </title> <booktitle> in Proc. COLING-92, </booktitle> <address> Nantes, </address> <month> Aug 23-28, </month> <pages> pp. 454-460. </pages>
References-found: 15

