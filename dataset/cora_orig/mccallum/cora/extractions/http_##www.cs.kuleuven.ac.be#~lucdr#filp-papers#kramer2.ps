URL: http://www.cs.kuleuven.ac.be/~lucdr/filp-papers/kramer2.ps
Refering-URL: http://www.cs.kuleuven.ac.be/~lucdr/filp.html
Root-URL: 
Email: stefan@ai.univie.ac.at  
Title: Stochastic Propositionalization of Non-Determinate Background Knowledge  
Author: Stefan Kramer 
Address: Schottengasse 3 A-1010 Vienna, Austria  
Affiliation: Austrian Research Institute for Artificial Intelligence  
Abstract: It is a well-known fact that propositional learning algorithms require "good" features to perform well in practice. So a major step in data engineering for inductive learning is the construction of good features by domain experts. These features often represent properties of structured objects, where a property typically is the occurrence of a certain substructure having certain properties. To partly automate the process of "feature engineering", we devised an algorithm that searches for features which are defined by such substructures. The algorithm stochastically conducts a top-down search for first-order clauses, where each clause represents a binary feature. It differs from existing algorithms in that its search is not class-blind, and that it is capable of considering clauses ("context") of almost arbitrary length (size). Preliminary experiments are favorable, and support the view that this approach is promising.
Abstract-found: 1
Intro-found: 1
Reference: [ Auer et al., 1995 ] <author> P. Auer, W. Maass, and R. Holte. </author> <title> Theory and applications of agnostic pac-learning with small decision trees. </title> <editor> In A. Prieditis and S. Russell, editors, </editor> <booktitle> Proceedings of the 12th International Conference on Machine Learning (ML95). </booktitle> <publisher> Morgan Kauf-mann, </publisher> <year> 1995. </year>
Reference-contexts: After propositionalization, we applied C4.5 [ Quinlan, 1993 ] . In table 1, we summarize the results for various methods in this domain. T2 <ref> [ Auer et al., 1995 ] </ref> induces 2-level decision trees. FOIL [ Quinlan, 1990 ] and Progol [ Mug-gleton, 1995 ] 2 are state-of-the-art ILP algorithms. M5 [ Quinlan, 1992 ] learns regression trees with linear regression models in the leaves. SRT [ Kramer, 1996 ] learns relational regression trees.
Reference: [ Cohen, 1994 ] <editor> W.W. Cohen. Pac-learning nondetermi-nate clauses. </editor> <booktitle> In Proc. Twelfth National Conference on Artificial Intelligence (AAAI-94), </booktitle> <year> 1994. </year>
Reference-contexts: DINUS [ Lavrac and Dzeroski, 1994 ] weakens the language bias of LINUS so that the system can learn clauses with a restricted form of new variables, namely determinate variables. This allows for the same transformation approach as the one taken in LINUS. Cohen <ref> [ Cohen, 1994 ] </ref> introduces a new restriction on non-determinate free variables called "locality constraint". This can be thought of in terms of schemata or cliches [ Silverstein and Pazzani, 1991 ] of literals.
Reference: [ Cohen, 1996 ] <author> W.W. Cohen. </author> <title> Learning trees and rules with set-valued features. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI-96), </booktitle> <pages> pages 709-716, </pages> <year> 1996. </year>
Reference-contexts: This type of transformation is called propositionalization. In general, full equivalence between the original and the transformed problem can only be achieved for certain subsets of first-order logic and certain types of background knowledge ( [ Lavrac and Dzeroski, 1994 ] , [ Co-hen, 1994 ] , <ref> [ Cohen, 1996 ] </ref> ). But even if there could be equivalent transformations theoretically, most interesting cases would still require feature subset selection. So for pragmatic reasons we should not expect the transformed problem to be equivalent to the original problem. <p> Zucker and Ganascia [ Zucker and Ganascia, 1996 ] proposed to decompose structured examples into several learning examples, which are descriptions of parts of what they call the "natural example". Cohen <ref> [ Cohen, 1996 ] </ref> introduced the notion of "set-valued features", which can be used to transform certain types of background knowledge. Srinivasan and King [ Srinivasan and King, 1996 ] presented a method for feature construction based on hypotheses returned by Progol [ Muggleton, 1995 ] .
Reference: [ Cook and Holder, 1994 ] <author> D.J. Cook and L.B. Holder. </author> <title> Substructure discovery using minimum description length and background knowledge. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 1 </volume> <pages> 231-255, </pages> <year> 1994. </year>
Reference-contexts: MILP [ Kovacic, 1994 ] is an ILP algorithm that performs stochastic search for single clauses to overcome the myopic behavior of greedy search. The outer loop of the algorithm employs the more conventional separate-and-conquer strategy. SUBDUE <ref> [ Cook and Holder, 1994 ] </ref> is an MDL-based algorithm for substructure discovery in graphs.
Reference: [ Geibel and Wysotzki, 1996 ] <author> P. Geibel and F. Wysotzki. </author> <title> Relational learning with decision trees. </title> <booktitle> In Proc. Twelfth European Conference on Artificial Intelligence (ECAI-96), </booktitle> <pages> pages 428-432, </pages> <year> 1996. </year>
Reference-contexts: In contrast to all previously discussed methods, this method works for all types of background knowledge. However, it is not yet clear why particularly these features should be useful for transforming relational learning problems. Geibel and Wysotzki <ref> [ Geibel and Wysotzki, 1996 ] </ref> propose a method for feature construction in a graph-based representation. The features are obtained through fixed-length paths in the neighborhood of a node in the graph.
Reference: [ Giordana et al., 1994 ] <author> A. Giordana, L. Saitta, and F. Zini. </author> <title> Learning disjunctive concepts by means of genetic algorithms. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 96-104, </pages> <year> 1994. </year>
Reference-contexts: The overall approach would not work, if the clauses in the population were the same extensionally. In other words, there would be no "division of labour" among the clauses (R3). (This is also the motivation for the universal suffrage selection algorithm presented in <ref> [ Giordana et al., 1994 ] </ref> .) We took a simple extension-driven approach to solve this problem: the algorithm only considers those refinements that yield clauses with an extension different from the extensions of clauses in the current population. <p> The constructed features are either "context-dependent node attributes of depth n" or "context dependent edge attributes of depth n". This method also works in general (for graphs), but using fixed-length paths obviously becomes prohibitive for large n. In contrast to SP, REGAL <ref> [ Giordana et al., 1994 ] </ref> is a concept learning algorithm. It is a full-fledged genetic algorithm. REGAL's universal suffrage selection algorithm is the first extension-driven approach to stochastic search in machine learning.
Reference: [ King and Srinivasan, 1997 ] <author> R.D. King and A. Srini-vasan. </author> <title> Prediction of rodent carcinogenicity bioassays from molecular structure using inductive logic programming. Environmental Health Perspectives, </title> <year> 1997. </year>
Reference-contexts: In most real-world domains, however, the chance of finding the correct concept during propositionalization is very small. So usually the work is divided by the propositionaliza-tion algorithm and by the subsequently applied learning algorithm. 3.2 Carcinogenicity Domain Next, we performed experiments in the carcinogenicity domain <ref> [ King and Srinivasan, 1997 ] </ref> . The database contains information about the carcinogenicity of 330 compounds, as classified by the US National Institute of Environmental Health Sciences (NIEHS). <p> Method Accuracy Default 55.00% Ames Test 63.00% C4.5 prune 58.79% C4.5 rules 60.76% T2 65.00% FOIL 25.15% Progol 63.00% SRT 72.46% SP/C4.5 prune 66.78% Table 1: Quantitative results for the carcinogenicity do main obtained by 5-fold cross-validation. 2 The experiment with Progol has been described in <ref> [ King and Srinivasan, 1997 ] </ref> . 4 Related Work In this section we briefly review related work on propo-sitionalization and stochastic search in machine learning and Inductive Logic Programming. LINUS [ Lavrac and Dzeroski, 1994 ] was the first system to transform a relational representation into a propositional representation.
Reference: [ Kovacic, 1994 ] <author> M. Kovacic. MILP: </author> <title> a stochastic approach to Inductive Logic Programming. </title> <booktitle> In Proceedings of the Fourth International Workshop on Inductive Logic Programming (ILP-94), GMD-Studien Nr. </booktitle> <volume> 237, </volume> <pages> pages 123-138, </pages> <year> 1994. </year>
Reference-contexts: In contrast to SP, REGAL [ Giordana et al., 1994 ] is a concept learning algorithm. It is a full-fledged genetic algorithm. REGAL's universal suffrage selection algorithm is the first extension-driven approach to stochastic search in machine learning. MILP <ref> [ Kovacic, 1994 ] </ref> is an ILP algorithm that performs stochastic search for single clauses to overcome the myopic behavior of greedy search. The outer loop of the algorithm employs the more conventional separate-and-conquer strategy.
Reference: [ Kramer, 1996 ] <author> S. Kramer. </author> <title> Structural regression trees. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI-96), </booktitle> <year> 1996. </year>
Reference-contexts: T2 [ Auer et al., 1995 ] induces 2-level decision trees. FOIL [ Quinlan, 1990 ] and Progol [ Mug-gleton, 1995 ] 2 are state-of-the-art ILP algorithms. M5 [ Quinlan, 1992 ] learns regression trees with linear regression models in the leaves. SRT <ref> [ Kramer, 1996 ] </ref> learns relational regression trees. The propositional learning algorithms listed here utilize global features available in addition to the non-determinate background knowledge. Quantitatively, SP/C4.5 performs quite well, and the improvement over other propositional algorithms is due to the newly constructed features.
Reference: [ Lavrac and Dzeroski, 1994 ] <author> N. Lavrac and S. Dzeroski. </author> <title> Inductive Logic Programming. </title> <publisher> Ellis Horwood, </publisher> <address> Chich-ester, UK, </address> <year> 1994. </year>
Reference-contexts: 1 Introduction A very large number of algorithms require a propositional representation, whereas many real-world learning problems are essentially relational. To bridge this gap, various researchers (e.g., <ref> [ Lavrac and Dzeroski, 1994 ] </ref> ) have proposed a transformation approach. This type of transformation is called propositionalization. <p> various researchers (e.g., <ref> [ Lavrac and Dzeroski, 1994 ] </ref> ) have proposed a transformation approach. This type of transformation is called propositionalization. In general, full equivalence between the original and the transformed problem can only be achieved for certain subsets of first-order logic and certain types of background knowledge ( [ Lavrac and Dzeroski, 1994 ] , [ Co-hen, 1994 ] , [ Cohen, 1996 ] ). But even if there could be equivalent transformations theoretically, most interesting cases would still require feature subset selection. <p> LINUS <ref> [ Lavrac and Dzeroski, 1994 ] </ref> was the first system to transform a relational representation into a propositional representation. The hypothesis language of LINUS is restricted to function-free constrained DHDB (deductive hierarchical database) clauses. This implies that no recursion is allowed, and that no new variables may be introduced. <p> <ref> [ Lavrac and Dzeroski, 1994 ] </ref> was the first system to transform a relational representation into a propositional representation. The hypothesis language of LINUS is restricted to function-free constrained DHDB (deductive hierarchical database) clauses. This implies that no recursion is allowed, and that no new variables may be introduced. DINUS [ Lavrac and Dzeroski, 1994 ] weakens the language bias of LINUS so that the system can learn clauses with a restricted form of new variables, namely determinate variables. This allows for the same transformation approach as the one taken in LINUS.
Reference: [ Muggleton, 1995 ] <author> S. Muggleton. </author> <title> Inverse Entailment and Progol. </title> <journal> New Generation Computing, </journal> <volume> 13 </volume> <pages> 245-286, </pages> <year> 1995. </year>
Reference-contexts: Cohen [ Cohen, 1996 ] introduced the notion of "set-valued features", which can be used to transform certain types of background knowledge. Srinivasan and King [ Srinivasan and King, 1996 ] presented a method for feature construction based on hypotheses returned by Progol <ref> [ Muggleton, 1995 ] </ref> . For each clause, each input-output connected subset of literals is used to define a feature. In contrast to all previously discussed methods, this method works for all types of background knowledge.
Reference: [ Quinlan, 1990 ] <author> J.R. Quinlan. </author> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 239-266, </pages> <year> 1990. </year>
Reference-contexts: After propositionalization, we applied C4.5 [ Quinlan, 1993 ] . In table 1, we summarize the results for various methods in this domain. T2 [ Auer et al., 1995 ] induces 2-level decision trees. FOIL <ref> [ Quinlan, 1990 ] </ref> and Progol [ Mug-gleton, 1995 ] 2 are state-of-the-art ILP algorithms. M5 [ Quinlan, 1992 ] learns regression trees with linear regression models in the leaves. SRT [ Kramer, 1996 ] learns relational regression trees.
Reference: [ Quinlan, 1992 ] <author> J.R. Quinlan. </author> <title> Learning with continuous classes. </title> <editor> In Sterling Adams, editor, </editor> <booktitle> Proceedings AI'92, </booktitle> <pages> pages 343-348, </pages> <address> Singapore, 1992. </address> <publisher> World Scientific. </publisher>
Reference-contexts: In table 1, we summarize the results for various methods in this domain. T2 [ Auer et al., 1995 ] induces 2-level decision trees. FOIL [ Quinlan, 1990 ] and Progol [ Mug-gleton, 1995 ] 2 are state-of-the-art ILP algorithms. M5 <ref> [ Quinlan, 1992 ] </ref> learns regression trees with linear regression models in the leaves. SRT [ Kramer, 1996 ] learns relational regression trees. The propositional learning algorithms listed here utilize global features available in addition to the non-determinate background knowledge.
Reference: [ Quinlan, 1993 ] <author> J.R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: This is an example of a combination that was found to be deactivating: new_f4 (A) :- functional_group (A, B, methyl), connected (B, C), functional_group (A, C, ring_size_5). After propositionalization, we applied C4.5 <ref> [ Quinlan, 1993 ] </ref> . In table 1, we summarize the results for various methods in this domain. T2 [ Auer et al., 1995 ] induces 2-level decision trees. FOIL [ Quinlan, 1990 ] and Progol [ Mug-gleton, 1995 ] 2 are state-of-the-art ILP algorithms.
Reference: [ Silverstein and Pazzani, 1991 ] <author> G. Silverstein and M.J. Pazzani. </author> <title> Relational cliches: Constraining constructive induction during relational learning. In L.A. </title> <editor> Birnbaum and G.C. Collins, editors, </editor> <booktitle> Machine Learning: Proceedings of the Eighth International Workshop (ML91), </booktitle> <pages> pages 203-207, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Mor-gan Kaufmann. </publisher>
Reference-contexts: Subsequently, a literal is selected for the specialization of the chosen parent clause with a probability proportional to the evaluation of the resulting clause. The refinement operator is a kind of specialization using schemata <ref> [ Silverstein and Pazzani, 1991 ] </ref> . Note that the only operator used in the algorithm is a refinement operator. Currently, the evaluation function employed is the inverse of the chi-square statistic. Due to lack of space, we cannot describe the expected frequencies here in detail. <p> This allows for the same transformation approach as the one taken in LINUS. Cohen [ Cohen, 1994 ] introduces a new restriction on non-determinate free variables called "locality constraint". This can be thought of in terms of schemata or cliches <ref> [ Silverstein and Pazzani, 1991 ] </ref> of literals. Newly introduced non-determinate variables may only be reused in literals within the same instantiated schema or cliche. Turney [ Turney, 1995 ] described a special purpose program for translating the "trains" problem of the East-West challenge into a propositional representation.
Reference: [ Srinivasan and King, 1996 ] <author> A. Srinivasan and R.D. King. </author> <title> Feature construction with Inductive Logic Programming: a study of quantitative predictions of chemical activity aided by structural attributes. </title> <booktitle> In Proceedings of the 6th International Workshop on Inductive Logic Programming (ILP-96), </booktitle> <year> 1996. </year>
Reference-contexts: Cohen [ Cohen, 1996 ] introduced the notion of "set-valued features", which can be used to transform certain types of background knowledge. Srinivasan and King <ref> [ Srinivasan and King, 1996 ] </ref> presented a method for feature construction based on hypotheses returned by Progol [ Muggleton, 1995 ] . For each clause, each input-output connected subset of literals is used to define a feature.
Reference: [ Turney, 1995 ] <author> P. Turney. </author> <title> Low size-complexity Inductive Logic Programming: the East-West challenge considered as a problem in cost-sensitive classification. </title> <booktitle> In Proceedings of the 5th International Workshop on Inductive Logic Programming (ILP-95), </booktitle> <pages> pages 247-263. </pages> <institution> Katholieke Universiteit Leuven, </institution> <year> 1995. </year>
Reference-contexts: This can be thought of in terms of schemata or cliches [ Silverstein and Pazzani, 1991 ] of literals. Newly introduced non-determinate variables may only be reused in literals within the same instantiated schema or cliche. Turney <ref> [ Turney, 1995 ] </ref> described a special purpose program for translating the "trains" problem of the East-West challenge into a propositional representation.
Reference: [ Zucker and Ganascia, 1996 ] <author> J.D. Zucker and J.G. Ganascia. </author> <title> Representation changes for efficient learning in structural domains. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <pages> pages 543-551, </pages> <year> 1996. </year>
Reference-contexts: Newly introduced non-determinate variables may only be reused in literals within the same instantiated schema or cliche. Turney [ Turney, 1995 ] described a special purpose program for translating the "trains" problem of the East-West challenge into a propositional representation. Zucker and Ganascia <ref> [ Zucker and Ganascia, 1996 ] </ref> proposed to decompose structured examples into several learning examples, which are descriptions of parts of what they call the "natural example". Cohen [ Cohen, 1996 ] introduced the notion of "set-valued features", which can be used to transform certain types of background knowledge.
References-found: 18

