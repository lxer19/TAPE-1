URL: http://www.cs.princeton.edu/prism/papers-ps/bilas-ima96-invited.ps
Refering-URL: http://www.cs.princeton.edu/prism/html/all-papers.html
Root-URL: http://www.cs.princeton.edu
Email: fBILAS, LIV, RUDRO, JPSg@CS.PRINCETON.EDU  
Title: SUPPORTING A COHERENT SHARED ADDRESS SPACE ACROSS SMP NODES: AN APPLICATION-DRIVEN INVESTIGATION  
Author: ANGELOS BILAS, LIVIU IFTODE, RUDRAJIT SAMANTA AND JASWINDER PAL SINGH 
Address: 35 OLDEN STREET  PRINCETON, NJ 08544  
Affiliation: DEPARTMENT OF COMPUTER SCIENCE  PRINCETON UNIVERSITY  
Abstract: As the workstation market moves form single processor to small-scale shared memory multiprocessors, it is very attractive to construct larger-scale multiprocessors by connecting symmetric multiprocessors (SMPs) with efficient commodity network interfaces such as Myrinet. With hardware-supported cache-coherent shared memory within the SMPs, the question is what programming model to support across SMPs. A coherent shared address space has been found to be attractive for a wide range of applications, and shared virtual memory (SVM) protocols have been developed to provide this model in software at page granularity across uniprocessor nodes. It is therefore attractive to extend SVM protocols to efficiently incorporate SMP nodes, instead of using a hybrid programming model with a shared address space within SMP nodes and explicit message passing across them. The protocols should be optimized to exploit the efficient hardware sharing within an SMP as much as possible, and invoke the less efficient software protocol across nodes as infrequently as possible. We present a home-based SVM protocol that was designed with these goals in mind. We then use detailed, application-driven simulations to understand how successful such a protocol might be and particularly whether and to what extent the use of SMP nodes improves performance over the traditional method of using SVM across uniprocessor nodes. We examine cases where the home-based SVM protocol across nodes is supported entirely in software, and where the propagation of modifications to the home is supported at fine grain in hardware. We analyze how the characteristics of our ten applications and their algorithms interact with the use of SMP nodes, to see what classes of applications do and do not benefit from SMP nodes, and determine the major bottlenecks that stand in the way of improved performance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. H. Bailey, </author> <title> FFTs in External or Hierarchical Memories, </title> <journal> Journal of Supercomputing, </journal> <volume> 4 (1990), </volume> <pages> pp. 23-25. </pages>
Reference-contexts: Protocol action is required only to fetch pages. The applications have different inherent and induced communication patterns [16, 28], which affect their performance and the impact on SMP nodes. FFT: The FFT kernel is a complex 1-D version of the radix p n six-step FFT algorithm described in <ref> [1] </ref>, which is optimized to minimize interprocessor communication. The data set consists of the n complex data points to be transformed and another n complex data points referred to as the roots of unity. <p> Obviously f N 2 <ref> [0; 1] </ref>. Note that the costs in the sequential execution are not directly involved in this definition. Alternative definitions with similar goals are also possible.
Reference: [2] <author> A. Bilas, L. Iftode, D. Martin, and J. Singh, </author> <title> Shared virtual memory across SMP nodes using automatic update: Protocols and performance, </title> <type> Tech. Rep. </type> <institution> TR-517-96, Princeton, NJ, </institution> <month> Mar. </month> <year> 1996. </year>
Reference-contexts: Erlichson et al. [12] conclude that the transition from uniprocessor to multiprocessor nodes is nontrivial, and performance can be seriously affected unless great care is taken. However, the protocol and system they assume is very different from ours. <ref> [2] </ref> is a preliminary version of this work. For this work, we use a much more detailed simulation environment and improved implementations of the the protocols. 10. Summary and Conclusions.
Reference: [3] <author> G. E. Blelloch, C. E. Leiserson, B. M. Maggs, C. G. Plaxton, S. J. Smith, and M. Zagha, </author> <title> A comparison of sorting algorithms for the connection machine CM-2, </title> <booktitle> in Proceedings of the 8th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <month> July </month> <year> 1991, </year> <pages> pp. 3-16. </pages>
Reference-contexts: It uses a different tree-building algorithm, where each processor first builds its own partial tree, and all partial trees are merged to the global tree after each computation phase. Radix: The integer radix sort kernel is based on the method described in <ref> [3] </ref>. The algorithm is iterative, performing one iteration for each radix r digit of the keys. In each iteration, a processor passes over its assigned keys and generates a local histogram. The local histograms are then accumulated into a global histogram.
Reference: [4] <author> M. Blumrich, K. Li, R. Alpert, C. Dubnicki, E. Felten, and J. Sandberg, </author> <title> A virtual memory mapped network interface for the shrimp multicomputer, </title> <booktitle> in Proceedings of the 21st Annual Symposium on Computer Architecture, </booktitle> <month> Apr. </month> <year> 1994, </year> <pages> pp. 142-153. </pages>
Reference-contexts: The SVM protocol can operate completely in software, or can exploit hardware support for automatic update propagation of writes to remote memories as supported in the SHRIMP multicomputer <ref> [4] </ref> (and in a different way in the DEC Memory Channel [13]). Having described the protocol, we use detailed simulation to examine how using k, c-processor SMPs connected this way compares in performance to using SVM across k fl c uniprocessor nodes, and whether the performance characteristics look promising overall. <p> Some recent network interfaces also provide hardware support for the propagation of writes at fine granularity (a word or a cache line, say) to a remotely mapped page of memory <ref> [4, 13] </ref>. This facility can be used to accelerate home-based protocols by eliminating the need for diffs, leading to a protocol called automatic update release consistency or AURC [16].
Reference: [5] <author> N. J. Boden, D. Cohen, R. E. Felderman, A. E. Kulawik, C. L. Seitz, J. N. Seizovic, and W.-K. Su, Myrinet: </author> <title> A gigabit-per-second local area network, </title> <journal> IEEE Micro, </journal> <volume> 15 (1995), </volume> <pages> pp. 29-36. </pages>
Reference-contexts: Simulated Platforms. The simulation environment we use is built on top of augmint [24], an execution driven simulator using the x86 instruction set runs on x86 systems. The simulated architecture (Figure 7) assumes a cluster of c-processor SMPs connected with a commodity interconnect like Myrinet <ref> [5] </ref>. Contention is modeled at all levels except the network links. The processor is P6-like, but is assumed to be a 1 intstruction per cycle (IPC) processor.
Reference: [6] <author> J. P. S. Chris Holt and J. Hennessy, </author> <title> Architectural and application bottlenecks in scalable DSM multiprocessors, </title> <booktitle> in Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Finally, each processor uses the global histogram to permute its keys into a new array for the next iteration. This permutation step requires all-to-all, irregular communication. The permutation is inherently a sender-determined one, so keys are communicated through scattered, irregular writes to remotely allocated data. See <ref> [6, 29] </ref> for details. Raytrace: This application renders a three-dimensional scene using ray tracing. A hierarchical uniform grid (similar to an octree) is used to represent the scene, and early ray termination and antialiasing are implemented, although antialiasing is not used in this study.
Reference: [7] <author> A. Cox, S. Dwarkadas, P. Keleher, H. Lu, R. Rajamony, and W. Zwaenepoel, </author> <title> Software versus hardware shared-memory implementation: A case study, </title> <booktitle> in Proceedings of the 21st Annual Symposium on Computer Architecture, </booktitle> <month> Apr. </month> <year> 1994, </year> <pages> pp. 106-117. </pages>
Reference-contexts: Holt et al. in [14] present a metric similar to protocol efficiency. They use it to characterize the performance of applications on a hardware cache coherent machine. Several papers have discussed the design and performance of shared virtual memory for SMPs <ref> [7, 12, 18, 23] </ref> for different protocols. Erlichson et al. [12] conclude that the transition from uniprocessor to multiprocessor nodes is nontrivial, and performance can be seriously affected unless great care is taken.
Reference: [8] <author> D. Culler and J. P. Singh, </author> <title> Parallel Computer Architecture, </title> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1998. </year>
Reference-contexts: The major advantages and disadvantages of a shared address space programming abstraction compared to explicit message passing are described in <ref> [8] </ref> (Chapter 3) and [25] and will not be covered here . Section 2 introduces the uniprocessor home-based protocols and describes the extensions to use SMP nodes. It identifies many of the tradeoffs that arise in designing such a protocol, and the positions that our protocol takes along them.
Reference: [9] <author> C. Dubnicki, A. Bilas, K. Li, and J. Philbin, </author> <title> Design and implementation of virtual memory-mapped communication on myrinet, </title> <booktitle> in Proceedings of the 1997 International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1997. </year>
Reference-contexts: Each network interface (NI) has two 1 MByte memory queues for incoming and outgoing packets. Network links operate at processor speed and are 16 bits wide. We assume a fast messaging system <ref> [9, 10, 22] </ref> that supports explicit messages. Initiating a message takes on the order of tens of I/O bus cycles. If the network queues fill, the NI interrupts the main processor and delays it to allow queues to drain. <p> This results in an uncontended network bandwidth of about 70-75 MBytes/s for a one-way page transfer, out of the theoretical 100 MBytes/s of the NI. The latency and bandwidth numbers are in agreement with the reported performance numbers for a real implementation <ref> [9] </ref>. Uncontended lock acquisition from a remote node costs 5800 processor cycles or 29s with no write notices or page fetches in the critical section, and from the local node it is about 2100 cycles.
Reference: [10] <author> T. Eicken, D. Culler, S. Goldstein, and K. Schauser, </author> <title> Active messages: A mechanism for integrated communication and computation, </title> <booktitle> in Proceedings of the 19th Annual Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992, </year> <pages> pp. 256-266. </pages>
Reference-contexts: Each network interface (NI) has two 1 MByte memory queues for incoming and outgoing packets. Network links operate at processor speed and are 16 bits wide. We assume a fast messaging system <ref> [9, 10, 22] </ref> that supports explicit messages. Initiating a message takes on the order of tens of I/O bus cycles. If the network queues fill, the NI interrupts the main processor and delays it to allow queues to drain.
Reference: [11] <author> A. Erlichson, B. Nayfeh, J. Singh, and K. Olukotun, </author> <title> The benefits of clustering in shared address space multiprocessors: An applications-driven investigation., </title> <booktitle> in Supercomputing '95, </booktitle> <year> 1995, </year> <pages> pp. 176-186. </pages>
Reference-contexts: Clustering processors together using a faster and finer-grained communication mechanism has some obvious advantages; namely prefetching, cache-to-cache sharing, and overlapping working sets <ref> [11] </ref>. The hope is that for many applications a significant fraction of the interprocessor communication may be contained within each SMP node. This reduces the amount of expensive (high latency and overhead) cross-node SVM communication needed during the application's execution.
Reference: [12] <author> A. Erlichson, N. Nuckolls, G. Chesson, and J. Hennessy, SoftFLASH: </author> <title> analyzing the performance of clustered distributed virtual shared memory, </title> <booktitle> in The 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1996, </year> <pages> pp. 210-220. </pages>
Reference-contexts: Holt et al. in [14] present a metric similar to protocol efficiency. They use it to characterize the performance of applications on a hardware cache coherent machine. Several papers have discussed the design and performance of shared virtual memory for SMPs <ref> [7, 12, 18, 23] </ref> for different protocols. Erlichson et al. [12] conclude that the transition from uniprocessor to multiprocessor nodes is nontrivial, and performance can be seriously affected unless great care is taken. <p> They use it to characterize the performance of applications on a hardware cache coherent machine. Several papers have discussed the design and performance of shared virtual memory for SMPs [7, 12, 18, 23] for different protocols. Erlichson et al. <ref> [12] </ref> conclude that the transition from uniprocessor to multiprocessor nodes is nontrivial, and performance can be seriously affected unless great care is taken. However, the protocol and system they assume is very different from ours. [2] is a preliminary version of this work.
Reference: [13] <author> R. Gillett, M. Collins, and D. Pimm, </author> <title> Overview of network memory channel for PCI, </title> <booktitle> in Proceedings of the IEEE Spring COMPCON '96, </booktitle> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: The SVM protocol can operate completely in software, or can exploit hardware support for automatic update propagation of writes to remote memories as supported in the SHRIMP multicomputer [4] (and in a different way in the DEC Memory Channel <ref> [13] </ref>). Having described the protocol, we use detailed simulation to examine how using k, c-processor SMPs connected this way compares in performance to using SVM across k fl c uniprocessor nodes, and whether the performance characteristics look promising overall. <p> Some recent network interfaces also provide hardware support for the propagation of writes at fine granularity (a word or a cache line, say) to a remotely mapped page of memory <ref> [4, 13] </ref>. This facility can be used to accelerate home-based protocols by eliminating the need for diffs, leading to a protocol called automatic update release consistency or AURC [16].
Reference: [14] <author> C. Holt, M. Heinrich, J. P. Singh, , and J. L. Hennessy, </author> <title> The effects of latency and occupancy on the performance of dsm multiprocessors, </title> <type> Tech. Rep. </type> <institution> CSL-TR-95-xxx, Stanford University, </institution> <year> 1995. </year>
Reference-contexts: Related Work. Our study on uniprocessor systems is consistent with the results obtained in [16] for their slow bus case, although we compare AURC against HLRC instead of LRC. HLRC and LRC have been compared for some applications on the Intel Paragon in [30]. Holt et al. in <ref> [14] </ref> present a metric similar to protocol efficiency. They use it to characterize the performance of applications on a hardware cache coherent machine. Several papers have discussed the design and performance of shared virtual memory for SMPs [7, 12, 18, 23] for different protocols.
Reference: [15] <author> L. Iftode, C. Dubnicki, E. W. Felten, and K. Li, </author> <title> Improving release-consistent shared virtual memory using automatic update, </title> <booktitle> in The 2nd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: Both protocols use timestamps to maintain ordering of events. The rest of this section first briefly discusses lazy release consistency, HLRC and AURC for uniprocessor nodes. More detailed descriptions can be found in the literature <ref> [15, 20, 30] </ref>. Then, we discuss the major design choices for extending the protocols to use SMP nodes efficiently, and the the specific choices made in our implementation. 2.1. Lazy Release Consistency. Lazy Release Consistency is a particular implementation of release consistency (RC).
Reference: [16] <author> L. Iftode, J. P. Singh, and K. Li, </author> <title> Understanding application performance on shared virtual memory, </title> <booktitle> in Proceedings of the 23rd Annual Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: This facility can be used to accelerate home-based protocols by eliminating the need for diffs, leading to a protocol called automatic update release consistency or AURC <ref> [16] </ref>. Now, when a processor writes to pages that are remotely mapped (i.e. writes to a page whose home memory is remote), these writes are automatically propagated in hardware and merged into the home page, which is thus always kept up to date. <p> Studies on different platforms have indicated that home-based protocols outperform outperform traditional LRC implementations, at least on the platform and applications tested, and also incur much smaller memory overhead <ref> [16, 30] </ref>. Having understood the basic protocol ideas, let us proceed to the main goal of this paper, to examine how and how well the protocols can be used to extend a coherent shared address space in software across SMP nodes. 3. Extending Home-based Protocols to SMP Nodes. 3.1. <p> Applications. In our evaluation we use the SPLASH-2 [28] application suite. We will now briefly describe the basic characteristics of each application. A more detailed classification and description of the application behavior for SVM systems with uniprocessor nodes is provided in the context of AURC and LRC in <ref> [16] </ref>. The applications can be divided in two groups, regular and irregular. 6.1. Regular Applications. The applications in this category are FFT, LU and Ocean. <p> In AURC we do not need to use a write through cache policy, and in HLRC we do not need to compute diffs. Protocol action is required only to fetch pages. The applications have different inherent and induced communication patterns <ref> [16, 28] </ref>, which affect their performance and the impact on SMP nodes. FFT: The FFT kernel is a complex 1-D version of the radix p n six-step FFT algorithm described in [1], which is optimized to minimize interprocessor communication. <p> We will separate these out, calling the former wait time and the latter protocol cost. As we saw in Section 5, wait time for locks is often increased greatly in SVM systems due to page misses occurring frequently inside critical sections and increasing serialization <ref> [16] </ref>, as well as to increased protocol activity at locks, which has the same effect. This makes locks much more expensive for SVM systems than for hardware-coherent systems. <p> Thus the performance of HLRC-1 is much better than AURC-1 and the benefit of using SMP nodes is much smaller. 9. Related Work. Our study on uniprocessor systems is consistent with the results obtained in <ref> [16] </ref> for their slow bus case, although we compare AURC against HLRC instead of LRC. HLRC and LRC have been compared for some applications on the Intel Paragon in [30]. Holt et al. in [14] present a metric similar to protocol efficiency.
Reference: [17] <author> D. Jiang, H. Shan, and J. P. Singh, </author> <title> Application restructuring and performance portability on shared virtual memory and hardware-coherent multiprocessors, </title> <booktitle> in Sixth ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: Access patterns are irregular and fine-grained. We use two versions of Barnes, which differ in how the shared octree is built and managed across time-steps. The first version (Barnes-rebuild) builds the tree from scratch after each computation phase. The second version, Barnes (space) <ref> [17] </ref>, is optimized for SVM implementations|in which synchronization is expensive|and it avoids locking as much as possible. It uses a different tree-building algorithm, where each processor first builds its own partial tree, and all partial trees are merged to the global tree after each computation phase.
Reference: [18] <author> M. Karlsson and P. Stenstrom, </author> <title> Performance evaluation of cluster-based multiprocessor built from atm switches and bus-based multiprocessor servers, </title> <booktitle> in The 2nd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: To reduce interrupts we can instruct idle compute processors to poll for requests and interrupt a random compute processor only when there are no idle processors <ref> [18] </ref>. It is interesting to note that each solution presented for protocol handling is expected to perform better but is more complex than the previous one. On a real system some choices may be difficult or too expensive to perform due to architectural and operating system limitations. <p> Holt et al. in [14] present a metric similar to protocol efficiency. They use it to characterize the performance of applications on a hardware cache coherent machine. Several papers have discussed the design and performance of shared virtual memory for SMPs <ref> [7, 12, 18, 23] </ref> for different protocols. Erlichson et al. [12] conclude that the transition from uniprocessor to multiprocessor nodes is nontrivial, and performance can be seriously affected unless great care is taken.
Reference: [19] <author> P. Keleher, A. Cox, S. Dwarkadas, and W. Zwaenepoel, Treadmarks: </author> <title> Distributed shared memory on standard workstations and operating systems, </title> <booktitle> in Proceedings of the Winter USENIX Conference, </booktitle> <month> Jan. </month> <year> 1994, </year> <pages> pp. 115-132. 21 </pages>
Reference-contexts: To reduce the impact of write-write false sharing LRC has most commonly been used with a software or hardware supported multiple-writer scheme. The first software-based multiple writer scheme was used in the TreadMarks system from Rice University <ref> [19, 20] </ref>. In this scheme, every writer records any changes it makes to a shared page during each time interval. When a processor first writes a page during a new interval it saves a copy of the page, called a twin, before writing to it.
Reference: [20] <author> P. Keleher, A. Cox, and W. Zwaenepoel, </author> <title> Lazy consistency for software distributed shared memory, </title> <booktitle> in Proceedings of the 19th Annual Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992, </year> <pages> pp. 13-21. </pages>
Reference-contexts: Relaxed memory consistency models are used to reduce the frequency of invocation of the expensive software protocol operations <ref> [20] </ref>. Much research has been done in this area, and many good protocols developed. One way to provide the programming model of choice in clusters, then, is to extend these SVM protocols to use multiprocessor (SMP) rather than uniprocessor nodes. <p> Both protocols use timestamps to maintain ordering of events. The rest of this section first briefly discusses lazy release consistency, HLRC and AURC for uniprocessor nodes. More detailed descriptions can be found in the literature <ref> [15, 20, 30] </ref>. Then, we discuss the major design choices for extending the protocols to use SMP nodes efficiently, and the the specific choices made in our implementation. 2.1. Lazy Release Consistency. Lazy Release Consistency is a particular implementation of release consistency (RC). <p> RC is a memory consistency model that guarantees memory consistency only at synchronization points. These are marked as acquire or release operations. In implementations of an eager variation of release consistency the updates to shared data are performed globally at each release operation. Lazy Release Consistency (LRC) <ref> [20] </ref> is a relaxed implementation of RC which further reduces the read-write false sharing by postponing the coherence actions from the release to the next related acquire operation. <p> To reduce the impact of write-write false sharing LRC has most commonly been used with a software or hardware supported multiple-writer scheme. The first software-based multiple writer scheme was used in the TreadMarks system from Rice University <ref> [19, 20] </ref>. In this scheme, every writer records any changes it makes to a shared page during each time interval. When a processor first writes a page during a new interval it saves a copy of the page, called a twin, before writing to it.
Reference: [21] <author> J. Nieh and M. Levoy, </author> <title> Volume rendering on scalable shared-memory MIMD architectures, </title> <booktitle> in Proceedings of the Boston Workshop on Volume Visualization, </booktitle> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: The partitioning and task queues are similar to those in Raytrace. The main data structures are the voxels, octree, and pixels. Data accesses are input-dependent and irregular, and no attempt is made at intelligent data distribution. See <ref> [21] </ref> for details. The version we use is also slightly modified from the SPLASH-2 version [28], to provide a better initial assignment of tasks to processes before stealing. This improves SVM performance greatly. Inherent communication volume is small.
Reference: [22] <author> S. Pakin, M. Buchanan, M. Lauria, and A. Chien, </author> <title> The Fast Messages (FM) 2.0 streaming interface. </title> <note> Submitted to Usenix'97, </note> <year> 1996. </year>
Reference-contexts: Each network interface (NI) has two 1 MByte memory queues for incoming and outgoing packets. Network links operate at processor speed and are 16 bits wide. We assume a fast messaging system <ref> [9, 10, 22] </ref> that supports explicit messages. Initiating a message takes on the order of tens of I/O bus cycles. If the network queues fill, the NI interrupts the main processor and delays it to allow queues to drain.
Reference: [23] <author> D. Scales, K. Gharachorloo, and C. Thekkath, </author> <title> Shasta: A low overhead, software-only approach for supporting fine-grain shared memory, </title> <booktitle> in The 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: Holt et al. in [14] present a metric similar to protocol efficiency. They use it to characterize the performance of applications on a hardware cache coherent machine. Several papers have discussed the design and performance of shared virtual memory for SMPs <ref> [7, 12, 18, 23] </ref> for different protocols. Erlichson et al. [12] conclude that the transition from uniprocessor to multiprocessor nodes is nontrivial, and performance can be seriously affected unless great care is taken.
Reference: [24] <author> A. Sharma, A. T. Nguyen, J. Torellas, M. Michael, and J. Carbajal, Augmint: </author> <title> a multiprocessor simulation environment for Intel x86 architectures, </title> <type> tech. rep., </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> March </month> <year> 1996. </year>
Reference-contexts: In the simulator the barrier owner sends all write notices to all nodes, and they decide which ones are relevant. 4. Simulated Platforms. The simulation environment we use is built on top of augmint <ref> [24] </ref>, an execution driven simulator using the x86 instruction set runs on x86 systems. The simulated architecture (Figure 7) assumes a cluster of c-processor SMPs connected with a commodity interconnect like Myrinet [5]. Contention is modeled at all levels except the network links.
Reference: [25] <author> J. P. Singh, A. Gupta, and J. L. Hennessy, </author> <title> Implications of hierarchical N-body techniques for multiprocessor architecture, </title> <journal> ACM Transactions on Computer Systems, </journal> <note> (1995). To appear. Early version available as Stanford Univeristy Tech. Report no. CSL-TR-92-506, </note> <month> January </month> <year> 1992. </year>
Reference-contexts: The major advantages and disadvantages of a shared address space programming abstraction compared to explicit message passing are described in [8] (Chapter 3) and <ref> [25] </ref> and will not be covered here . Section 2 introduces the uniprocessor home-based protocols and describes the extensions to use SMP nodes. It identifies many of the tradeoffs that arise in designing such a protocol, and the positions that our protocol takes along them.
Reference: [26] <author> J. P. Singh, A. Gupta, and M. Levoy, </author> <title> Parallel visualization algorithms: Performance and architectural implications, </title> <booktitle> IEEE Computer, </booktitle> <month> 27 </month> <year> (1994). </year>
Reference-contexts: The major data structures represent rays, ray trees, the hierarchical uniform grid, task queues, and the primitives that describe the scene. The data access patterns are highly unpredictable in this application. See <ref> [26] </ref> for more information. The version we use is modified from the SPLASH-2 version [28] to run more efficiently on SVM systems.
Reference: [27] <author> K. Skadron and D. W. Clark, </author> <title> Design issues and tradeoffs for write buffers, </title> <booktitle> in The 3nd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> Feb </month> <year> 1997. </year>
Reference-contexts: The processor is P6-like, but is assumed to be a 1 intstruction per cycle (IPC) processor. The data cache hierarchy consists of a 8 KBytes first-level direct mapped write-through cache and a 512 KBytes second-level two-way set associative cache, each with a line size of 32B. The write buffer <ref> [27] </ref> has 26 entries, 1 cache line wide each, and a retire-at-4 policy. Write buffer stalls are simulated. The read hit cost is one cycle in the write buffer and first level cache and 10 cycles in the second-level cache. The memory subsystem is fully pipelined.
Reference: [28] <author> S. Woo, M. Ohara, E. Torrie, J. Singh, and A. Gupta, </author> <title> Methodological considerations and characterization of the SPLASH-2 parallel application suite, </title> <booktitle> in Proceedings of the 23rd Annual Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1995. </year>
Reference-contexts: The cost of barrier synchronization can be seen to go up sharply with the number of write notices produced in the previous interval, since these are communicated to the barrier master and from there to all nodes. 6. Applications. In our evaluation we use the SPLASH-2 <ref> [28] </ref> application suite. We will now briefly describe the basic characteristics of each application. A more detailed classification and description of the application behavior for SVM systems with uniprocessor nodes is provided in the context of AURC and LRC in [16]. <p> In AURC we do not need to use a write through cache policy, and in HLRC we do not need to compute diffs. Protocol action is required only to fetch pages. The applications have different inherent and induced communication patterns <ref> [16, 28] </ref>, which affect their performance and the impact on SMP nodes. FFT: The FFT kernel is a complex 1-D version of the radix p n six-step FFT algorithm described in [1], which is optimized to minimize interprocessor communication. <p> The major data structures represent rays, ray trees, the hierarchical uniform grid, task queues, and the primitives that describe the scene. The data access patterns are highly unpredictable in this application. See [26] for more information. The version we use is modified from the SPLASH-2 version <ref> [28] </ref> to run more efficiently on SVM systems. <p> The main data structures are the voxels, octree, and pixels. Data accesses are input-dependent and irregular, and no attempt is made at intelligent data distribution. See [21] for details. The version we use is also slightly modified from the SPLASH-2 version <ref> [28] </ref>, to provide a better initial assignment of tasks to processes before stealing. This improves SVM performance greatly. Inherent communication volume is small. Water: This application evaluates forces and potentials that occur over time in a system of water molecules.
Reference: [29] <author> S. C. Woo, J. P. Singh, and J. L. Hennessy, </author> <title> The performance advantages of integrating message-passing in cache-coherent multiprocessors, </title> <booktitle> in Proceedings of Architectural Support For Programming Languages and Operating Systems, </booktitle> <year> 1994. </year>
Reference-contexts: The transposes are blocked to exploit cache line reuse. To avoid memory hot-spotting, submatrices are communicated in a staggered fashion, with processor i first transposing a submatrix from processor i + 1; then one from processor i + 2; etc. More details can be found in <ref> [29] </ref>. We use two problem sizes, 256K (512x512) and 1M (1024x1024) 11 elements. LU: The LU kernel factors a dense matrix into the product of a lower triangular and an upper triangular matrix. <p> Fairly small block sizes (B=8 or B=16) strike a good balance in practice. Elements within a block are allocated contiguously to improve spatial locality benefits, and blocks are allocated locally to processors that own them. See <ref> [29] </ref> for more details. We use two versions of LU that differ in their organization of the matrix data structure. The contiguous version of LU uses a four-dimensional array to represent the two-dimensional matrix, so that a block is contiguous in the virtual address space. <p> Finally, each processor uses the global histogram to permute its keys into a new array for the next iteration. This permutation step requires all-to-all, irregular communication. The permutation is inherently a sender-determined one, so keys are communicated through scattered, irregular writes to remotely allocated data. See <ref> [6, 29] </ref> for details. Raytrace: This application renders a three-dimensional scene using ray tracing. A hierarchical uniform grid (similar to an octree) is used to represent the scene, and early ray termination and antialiasing are implemented, although antialiasing is not used in this study.
Reference: [30] <author> Y. Zhou, L. Iftode, and K. Li, </author> <title> Performance evaluation of two home-based lazy release consistency protocols for shared virtual memory systems, </title> <booktitle> in Proceedings of the Operating Systems Design and Implementation Symposium, </booktitle> <month> Oct. </month> <year> 1996. </year> <month> 22 </month>
Reference-contexts: Both protocols use timestamps to maintain ordering of events. The rest of this section first briefly discusses lazy release consistency, HLRC and AURC for uniprocessor nodes. More detailed descriptions can be found in the literature <ref> [15, 20, 30] </ref>. Then, we discuss the major design choices for extending the protocols to use SMP nodes efficiently, and the the specific choices made in our implementation. 2.1. Lazy Release Consistency. Lazy Release Consistency is a particular implementation of release consistency (RC). <p> Studies on different platforms have indicated that home-based protocols outperform outperform traditional LRC implementations, at least on the platform and applications tested, and also incur much smaller memory overhead <ref> [16, 30] </ref>. Having understood the basic protocol ideas, let us proceed to the main goal of this paper, to examine how and how well the protocols can be used to extend a coherent shared address space in software across SMP nodes. 3. Extending Home-based Protocols to SMP Nodes. 3.1. <p> Related Work. Our study on uniprocessor systems is consistent with the results obtained in [16] for their slow bus case, although we compare AURC against HLRC instead of LRC. HLRC and LRC have been compared for some applications on the Intel Paragon in <ref> [30] </ref>. Holt et al. in [14] present a metric similar to protocol efficiency. They use it to characterize the performance of applications on a hardware cache coherent machine. Several papers have discussed the design and performance of shared virtual memory for SMPs [7, 12, 18, 23] for different protocols.
References-found: 30

