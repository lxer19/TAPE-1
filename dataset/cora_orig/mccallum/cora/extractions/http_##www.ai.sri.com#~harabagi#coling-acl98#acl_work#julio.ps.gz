URL: http://www.ai.sri.com/~harabagi/coling-acl98/acl_work/julio.ps.gz
Refering-URL: http://www.ai.sri.com/~harabagi/coling-acl98/acl_work/acl_work.html
Root-URL: 
Email: fjulio,felisa,irina,juancig@ieec.uned.es  
Title: Indexing with WordNet synsets can improve text retrieval  
Author: Julio Gonzalo and Felisa Verdejo and Irina Chugur and Juan Cigarran UNED 
Address: 28040 Madrid Spain  
Affiliation: Ciudad Universitaria, s.n.  
Abstract: The classical, vector space model for text retrieval is shown to give better results (up to 29% better in our experiments) if WordNet synsets are chosen as the indexing space, instead of word forms. This result is obtained for a manually disambiguated test collection (of queries and documents) derived from the Semcor semantic concordance. The sensitivity of retrieval performance to (automatic) disambiguation errors when indexing documents is also measured. Finally, it is observed that if queries are not disambiguated, indexing by synsets performs (at best) only as good as standard word indexing. 
Abstract-found: 1
Intro-found: 1
Reference: <author> J. Gonzalo, M. F. Verdejo, C. Peters, and N. Cal-zolari. </author> <title> In press. Applying EuroWordnet to multilingual text retrieval. </title> <journal> Journal of Computers and the Humanities, </journal> <note> Special Issue on EuroWordNet. </note>
Reference: <author> D. K. Harman. </author> <year> 1993. </year> <booktitle> The first text retrieval conference (TREC-1). Information Processing and Management, </booktitle> <volume> 29(4) </volume> <pages> 411-414. </pages>
Reference-contexts: However, it has produced few successful experiments. For instance, (Voorhees, 1994) manually expanded 50 queries over a TREC-1 collection <ref> (Harman, 1993) </ref> using synonymy and other semantic relations from WordNet 1.3. Voorhees found that the expansion was useful with short, incomplete queries, and rather useless for complete topic statements -where other expansion techniques worked better-.
Reference: <author> S. Landes, C. Leacock, and R. Tengi. </author> <year> 1998. </year> <title> Building semantic concordances. In WordNet: An Electronic Lexical Database. </title> <publisher> MIT Press. </publisher>
Reference-contexts: A new, bigger version has been made available recently <ref> (Landes et al., 1998) </ref>, but we have not still adapted it for our collection. We have adapted Semcor in order to build a test collection -that we call IR-Semcor- in four manual steps: * We have split the documents to get coherent chunks of text for retrieval.
Reference: <author> G. A. Miller, C. Leacock, R. Tengi, and R. T. Bunker. </author> <year> 1993. </year> <title> A semantic concordance. </title> <booktitle> In Proceedings of the ARPA Workshop on Human Language Technology. </booktitle> <publisher> Morgan Kauffman. </publisher>
Reference-contexts: The next section describes the test collection that we have produced. The experiments are described in Section 3, and the last Section discusses the results obtained. 2 The test collection The best-known publicly available corpus hand-tagged with WordNet senses is Semcor <ref> (Miller et al., 1993) </ref>, a subset of the Brown Corpus of about 100 documents that occupies about 11 Mb. (including tags) The collection is rather heterogeneous, covering politics, sports, music, cinema, philosophy, excerpts from fiction novels, scientific texts...
Reference: <author> G. Miller. </author> <year> 1990. </year> <title> Special issue, Wordnet: An on-line lexical database. </title> <journal> International Journal of Lexicography, </journal> <volume> 3(4). </volume>
Reference-contexts: 1 Introduction Text retrieval deals with the problem of finding all the relevant documents in a text collection for a given user's query. A large-scale semantic database such as WordNet <ref> (Miller, 1990) </ref> seems to have a great potential for this task. There are, at least, two obvious reasons: * It offers the possibility to discriminate word senses in documents and queries. This would prevent matching spring in its "metal device" sense with documents mentioning spring in the sense of springtime.
Reference: <author> H. T. Ng. </author> <year> 1997. </year> <title> Exemplar-based word sense disambiguation: Some recent improvements. </title> <booktitle> In Proceedings of the Second Conference on Empirical Methods in NLP. </booktitle>
Reference-contexts: Some of the best results on a comparable setting (namely, disambiguating against Word-Net, evaluating on a subset of the Brown Corpus, and treating the 191 most frequently occurring and ambiguous words of English) are reported reported in <ref> (Ng, 1997) </ref>. They reach a 58.7% accuracy on a Brown Corpus subset and a 75.2% on a subset of the Wall Street Journal Corpus. A more careful evaluation of the role of WSD is needed to know if this is good enough for our purposes.
Reference: <author> R. Richardson and A.F. Smeaton. </author> <year> 1995. </year> <title> Using Wordnet in a knowledge-based approach to information retrieval. </title> <booktitle> In Proceedings of the BCS-IRSG Colloquium, </booktitle> <address> Crewe. </address>
Reference-contexts: Voorhees found that the expansion was useful with short, incomplete queries, and rather useless for complete topic statements -where other expansion techniques worked better-. For short queries, it remained the problem of selecting the expansions automatically; doing it badly could degrade retrieval performance rather than enhancing it. In <ref> (Richardson and Smeaton, 1995) </ref>, a combination of rather sophisticated techniques based on WordNet, including automatic disambiguation and measures of semantic relatedness between query/document concepts resulted in a drop of effectiveness. Unfortunately, the effects of WSD errors could not be discerned from the accuracy of the retrieval strategy.
Reference: <editor> G. Salton, editor. </editor> <year> 1971. </year> <title> The SMART Retrieval System: Experiments in Automatic Document Processing. </title> <publisher> Prentice-Hall. </publisher>
Reference-contexts: be used to evaluate automatic summarization systems (measuring the semantic relation between the manually written and hand-tagged summaries of IR-Semcor and the output of text summarization systems) and other related tasks. 3 The experiments We have performed a number of experiments using a standard vector-model based text retrieval system, Smart <ref> (Salton, 1971) </ref>, and three different indexing spaces: the original terms in the documents (for standard Smart runs), the word-senses corresponding to the document terms (in other words, a manually disambiguated version of the documents) and the WordNet synsets corresponding to the document terms (roughly equivalent to concepts occurring in the documents).
Reference: <author> M. Sanderson. </author> <year> 1994. </year> <title> Word sense disambiguation and information retrieval. </title> <booktitle> In Proceedings of 17th International Conference on Research and Development in Information Retrieval. </booktitle>
Reference-contexts: This impression is founded on the results of some experiments measuring the role of Word Sense Disambiguation (WSD) for text retrieval, on one hand, and some attempts to exploit the features of Word-Net and other lexical databases, on the other hand. In <ref> (Sanderson, 1994) </ref>, word sense ambiguity is shown to produce only minor effects on retrieval accuracy, apparently confirming that query/document matching strategies already perform an implicit disambiguation. Sanderson also estimates that if explicit WSD is performed with less than 90% accuracy, the results are worse than non disambiguating at all. <p> From the plot, it can be seen that: * Less than 10% disambiguating errors does not substantially affect performance. This is roughly in agreement with <ref> (Sanderson, 1994) </ref>. * For error ratios over 10%, the performance degrades quickly. This is also in agreement with (Sanderson, 1994). * However, indexing by synsets remains better than the basic Smart run up to 30% disambiguation errors. <p> From the plot, it can be seen that: * Less than 10% disambiguating errors does not substantially affect performance. This is roughly in agreement with <ref> (Sanderson, 1994) </ref>. * For error ratios over 10%, the performance degrades quickly. This is also in agreement with (Sanderson, 1994). * However, indexing by synsets remains better than the basic Smart run up to 30% disambiguation errors. From 30% to 60%, the data does not show significant differences with standard Smart word indexing. This prediction differs from (Sanderson, 1994) result (namely, that it is better not to disambiguate below <p> This is also in agreement with <ref> (Sanderson, 1994) </ref>. * However, indexing by synsets remains better than the basic Smart run up to 30% disambiguation errors. From 30% to 60%, the data does not show significant differences with standard Smart word indexing. This prediction differs from (Sanderson, 1994) result (namely, that it is better not to disambiguate below a 90% accuracy). The main difference is that we are using concepts rather than word senses.
Reference: <author> A.F. Smeaton and A. Quigley. </author> <year> 1996. </year> <title> Experiments on using semantic distances between words in image caption retrieval. </title> <booktitle> In Proceedings of the 19 th International Conference on Research and Development in IR. </booktitle>
Reference-contexts: Unfortunately, the effects of WSD errors could not be discerned from the accuracy of the retrieval strategy. However, in <ref> (Smeaton and Quigley, 1996) </ref>, retrieval on a small collection of image captions that is, on very short documents is reasonably improved using measures of conceptual distance between words based on WordNet 1.4. Previously, captions and queries had been manually disambiguated against WordNet. <p> These results are in agreement with (Voorhees, 1994), but it remains the question of whether the conceptual distance matching would scale up to longer documents and queries. In addition, the experiments in <ref> (Smeaton and Quigley, 1996) </ref> only consider nouns, while WordNet offers the chance to use all open-class words (nouns, verbs, adjectives and adverbs). Our essential retrieval strategy in the experiments reported here is to adapt a classical vector model based system, using WordNet synsets as indexing space instead of word forms.
Reference: <author> A. Smeaton, F. Kelledy, and R. O'Donnell. </author> <year> 1995. </year> <title> TREC-4 experiments at dublin city university: Thresolding posting lists, query expansion with Wordnet and POS tagging of spanish. </title> <booktitle> In Proceedings of TREC-4. </booktitle>
Reference-contexts: Query expansion with WordNet has shown to be potentially relevant to enhance recall, as it permits matching relevant documents that could not contain any of the query terms <ref> (Smeaton et al., 1995) </ref>. However, it has produced few successful experiments. For instance, (Voorhees, 1994) manually expanded 50 queries over a TREC-1 collection (Harman, 1993) using synonymy and other semantic relations from WordNet 1.3. <p> Voorhees found that the expansion was useful with short, incomplete queries, and rather useless for complete topic statements -where other expansion techniques worked better-. For short queries, it remained the problem of selecting the expansions automatically; doing it badly could degrade retrieval performance rather than enhancing it. In <ref> (Richardson and Smeaton, 1995) </ref>, a combination of rather sophisticated techniques based on WordNet, including automatic disambiguation and measures of semantic relatedness between query/document concepts resulted in a drop of effectiveness. Unfortunately, the effects of WSD errors could not be discerned from the accuracy of the retrieval strategy.
Reference: <author> Ellen M. Voorhees. </author> <year> 1994. </year> <title> Query expansion using lexical-semantic relations. </title> <booktitle> In Proceedings of the 17th Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval. </booktitle>
References-found: 12

