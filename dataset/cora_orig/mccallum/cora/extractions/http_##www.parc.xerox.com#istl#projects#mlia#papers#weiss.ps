URL: http://www.parc.xerox.com/istl/projects/mlia/papers/weiss.ps
Refering-URL: http://www.public.iastate.edu/~CYBERSTACKS/Aristotle.htm
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: brillg@cs.jhu.edu  
Author: Scott A. Weiss and Simon Kasif and Eric Brill fweiss, kasif, 
Keyword: Classification Experiments  
Note: A Domain For Text Classification  
Affiliation: Dept. of Computer Science The Johns Hopkins University  
Abstract: Text Classification in USENET Newsgroups: A Progress Report Abstract We report on our investigations into topic classification with USENET newsgroups. Our framework is to determine the newsgroup that a new document should be posted to. We train our system by forming "meta-documents" that represent each topic. We discuss our experiments with this method, and provide evidence that choosing particular documents or words to use in these models degrades classification accuracy. We also describe a technique called classification-based retrieval for finding documents similar to a query document. Most work in classification has involved articles taken off of a newswire, or from a medical database(Lewis 1992). In these cases, correct topic labels are chosen by human experts. The domain of USENET newsgroup postings is another interesting testbed for classification. The "labels" here are just the newsgroups to which the documents were originally posted. Since users of the Internet must make this classification decision every time they post an article, this is a nice "real life" application of text categorization. Our approach is to create a model for each group, and then compare future documents to each model to find the best match. Newsgroups have been studied in other projects. Mostly there has been work done in filtering. In this problem, users attempt to select documents from a large pool that match their interests or are similar to other documents. The SIFT project at Stanford(Yan & Garcia-Molina 1995) has users submit profiles of their interests. It then uses new documents as queries on the profiles. At CMU, the Newsweeder project(Lang 1995) has users rate new documents in terms of relevance, and attempts to learn these ratings to distinguish if future postings are relevant. We will address this problem using the techniques we tried for classification. Our underlying system is SMART, developed by Salton(Salton 1971). SMART indexes a collection of text documents by first removing stopwords, and then stemming words to join those with similar meanings. It then creates a vector for each document whose features represent the remaining terms. A query document is converted to a vector in the same manner, and then compared to each document in the collection by computing the cosine of the angle between the two vectors. Our job then is to create a set of metadocuments that represent each of the newsgroups we wish to classify. We then match a posting with the metadocument that gives the highest cosine similarity. There were three major approaches that we took towards forming the topic models. The first method is naive pooling. For each group, we form a single document that is the concatenation of all of that topic's representatives in the training set. This method involves no analysis of documents. It creates a very large feature space using ostensibly many irrelevant terms. It can incorporate "fluke" documents that are off-topic or without content. Rather than take all of the documents in the training set, we can try to identify those that are most useful in discriminating topics. We call this method document selection. Using SMART, we use each training set posting as a query on the remaining training set elements. Consider a particular document Q, and let N be the number of representatives of Q's group in the training set. (In our experiments, N = 100 for all groups.) The documents most similar to Q should be exactly those from its own newsgroup. Hence, optimally, Q should appear amongst the top N retrieved documents exactly for those postings from its own group. While it is foolish to expect this behavior for all documents (and possibly for any), those documents that come close to it could be good discriminators. We thus use each document in our training set as a query on the entire set. A document is considered to be retrieved by a query if it appears in the top N documents. (This is the breakeven point where recall equals precision.) For each posting, we count how many times 
Abstract-found: 1
Intro-found: 1
Reference: <author> Goldberg, J. </author> <year> 1995. </year> <title> Cdm: An approach to learning in text categorization. </title> <booktitle> In Proc. of TAI 95, 7th IEEE International Conference on Tools with Artificial Intelligence. </booktitle>
Reference: <author> Lang, K. </author> <year> 1995. </year> <title> Newsweeder: Learning to filter netnews. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> 331-339. </pages>
Reference: <author> Lewis, D. D. </author> <year> 1992. </year> <title> Representation and Learning in Information Retrieval. </title> <type> Ph.D. Dissertation, </type> <institution> Department of Computer and Information Science, University of Massachusetts. </institution>
Reference: <author> Salton, G. </author> <year> 1971. </year> <title> The SMART Retrieval System Experiments in Automatic Document Processing. </title> <publisher> Pren-tice Hall. </publisher>
Reference: <author> Yan, T., and Garcia-Molina, H. </author> <year> 1995. </year> <title> Sift a tool for wide-area information dissemination. </title> <booktitle> Proceedings of the 1995 USENIX Technical Conference 177-186. </booktitle>
References-found: 5

