URL: http://www.cs.berkeley.edu/~russell/papers/aaai98-speech.ps
Refering-URL: http://www.cs.berkeley.edu/~russell/publications.html
Root-URL: 
Email: fzweig,russellg@cs.berkeley.edu  
Title: Speech Recognition with Dynamic Bayesian Networks  
Author: Geoffrey Zweig and Stuart Russell 
Address: Berkeley, California 94720  
Affiliation: Computer Science Division, UC Berkeley  
Abstract: Dynamic Bayesian networks (DBNs) are a useful tool for representing complex stochastic processes. Recent developments in inference and learning in DBNs allow their use in real-world applications. In this paper, we apply DBNs to the problem of speech recognition. The factored state representation enabled by DBNs allows us to explicitly represent long-term articulatory and acoustic context in addition to the phonetic-state information maintained by hidden Markov models (HMMs). Furthermore, it enables us to model the short-term correlations among multiple observation streams within single time-frames. Given a DBN structure capable of representing these long- and short-term correlations, we applied the EM algorithm to learn models with up to 500,000 parameters. The use of structured DBN models decreased the error rate by 12 to 29% on a large-vocabulary isolated-word recognition task, compared to a discrete HMM; it also improved significantly on other published results for the same task. This is the first successful application of DBNs to a large-scale speech recognition problem. Investigation of the learned models indicates that the hidden state variables are strongly correlated with acoustic properties of the speech signal. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bourlard, H., and Morgan, N. </author> <year> 1994. </year> <title> Connectionist Speech Recognition: A Hybrid Approach. </title> <address> Dordrecht, The Netherlands: </address> <publisher> Kluwer. </publisher>
Reference: <author> Davis, S., and Mermelstein, P. </author> <year> 1980. </year> <title> Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences. </title> <booktitle> IEEE Transactions on Acoustics, Speech, and Signal Processing 28(4) </booktitle> <pages> 357-366. </pages>
Reference-contexts: The data was processed in 25ms windows to generate 10 mel-frequency cepstral coefficients (MFCCs) <ref> (Davis & Mermelstein 1980) </ref> and their derivatives every 8.4ms. MFCCs are generated by computing the power spectrum with an FFT; then the total energy in 20 different frequency ranges is computed. The cosine transform of the logarithm of the filterbank outputs is computed, and the low-order coefficients constitute the MFCCs.
Reference: <author> Dean, T., and Kanazawa, K. </author> <year> 1989. </year> <title> A model for reasoning about persistence and causation. </title> <booktitle> Computational Intelligence 5(3) </booktitle> <pages> 142-150. </pages>
Reference-contexts: When the variables represent a temporal sequence and are thus ordered in time, the resulting Bayesian network is referred to as a dynamic Bayesian network (DBN) <ref> (Dean & Kanazawa 1989) </ref>. These networks maintain values for a set of variables X i at each point in time. X ij represents the value of the ith variable at time j. These variables are partitioned into equivalence sets that share time-invariant conditional probabilities. Bayesian Network Algorithms.
Reference: <author> Dupont, S.; Bourlard, H.; Deroo, O.; Fontaine, V.; and Boite, J.-M. </author> <year> 1997. </year> <title> Hybrid HMM/ANN systems for training independent tasks: Experiments on PhoneBook and related improvements. </title> <booktitle> In ICASSP-97, </booktitle> <pages> 1767-1770. </pages> <address> Los Alamitos, CA: </address> <publisher> IEEE Computer Society Press. </publisher>
Reference: <author> Ghahramani, Z., and Jordan, M. I. </author> <year> 1997. </year> <title> Factorial hidden Markov models. </title> <booktitle> Machine Learning 19(2/3). </booktitle>
Reference: <author> Lauritzen, S. L. </author> <year> 1995. </year> <title> The EM algorithm for graphical association models with missing data. </title> <journal> Computational Statistics and Data Analysis 19 </journal> <pages> 191-201. </pages>
Reference-contexts: As with HMMs, there are standard algorithms for computing with Bayesian networks. In our implementation, the probability of a set of observations is computed using an algorithm derived from (Peot & Shachter 1991). Conditional probabilities can be learned using gradient methods (Russell et al. 1995) or EM <ref> (Lauritzen 1995) </ref>. We have adapted these algorithms for dynamic Bayesian networks, using special techniques to handle the deterministic variables that are a key feature of our speech models (see below). A full treatment of these algorithms can be found in (Zweig 1998).
Reference: <author> Lee, K.-F. </author> <year> 1989. </year> <title> Automatic speech recognition: The development of the SPHINX system. </title> <address> Dordrecht, The Netherlands: </address> <publisher> Kluwer. </publisher>
Reference-contexts: Observation variables are shaded. This simple picture ignores the issues of parameter tying and phonetic transcriptions. keep the number of parameters manageable with these multiple observation streams, a further conditional independence assumption is typically made <ref> (Lee 1989) </ref>: P (o i js i ) = j j Bayesian Networks A Bayesian network is a general way of representing joint probability distributions with the chain rule and conditional independence assumptions. <p> Their derivatives were quantized in a second codebook. The C 0 and delta-C 0 coefficients were quantized separately with size-16 codebooks, and concatenated to form a third 256-valued data stream. We performed mean-cepstral subtraction for C 1 through C 10 , and speaker normalization for C 0 <ref> (Lee 1989) </ref>. The effect of mean-cepstral subtraction is to remove the transmission characteristics of telephone lines. Speaker normalization scales C 0 to the overall power level of a speaker by subtracting the maximum value, so that the resulting values can be compared across utterances.
Reference: <author> Peot, M., and Shachter, R. </author> <year> 1991. </year> <title> Fusion and propagation with multiple observations. </title> <booktitle> Artificial Intelligence 48(3) </booktitle> <pages> 299-318. </pages>
Reference-contexts: These variables are partitioned into equivalence sets that share time-invariant conditional probabilities. Bayesian Network Algorithms. As with HMMs, there are standard algorithms for computing with Bayesian networks. In our implementation, the probability of a set of observations is computed using an algorithm derived from <ref> (Peot & Shachter 1991) </ref>. Conditional probabilities can be learned using gradient methods (Russell et al. 1995) or EM (Lauritzen 1995). We have adapted these algorithms for dynamic Bayesian networks, using special techniques to handle the deterministic variables that are a key feature of our speech models (see below).
Reference: <author> Pitrelli, J.; Fong, C.; Wong, S.; Spitz, J.; and Leung, H. </author> <year> 1995. </year> <title> Phonebook: A phonetically-rich isolated-word telephone-speech database. </title> <booktitle> In ICASSP-95, </booktitle> <pages> 101-104. </pages> <address> Los Alamitos, CA: </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: The articulator network was initialized to reflect voicing, and the chain network to reflect temporal continuity. Experimental Results Database and Task As a test-bed, we selected the Phonebook database, a large-vocabulary, isolated-word database compiled by researchers at NYNEX <ref> (Pitrelli et al. 1995) </ref>. The words were chosen with the goal of "incorporating all phonemes in as many segmental/stress contexts as are likely to produce coarticulatory variations, while also spanning a variety of talkers and telephone transmission characteristics." These characteristics make it a challenging data set.
Reference: <author> Rabiner, L. R., and Juang, B.-H. </author> <year> 1993. </year> <title> Fundamentals of Speech Recognition. </title> <publisher> Prentice-Hall. </publisher>
Reference-contexts: Introduction Over the last twenty years, probabilistic models have emerged as the method of choice for large-scale speech recognition tasks in two dominant forms: hidden Markov models <ref> (Rabiner & Juang 1993) </ref>, and neural networks with explicitly probabilistic interpretations (Bourlard & Morgan 1994; Robinson & Fallside 1991). Despite numerous successes in both isolated-word recognition and continuous speech recognition, both methodologies suffer from important deficiencies. <p> The conditional probability parameters in HMMs are usually estimated by maximizing the likelihood of the observations using the EM algorithm. Once trained, the HMM is used to recognize words by computing P (ojM i ) for each word model M i . For details, the reader is referred to <ref> (Rabiner & Juang 1993) </ref>. In this paper, we will be concerned with discrete observation variables, which can be created from the actual signal by the process of vector quantization (Ra-biner & Juang 1993).
Reference: <author> Robinson, A., and Fallside, F. </author> <year> 1991. </year> <title> A recurrent error propagation speech recognition system. </title> <booktitle> Computer Speech and Language 5 </booktitle> <pages> 259-274. </pages>
Reference: <author> Russell, S.; Binder, J.; Koller, D.; and Kanazawa, K. </author> <year> 1995. </year> <title> Local learning in probabilistic networks with hidden variables. </title> <booktitle> In IJCAI-95, </booktitle> <pages> 1146-52. </pages> <address> Montreal, Canada: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Bayesian Network Algorithms. As with HMMs, there are standard algorithms for computing with Bayesian networks. In our implementation, the probability of a set of observations is computed using an algorithm derived from (Peot & Shachter 1991). Conditional probabilities can be learned using gradient methods <ref> (Russell et al. 1995) </ref> or EM (Lauritzen 1995). We have adapted these algorithms for dynamic Bayesian networks, using special techniques to handle the deterministic variables that are a key feature of our speech models (see below). A full treatment of these algorithms can be found in (Zweig 1998).
Reference: <author> Smyth, P.; Heckerman, D.; and Jordan, M. </author> <year> 1997. </year> <title> Probabilistic independence networks for hidden Markov probability models. </title> <booktitle> Neural Computation 9(2) </booktitle> <pages> 227-269. </pages>
Reference: <author> Zweig, G., and Russell, S. J. </author> <year> 1997. </year> <title> Compositional modeling with dpns. </title> <type> Technical Report UCB/CSD-97-970, </type> <institution> Computer Science Division, University of Califor-nia at Berkeley. </institution>
Reference: <author> Zweig, G. </author> <year> 1998. </year> <title> Speech Recognition with Dynamic Bayesian Networks. </title> <type> Ph.D. Dissertation, </type> <institution> University of California, Berkeley, Berkeley, California. </institution>
Reference-contexts: We have adapted these algorithms for dynamic Bayesian networks, using special techniques to handle the deterministic variables that are a key feature of our speech models (see below). A full treatment of these algorithms can be found in <ref> (Zweig 1998) </ref>. DBNs and Speech Recognition Like HMMs, our DBN speech models also decompose into a pronunciation model and an acoustic model.
References-found: 15

