URL: http://www-csag.cs.uiuc.edu/papers/hpdc7-giannini.ps
Refering-URL: http://www-csag.cs.uiuc.edu/papers/index.html
Root-URL: http://www.cs.uiuc.edu
Email: achieng@cs.uiuc.edu  
Title: A Software Architecture for Global Address Space Communication on Clusters: Put/Get on Fast Messages  
Author: Louis A. Giannini and Andrew A. Chien 
Address: fgiannini,  
Affiliation: Department of Computer Science University of Illinois at Urbana-Champaign  
Abstract: However, new high-performance cluster messaging systems now allow global address space mechanisms to be realized efficiently in software. We describe a high performance one-sided communication model that is implemented as a software layer on top of the Illinois Fast Messages (FM) system. We evaluate several different software implementation architectures for the remote agent, characterizing their differing performance characteristics. Our Put/Get-FM implementation achieves peak bandwidths for put/get operations of 67 MBytes/s, overheads of a few microseconds, and remote-read latencies as low as 26 microseconds on a Myrinet-connected PC cluster. This implementation was released publicly as part of HPVM 1.0 in August 1997, and is receiving significant usage. It has been used for an implementation of the Global Arrays library and also serves as a back-end target for PGI's commercial HPF compiler. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. H. Arpaci, D. E. Culler, A. Krishnamurthy, S. G. Stein-berg, and K. Yelick. </author> <title> Empirical evaluation of the CRAY-T3D: A compiler perspective. </title> <booktitle> In Proceeedings of the International Symposium on Computer Architecture, </booktitle> <pages> pages 320331, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Discussion As is shown in the performance data, the put/get layer achieves high absolute performance. On the basis of hardware and dollar system cost, this version performs quite well when compared to the Cray T3D <ref> [1] </ref>. While the T3D has significantly better latency and bandwidth, it comes at a high cost. Also, our system performs favorably when compared to other cluster based approaches [10]. Furthermore, our put/get system is efficient, delivering nearly all of the underlying FM performance, and with modest additional overhead. <p> The lack of an interrupt-driven notification makes bounding response latency (and therefore get latency) difficult. 7. Related Work There are a large number of parallel computing systems which support put/get models either in hardware or software. The Cray T3D and T3E have extensive remote access mechanisms in hardware <ref> [1, 15] </ref>. The Fujitsu AP1000+ [8] also has hardware based put/get primitives. In addition, there are several cluster-based interconnects which support global address space mechanisms either di rectly or indirectly on the card. These include Myrinet [16], ServerNet [9], and certain variants of VIA [5].
Reference: [2] <author> B.-H.Lim, P.Heidelberger, P.Pattnaik, and M.Snir. </author> <title> Message proxies for efficient, protected communication on SMP clusters. </title> <booktitle> In Proceedings of the 3rd International Symposium on High Performance Computer Architecture, </booktitle> <month> February </month> <year> 1997. </year>
Reference-contexts: However to date, these hardware-based approaches have not spread to more generic kinds of network interfaces. There have been several put/get implementations for clusters also. These include the LAPI [17], Global Array (GA) toolkit [13, 12], Active Messages/Split-C [10], and Message Proxies <ref> [2] </ref>. The LAPI interface for the IBM SP uses an interrupt driven put/get system, achieving predictable latencies. In comparison, our put/get system can achieve lower latencies and lower overheads, but does so at the cost of dedicating a processor for the remote agent.
Reference: [3] <author> R. Barriuso and A. Knies. </author> <title> SHMEM User's Guide. Cray Research, </title> <publisher> Inc., </publisher> <month> May </month> <year> 1994. </year>
Reference-contexts: Put/Get Application Programming Interface Our put/get system application programming interface is based the Cray T3D shmem API <ref> [3] </ref>. This interface was chosen as a base because of its relative simplicity and widespread use. We implemented only a subset of the original interface and also added several routines. A complete listing of the FM-Shmem interface is in Table 1.
Reference: [4] <author> A. Chien, S. Pakin, M. Lauria, M. Buchanan, K. Hane, L. Giannini, and J. Prusakova. </author> <title> High performance virtual machines (HPVM): Clusters with supercomputing APIs and performance. </title> <booktitle> In Proceedings of the Eighth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> Min-neapolis, Minnesota, </address> <month> March </month> <year> 1997. </year>
Reference-contexts: Finally, Section 6 discusses the significance of the results, section 7 describes related work, and section 8 concludes the paper. 2. Background The work described in the paper was done in the context of the High Performance Virtual Machines (HPVM) <ref> [4] </ref> project. HPVM's goal is to create a software infrastructure which enables high-performance parallel computing on PC clusters. Currently we are focusing our efforts on clusters of Pentium Pro and Pentium II PCs running Windows NT and connected by Myrinet.
Reference: [5] <institution> Compaq Computer Corp., Intel Corp., and Microsoft Corp. Virtual Interface Architecture specification, </institution> <month> December </month> <year> 1997. </year>
Reference-contexts: Design Alternatives There are a variety of implementation approaches for remote agents, involving a range of hardware and software support <ref> [10, 8, 9, 5, 12, 17] </ref>. As illustrated in Figure 1, the remote agent can consist of hardware on the network interface, an interrupt handler, or a host thread. In particular, the actions on the remote node needed to fulfill a get request are shown. get implementation. <p> In particular, the actions on the remote node needed to fulfill a get request are shown. get implementation. As can be seen, the NIC directly accesses the physical memory to service the request, bypassing the host CPU. Variations on this approach are implemented in <ref> [6, 8, 9, 5] </ref>. In Figure 1 (b), the actions required by the interrupt- driven get implementation are illustrated. After the request message arrives at the NIC, it interrupts the processor. <p> The Fujitsu AP1000+ [8] also has hardware based put/get primitives. In addition, there are several cluster-based interconnects which support global address space mechanisms either di rectly or indirectly on the card. These include Myrinet [16], ServerNet [9], and certain variants of VIA <ref> [5] </ref>. However to date, these hardware-based approaches have not spread to more generic kinds of network interfaces. There have been several put/get implementations for clusters also. These include the LAPI [17], Global Array (GA) toolkit [13, 12], Active Messages/Split-C [10], and Message Proxies [2].
Reference: [6] <institution> Cray Research, Inc., Eagan, MN. Cray T3D System Architecture Overview, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: In particular, the actions on the remote node needed to fulfill a get request are shown. get implementation. As can be seen, the NIC directly accesses the physical memory to service the request, bypassing the host CPU. Variations on this approach are implemented in <ref> [6, 8, 9, 5] </ref>. In Figure 1 (b), the actions required by the interrupt- driven get implementation are illustrated. After the request message arrives at the NIC, it interrupts the processor.
Reference: [7] <author> C. Dubnicki, A. Bilas, Y. Chen, S. Damianakis, and K. Li. VMMC-2: </author> <title> efficient support for reliable, connection-oriented commnication. </title> <booktitle> In Proceedings of Hot Interconnects V. IEEE, </booktitle> <year> 1997. </year>
Reference-contexts: FM provides direct user-level access to the network and supports multiple processes per node. There are a number of other cluster-based messaging systems which have design goals and performance similar to FM. These include Active Messages [11], PM [18], U-Net [19], and VMMC-2 <ref> [7] </ref>.
Reference: [8] <author> K. Hayashi, T. Doi, T. Horie, Y. Koyanagi, O. Shiraki, N. Imamura, T. Shimizu, H. Ishihata, and T. Shindo. </author> <title> AP1000+: Architectural support of PUT/GET interface for parallelizing compiler. </title> <booktitle> In Proceedings of ASPLOS-VI, pages 196207, </booktitle> <address> San Jose, California, </address> <year> 1994. </year>
Reference-contexts: This basic difference can greatly reduce synchronization requirements between the source and destination processes. For certain applications, especially those with dynamic and irregular communication, put/get models can greatly increase performance and decrease programming effort <ref> [8] </ref>. This is why these interfaces have long been used in high-performance parallel computing. They are used both by compilers such as those for HPF to generate parallel code, and by programmers directly. For these reasons it is important that clusters efficiently support these interfaces. <p> Design Alternatives There are a variety of implementation approaches for remote agents, involving a range of hardware and software support <ref> [10, 8, 9, 5, 12, 17] </ref>. As illustrated in Figure 1, the remote agent can consist of hardware on the network interface, an interrupt handler, or a host thread. In particular, the actions on the remote node needed to fulfill a get request are shown. get implementation. <p> In particular, the actions on the remote node needed to fulfill a get request are shown. get implementation. As can be seen, the NIC directly accesses the physical memory to service the request, bypassing the host CPU. Variations on this approach are implemented in <ref> [6, 8, 9, 5] </ref>. In Figure 1 (b), the actions required by the interrupt- driven get implementation are illustrated. After the request message arrives at the NIC, it interrupts the processor. <p> Related Work There are a large number of parallel computing systems which support put/get models either in hardware or software. The Cray T3D and T3E have extensive remote access mechanisms in hardware [1, 15]. The Fujitsu AP1000+ <ref> [8] </ref> also has hardware based put/get primitives. In addition, there are several cluster-based interconnects which support global address space mechanisms either di rectly or indirectly on the card. These include Myrinet [16], ServerNet [9], and certain variants of VIA [5].
Reference: [9] <author> R. Horst. Tnet: </author> <title> A reliable system area network for i/o and ipc. </title> <booktitle> In Proceedings of Hot Interconnects, </booktitle> <year> 1994. </year>
Reference-contexts: Design Alternatives There are a variety of implementation approaches for remote agents, involving a range of hardware and software support <ref> [10, 8, 9, 5, 12, 17] </ref>. As illustrated in Figure 1, the remote agent can consist of hardware on the network interface, an interrupt handler, or a host thread. In particular, the actions on the remote node needed to fulfill a get request are shown. get implementation. <p> In particular, the actions on the remote node needed to fulfill a get request are shown. get implementation. As can be seen, the NIC directly accesses the physical memory to service the request, bypassing the host CPU. Variations on this approach are implemented in <ref> [6, 8, 9, 5] </ref>. In Figure 1 (b), the actions required by the interrupt- driven get implementation are illustrated. After the request message arrives at the NIC, it interrupts the processor. <p> The Fujitsu AP1000+ [8] also has hardware based put/get primitives. In addition, there are several cluster-based interconnects which support global address space mechanisms either di rectly or indirectly on the card. These include Myrinet [16], ServerNet <ref> [9] </ref>, and certain variants of VIA [5]. However to date, these hardware-based approaches have not spread to more generic kinds of network interfaces. There have been several put/get implementations for clusters also. These include the LAPI [17], Global Array (GA) toolkit [13, 12], Active Messages/Split-C [10], and Message Proxies [2].
Reference: [10] <author> A. Krishnamurthy et al. </author> <title> Evaluation of architectural support for global address-based communication in large-scale parallel machines. </title> <booktitle> In Proceedings of ASPLOS-VII, </booktitle> <year> 1996. </year>
Reference-contexts: Design Alternatives There are a variety of implementation approaches for remote agents, involving a range of hardware and software support <ref> [10, 8, 9, 5, 12, 17] </ref>. As illustrated in Figure 1, the remote agent can consist of hardware on the network interface, an interrupt handler, or a host thread. In particular, the actions on the remote node needed to fulfill a get request are shown. get implementation. <p> On the basis of hardware and dollar system cost, this version performs quite well when compared to the Cray T3D [1]. While the T3D has significantly better latency and bandwidth, it comes at a high cost. Also, our system performs favorably when compared to other cluster based approaches <ref> [10] </ref>. Furthermore, our put/get system is efficient, delivering nearly all of the underlying FM performance, and with modest additional overhead. The put/get performance, particularly the low get latency, is sensitive to the dedication of a CPU for the remote agent thread. <p> However to date, these hardware-based approaches have not spread to more generic kinds of network interfaces. There have been several put/get implementations for clusters also. These include the LAPI [17], Global Array (GA) toolkit [13, 12], Active Messages/Split-C <ref> [10] </ref>, and Message Proxies [2]. The LAPI interface for the IBM SP uses an interrupt driven put/get system, achieving predictable latencies. In comparison, our put/get system can achieve lower latencies and lower overheads, but does so at the cost of dedicating a processor for the remote agent.
Reference: [11] <author> A. Mainwaring and D. Culler. </author> <title> Active messages: </title> <booktitle> Organization and applications programming interface, </booktitle> <year> 1995. </year>
Reference-contexts: FM provides direct user-level access to the network and supports multiple processes per node. There are a number of other cluster-based messaging systems which have design goals and performance similar to FM. These include Active Messages <ref> [11] </ref>, PM [18], U-Net [19], and VMMC-2 [7]. <p> They both support only the primary data movement routines in hardware. Furthermore, because they are hardware based they operate only on physically addressed and pinned buffers. The Active Messages project has chosen a different approach, integrating the put/get primitives deeply into the AMII <ref> [11] </ref> system. The earlier versions of Active Messages did not provide put/get capability. Deeper integration enables the put/get transfers to move data outside of the messaging buffer pool, finessing deadlock and flow control issues.
Reference: [12] <author> J. Nieplocha and R. Harrison. </author> <title> Shared memory NUMA programming on I-WAY. </title> <booktitle> In Proc. 5th IEEE Symp. on High Performance Distributed Computing, </booktitle> <year> 1996. </year>
Reference-contexts: Design Alternatives There are a variety of implementation approaches for remote agents, involving a range of hardware and software support <ref> [10, 8, 9, 5, 12, 17] </ref>. As illustrated in Figure 1, the remote agent can consist of hardware on the network interface, an interrupt handler, or a host thread. In particular, the actions on the remote node needed to fulfill a get request are shown. get implementation. <p> These include Myrinet [16], ServerNet [9], and certain variants of VIA [5]. However to date, these hardware-based approaches have not spread to more generic kinds of network interfaces. There have been several put/get implementations for clusters also. These include the LAPI [17], Global Array (GA) toolkit <ref> [13, 12] </ref>, Active Messages/Split-C [10], and Message Proxies [2]. The LAPI interface for the IBM SP uses an interrupt driven put/get system, achieving predictable latencies.
Reference: [13] <author> J. Nieplocha, R. J. Harrison, and R. J. Littlefield. </author> <title> Global Arrays: A portable shared-memory programming model for distributed memory computers. </title> <booktitle> In Supercomputing '94, </booktitle> <year> 1994. </year>
Reference-contexts: The Put/Get-FM system was released publicly as part of HPVM 1.0 in August 1997, and is receiving significant usage. We have used our put/get layer as the basis for two other communication interfaces: as the underlying communication layer for a port of Global Arrays <ref> [13] </ref> and as a target by the Portland Group's commercial High Performance Fortran (HPF) compiler. The remainder of the paper is organized as follows. Section 2 briefly provides some background. In Section 3, we describe the 32-bit adaptation of the Shmem put/get API that our system implements. <p> These include Myrinet [16], ServerNet [9], and certain variants of VIA [5]. However to date, these hardware-based approaches have not spread to more generic kinds of network interfaces. There have been several put/get implementations for clusters also. These include the LAPI [17], Global Array (GA) toolkit <ref> [13, 12] </ref>, Active Messages/Split-C [10], and Message Proxies [2]. The LAPI interface for the IBM SP uses an interrupt driven put/get system, achieving predictable latencies.
Reference: [14] <author> S. Pakin, V. Karamcheti, and A. A. Chien. </author> <title> Fast Messages: Efficient, portable communication for workstation clusters and mpps. </title> <journal> IEEE Concurrency, </journal> <volume> 5(2):6073, </volume> <month> April-June </month> <year> 1997. </year>
Reference-contexts: HPVM's goal is to create a software infrastructure which enables high-performance parallel computing on PC clusters. Currently we are focusing our efforts on clusters of Pentium Pro and Pentium II PCs running Windows NT and connected by Myrinet. Illinois Fast Messages <ref> [14] </ref> is the centerpiece of the HPVM project. FM is one of several new highly-efficient cluster based messaging systems. It uses an active message style interface and is designed to provide high-performance for all message sizes using a single, simple interface.
Reference: [15] <author> S. L. Scott. </author> <title> Synchronization and communication in the T3E multiprocessor. </title> <booktitle> In ASPLOS-VII, </booktitle> <pages> pages 2636, </pages> <address> Cambridge, Massachusetts, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: The lack of an interrupt-driven notification makes bounding response latency (and therefore get latency) difficult. 7. Related Work There are a large number of parallel computing systems which support put/get models either in hardware or software. The Cray T3D and T3E have extensive remote access mechanisms in hardware <ref> [1, 15] </ref>. The Fujitsu AP1000+ [8] also has hardware based put/get primitives. In addition, there are several cluster-based interconnects which support global address space mechanisms either di rectly or indirectly on the card. These include Myrinet [16], ServerNet [9], and certain variants of VIA [5].
Reference: [16] <author> C. Seitz. </author> <title> Myrinet a gigabit-per-second local-area network. </title> <booktitle> In Proceedings of Hot Interconnects, </booktitle> <year> 1994. </year>
Reference-contexts: The Cray T3D and T3E have extensive remote access mechanisms in hardware [1, 15]. The Fujitsu AP1000+ [8] also has hardware based put/get primitives. In addition, there are several cluster-based interconnects which support global address space mechanisms either di rectly or indirectly on the card. These include Myrinet <ref> [16] </ref>, ServerNet [9], and certain variants of VIA [5]. However to date, these hardware-based approaches have not spread to more generic kinds of network interfaces. There have been several put/get implementations for clusters also.
Reference: [17] <author> G. Shah et al. </author> <title> Performance and experience with LAPI a new high-performance communication library for the IBM RS/6000 SP. </title> <booktitle> In IPPS/SPDP '98. </booktitle>
Reference-contexts: Design Alternatives There are a variety of implementation approaches for remote agents, involving a range of hardware and software support <ref> [10, 8, 9, 5, 12, 17] </ref>. As illustrated in Figure 1, the remote agent can consist of hardware on the network interface, an interrupt handler, or a host thread. In particular, the actions on the remote node needed to fulfill a get request are shown. get implementation. <p> These include Myrinet [16], ServerNet [9], and certain variants of VIA [5]. However to date, these hardware-based approaches have not spread to more generic kinds of network interfaces. There have been several put/get implementations for clusters also. These include the LAPI <ref> [17] </ref>, Global Array (GA) toolkit [13, 12], Active Messages/Split-C [10], and Message Proxies [2]. The LAPI interface for the IBM SP uses an interrupt driven put/get system, achieving predictable latencies.
Reference: [18] <author> H. Tezuka, A. Hori, and Y. Ishikawa. </author> <title> PM: A high-performance communication library for multi-user parallel environments. </title> <type> Technical Report TR-96-015, </type> <institution> Tsukuba Research Center, Real World Computing Partnership, </institution> <month> Novem-ber </month> <year> 1996. </year>
Reference-contexts: FM provides direct user-level access to the network and supports multiple processes per node. There are a number of other cluster-based messaging systems which have design goals and performance similar to FM. These include Active Messages [11], PM <ref> [18] </ref>, U-Net [19], and VMMC-2 [7].
Reference: [19] <author> T. von Eicken, A. Basu, V. Buch, and W. Vogels. U-Net: </author> <title> A user-level network interface for parallel and distributed computing. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 4053, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: FM provides direct user-level access to the network and supports multiple processes per node. There are a number of other cluster-based messaging systems which have design goals and performance similar to FM. These include Active Messages [11], PM [18], U-Net <ref> [19] </ref>, and VMMC-2 [7].
References-found: 19

